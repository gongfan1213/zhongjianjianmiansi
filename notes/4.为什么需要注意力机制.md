还提到一个关键点，就除了刚刚抓取这个关键信息以外，腾讯选就注意力机制还有什么样的价值？我们为什么要用它？第一个点就在于我们去学习和理解自然语言处理，大家如果有一些相关基础的话都会知道它会用到一种神经网络，叫做循环神经网络，用来解决很多自然语言处理的问题。但是循环神经网络它天然有一些挑战，就比如说它去解决一些很长的序列的问题的时候，它会像这个卷积神经网络一样。就我们都知道神经网络越来越深。在我们做计算机视觉的时候，一千层的神经网网络他就没法去做这个反向传播了，它的迭代的机制失效了，因为它已经超过了我们这个浮点数的精度的上限了，那这个时候怎么办呢？我们知道有resnel这种很巧妙的做法，对吧？
我跨层连接。那在RNN这个领域也是一样的。我们去解决这个呃自然语言处理的时候，把每一这个词变成了这个RN当中的一个序列当中的一个神经元，或者说你可以列成一个一个的串起来的这一段话，这样你讲话这个文字一个一个这样列在这儿。然后他去解决这个长度越来越长的问题的时候也是一样。说句简单一点话，就是我我说话说太久了，你就忘了我前面说什么了。这个是非常常见的。那么RNN原生的RN有这样的问题。LSTM其实是一个RNN的变体，并且被广泛的应用在未来真正的这个循环神经网络的各种场景里面。LSTM就是一个类似于我的这个internet一样的去跨层连接。
STM它自己在每一个词去重处理。你前文的时候，它有一个你可以理解成有一个小的记忆单元，它一直在那记着呢，然后就像你拿了一个小本本一样。这个你可以想象成一个这样的一个场景，就是好记性不如烂笔头。然后你可以把以前说过的话记下来，有一张纸，但是你这个烂笔头也不能无限的机。你不管怎么记，前面说了100个字、1000个字、1万个字，你都只能记一页的A4纸。这个就是LSTM的核心，它虽然能记，但它能记的容量有限，所以你要不断的祭，你他说了1000个字的时候，你这一页就记满了，那你就要把你这一页纸再浓缩，再去抽象。
这个是我们的RNN发展的一个过程。但是注意力机制也可以解决这些问题。注意力机制也能解决这个序列长度增加的时候性能下降，包括这个计算效率降低的问题。
那他怎么做的呢？就是我们待会儿会去讲他的这个原理。就是我们刚刚提到你有一页A4纸，这个A4纸你要去记，但这个具体怎么去抽象它还是个问题，对吧？虽然你有一页A4纸，你能够去记之前说的很多内容，但你怎么去抽象它，这是一个套路，这是一个人的能力。
就是好学生跟差学生都在记课堂笔记，谁的这个课堂笔记记得质量好，谁的这个课堂笔记记得是跟他自己脑子里面的东是匹配起来的。因为你要知道你这一页A4纸是要跟你脑子里面已经记下来什么东西是配合着用的。你去看别人的学习笔记为什么没什么效果？因为你脑子里装的东西跟他不一样，你们记的重点也不一样。注意力机制是通过这个方式来解决问题的。就是我能够给你一张纸，但是我还教了你怎么样去记笔记，这个记笔记甚至就不要那页A4纸了。就是你在学的过程当中提升你的学习效率，你可以这么理解，这是它的一个很重要的一个特点和优势，这也是它被引入INN当中的最重要的一个原因。
第二个就是说其实注意力机制它是一个学习套路。这个学习套路在IN在自然语言处理这个领域可以用，在计算机视觉领域也一样可以用。甚至在跨模态多模态的任务和推荐系统领域里面也都能用。因为它本身是一个套路，这个套路我们会庖丁解牛，逐步给大家做讲解。同时还能够解决这个IN的问题，也能够在多项任务当中去使用，并也取得了很好的性能提升，拿结果拿到了好的结果。
还有一点就是说它能够提高可解释性。就是我们的神经网络大家一直都说是一个黑盒子，不知道它里面学了什么东西。那么注意力机制可以去提升它的可解释性，可以可视化的给你看我的神经网络到底学了什么。同时还能把一些特征更明显，把这个特征给抛出来，让大家看的更明显。
所以我们总结一下，它解决这个INN的问题主要就解决这几个问题。第一就是我们这个RN当中的这个encode decode这种传统的这种编码解码器模型当中会有一些信息损失，包括还无法进行输入输出的对齐，它能解决这些问题。同时它也允许我们这个解码器，就是我们这个大家可以理解这个就像翻译一样，这个编解码过程这样一个机器翻译就是一个最典型的编解码这种模型的实例。那么它允许什么呢？它允许解码器访问编码器的完整的输入序列，然后通过注意力权重选择性的去关注一些相关信息，这个就是我们这样，你现在在听我这个课程，然后这节课我有很多的内容，有的内容是打比方，有的内容是一些关键知识点。每个人学习的重点不一样，注意力机制就是希望因材施教，每个人他关注的重点都不一样，他通过这个方式来学，我们具体会讲怎么写，然后它能够自动的去学习这个注意力的权重。就是相当于你可以理解成有一个好学生他很厉害，他上语文课、上数学课、上编程课，他的注意力权重是不一样的。实际上也应该这样做，我们直观的去想象，让它能构建一个上下文向量，这个上下文向量就是它的整个注意力机制的一个核心，这个在待会儿我们讲它具体实现的时候会跟大家讲解。
同时它也能够提高模型的性能，改善输出的质量，提供更好的解释性。这个是它在这个问题回应上面的一些很重要的优势。然后我们还提到他在很多任务上都达到了一个更新的性能。就比如说我们看到OpenAI的官方发布出来的GPT4，包括T3.5做到的一些在基准测试上的一些结果。我们能看得到这个绿色的部分是GPT4的成果。GPT4是更广泛的使用了注意力机制，包括GPT3.5，他们都在各种各样的基准测试上拿到了很好的结果。这些结果在以前的各种语言模型或者说AI的模型里面是做的不好的那他为什么能做的更好呢？就是因为引入了注意力机制最底层的技术手段。在没有注意力集中之前，其实大家做的很一般，包括像各种各样的考试，我们能看到在美国的一些高中生的测试，包括一些职业考试里面，它都能在比如说一个班级有100个人，他能做到前五甚至前三这样的一个成绩，那就是非常高的一个水平了。
然后我们还提到了他的这个可解释性，就比如说我们在做这个深度学习，经常会有各种识别任务那我们在识别任务里面训练的这个神经网络到底学到了什么？我们能看得到这幅图里面的这个是一组的图。每一个数字下面，其实标的是我们输入的这个图像。那旁边的右边的这一幅，其实是它的一个你可以理解通过注意力机制学到的一些他的对于这幅画面里面最最注意力权重最高的部分。如果我们通过这幅图，我们能把右边这一幅带有注意力权重的输出，我们通常也叫激活层的一些输出，能够看得到其实他把很多背景的信息就过滤掉了。简单来说就是背景不重要。我们就像这幅当前我们看到这个画面里面有很多白色的背景，这背景不重要，没有有效的信息。有效的信息其实是前景，就是我们前面的这个关键物体。所以我们能看得到通过这个注意力机制，我们能够把神经网络学了什么更好的展示出来。
甚至我们还可以学不同的类别，我们可以给注意力机制且额外的输入，这个也是transformer用到的一个多头注意力其实很重要的一个点。就比如说这里我们学到了这个飞机，但是飞机其实旁边还有云彩，那我们可能就可以做两个两个不同的这你可以认为就像我们学数学，学物理是两条线一样。我这边重点学的是飞机，另一边重点学的是这个白云。这个要这样做也是可以的对。然后除了刚刚讲的这个，我们在计算机视觉领域以外，其实在自然语言处理领域，注意力机制更重要。也是我们大语言模型，包括我们做机器翻译，逐步发展过来，注意力机制真正体现价值的一个地方。
我们接下来讲这个注意力机制的理论的这个论文，也是一个机器翻译的场景，机器翻译是语言处理里面最重要的一个基准测试的一个任务。就相当于它能体现你这个模型的水平能力，好不好？就跟我们的人都要参加高考一样。这个机器翻译就是一个最早的高考。左边这幅图其实是展示了注意力机制的一个可视化的一个效果。在自然语言这个任务上，尤其是在机器翻译这个特定的一个基准测试上。
我们怎么看这幅图呢？首先这幅图是一个混淆矩阵的灰度图，这里只有灰度值越亮的部分表示它的相关性越高。我们能看到英语和法语之间，我们要做翻译的时候，词跟词之间是有一个对应关系的。就比如说我们都学过英语，知道这个boy是男孩，girl是女孩。那么我们假设把这个地方的法语换成中文的话也是一样的。
那么在这篇论文当中，他给出了一些实验结果，就比如说我们能看到这里有一个叫做area，就是区域跟这个zone这两个。这个zone其实是一个法语词，现在我们应该也能在一些地方看到zone这个词我就是我小时候用中国移动的手机，它会有中国移动什么动感地带，当时那个广告语里面就mine zone好像是这么一个词。这个zone其实是一个区域的意思，它跟这个area相关性最高，所以这幅图其实是把注意力机制的可视化给大家做了一个很直观的展现。不管是在自然语言，还是在我们的计算机视觉，就在我们的视觉部分，其实它都能够把一些神经网络的不可明说的这些我们到底学了什么，这个黑匣子给大家做一个展现。这是它的一个。
第三个特点，我们通过这样的一个混淆矩阵这个关联关系，就能看得出来他其实这个网络学的好不好。学得好的话，那这个词的关联就应该是对的。比如说学得好的话，那这个视觉里面的这个主体这个前景就应该是被高亮突出出来的那这是它的第三个特性，我们总结一下为什么要用注意力机制。第一，它能解决RNN的很多问题。第二，它在很多不同类型的任务上，包括自然语言、视觉推荐系统等等，都取得了非常好的成果。第三，它增加了我们深度学习神经网络的可解释性。包括一些特征的明显就叫做characteristics，就是我们的这个特征，我们所谓学到这些特征能够让它突出出来，特征化能够更明显，这个其实就是注意力机制的价值。
讲到了它是什么，它有什么样的价值。最关键的事情是说，怎么样去实现这个注意力机制呢？我们接下来看看我们刚刚讲到了机器翻译这个场景。在机器翻译里面，其实注意力机制是最先被引入到机器翻译这个问题场景里的。并且纪翻译作为早期也比较早期，就十年以前，十年前自然语言处理里面最重要的一个任务，它是非常有代表性的。简单来说就谁把这事儿干好了谁就牛逼。
那么机器翻译怎么引入注意力机制？它又是如何工作的？我们接下来通过这节课再跟大家分享一下。
我们刚刚有提到encoder。Decoder的架构就是这个编码器解码器的这个架构，这是非常经典的一个架构，大家如果有过基础的话，应该看过类似的网络架构的一个图。那这幅图上下两个不一样的架构是想说明，上面这幅图是一个单纯的encoder decoder的架构，编解码的一个架构。那下面是编解码的架构，但是增加了注意力网络或者说注意力机制。那这两个架构有什么不同应该很一目了然。
Encoder跟decoder的部分看起来都一模一样，不同的点在于中间这一部分，对于传统的架构来说，我的机器翻译怎么做的呢？就是我这里每一个单词，我都会把它串联起来。相当于我学这个单词它应该表达成一个什么向量的时候，我不只是把它丢到我的这个RN这个神经网络里，我还要把它的前文学下来。
这个道理也好理解，就是我们你想象一下你是怎么学英语的。回忆一下你学英语的时候，最早是背单词，对吧？但现在大家都知道背单词，死记硬背这个方式不好。我们要学短语，我们要学从原文里面去学学完形填空，我们要读整篇的文章。然后读完形填空的时候，我们就能发现这个上下文给了很多的内容，你只需要把那个空空填好。其实我们后面在做IN的时候，你去理解它的这个原理也是一样的。
我们去学习一门语言，学习这个机器翻译的任务，我现在是不真的把这个语言本身理解到位了呢？其实没有。你可以理解把翻译学的好和理解这个语言本身是两回事儿。就是计算机和人还是不一样的。计算机在学这个单词对应着另一个单词，就跟我们刚刚说中文对应着area的时候，它是不是真的抽象理解出来了area是有一个画面呢？它其实没有，那是另一门技术了，人的大脑很厉害，就是我们要让计算机变得跟人的大脑一样厉害是非常难的。因为当你真的做到那一天的时候，我们就像上帝一样，我们能造出一个完整的人了，因为大脑是最关键的。剩下那些都可以通过一些机械专业或者说自动化专业的一些手段，给它加一些机器人相关的一些控件就好了。
所以大家首先要理解一点，就是我们人会做翻译，是我们人真的理解了那个词或者那个语言本身。我们有这样的一个文化积累，我们有这个图像抽象的能力。但是机器做翻译，它其实就是找对应关系，这也是一个概率学问题，是一个数学问题。所以说计算机去做翻译是一个数学问题，这个是大家一定要去理解这个机器翻译最关键的一个点。但是不管它是什么问题，我们要把这个问题解决掉。在这个学习过程当中，我们就有提它既然是一个数学问题，是一个概率学问题。
那么我们要学习它就不能只学这个单词本身。我们要去学它的前面有什么样的单词，相当于就学它的上下文当中的上文。这个是INN的一个很关键的一个设定，就是它从左往右有一个序列，这个序列把它的每一个单词变成了一个向量。这个向量我们通过一个IN变成了一个隐，就是隐藏层的一个你可以理解成就是这个单词变成了一个高维的向量，这个高维的向量就是这个hidden state，就我们这儿的这个隐藏状态一、隐藏层的状态二和隐藏层的状态3。那么这三个状态不等于这个单词本身，那对于第一个单词来说，对于句首的第一个单词来说，它就是单纯的一个加工对吧？
把我们的第一个单词变成了学习的一个向量，就是我们刚刚看到的那些不太好可视化的所谓的黑盒子的内容。但是我们到第二个单词的时候，你就会发现这里的hidden state 2，他不是单纯的这个单词，而是他把这个单词和之前这个单词，就hidden state一和这个单词都作为它的输入，对吧？然后变成了hidden state 2。类似的到hidden state 3的时候，就把这三个单词串起来，变成了这个状态。这个状态就是集合了最新的单词和前面所有的单词。他之前就是通过这样的一个方式去表达原来这一整句话的那我们要学的这个RNN要学的这个翻译模型，就这个RNN本身它是两部分组成，我们把它编码成了一个一串数字的一个高维的向量。这个过程像什么事情就像我现在在这里讲，我们叫编解码。
怎么理解？编解码就是有一种特定的编码方式。比如说我说话这是一个声音，对吧？它会震动，然后它就会被我的电脑给记下来。记下来之后，他会有一些声音处理的单元，然后会有包括这个视频的画面，通过这个DSP，最终变成了一堆数字信号。这堆数字信号在你看这个视频的时候，其实它是通过数字信号又变成了这些图像和声音，对吧？那这其实是一个典型的编解码过程，只不过这个编解码符合一些标准规范，它是符合这个标准格式的。比如说这个视频有什么H264对吧？
H263的编解码格式，声音可能是其他的一些编解码方式，没有视频向大家了解这么多，它是固定的套路，但是RN它是要学的，因为我无法把语言理解到位，就是语言本身没法用一个固定的编码方式来编码。因为它背后或是那些语言背后的含义就是说语言学有意思的地方。所以我们要通过你输入的语料越多，我就越能理解你的含义。其实就是我越能学会这些语言背后的一个概率分布。
现在我们把这一堆这三个单词压缩成了这样的一个编码，叫hidden state 3。然后我们需要有一个解码器，解码器把它变成另一种语言的各个单词am a student，并且它们是这个单词的数量甚至可以是不一样的。这个也很好理解，就我同一门语言到另一门语言，它的词的这个个数可能是不一样的，跟语言本身有关。
那么注意力网络它的区别在哪儿？就是我们开始有提到IN有一个很重要的问题，这里只有三个单词。假设这里有1万个单词，那这一个state就是我们不管他拉的多长，假设这个state是一千维的一个向量。一千维的一个向量要去表示1万个单词，它也是表达不全的，它一定会有信息丢失。就我们开始说的，你这一节课你听了很多的内容，你不可能全部都记下来，因为脑容量有限，就跟这个state这个向量他的表达能力有限。
注意力网络最大的一个区别就是你继续去记，你在这里你把这个state 1 state 2继续记下来。但是我给到解码器的时候，我不是只给你一个hidden state 3，我把他们前面的一和二也都给你。简单来说就是你之前记了的所有的这些工中间状态，我也都给到我的解码器，让他能够去记下来。
这个过程像什么呢？就像我这儿每一页讲完之后，就我们这个课件有很多页，我每一页讲完之后，我都可以让一个同学记下来。这个同学记了一个笔记，他记了一页A4纸，另一个同学又记了一页A4纸。一直到我讲完之后，第三个同学又寄了一页A4的纸。然后这三张纸都是我的课程笔记，我交给一个另一个同学或者交给老师。这三页课堂笔记我都交给老师了。
老师通过这三页课堂笔记，另一门老师或者说没来上课的同学，我们假设就有一个同学，同学的同学，第四个同学他没去上课，他就看了这三个同学的课堂笔记。那他要去学学到底这个课里面讲了什么内容，它其实就是在解码，只不过这个过程我们把它想象成一个机器翻译的过程。这个同学他就是要把这三个同学的笔记都记下来，并且这三个同学听课还听的都不一样。有的同学只听了上半节课，有的同学听了前面两节课，第三个同学是把整堂课都记下来了。但无论如何，这个思路大家应该能理解，它的好处就是你这三个同学各自记得一定都有侧重，大家记得内容不一样。那我把这三份笔记拿到一起来学习，肯定是比我只从第三个同学这拿一个笔记要好很多。这个信息量都不一样，对吧？但是通过这样的一个方式，我们直观理解了，那他具体怎么做的呢？
我们要看一看一篇很重要的论文，其实这篇论文的这个思想是14年的时候提出来的。然后我们现在这幅图是来自于2021年的一篇综述，这篇综述叫做安安attentive survey of attention models。这篇论文是2021年发表的一篇关于注意力机制的综述的论文。这些论文我们都会放在课程平台上，大家可以去详细的去阅读理解。但是这个思想就是这个呃encoder decoder with attention model的这个思想，是14年的时候，本就和他的学生发明的。待会儿我们会讲。
具体来看这个注意力机制怎么回事我们看到左边是我们的没有注意力机制的一个网络，右边是带了注意力机制一个网络，跟我们刚刚看到这里是一模一样的这里就是没有注意力机制的，这边是有，那它的区别在哪？通过左边这幅图可以更抽象的理解这个H1、H2、H三是我们的课堂笔记，对吧？X1、X2、X3是我在这儿念的这些内容，我哔哔哔说了一大堆话，然后有课堂笔记记下来了。
接下来之后，之前的没有助力机制，网络就是人传人对吧？一直到最后一个第三个同学，他把他的笔记给到我们的这个没来听课的同学。然后现在不一样了，假设我没来听课同学也可以有很多，对吧？有两个没来听课的同学，他们都需要从我这三份笔记里面去学习内容。
因为这两个没来听课的同学，他们可能本身的关注重点不一样。比如说我这个同学，我就是想听一下前面讲的这个AI历史的事实发展。另一个同学是想讲这节课我讲的这个注意力机制的价值是什么？还有一个同学是想听这个注意力机制的原理是什么？它当然重点就不一样了。重点不一样，我怎么样能够让关注不同重点的没来听课的同学都听到重点呢？
这就引入一个我们刚刚提到的很有意思的概念，叫上下文向量context Victor这个部分。这个context Victor它的形式化定义也很简单，我们看到这里有一个叫做阿尔法和H的东西。H其实就这个课堂笔记对吧？
阿尔法是什么呢？阿尔法叫做注意力权重。就是我作为没来听课同学，我当然知道我重点关注的是什么，我想要重点听的是什么。所以当我拿到这三份笔记的之后，我会发现这个三号同学记的内容太多了，我不想听。那我就把这个三号同学的注意力权重给他降低一点。我重点想听的是一号同学记得这个笔记，因为一号同学记得这个笔记全是我想听的，没有杂音。二三这个同学这些记得这些内容我都已经会了，我不想听。那就把这两个权重都降低，把这个权重拉大一点，这个就是一个上下文向量。
这样我们就会发现，相比于这个没有注意力机制的这个网络，那有注意力机制这个网络增加了一个额外的输入，就是我们的这个C就我们的这个上下文向量。他跟左边这幅图比起来，他既把只有两个没来听课的同学，对吧？第一个没来听课同学看完了资料告诉下一个同学。这儿他能去做一些筛信息的收集和删减。然后同时它还可以从我们刚刚提到这个上下文向量，它总览了就相当于你这个S一这个同学没来听课，你看完了课程笔记，你跟我讲了一通，我也可以直接去看课程笔记。
我通过我自己关注的重点，我再有一理解，这是我的C2，通过这个描述大家应该能比较好的理解了，对吧？同时这个过程当中，我们通过这个context vector能够带来一个最大的价值。就是我不再是人传人，不再是击鼓传花。我的解码器当中的每一个同学，他都能够完整的获取到我们的输入这一侧，就是我们编码器当中的所有的输入内容，并且由他自己来决定哪一个内容重要，哪个内容不重要。那这个决定的结果就变成了注意力的权重，那最终有一个注意力权重的矩阵。因为对于每一个S就对于每一个解码器当中的输出来说，它都需要有这样的一个阿尔法。你可以认为一个一维的阿尔法对吧？然后有多少个S这就叠起来有多少个就变成一个二维的一个矩阵。
所以这个C或者说我们叫做context vector，它最重要的一个点就是把注意力这件事儿实现了。有一个形式化的一个定义。它核心就是我作为一个解码器。比如说翻译任务，我想要知道我翻译出来这个area这个词，它到底应该对应着法语里面的哪个词的时候，那么就可以通过这样的一个注意力权重来实现学的最好就是我甚至阿尔法，另外两个阿尔法都是零。那你真正跟它相关那个重的那个阿尔法，大家还记得那个矩阵对吧？就变成了一。
那那他就直接从这个重就输过来了让他通过这样的一个方式捕捉了我们的输入序列，这是一个输入序列，是我们的所有的笔记和我们的这个位置之间的一个相关性。这个是我刚才我讲到这个S还有三个部分的输入，从这个S阶减一上一个同学到CJ到YJ减1Y是这个输出的一个结果，就跟我们X要直接输进来一样，他把这个输出的结果也要丢进来，这个也是比较直观的。就是他把这个你可以理解成你的这个没来听课的同学，一号他还可以输出一个笔记，他把他的输出笔记给你，你学习了他的这个成果。
这么样的一个打比方，希望大家能理解这个注意力机制的这个过程，这里我们再展开稍微讲一讲这个注意力机制当中的一些细节。首先我们刚刚提到注意力权重是用来表达我的最终的输出的。这个同学就没来听课，这个同学也就是我们要翻译出来这个目标语言，它跟我们的这个输入当中哪一个词重点关系最大。通过这个注意力权重，阿尔法这么一个矩阵也好，具体的一个也好来表达。然后它怎么来的呢？它其实是在我们的论文当中做了一个抽象，把这个阿尔法IJ等于这个PEIJ，我们把这个稍微展开来讲一讲，这个跟后面transformer也有关系，那他捕捉了这个reliance，就是它的关联性，就H和S之间的相关性，就把这个纵横area找到了，找对了。
但是具体来看，它这又套了一层函数P这是一个分布函数，里面输入的是一个EIJ，那什么意思呢？就是EIJ等于这个ASJ减1HI，就等于这样的一个矩阵。整体这个矩阵我们把它叫做我们把这个过程给大家再做一个拆解，大家可以比较好的去看懂看看懂这个过程。
这个A叫做对齐的函数aligned方向。我们刚刚看到是一个具体的一个向量，它是这样乘出来的。但是我们有很多个注意力权重对吧？RYG他们整体组成了这么一个矩阵。这个矩阵其实就是我们要学习的东西。就我们在讲learning对吧？
我们讲deep learning，march learning在学。其中这个注意力权重就是我们要学的一个参数。这个参数整体构成了一个矩阵，这个矩阵可以作用于我们的输入和输出，就是我们的SI减一和我们的HI。然后整体的这个函数我们叫做alignment function，相当于我们在学这个，然后我们要去取一个EIJ就是去取J减一和HI那相当于就取出了这样一对关系，就S2和当中的某一个HI的这个关系。那么这里这个对齐函数最重要的一个点，它捕捉的一个什么呢？它就是把这个query和key的这个关联关系，它通过这个elegant function找到了。
简单来说，我们把这个展开给大家整体做一个说明，我们整体来看一下，这个公式其实并不复杂。但是通过这个公式这个形式化定义，我们能从attention进一步扩展到transformer和所有的大模型。我们再来捋一下这个逻辑当中，这个里面的变量定义很多，但是我们都是纸老虎，我们梳理一下就能明白。首先H是什么？输入的这个笔对吧？S是没来的同学的笔记。这么理解，阿尔法是我要关注的这个重点。我每一个同学没来听课同学我都有一个关注重点，我关注哪一个？听课同学的笔记都有一个重点，这是他的具体的一个的注意力权重。
这个注意力权重组成的矩阵我们叫做alignment function，叫做我们的对齐函数。对齐其实就这么一个意思，就把输入输出，包括它的位置对齐。同时为了形式化定义更好看，我们抽象出了一个东西叫E，就是这里的这个EE等于AKQ，AKQ是什么呢？就这里就我们这里鼠标指向了这里这个E等于AKQ，那它可以有具体取某一个IJ具体某一个值的这个形式，也可以写成矩阵化的形式，就E等于AKQ。那这个K是什么呢？K就是我们的key representation。
大家现在都知道我们做这个注意力关键其实要找的就是这个key。我到底哪些是我的这个重点？就我这的S2我要关注的重点是什么？Q是我的输入，是我的query，就是我提的这个你可以理解，就是我给你的这一串输入题的问题。简单来说就是如果是一个翻译任务的话，我给你说了一长串的话，就是刚刚提到那一长串的话。但对于我一个具体的待翻译的值，就比如说这个area来说，它就是我的key。一个具体的key，那他要关注的值就是要从query里面找到那个zone，那他这个E就是用来干这个事儿的，用来表征我具体的一个key跟Q之间的一个关联关系。然后我每一个词都组成了一个矩阵，最终就变成了这个align function。
这里大家应该明白了对吧？但是E把这个关系抽出来之后，它是一个注意力权重的值。那这个值它可能是比如说55、68、77都有可能。但这样的一个值作为权重来说不太好归一化。我们大家都知道归一化要干什么事情，就是把所有的这个注权重，比如说这里的C2，它这里的阿尔法212223，我希望这些值加起来最好是等于一的那这样的话我们能够去做一个很好的数学上的计算。所以这里会套一个soft max，这个softmax要干的事情就是简单来说把我们的这个注意力权重的这个值它通过softmax变成一个0到1之间的值。并且使得我这样的一个横线，就我这里的这个一个横线当中和这个注意力的权重经过这个softmax之后，它变成了注意力分数，就是一个0到1之间的是它这一横线的注意力分数加起来是等于一的，这样就很好的能够去生成上下文的向量，就是我们的这个context the vector。
整体来看通过一堆的等价变换，我们把注意力机制变成了这样的一个形式化定义。这个形式化定义就是最外层套了一个soft max这个softmax就是为了把我们的注意力权重变成一个0到1之间的注意力的分数，使得一个特定的输出，它的所有相关的注意力分数加起来等于一。然后这个注意力分数是通过soft max来的那这个softmax输入是一个注意力权重，这个注意力权重就是我们的alignment function当中的一个具体的一个位置上的一个值，对吧？这个具体的位置的值其实捕捉的就是我输入的这个Q之间的一个关系。
通过我的aligned，其实这个地方AKIQ就是等于一个E，对吧？等于一个具体的EIJ。这个具体的EIJ变成了一个0到1之间的值。然后这里我们把这个H变成了一个V大家能看到这里乘上的是一个VI这是为什么呢？是因为我们我们可以这么想象一下，就是一个实际的场景里面，我除了把老师讲的这些X1、X2、X3，这是我的输入对吧？
通过几个同学变成了学习笔记，学习笔记给到后面的这个同学拿去抄拿去学习。但是我为了让后面的同学能够记得多一点，我也许不只是给了他学习笔记，也许我还录了音，对吧？我还录了像。这些内容其实可以变成后面这些同学额外的一些学习资料。
这些额外的学习资料，这样的场景是很常见的那这个大家应该能理解。所以这里我们补充了一句话，就在一些特定的场景里面，可能有一些额外的这些输入，那我们就把X加上这些额外的输入统一变成V，乘上这个V然后最终变成了一个什么样的形式呢？我们把这一堆等价变换讲完之后，变成了一个最关键的三个参数。QK是什么？已经讲清楚了，输入的这一堆query和我们的想要的关键位置的KK key，以及最后的完整的输入V这是三个最重要的参数，然后被这个A包裹起来，这个A就是我们的注意力机制attention。这个其实是注意力机制，我们给它刨丁解牛。
大家理解下来最重要的一个形式，最通用的一个形式。这个形式里面有很多的数学上的一些定义，大家理解的话就是输入输入变成了学习笔记，就是我们的隐藏层，这跟传统的RN一样。然后如果我们忘掉中间alignment function的话，就是输出就是我们的这个解码，就是decoder hidden state，就是这一个一个的S然后这个输出解码的结果是这一个最终输出结果Y1、Y2、Y3对吧？
在这个过程当中我们引入了一个alignment function。这个align的function就是用来学输入跟输出。在一个特定的输出上，它到底跟哪个输入关联度最高，通过alignment来学，这个alignment function是学出来的一个矩阵。这个矩阵里面的这些注意力权重就表达了我针对一个特定的输出，哪些输入跟它高度相关。然后这个注意力的权重本身是一个比较不太好确定的值。通过一个distribution function这个分布函数，我们这个P变成了一个0到1之间的注意力分数。我们再看一下这个形式，整体的这篇论文的思想就来自于这里。
2014年叫做neural machine translation by jointly learning to align and translate这篇文，这篇文章是本就和他的这个学生叫banana，这两位一起发布了这篇文章，但还有位二作。这篇文章最重要的价值，第一，我们刚一直在讲机器翻译，机器翻译是一个自然语言处理非常难的问题。他第一次把这个机器翻译当中的翻译问题和对齐问题，用一个网络给搞定了。这个网络就叫注意力的网络，当然当时还不叫这个名字，但后来大家都引用了这篇文章的思想去做这个事情。
大家再回顾一下，刚刚我们把翻译问题当中的某两个词area area跟这个中文关联起来了。这个关联起来就是通过学习了一个alignment function。以前我没有aligned function的时候我能做翻译，但是我不知道这个词跟这个词关联度到底怎么样，我就相当于一个黑盒子。但是通过alignment function里面的这个注意力机制，我把这个对应关系、位置关系、这个敏感度、这个关注度也学出来了。他同时做了翻译和对齐。这个相当于用一个网络架构做了两个任务，这个是一个很重要的思想。就是以前大家都是搞NLP的时候，就是干一个任务做一个模型，最后把很多模型缝合起来，串起来。但是能不能用一个网络去学很多任务，这个是一个很关键的事情。
并且他是使得我们对语义理解有了一个新的思路，这个transformer其实就沿着这个思路去做的那这个是attention mechanical非常重要的一个贡献，也是所有后续做这个大语言模型都在在引用的一个思想。那我们再回到这个注意力机制这个模型上，其实我们能看到注意力机制有很重要的几个点。第一，就刚刚讲的那些形式化定义，但其中还有一个点就是它的element function，就我们去学的这个对齐的这个函数。
这个对齐函数其实很很值得玩味。就是对齐函数本身，其实它是一个大家看到我左边这幅图，有各种各样的对齐函数，对齐函数它是一个需要去学出来的东西。但是学它的时候我可以符合某种形式，就相当于学的这个形式有很多种。比如说我可以学这个similarity，就我的相似度包括这个点击，scale的这个点击。这个其实就是最后transformer用的方法，学的这样的一个方法。就是你可以理解成我去关注这个输入跟输出的时候，这个对齐函数本身，我当然有超参数需要去学，那是具体的一个值了。但是这个对齐函数是有很多种选择的那好，其实追根溯源，这种对齐函数的这个学习方式，在一九六几年的时候就有一篇文章。那个时候提出了一些非参数化的方式来学习输入跟输出之间的关系，这也是很多人今天还在聊，就是14年这篇文章不是开山之作，其实一九六几年就是这个思想，这么说也不完全错，是也是他对的地方。
为什么？因为去学习对齐的核心其实就是有两个概念，我要把他们这两个概念之中的一些局部概念对上号，这个是对齐要干的事儿。但是是你要知道我去学这个过程是有很多种方式的。一种就是这里看到的参数化的方式。参数化的方式就是说我给你一个函数数，这个函数里面还有一些参数是没有定下来的。超参数根据我输入的数据你去学它就学出来，结果是适合你输入这堆数据的分布的。
还有一种方式就是我没有超参数，我是非参数化的方法。比如说我给你一个核函数，这个核函数是定下来的，那么这个定下来的核函数你直接拿去用就好了。只有跟我们最早搞这个识别任务，搞这个图像识别的时候，我们有这个专家已经很多年经验做出来的一些卷积核，比如说我用做边缘检测的这种什么，包括一些经典的算子。这些算子其实是很多年的经验，并且它也符合一些数学上的要求，可以把边缘检测出来，明暗检测出来之类的。这些都是一些非参数化的方法。
在自然语言处理里面也是一样的，有一些经典的核函数的一些方法可以直接用，但它的适应性有限。尤其是在语义的这个理解上面，它的适应性有限。所以说后来才会提出这些参数化的方法，让我根据你的特定输入数据去学出一个特定的超参数，符合你特定输入数据的概率分布。它的这个适用范围就更广。虽然它不是一个通用AI就是什么问题都能解决，但是至少一个领域或者说绝大部分的常识它都不能解决了。
这个是我们在讲这个注意力机制它未来发展的一个很重要的一个设定。它的这个对齐函数本身是有很多种不同类型选择的，这个应该也很好理解对吧？就是每个人都有自己的学习套路，但这个学习套路在不同的学科上面，你要有不同的注意力权重，这个过程就跟我们人学习一样。你想象一下这二三十年来三四十年来你的学习过程，你已经学会了对不同的人见人说人话，见鬼说鬼话。其实就是这个逻辑。你知道在什么样的场景里面该说什么样的话。因为你用了不同的aligned function，或者你用了同一个aligned function。但是在不同的场景里面，它的超参数学的不一样，就这个意思。
那刚我们其实有讲到这个过程，就是alignment function学出来的这个值，它是一个alignment这个券就是我们的对齐的这个值。那这个对齐的，这里有可能要把中文和英文再给大家声明一下。刚刚应该说说反了，就是在有一些文章里面有去讲，就是我们直接输出来这个词，就这个a alignment叫element的分数，不叫权重，叫aligned。这个分数在这幅图里面有不一样的说法，但这个英文跟中文的翻译不重要，有的地方叫alignment，有的地方叫attention。但是简单来说，我们通过更通用的形式，所算出来的这个值，我们通常把它叫做alignment score。
就我们对齐的一个的分数，刚刚我把它叫的是注意力的分数，但其实我们可以在更通用的形式上把它叫做对齐的一个分数score。因为到这一步大家应该能理解了，其实A这个对齐函数也是一个可替换的东西，不是一个死的东西。那么它输出的值是对齐的分数。
对齐的分数是各种各样的值。我们为了把它规划做一个softmax，就是我们这个P套了一个分布的一个函数。那么到了这个分布函数之后，就变成了attention with。这个就回到我们刚刚看到那个C里面那个阿尔法，就具体的这个值。这个attention with是一个规划的值，构造出这个上下文的向量context vector，然后整个这一块我们把它叫做attention layer，就是我们的注意力程。以前是没有这一层的，以前把这一层完全抛掉，是可以直接对上号的。就是我们的encode decode的结构，因为引入了注意力，使得我们现在的经网络可以主动的去学习特定的词跟词之间的这个关系，对应关系，这个对齐的关系。


![image](https://github.com/user-attachments/assets/d2cb54e3-ccd2-4a75-b59d-ab2736bb7353)

