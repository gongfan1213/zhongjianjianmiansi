# 第4章 Chapter 4
提示工程的基本思路和技巧
# 4.1 提示工程基础知识
## 4.1.1 提示工程的基本思路


提示工程即为通过构建合适的提示词，提供相关的信息，来调教大语言模型（Large Language Models，LLM）帮助人们完成所需的任务。那么如何来进行调教？这涉及如下几个方面。总体的出发点是可以把大语言模型想象成一个强大的助手。那么在给这个助手分派任务的时候，就需要考虑这个助手擅长什么，想让它做什么，然后对它做出来的“样品”如何进行评估。

第一，提示词的撰写是基于对大语言模型的特点的基本认知，即大语言模型擅长的部分和（暂时）不足的部分。

第二，提示词是基于用户对任务的透彻理解，并且通过语言清晰地表述出来。把大语言模型想象成一个强大的工作秘书，它拥有广博的知识并精通各种任务。但是，对于用户需要完成的任务，语言模型并不知晓，特别是行业内的细分领域的知识和具体任务的要求。这就需要用户在撰写提示词的时候，首先把任务理解透彻：需要做什么，有哪些知识需要通过提示的方式告诉语言模型。正如我们在工作中给人分派任务一样，往往一个简单的任务描述是不够的，而需要我们将各方面的要求阐释清楚甚至提供好的样例来使对方准确理解我们的意思，对大语言模型亦然。


第三，提示工程要求用户有一套方法来评估提示词的好坏。语言模型固然强大，最后它输出的结果可能不符合要求，好的提示词应当能够在多次使用中使模型输出优质的结果。

第四，提示工程是需要反复迭代的。正如与人类助手的合作时也往往需要多次修改迭代一样，和大语言模型的“合作”需要用户反复基于返回的结果对提示词进行调整。对于语言模型非常熟悉的、在它训练时候就已经见过多次的任务，可能只需要少量迭代就可以。但是随着我们希望用语言模型来完成更多更复杂的任务，或者深入到垂直领域，对提示词的调试就显得尤为必要。

总而言之，以当前大语言模型出色的人类语言理解和表达能力，人们完全可以把它想象成一个助手，然后考虑如何跟它进行有效的沟通来完成手头的任务。因此，提示工程表现得更像是一门艺术。许多实操的案例表明，写出好的提示词需要发挥想象力。某些看似不经意的改动可能带来出其不意的效果。如Isola的推文指出在提示词里加入一些特殊词语可以提升图片的质量 ，以及由Kojima等人提出的“让我们一步一步思考（Let’s think step by step）”提示，可以大幅提高大语言模型在推理任务上的正确率。

OpenAI专门发布了“获得更佳提示结果的6条策略”（https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results），来指导人们使用提示工程，这些策略包括：

1）编写清晰的指令：提示者应直接说明自己的需求，例如，要求大语言模型简短答复，或要求专业级答案等。需求表达得越清晰，得到的回复也就越准确。


诚然，这些技巧的普适性是有待商榷的，此处仅想借助这些例子表明提示工程在这里也表现得像一种艺术。

2）提供参考文本：为了对抗大语言模型可能产生“幻觉（hallucination）”的情况，在提示时最好同步提供含有准确的原始信息的参考文本。

3）分解复杂任务：将复杂任务拆分为更简单的子任务，或者基于早期任务的输出构建后续任务的输入，都可以有效地降低大语言模型回复的错误率。

4）给模型思考的时间：要求大语言模型在给出答案之前提供思考的过程，可以帮助它更可靠地推理出正确的答案。

5）使用外部工具：利用其他工具的输出来弥补大语言模型的不足，例如，目前被称为“检索增强生成”的方法就是利用文本检索工具获得文档信息。 

6）系统化测试改进：通过与标准答案的对比来评估大语言模型的输出，以确认更改提示对提升模型性能有效。

依据具体情况综合使用上述策略，能够帮助用户更有效地利用大语言模型获取准确的结果。

# 4.1.2 提示工程的特点

本小节具体介绍提示工程有哪些特点。正如4.1.1小节所述，提示工程需要基于大语言模型的特点。当前的大语言模型，如ChatGPT的前身GPT-3，根据Brown等人公开的论文，拥有1750亿（175B）参数，利用了4290亿token的网络爬取的数据，670亿token的书籍，和30亿wikipedia的数据。没有任何一个人可以有如此大的阅读量，遑论将如此庞大的信息组织在一起。当然，所有人类的知识量总和起来是有这么多的，但是每个领域每个人所获得的知识是割裂的，没有易扩展（scalable）的方式将它们汇总叠加融合。而大语言模型使这成为可能。它见过几乎所有公开的网页，学习过几乎所有领域的知识。因此，我们在撰写提示的时候，就可以先想一想，这个任务是不是大概率它已经见过很多次了，还是对它来说比较新的任务。例如，我们想让语言模型解释一个词的含义，那么可以想象，它在训练的过程中已经见过了大量网上词典和百科词条，自然可以做得很好，除非这个词的意义非常生僻，在网上都很难见到，或者甚至只有在某个尚未完成电子化的古书中存在。再例如，我们想让语言模型写一份请假条，那么它必定已经见过网上成千上万的类似文本，只要简单告知这个任务它就可以做的很好。相反，如果我们想让它撰写一份行业趋势发展报告，那么它大概率会陷入空洞无物、泛泛而谈的毛病。这就需要我们来提供更多的“干货”和背景资料来使报告更有深度。

此外，由于大语言模型见过的资料是跨领域的，而所有这些信息都蕴含在整个庞大的神经网络之中，它自发地建立了不同概念和知识的内在联系。这使得它能很好地帮助人们打开思路，发现新的不同领域交叉带来的新方向和新角度，甚至帮助人们意识到思维的盲点。因此，完全可以利用大语言模型的这个特点来使得人们的工作更富创造力。这个时候，应当在提示的时候尽量激发语言模型的这个特点，促使它提供更多多角度的思考方式。

最后，大语言模型拥有强大的多语言理解和表达能力，以及强大的代码能力。这是因为大语言模型在训练时使用的目标函数就是基于上下文词条预测的准确率。这意味着，在语言润色、修饰、翻译这些常见的自然语言处理任务的时候，大语言模型能够取得非常出色。因而，通常的提示词撰写都是基于自然语言。当然，在文献中也有很多工作发现最优的提示词并不一定是人类可读的。但是这样的提示词往往需要通过数学优化方法（梯度下降、遗传算法、强化学习等）得到，获得的时候较为烦琐，而且其在相似任务或不同模型间的可迁移性尚未有系统的研究。此外，从工程应用的角度来讲，让一个提示词是人类可理解的也方便后续的维护和改进。

上面从大语言模型的特点出发介绍了提示工程如何利用这些特点。下面再介绍提示工程师在这其中发挥的作用。

第一，如4.1.1小节提到的，用户需要提供完成目标任务的具体信息和细分领域知识。即回答我需要它完成什么，什么是语言模型已知的和什么是语言模型未知的。这些问题看似简单，在解决具体应用问题的时候并不尽然。例如，用户希望用语言模型来完成文本的分类，检测文本中是否有歧视性的语言。那么就需要首先理解清楚，对于当前的应用场景，什么是有歧视性的语言。这里面肯定有很多模棱两可的语言需要处理。再例如，用户用语言模型来做检索句扩展（query expansion），那么很显然，对于一个语句作扩展可以有多种可能的目的，每种目的的要求是不一样的。可能是希望通过扩展使得检索句更符合用户的意图，可能是希望更广泛地涵盖到这个检索句可能的各种不同的意义，可能是希望换用一种更常见的更能够搜到相应文档的表述等。每一种具体的目的，决定了提示词应该怎样优化，和相应地，在提示词中提供什么样的细节信息。

第二，大型任务需要人来预先进行分解。虽然语言模型能够接受的输入长度在不断提高，拆解复杂任务仍然是达到理想输出所必要的。以撰写论文为例，可以分解成论文选题、分角度文献搜集、小节撰写等阶段，每个阶段的优化目标是不同的。选题阶段，着重在利用大语言模型来进行头脑风暴，发现新颖的观察角度和研究方向。分角度文献搜集阶段着重在利用大语言模型广泛的阅读量，提供已有工作的总结，帮助我们迅速了解一个方向的已有工作。当然，由于语言模型也容易犯事实性错误，这一阶段的结果也需要提示工程师对结果进行甄别核实。但即便如此，往往大语言模型提供的结果也非完全错误，而是对帮助我们进一步查找精确信息提供了有效关键词和基础概念。小节撰写阶段着重在语言的准确性和合规性。对此，我们可以在提示词中提及相关的文字风格方面的要求。综上，对于复杂的任务，或者任何语言模型做得不够理想的任务，提示工程的一大方向就是将这个任务作进一步细分，对每个子任务设计不同的优化目标和提示。 

第三，大语言模型的输出需要人来把关。除了肉眼可以迅速判断的质量好坏以外，这里所说的把关涉及以下几个方面。

1）大语言模型是一个概率模型，每次输出的结果可能不同。一个提示词的好坏，不能仅凭单次的结果，而应当根据多次结果的综合。即使是在小规模数据上已经调试好的提示，在更大样本集上运行的时候，仍有一定概率有部分结果不符合用户的要求。 

2）众所周知，大语言模型可能“一本正经地胡说八道”，即所谓“幻觉”。由于它总是会表现得非常自信而肯定，当它在提供虚假信息的时候，人就会很容易被误导。如果应用场景对事实性错误容忍度较低，那么检查结果的正确性就极为重要。 

3）同一个提示词在不同的模型上效果可能有差别。特别是我们的提示词如果是在一个大模型上调试好的，那么换到一个更小的模型往往质量会下降，而需要进一步调试。一个经过人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF）调试过的模型和原始的模型在输出的分布上也会不同，因此也需要对提示词作调整。 

4）大语言模型对一些人类觉得没有区别的东西可能意想不到地敏感。更准确地说，我们需要确保大语言模型没有错误地理解我们的要求，没有觉察到输入中的一些任意模态（pattern），而导致结果的统计偏差（bias）。例如，人们在提示词中列出了一系列信息，那么这些信息的排序有可能会引入额外的偏差，如有可能语言模型会认为前面的更重要因而前面的信息会在结果中占到更大的权重（或反之）。再例如，人们在提示词中列出了一堆需要模型来打分的样本，那么不同的分割符可能对结果质量有影响。这些往往在一次试验中难以观察到，而需要多次测试和采用不同的样本。

# 4.1.3 提示调试涉及的因素

一个最简单的提示即为对任务的描述，即指令（instruction），以及当前需要模型进行处理的输入（input）。对于复杂的任务，我们还需要提供所需的上下文信息（context）或提供示例（demonstration）。在图4-1所示的例子中，指令即为告知语言模型我们希望它对推文（tweet）进行情感分类，示例即为两个具体例子，而输入即为当下需要语言模型进行判断的推文。对提示词进行调试包含对提示词这些要素的选用和改进。具体的技巧会在后续小节介绍。这里介绍两个具有普遍性的因素：创新性和稳定性的取舍，以及输出结果的格式控制。

请对推文进行情感分类 指令

推文：今天天气很好

情感：正向

推文：这家店的菜，味道真是一言难尽

情感：负向 示例

推文：制作精良，故事引人人胜，值得一看 输入

情感： 输出引导词


![image](https://github.com/user-attachments/assets/7155de49-995a-42c8-8b10-49b8b9197468)


图4-1 一个简单的包含指令和示例的提示

一般而言，在对提示词进行调试时面临结果创新性和稳定性的取舍。对于有的任务，我们看中结果的创新性，如生成新的想法、撰写有趣的故事等，这时最希望模型给出的是那些不那么常见的、能让人眼前一亮的结果，而非常见结果。相反对于有的任务，我们知道是有正确答案的，如上述的推文分类，或者推理类的任务等。这时我们希望模型在多次运行时候都能够返回正确的结果，即结果是稳定的。显然，通过改变提示，可以促使模型的结果更加有创新性或更加稳定。在提示不变的情况下，也可通过对采样参数的调整来实现在两者间的取舍。

大语言模型内部的神经网络的输出结果实际是一个词向量的分布，最后输出的文本由在这个分布上进行采样而获得。按照OpenAI官方文档，常见的有以下几个参数。

1）温度（0-1之间）：高的温度使得生成的文本更加随机，即模型更可能采样到低概率的词；低温则使结果更加确定（但仍有随机性）。

2）Top_p：使得模型只考虑从概率较大的token中采样，其效果和温度是类似的，因此只使用两者之一就可以。 

3）Presence and frequency penalty：用于抑制模型重复当前文本中已有的词语，即鼓励模型转移到新的句式或者开启新话题。

对于具体的应用，合适的取值需要调试。一般而言，对于有确定答案的、非发散性的任务，可以设置温度为零，且不采用Presence and frequency penalty。

在调试的时候需要对模型输出结果的格式进行控制。例如在推文分类任务中，只需要结果输出positive或negative。但是模型往往还会有一定概率生成冗长的其他句子，如解释为什么是positive，或者附加说明这个结果比较模棱两可等。这样“非常规”的结果不但使得输出的句子过长使token配额（quota）消耗更快，也给输出结果的后处理带来麻烦（即后处理函数需要处理更多的边界情况）。一种方式是用户在提示词里面告诉模型不要输出这一类信息。如“don’t output any reasons”或“don’t output anything else”。但是通常语言模型对于“不要做某事”的指令，并不能够很好地执行。一个常见的提示工程的小贴士是尽量把所有的“不要做某事”改写成“做什么”。如果确实需要告诉模型不要做什么，那么模型可能对这个否定表达的方式非常敏感，因而需要用户对提示词写法进行更多的调试。对于上面的例子，更有可能来规范输出结果的一种指令是“output only ‘positive’ or ‘negative’, and nothing else”。（当然此处仅为举例如何调整提示词来控制模型输出的结果。具体效果会依模型不同而有所不同，而提示词也可能需要相应调整。）另外，通过给出样例也能起到帮助规范输出格式的效果，下文对此将予以详述。

# 4.1.4 提示效果评估

本小节介绍如何对提示效果进行评估。如上所述，对输出的有效衡量是后续迭代的基础。对于单次使用的提示，一般只需要肉眼观看模型的输出是否符合要求，然后修改提示词直至满意。但是这仅表明这个提示词对于当前这个单一的输入，可以得到想要的结果，对于更多的不同的输入，这个提示词是否有效仍是未知的。特别是如果用户需要拿这个提示词在实际产品中反复使用，那么提示词的可靠性需要全面的评估。

首先，需要建立一个真值（ground-truth）数据集，用来评估提示词。对于常见的任务，可以寻找公开的数据集，如Natural-Instructions涵盖的1600+个自然语言处理任务，以及StrategyQA、ARC、Common-sense reasoning等推理任务数据集。然而对于实际的商用场景和细分领域，往往没有现成的数据集，或者公开数据集的标注标准与应用需求有出入。这时一般的做法是搜集该问题有代表性的数据样本进行人工标注。对于这些标注，需要确保它们是高质量的，确实可以作为“真值”来指导提示句的迭代。为减少人工工作量，可以从少量的数据开始，然后用主动学习（Active Learning）的方法，不断找出当前提示词下输出结果不佳的样本，将其添加到真值数据集中，再据此改进提示词。 

其次，需要定义一个指标（metric）来衡量提示词在真值数据集上的总体表现。指标的定义跟一般机器学习模型的衡量指标类似。对于二分类任务，可用ROC AUC、PR AUC、precision/recall、F1 score等。对于排序任务，可用NDCG（Normalized Discounted Cumulative Gain）等。对于文本生成任务，如果这个任务有相对的标准答案，那么可以用常见的BLEU、ROGUE等指标衡量模型输出和标准答案的综合匹配度。除此之外，对于需要落地应用的提示词，也应考虑产品端的优化目标来定制相应的指标。
# 4.1.5 工具和资源

当前，提示词的撰写已成为互联网上流行的话题，相信读者可以搜索到很多好的样例作为借鉴。下面仅举几个可能有帮助的网站。

“promptoMANIA: AI art community with prompt generator”可以帮助用户撰写用于图像生成的提示。

“PROMPTME THEUS - ChatGPT prompt engineering toolkit”帮助用户撰写ChatGPT的提示。

“GitHub awesome-chatgpt-prompts”搜集了许多不错的ChatGPT用法和相关提示词。

“Public Prompts”是图像生成提示词的集合。

当然，既然提示词的撰写是一个“文本生成”任务，那大语言模型本身也可以是一个提示撰写的有效工具。用户可以用一个简单的指令让它帮助生成若干可用的提示词，或者输入一个自己写的提示词请它作润色。如图4-


![image](https://github.com/user-attachments/assets/e27eb234-414f-4cc2-8464-ca82c6f8d6ae)


4-2所示，语言模型帮助生成了10个用于总结新闻文章的提示词供选择。

请帮我写10个英文的prompt，用来让GPT总结长文章的内容

1. “Can you provide a concise summary of the main points discussed in the article?”

2. “In a few sentences, summarize the key arguments made in the article.” 

3. “What are the main findings or conclusions drawn by the author in the article?” 

4. “Could you distill the article's main ideas into a brief summary?” 

5. “Summarize the article's main points in your own words.” 

6. “What were the primary points addressed in the article?” 

7. “Give me a brief overview of the article's main themes.” 

8. “In a nutshell, what was the main message conveyed by the author in this article?” 

9. “If you were to summarize the article for someone who hasn't read it, what key points would you highlight?” 

10. “Provide a concise summary of the article's content and main takeaways.” 

图4-2 用OpenAI GPT-3.5-turbo模型生成英文提示

用大语言模型生成提示词有几个好处。如果想用你的非母语撰写提示（如中文母语者撰写英文提示），那么模型生成的提示能更好地符合该语言的用词和语法习惯。而且因为提示词是大语言模型自己生成的，能够更好地为它所理解。同时，我们可以根据模型生成的提示词来判断它是否准确和全面地理解了用户的意思。后文进阶技巧中将结合已有的研究进行详述。 

# 4.2 提示工程基本技巧 

本节开始将展开介绍一些具体的提示工程技巧。值得注意的是，随着大语言模型的更新迭代，对最新版本的模型而言，有些技巧变得不像过去那么重要，但本书仍会花简短的篇幅进行介绍，目的有二：其一，希望帮助读者层层递进地理解用提示词来调教大模 
