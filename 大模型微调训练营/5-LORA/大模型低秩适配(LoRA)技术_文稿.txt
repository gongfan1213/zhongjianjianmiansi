	大家可以看见吗？hello. 可以听见声音吗？可以的话，写一个一对一，应该都可以看见和听见。好。好好好，看来星期天的晚上大家还是挺积极的那我们就正式开始今天的这节课。今天这节课可能不一定会讲满3个小时，因为我们希望今天能够把这个prompt这件事儿给讲明白。就整个基于prompt的这些突然的方法，不管是我们上一节课学到的adapter，还是我们知道soft prompt这一类的技术。比如说我们提到的这个prefix，prom tuning和P20这几类，这个soft prompt技术其实也都是PFT的一个分支，那么今天我们会把PFT做一个小小的总结，然后PFT里面我们有提到，除了上节课讲到的这个适配器和我们的软提示这两种技术以外。
	还有一个很重要的技术路线。都是说怎么样用低的成本，用比较少的训练参数来撬动整个大模型的训练。这就是我们今天会讲到的lora，也就是这个大模型的机制适配的技术。整个Laura这条线呢，其实需要有一点点的数学的要求，所以我会讲的慢一点。大家如果有什么问题也都可以在评论区里面提出来，我们可以及时的来做沟通。然后把lao ora讲完之后，可能还会再讲两个paper，一个是UDPELT，就是一个统一的大语言模型，高效微调的框架，还有一个叫IA3，这两个都是2022年的论文，各自也都在PFT这个分类技术里面有自己的一块独特的定位，所以拎出来两个有代表性的技术跟大家分享一下。
	好，那我们就正式开始今天的这个Laura相关的PFT技术的分享，然后最后我们会来总结和QA。首先大家如果还有印象的话，我们刚刚有讲过这个apter和这个soft proms这两种大语言模型微调的方法。Adapter方法其实是在我们的transformer这个网络模块里面，在FFN就是我们的fit forward的network后面增加了两个adapter的小模块，这两个小模块是我们真正adapter tuning去训练的模型参数。而原来的transformer的这些模块当中的计算量大的参数其实是不训练的。
	他在早期2019年提出来的时候，是取得了一个非常好的微调的结果。当时是比较趋近于19年的全量微调的一个水平。但是它也有它很严重的一个缺陷。
	现有PFT也是站在2022年，也不是今天的这个PFT以及PFT的技术，每天都有一些新的推一些新的论文出来。那么adapter方法在当时是不错的。但他在今天我们来看，尤其是在大家已经学习了PFT这么多不同的方法之后，就会知道而adapter方法它其实是增加了模型的深度，这个模型的深度我们能看得到，在原来的大模型上增加了adapter的模块，而这个adapt模块其实是相对来说是纵向增加的。这个纵向增加的模块在训练完之后，我们要把整个大元模型加载进来。加载同样就要加载你新加的adapt模块，不然你就相当于没训练那加了的这些adapter模块是有计算量的，所以某种层面上它会增加模型的推理延时。尤其是基于19年这篇paper之后，后续还有很多adapter的文章，我们今天也会做一些简单的提及。
	所以说adapter方法的这个挑战和局限就在这儿。第一是因为它增加了推理的延时。第二就是说它的训练方法使得它的GPU消耗还是很高的。它需要加载整个大语言模型，因为它没法剥离出来，它是在整个大语言的模型的这个网络结构里面加了一个小模块。
	那么像prom tuning、prefix tuning和P2I这些soft proms方法也有一些问题，最明显的问题就是我们在看perfect student的这个paper的时候，会发现它很难训练，尤其是他在一些序列标注任务，这一类需要强调自然语言理解能力的这种任务下游任务上面其实是很训练的。并且他的这个训练完成之后，我们会发现它有一个叫做虚拟的token，或者叫这个virtual tokens。这个virtual tokens是会在原来的这个预训练模型，就这个大语言模型。假设本身是支持2K的token，那你的prefix或者你的prom ti有可能会占据一两百个token。
	这个其实是一个相对来说一个比较大的缺陷了。因为你的模型有一部分的可用长度被剪短了，并且一些复杂任务说不定还会用的更多，比如说alt GPT这种agent的任务。如果你要基于这个翻译tune的方式，PFT的方式，然后又是用这种soft proms的方式，那你就会浪费掉很多的相当于可用的序列长度。这个是一个比较现实的问题，就soft prompt的这个方法这一类方法。所以就我们目前学到的这两个大的类别，adapter和这个soft proms这一类方法，他们都比较难以兼顾高效率和高质量，效果通常也没法达到全量微调的一个效果。所以我们就要讲今天的这个很重要的一个分支。
	并且他的这种思维或者说方法论也是比较通用的，不仅仅能作用于大语言模型，也有很多的同学知道纹身图，stable diffusion，也有Laura的一些微调，这就是因为lara的这个思想，它本身就不太是说绑定在大语言模型上的。那我们会逐步的去把它给大家做一个庖丁解牛，并且会去讲Laura的进阶版本这个AW ora。所以简单来说就是用一个更小的所谓的低质的本质维度，或者我们叫更小的矩阵，来解决原来大语言模型参数过多的一个难以训练的问题。核心来看，其实就是2021年，也就是两年前微软发了一篇paper，这篇paper就叫Laura，low rec adaption of a large language models，就是低质适配，这个低质适配主要是作用于大语言模型，然后微软自己也开源了，Laura的这个实现在给他上简单来看就是右下角这幅图可以比较好的表达出roa的一个核心思想。
	就是说我们能看到这里有一个这幅图里面是一个从底到顶，或者说从下往上的一个做计算的一个结构，最下面的这个X是我们某一个局部的输入，然后它有两条路可以走，左边这条路叫做预训练模型的模型权重，然后是一个地位的一个高维的这个地小地地层地的一个高维的一个空间。这个高维的空间原来的模型权重，然后我们现在希望用一个比较小的低维的模型空间，就能够去表达出原来的这个高维空间。所以这里有两个小矩阵，A和B都是梯形的，这里有一个小的R然后这个R原则上是远远小于D的，可能是2到3个数量级的一个减少。甚至后面IA3可能小了这个四个数量级，就可能是它的1‰甚至1‱的一个维度了，然后RR是远远小于D的。那么我们实际上要训练的，以前可能是说我们要训练这个预训练的参数，要么是在它里面嵌入一个adapter，要么是在它的某些层增加一些前缀。
	Na有一个新思路是说我就不管左边这条路了，左边有一个预训练的权重，右边是我们要新训练的一个模型，这个模型很小，在这儿它有一个自己的称称呼，叫做更新矩阵updated的一个metrics，分别是A和B，预训练方式，这个初始化的方式也不一样，矩阵B初始化是0，矩阵A是一个正态分布，具体怎么做，我们可以来看一看。整个lora它的核心其实就是在原来的这个。你可以理解原来有一个神经网络，然它的连接是已经定义好的，并且已经预训练成功了。现在我们在旁边增加一个附加的一个网络通路，或者说有的叫做外挂的一个通路。这个通路它的核心就是用小矩阵去模拟大矩阵。
	那怎么样去模拟呢？其实我们可以看一下，核心其实是我们整个右边这幅图，就等于这里这条公式H就是我们最终在这个局部环境里面输出的一个模型的一个值，就跟我们看到大模型有一个Y输出的最终的输出之一，但是我们知道整个大元模型里面充满了矩阵相乘，那每一个小的模块都可以被抽象出右边这个罗拉这幅图的一个结构。然后原来的这个大元模型的微调是一个什么样的数学形式化的定义呢？其实是我们看到H，就是我们新的这个H它如果要去适应我们的下游的任务，通常我们是需要去做微调。
	微调调的是什么？我们可以看到W0乘上X加上德耳塔W乘上X那么W0你可以认为是我们原来的预训练的权重，我们就以右边这幅图的这个蓝色的方块，可能就是原来的W0。那么这个德耳塔W就是我们通过通过我们下游任务特定任务的一个训练集。所对原来的这个大的矩阵，这个地层地，这个高维空间。因为它没有去动过空间的维度，所以就是在原来这个空间里面调整了一定的模型权重，就是这个delta w这个是我们之前讲过的那一大类PFT的一个核心思想。那么lora它的核心思想是说，我把这个等式做了一个变换，我把这个delta w从一个地层低维的高维空间变成了一个B层A乘X这么一个很取巧的方式来实现同样的值，就德耳塔W乘X同样的值。
	我的X怎么就能跟BA这两个小矩阵相乘达到delta w跟X，这个是我们Laura后面要解决的一个核心的问题。那怎么怎么做的呢？我们可以看到，第一这是两个小模型，他们在做矩阵相乘，BA其实约等于这个delta w而且我们看这个图也能看得出来，BA它的输入和输出的这个维度都是D就跟原来X要求的这个维度是一样的。那么就跟这里我们看到的这个矩阵A其实它是一个地层R这么一个矩阵，然后这个矩阵B其实是一个R乘D，当然如果我们H就我们下游的这个输出，它的要求不是一个R型的或者说维度为R那你也可以去做调整，在B这个矩阵这去做调整就好了。
	整体来看，首先我们保证A和B相乘之后的维度，就它输入输出相乘之后，这个维度跟原来的这个W保持一致。如果我们的原来的W是一个D乘K的矩阵，那我只需要把B变成一个跟原来的这个K一致的矩阵就好了。因为它最终乘起来是这样的一个递乘K，这个是比较简单的一个线性代数的一个相乘的一个基础。就是大家如果这个维度看不明白的话，可以稍微补一补。然后两个模型相乘之后，维度首先保持一致了，那至少可以乘下去。因为矩阵相乘这种向量相乘跟标量一个自然数相乘，最重要的是维度得一致，不然都没法相乘，那保证我们首先能运算了。
	第二步就提到了我们最关键的概念，低质分解，这个低值分解其实就是我们这儿看到的A矩阵，要将这个输入的一个D维矩阵降到一个R维。这个R维有一个或者说我们把整个A矩阵和B矩阵都叫是我们新增加的更新矩阵或者说增量矩阵。它是在原来的模型里没有，我新增加的这个矩阵，也就是我们这个delta w原来要被替代掉的这一部分这个增量矩阵。
	这个R其实是一个矩阵的质，就是我们rank在音频里面，它其实评价这个R的含义，它的数学含义就是如果有一个高维空间，这个高维空间它的表达是有冗余的那它一定可以通过降维的方式来降低它的维度，降到不能再降为止，那这个维度就是它的这个制，就是它的rank。然后既然有这个rank，也会有它的本身的向量。这个向量其实就是假设这个R是十维，那么就会有一个十维的一个方向向量，或者单位向量用来表达它是一个十维的一个空间，这个都是一些线性代数的基础，然后通常这个R也会远小于D因为R的出现使得它的计算量降低了，模型的参数量也直接降低了。我们就从右边这个图的几何上面来看，都能看出来一个A和B的这个维度肯定是比这个远程原来的这个W要小得多的。那这个小的这个部分，本身它的模型也变小了，因为B乘A的这个矩阵要小的多，然后如果把R取D的10分之1或者1%，这个是一个小了很多个数量级的一个比例。这也是为什么使用柔软来进行模型微调的时候，你要调的模型参数量可能只有原来的整个大语言模型的千分之几，就是这样来的。所以通过这个低质分解，你可以减低模型的参数量和它的计算量。同时我们要怎么样把这个B和A又变回到data w就是刚才讲到的这个B矩阵。
	它会把这个R维的数据，就是一个低维的数据又映射回原来的这个地位，或者说你要求的其他的维度，比如说K位，那也可以映射回去，这个完全取决于你的整个神经网络里面，X到H就是通过这个神经网络模块之后，你的维度有没有做变化，这些都是可以去做调整的。那么通过这样的一些低质适配的方式，我们会发现第一整个计算资源降低了，就是你需要用的算力降低了。同时你需要用来存储模型的显存，因为它参数量降低了也变小了。
	还有一个很有意思的事情是说，其实你通过BA这个矩阵在模拟的是这个德耳塔W也就是在模拟这个W模拟整个大语言模型。我们其实整个lora的方法是想说，能不能用两个小矩阵去模拟出一个更大的模型，这个其实是有一定的数学依据可以参考的。就像我们刚刚在讲，整个就算从数学的角度来看，不是每一个高维空间它都是很很都是有一定冗余的。这么讲，我们大语言模型也不例外。但是大语言模型它要把参数做这么大也是有它的道理的。因为他要学习的知识类别太多了，所以如果他一开始就把这个模型参数的维度搞得非常低的话，其实这是一个很经典的问题。
	就是我们在机器学习，包括TN sor flow的这个视频预览课里都跟大家讲过过拟合和欠拟合的问题。如果一开始我的模型过于小，而我的数据量要训练的数据比较大的时候，会容易出现一个什么问题呢？那就是欠拟合。因为你的模型复杂度太低了，不足以支撑你去拟合这个庞大的训练集里面的这个数据分布。但是如果你把你的模型搞得非常大，甚至超超过我要你和这个数据分布的需求的时候，那显然你训练出来的这个模型有可能是需要去降维的。而这个降维并不太会影响太多模型的性能。这个是罗拉的一个出发点和它的一个灵感。通过他的实验结果我们也能看得出来，确实如此。
	尤其是当我们的预训练语言模型是在训练知识，就学习各种各样的知识，然后把这个知识记在模型权重点。然后na这种微调的技术，或者说其他的PFT的微调技术，都是说你只需要去关注下游的特定任务。那下游的特定任务原本就不需要用预训练模型里面的所有知识。所以你去降维，然后把一些这个任务不需要的指示给他，你可以说是删掉也好，或者说移除掉也好，对他的影响也不大，这就像我们都说，尤其这两天大家如果看twitter的话，这个elon musk就发了一个twitter。
	就是讲你学习的数学，或者说你学到的知识。小学的时候是A加B，然后可能中学的时候变成了比较复杂的几何运算，到大学的时候可能就变成了现在我们这里看到的矩阵相乘，或者说求这个积分，求微积分。最后你的工作可能就是一个excel的数学公式，这个就相当于有大部分的人其实他用不到这些数学的知识点，但总有人会用到。就像他的twitter有一个很有名的人去做了回复，就是图灵奖的获得者这个乐坤，就是CNN支付，或者叫乐坤就回复这个。是的，可能你的工作是这样，但我需要这个数学的知识，也就是一个意思。每个人你的成长环境，你的这个学习过程是学了很多东西的，但不是你学到的所有东西。
	针对你的一个特定下游任务都是需要的。那么这个时候用na来代替或者说来替代你这个大模型，然后只关注你下游任务的这一部分参数是可行的。这个是lora的一个核心逻辑。
	那么我们再看lord a相比刚刚这两个方法有什么优势第一个就是说跟adapter相比，我们都知道这个Laura它在推理阶段，它它因为已经替换掉了整个W了，所以它其实是不需要再去计算原来的这个预训练模型这一部分的这个推理的。就是你可以理解成那个transformer。以前的adapter需要把transformer算一遍，再把自己的adapter模块，因为它是加嵌入到这个transformer原来的这个模块里的，所以还需要额外增加一些计算量。但是Laura不会，因为Laura本来就把大模型变成两个小模，然后两个小模型总总的这个参数量少，然后它乘起来的这个计算量也少。所以这种替换其实是更快的，就不会带来更多的计算量，也不会带来延时，反而会更快。
	同时因为它在模拟整个矩阵，所以它有可能或者说从原理上来说，它其实跟全参数微调是类似的。这个逻辑就是如果你进行全参数微调，你就算把整个模型加载进来，你针对这个特定下游任务去微调的时候。大部分跟这个任务无关的那些参数，你其实是没有必要加载进来的。但因为你用的是全参数微调的技术，它也得加载进来你才能调。而罗马是说，我就一开始就搞两个小矩阵去模拟你的这个大据。那我就争取训练出来的就是那个最终降维之后，移除冗余之后留下来的这个矩阵。
	所以某种层面上也有人经常讲，Laura其实是在模拟全参数微调，是这个意思，所以就跟下面我解释的这个一样，对模型关键部分的一些低质调整，是能够实现全参数微调的一个模拟的。所以从这个视角来看，相对于lora来说，如果我们有一些对推理速度有要求，并且对于这个模型的性能有要求的一些场景，用lora是肯定靠谱的。就比如说我们最典型的这个纹身图的场景，大家都知道如果你自己部署过一个stable diffusion，或者说部署过类似的一些纹身图的模型，然后你要去生成一个相对来说，比如说512乘512或者1024乘1024的一个分辨率的图。在一个GPU，在T4的GPU或者说类似的GPU上面，可能都是要5到10秒钟才能生成。但是这已经是使用了一些微调的技术之后的小模型。但如果用原来的早期的模型可能就会更慢。这就是因为本身它不需要再去算那么多的大模型的推理的矩阵相层的计算量了。它就剩两个小模型，小模型相乘就够了。
	那么相比soft prompt这一类方法，lora有什么优势呢？第一就是他其实是做了一些更深层次的模型修改。我们都知道soft prompts有好几类不同的方法，有的是像prom too I，他最早提出来，他可能是直接在外部都不会去改这个transformer本身而prefix它可能是去在transformer的一些不同层增加了一些新的mlp就是新增加了一些神经网网络的模块。在原来的这个layer的前面，像这个P20也是类似的。在embedding层增加了一些新的神经网络的参数。如果你是用的P20v two，可能还会有更深层的一些前缀的增加，这些东西也都是为了调整我们的整个端到端的模型，让它生成一些prompt的时候更好。但即使如此，我们都会发现一个问题就是他们为了能够把这个训练方法运行起来，他们会去冻结原来的整个transformer的这个参数，就是那个大模型的参数，然后它只会去调它嵌入进来的这部分新的参数。
	但是带来一个问题就是说就跟我们去理解这个raz NET1样。就是为什么一开始的CV的这些深度神经网络去微调的时候，可能就只微调最后一层，就最顶这一层，top layer这一层，这一层可能是一个全连接网络。然后我们看到后面大家也会去微调，这个后面几层的全连接都会去微调，然后还会有一些人去微调。可能除了这些全连接网络之后，再往前一层的这个卷积，CNN它也会去调。就是因为可能CN这一层学到的一些图像信息，在你的这个下游的特定的领域里面，它它的这个变化还挺大的那如果你只去修改这个全链阶层，可能就达不到你要的效果。
	类似的其实soft proms都有这一类问题。就是因为transformer的这一部分参数是不改不去改的，而这个na本身它其实就直接跳出这个坑，我就不用去动你的transformer。原来这个参数我就在训练过程当中，就已经把你的原来的这个大模型，这个高维矩阵给降维成一个小的模型。然后用这个小的模型去模拟，然后它的输出结果，只要我的loss设计的相对来说还比较好，就能够去模拟你的这个大模型输出的结果。其实这个方式是更加深层次的一个模型修改，而这个更深层次通过这个方式去理解，而不是说像这个P20V1和P20V2，在原来的transformer网络上面再去比较深的去做修改。
	当然这也是一条路线，就包括提出这个prefix tuning的这个团队，stanford这个团队也提出过叫做deep prom ti，大家有兴趣可以去看这篇paper。然后同时因为它本身不需要加前缀，它也不会牺牲这个输入空间，这个也是比较直接的。第三个就是说它直接作用于这个模型结构，然后这个要怎么理解？我们可能刚刚我们的描述都是说把整个大模型替换成一个小模型。但是这个地方就要着重跟大家讲，就是Laura它本身是一个矩阵层面上的一个替换，大家再回想一下刚刚一个大的W和两个小的AB，然后这个东西它是一个矩阵变换，而这个矩阵的变换是不等于整个大模型的。
	我们知道整个大模型它的形式化定义甚至都很难写出来。所以我们可以想一想，整个transformer里面它有哪些模块。有我们的q query，有我们的k key，还有我们的value。然后Q和K其实都是通过我们输入的这个X跟一个特定的矩阵WQ和WK乘出来的那我们刚刚说的这个特定的层其实就是指比如说这个WQ或者WK或者说最终直接输入的这个WV这些权重矩阵来进行调整，而不是整个被替换成。所以它其实整个你可以认为Laura的微调里面充满了很多的小B和小A去用一对一对的BA去替换对应的一个W而整个大元模型里面充满了W对吧？你的不同层或者说同一层不同的模块都有W。所以这个直接作用就是指这么一个意思，它仍然是有一个大语言模型的大的价值，就比如说你的这些非线性层，你的这些softmax肯定是不用变的，这个是lora的通用性的一个底层逻辑。
	就是它本身是在你原来的一个网络结构里面，有比较高维的矩阵被替换成两个小矩阵。这两个小矩阵上下游不需要被替换的这些部分它还保留着的。所以它才能通过这种方式去拟合出或者说模拟出你的那个大的W啊啊啊所以这也就是下面提到的灵活性和适应性，当然它最终也是在模拟全参数微调的一个效果，所以它相比于soft prompt这一种方法，它能够第一是更深入，第二就是不用占用一些额外的一些输入空间来修改模型，然后也能够提高一些提供一些更高的灵活性和适应性，包括我们看到这个hugin face的PFT这个库，也是我们后面会用的这个库。它就可以直接通过lora去加载它被替换掉的这个小B和小A而不去加载那个W大家去想象一下这个过程，其写代码是可以实现的。就是你在加载整个大语言模型权重的时候，本来这个地方应该加载一个大的W，你把它加载成了一个BA然后在实现他的前推理过程当中，也把这个公式给改过来就好了。其实这个是能够大大减少我们想要的这个GPU的一个显存。所以这种方式是目前来看通用性比较好，并且能够跨任务跨这个模型而去使用的一种高效微调的技术。
	那它的实验结果怎么样呢？我们可以看到这幅图里面其实是在两个不同的基准测试上面做了一个对比。左边是叫做viki circle，右边是这个marky mark NLI，然后下面是这个要训练的模型参数，其实你可以约等于你的这个训练的一个成本。因为你的模型要训练的参数越多，你需要的GPU显存越多，然后你需要的这个计算量越大，你的训练时间可能越长。
	然后我们看到如果是这样来理解右上角这个反应，two就非常好好知道为什么在这儿了。第一five它的这个模型训练参数肯定是最多的，它跟高效微调就完全不在一个数量级。这里我们能看到几乎有四个数量级的一个差距，然后这个纵轴是我们验证机上面的一个准确率，因为我们都是基于一个特定的下游任务去做微调，lauda其实跟我们的fine two几乎是一个准确确率，但是它需要使用的这个训练参数是非常少的。而lora相比于其他的我们看到的这个prefix，或者说这个adapter，以及这个prefix加上embedding这种复合技术比起来，它的训练参数其实并没有变多，但它的准确率非常高。所以它是一个相对来说这两者之间取得一个比较均衡的一个微调技术，同样的在另一个基准测试上也是如此。
	Laura本身我们刚刚有提到，除了刚刚的这个soft prompt和adapter技术以外，其实还有一些我们没有去讲的PFT的这个路线。但现在可能用的不多了。就比如说这里我们看到有一个叫做beat fit，这个其实是讲在选择性的，或者叫selective measures这一类的PFT的技术，然后包括这个beat fit，还有一些其他的一些这一类的技术。
	然后整个我们看到fine phone作为一个基准测试。要训练的模型的这个参数是125M，也就是1.25亿的训练的模型参数。而lora其实只有30万，但是他们取得的基准测试的结果，我们可以看到上面这一行87.6 87.5正-0.3，然后95.1 93.3，其实他只用了这个30万，跟1.25亿的这个训练的参数总量比起来效果是差不多的。甚至在一些特定的基准测试集上面是超过了这个反应2，然后类似的一些不同的大语言模型，不同规模base large或者超大的都能看出来roa的效果。它几乎是使用1‰，或者说这个2‰的训练总量参数的训练总量达到这个find two的一个训练效果。那你的训练成本其实就是原来的这个1‰到2‰的一个训练成本，这个是非常夸张的。
	然后类似的在同样的一个模型GPT3上面，因为这个GPT3本身是一个相对来说很大的一个模型，1751的一个大语言模型。在这个GPT3上面，我们对比了lara和刚刚提到的这几类技术，我们可以看得到其实lora是非常好的一个表现，并且当我们把这个训练的参数总量提到了十倍，它的训练结果也是有一定提升的。但这就埋下了一些坑，就是最最开始微软提出来这个roa非常好，并且它的设计也很巧妙，但它也带来了一些这个坑。这个坑其实从这个训练结果里面也能看出一些眉目。就比如说GPT3使用loa，然后它的训练参数量变成了原来的将近十倍的时候，在这个MNLIM这么一个基准测试上甚至还掉了0.1，这个是为什么？我们待会儿会在ADALA的时候去看看。好，那么在GPT2上面也是一样，柔软也取得了非常好的一个效果。
	然后接着在微软提出这篇论文的时候，他也自己意识到了，就我们刚刚在讲的这个过程当中，我们看到GT3的这个大元模型使用Laura之后，它的这个训练的参数总量有一个十倍的提升。但是在特定的测试集上面甚至还表现更差了。这个是为什么？这个就涉及到lora这个技术，它到底会有哪些超参数需要调整。这里我重点拎出来来讲，就是因为整个其实后面对于ra的演进也是这个维度上面的一些演进。
	第一就是说我们看到这幅图里面是两个训练结果，就是在微软的这篇paper里。好，然后我们结合这个transformer的market attention这个结构来跟大家讲一讲到底问题出在哪儿。我们首先看到这个标题也很写的很清楚，就是我们的权重矩阵的种类和这个质，就我们的本真质这个r rank，他们的这个选择对于训练效果其实是有很大的影响的。怎么看这个实验结果的图呢？我因为我后来发现好像不细讲这个实验结果，大家不一定整的明白，所以今天我稍微讲细一点，看大家对这个讲细点是希望这个讲细节，还觉得讲细讲的太细会耽误时间，大家待会儿也可以评论一下。
	我们可以看到每一列其实是不同的，这个你可以认为是不同的训练超参数得到的一些结果。那么我们看到上面这里有WQKVO这四个孤立的权重矩阵。如果只去对他们进行Laura的微调得到的一个结果，然后当然这个rank r本身也是一个超参数，你可以选择把它降到这个D层8，然后也可以选择降到D乘2，两维的都可以。然后WQKWQV以及WQKVO4个重要的矩阵，这四个都是很大的矩阵。然后这四个矩阵你的这个矩阵一起去做这个Laura也是那是一个组合，他们其实可以看到不同的一个训练结果。
	从首先从这个最终结果来看孤立的去调这个R，未必是有很好的一个结果。然后就这下面的这个R1、R2、R3、R4，其实是取得的这个效果变化不大的。我们看到下面这一列，R等于一、R等于2、R等于4、R等于8、等于64。然后横着看，这个就是指我的权重矩阵的种类。的选择不变，单纯去调整R那么这个变化其实不大的。所以我们要知道为什么刚刚提到10倍的训练参数反而还下降了。你要知道十倍的训练参数对于loa这种微调技术来说，其实核心就是那个R的一个调整。你去想象一下这个lora的重要的训练参数在哪儿？
	其实就是那个B和A然后B和AD是不变的，R是可变的那当R的这个数量变成十倍之后，自然它的训练参数总量肯定就会有一个巨大的增长。当然不一定是1比1比1的关系。所以为什么刚刚的这个GT3上的实验结果是那样的，就是因为R本身在你的wait type就权重矩阵不变的情况下，单独去调整R不一定有什么好的效果。
	当然R有它最佳它的一个值，就像我们任何的一个曲线，它有一个全局最优值，或者相对来说比较大的一个局部最优值。那么R一定有它的最优值，并且我们还会观察到不同的weight type的组合，哪怕是在一个相同的基准测试上，不同的组合它的最佳的R也是不一样的，就是它最好的这个R也是不一样的。然后我们比如说这里的这个v ki circle，我们看得到它这里的最佳值是74.1，这个74.1是WQKVO4个都一起进行lora的微调之后，R只选了一，它反而取得了一个最佳值。换句话说其实它的模型参数反而是最小的，它要训练的。然后类似的在另一个基准测试mart NLI上面也有一些一样的表现。就是RR本身对于我们的这个下游任务，或者说对于我们lura最终出来的这个模型的效率性能，其实影响的这个点没有那么大。
	反而这个wait type怎么去选很重要，这里我们再引申讲一讲这个wait type。我们可以看得到整个最核心的transformer这个网络结构，其实就是右边这幅图，大家如果有这个不是很熟悉的话，回去再看一看这个第二节课，应该是第二节课讲过这个transformer。然后很多同学也在问这个QKV到底是什么？我这儿再花点时间给大家讲一讲。很多同学没有听听得特别整明白。首先这个QKV它不是我们直接输入的这个X那就我们Y等于WX，对吧？那么它不是我们直接输入的这个X它是什么呢？首先他们都来自于X，大家如果手上有纸笔的话，可以去写一写。
	或者再回想一下我们讲这个attention QKV的这个公式的时候，这个scale dot product attention中间这个紫色的方块，他是怎么样处理出这个QQV的。其实是把我们的这个q query和我们的key的转置相乘。相乘之后，其实是为了得到一个什么东西呢？得到的一个东西是Q和K之间的一个关联度，或者说相关性相似度。这也是为什么我们最早讲attention mechanical的时候，有讲这个sequence to sequence这个网络去运用注意力机制的时候，捕捉的是输入的这个长城变量H和输出的隐藏层变量S之间的一个相关性。这个相关性由attention with来表达，那么这个self attention它没有这个encoder decoder两个神经网络的隐藏层变量，它就是自己，它就输入的X所以这个时候它的Q和K其实都是由X变化而来的，只不过这个Q和K他要捕捉的是什么？捕捉的是这个query这一段自然语言的query里面什么key，就一些key words也好，key token也好，这个key这这个query里面哪些key是最关键的。因为它跟整个query之间是有一个自注意力，就是哪些key是这句话里面的不是废话，是重点，哪些是废话，他的注意力权重值就低一点。
	就像我们看transformer这篇paper，他的这个实验结果里面有写。比如说这个意思，这个单词到底是指的application，还是指的是law。然后他跟the可能关系比较大一点，因为它那个颜色比较深，那个就是他的注意力权重值，它跟另一些介词，比如说什么to之类的，它的这个关系就浅，因为那个就不重要。
	这个Q和K都是通过X跟一个对应的WQ和WK做了这个矩阵变换之后得到的这个向量，然后Q和K相乘之后，再除以了一个根号下面的DK我不知道大家还有没有印象，这个公式是非常经典的一个soft max的一个公式。这个公式就是为了把Q和K乘出来的这个mask矩阵再规划到一个0到1之间，然后最后再乘上这个V，这个V还是X，不过它乘的是WV的矩阵，目的是为了保留X自己，就相当于V就更约等于这个输入的X而Q和K他们造出这两个向量，这个transformer造出这两个向量就是为了捕捉。这个输入的X里面，这个核心的key和这个query之间的哪些有关系，然后他们放在一起叫做attention的QKV。刚刚说的这个公式，就是这个中间的紫色的部分，然后整个transformer的经典的结构里面有H个不一样的scale的dot product attention，就是并列在一起的。最后在contract拼接在一起，拼接在一起之后通过一个变换之后变成了WO所以WO某种层面上比QKV的这个矩阵还大。就是我们这看到有一个WO，它是原来的这个论文截图里没有，我这边做了一个简单的图例的注释，那么最终输出的这个WO会变得更大。
	从这个视角大家就能想象一下，如果transformer层次比较深，你第一层这个H比如说叠了六个或者说十个，那么这个矩阵的这个维度就变成很大了。然后你的这个transformer层又从十几层变到了快100层，那么整个叠加是一个非常夸张的一个增长速度。这个矩阵。所以大语言模型需要消耗这么多的显卡，它是在传输功能这一层消耗了绝大部分的算量。我记得这个也是某一个同学提出来的问题。
	好，然后这个就是在罗尔提出这篇paper的时候就已经提过的一个问题，就是我们的权重的种类的选择排列组合。然后我们看到这只是一个transformer的一个小模块这只是一个encoder的一个小模块。然后一个transformer一层transformer就有六个这样的模块，甚至在decoder里面也有这样的模块。每一个都有自己的W每个W其实都可能对它的最终结构有影响。这个时候就很玄学了。他虽然不需要像soft prompt那样去担心我要怎么样去造一些prompt，比如说我们的prefix tuning，他需要去用按照p tuning这篇paper的讲法叫prompt这个generator去手工生成一些prints去适应下游任务。它没有这个烦，但是他有的烦恼是说这堆wait type和rank r到底要怎么去选，这些超参数要怎么样去做，其实是他的烦恼。
	类似的我们还可以看到在其他的一些测试集，比如说这个BLEU list，meta不同的测试集上面，同样的一个GPT2，或者说这个呃GPT2的medium，就中中间这一档的GPT two这个模型，它用不同的rank也能取得一些不错的结果。而且比较容易看到的是，rank取从16到512的时候，lost都最终掉到了这个位置。所以整个rank这个超参数它是未来有很多的可优化空间的。简单来说就是你不用那么大的rank，不用那么高的训练成本，就能取得一个不错的成果。有一个最舒服的ROI最高的一个rank，需要去把需要去被找出来。这个是Laura提出来的时候，他就已经留下的一些伏笔，也是未来大家可以去努力工作的一个方向，所以刚刚我们把Laura讲了一下，我不知道大家关于lora现在有没有问题，没有的话我们就直接讲这个ADA的这个Laura。这个是ICLR2023收录的这个会议论文，也是微软和交加理工一起做的一篇paper，沿着原来的这个思路，当然还有普林斯顿，大家关于罗拉有什么问题？
	看大家有什么新问题，因为有可能之前问的问题现在已经听明白了。
	那就先往后讲，我先往后讲，我们这样可能大家能赶紧把这个一起听完，可能会感觉好一点。我们来接着看啊，整个Laura的核心思想是什么？就是针对性的，就是对下游的各种任务针对性的去增量训练一个小模型，然后这个小模型的简写其实就是这个新的W，它等于这个W0，就原来的这个预训练的模型权重加上这个delta w而这个delta w其实就是我们要去训练的这个lora的小模型。
	那lora的问题也很突出，刚刚我们也讲了，第一就是它的这个超参数，现在我们可以把它理解为超参数了。因为na本身这个微调的方法也可以被作为一种learning的方法。然后这个超参数的这个增量矩阵，就新训练的这个增量矩阵这个R，它是没法自适应去调整的，它是我们在一开始训练lora的时候就需要去设置的一个值。然后第二就是在罗拉的21年的论文里面，他其实本来想的是降维。用小矩阵模拟这个大矩阵在特定任务上面的一个表现。但是它其实低估了这个权重矩阵的种类和不同层，那就相当于不同的层有不同的权重的矩阵。他们的选择本身对我们的微调结果可能影响更大。
	还不只是这个R的大小，然后在原来的那个paper里面，还有一个问题是说，我们也看到了它微调的是这个W的这个QKV，以及我们最终输出一个O，但是他没有去微调fit forward的这个模块，我们知道transformer那个结构里面最重要的就是一个my head attention接了一个free forward network，然后后面又去接了一个这个mart head，又接了这个fit forward adapter。就是说不去动这个matter attention，直接在原来的fit forward的后面增加了一个模块叫adapter。而这个lora一开始是没有去微调FFN这个模块的那也就是前馈网络。
	所以ADA罗A提出了一个针对性的解决方案，来解决原来的这个Laura论文里面没有解决好的这些问题。第一个就是既然我们去降维，用这个BA去替代这个矩阵，那么要去替代它，那怎么样去替代呢？就是怎么样去做降维更好？其实本质是这个问题，就怎么样找出一个BA更好。这个时候有很多经典的方案，在经典的这个机器学习时代就有一个叫做SVD的期值分解的方法，这个待会儿我们也会简单提一提。并且ADA罗A的核心其实就是把这个SVD用到了极致，怎么样用SVD去提升这个矩阵低质分解的性能，第二就是说可以对模型进行减值。
	其实我们本身整个这个过程，罗拉的过程就是想降维，其实减脂的含义就是这个大语言模型里面不是每个参数都都有用的，然后我其实只需要把那些最有用的参数用起来，那些不相关的参数我甚至可以去给它干掉，或者不用它。那怎么样找出那些最有用的参数，其实这个过程是对一个巨大的千亿级的模型参数去建模，就相当于我们把这些模型参数，每一个单独的参数都当成我们的一个需要去建模的对象。每一个参数它都有自己的一个重要性，我们要对这个重要性进行评分。这个其实是ADA罗A里面做了很重要的一个事情，并且这个也是之前这个团队的一些成果的一个进一步的进一个扩展。
	第三个就是说既然R它没有办法自适应的调整，同时我们也不能拍脑袋的去决定在这个任务和这样的一个特定的训练集上哪个R好。那能不能让他这个动态的自适应的去调整这个R，这个其实是ADA这篇lora非常重要的一个贡献，也是现在很新的一个研究成果，在SLR的这个会议上面，我们再看一下这个图这幅图里面其实有提到这个我们我把这个稍微调整一下。我们可以看到左边这幅图是最早的我们看到的adapter的tony的一个截图。然后adapter fly，我们再回想一下整个adapter的模块核心是什么？这个核心其实就在我们刚刚看到的左边，我们又再加那个红框了。
	我用鼠标这个Martin head attention，这个部分我们用了罗A然后ada跟他的不同是说这些灰色的muti head attention和fed forward layer我都不去做调整，我只调整这里的adapter layer，就是在fit for world后面，包括这里的第二个fit ball ard的后面增加的这个dep的模块，这个是我们的经典的adapt I的一个网络结构。并且它内部的结构也很像我们。包括像Laura，包括像其他的一些soft pro prompt的一些技术，都会去用到这么一个降维，就是down project和up project。然后这里是一个小的网络，然后通过这个方式，以比较低的成本去训练一个下游任务的小模型。
	从这个图里面其实是我们的ADA lura里面的一个截图，右边它选择了不同的矩阵，就是我们开始提到的所谓的权重矩阵，然后也选了一些不同的层，就选择不同的layer来进行微调，得到的结果不一样。这里的WQKVO我们应该比较熟悉了，然后还有两个是F1和F2，就是我们要放在这儿的一个原因。这个F1和F2就是指的我们经典的transformer这个结构里面的feed forward。然后F1是指的前面的这个前馈网络，F2是指的第二个这个前馈网络里面对应的这个W它的权重。
	所以在ADA罗A这篇文章里面，它就回应了之前的一些问题。就是我其实不只是可以调attention，我甚至要去调这个feed forward。并且从实现结果来看，调整feed forward更重要。我们看到通过调整这个feed for feed forward里面的权重举证，它取得的效果更好。在MNLI这样的一个基准测试上，然后我去调整不同层，其实结果也会有不同。这个是ADA lara当时最直观的一个结果，就是不只是要调整注意力，也要调整这个前后神经网络，甚至前后神经网络的结果更好，包括后面还有很多在并行的去做adapt的一些方法，其实就是类似的，待会儿我们讲到这儿再说。
	那就来看这个ADLA的一个具体实现，就是它它这玩意儿怎么运行的，最核心的其实是这个期值分解，这个期值分解，如果这个有基础的同学应该一看就明白。如果咱们之前没有接触过的，我简单跟大家解释解释一下它是怎么样运行的。它的这个逻辑简单来说，第一，它是在很早以前，在数学和信号处理里面就会用到的一种矩阵分解技术。这个矩阵分解技术其实就跟右边我们看到这个图一样他要干什么事情呢？它其实就是能它它是一种技术。然后你给我一个任意的一个M乘M乘N的一个矩阵，比如说这里的一个矩阵A任意的一个矩阵A它可以有一个SVD的一个分解。这个分解会把这个大矩阵变成三个矩阵相乘的结果，其中这个有两个矩阵叫做U和V是正交矩阵，然后这两个矩阵是正交的，然后中间这个是它的一个对角矩阵，然后对角矩阵上面有一堆的这个值，对角线上面这些值就是它的奇异值。也是我们整个这个ADA罗A里面会见到的lamda很重要的一个本增值。
	然后简单来说，通过SVD它是一个固定的一个算法，能够把一个特定的任意的，或者说你在这个模型里面是一个特定的W，对于数学上来说，它就是一个任意的W任意的一个高维矩阵，它可以把它去做一个分解，然后得出来对应的UV以及这个对角矩阵，也就得到了里面的这个特征值。然后它的应用领域，其实也非常的广泛，在一些数据科学和机器学习的领域里面，通常是用来降维的，然后也可以去做一些噪声过滤、数据压缩。在信号处理里面。在早期的自然语言的各种各样的算法里面，通常是用来提取这个语义结构的。所以这里我们也能看得到，本身我们的这个W这个权重矩阵里面就有一些语义信息，用SVD来提取其实是比较自然的一个结果。那这个过程其实就变成了，这是我们刚刚看到的这个Laura的一个结构这个结构就发生了变化，我们会围绕着这个变化来仔细讲到底是怎么在玩这个ADA的。
	罗软了，我们看到跟上面这个SVD的这个分解，我们做了一个对应P就是指当然这个我找不到跟它完美的这个数学公式上一样的图，大家有兴趣可以补一个。就这个左期向量和右期向量，其实就是这里的P和Q，对应着上面的UV这个对角矩阵，就这里的这个对角矩阵，我们本来是有一个BAX。为什么有一个BAX？因为BAX是想去模拟这个变化的delta w delta WX但这个delta其实也可以被我们用另一种方式去做分解，就是用SVD的方式去做分解。原来这个德耳塔W是需要BA相乘来做拟合的那现在把这个德耳塔W用SVD来做拟合，那么就只需要一个SVD的一个经典结构，就是对角矩阵。对角矩阵加上左右的七相量相乘，就可以变成这样的一个形式，这个形式很好。
	并且ADA Laura这篇paper里面，在原来的lora的目标函数里面还新增加了一个正则项，一个regnier ation的一个惩罚项。这个惩罚项目的输入，或者说他的整个新增加的政策项，为了确保什么呢？其实就确保我们的七向量这个P和Q它是正交的。因为它如果是正交的，这个其其值的分解过程，SVD的这个分解过程计算就会相对来说比较稳定，不会带来一些额外的不必要的分解。因为SVD的这个分解要求就是我们的U和V是正交的。
	那么如果我们在训练过程当中，然后大家都知道这个反向传播的过程当中，你会去更新这些参数。如果P和Q没有确保它增加性，那有些结果就是没有用的。所以新增加了一个RPQ的一个正则项，是下面这个方式去做的计算，我这儿就不再展开了。
	好，我们接着再看到底是怎么玩的对吧？就是我们知道矩阵可以这么分解了，从原来的B和A变成了三个矩阵，一个P一个Q还有一个对角矩阵。然后我们知道这么分解了，但是怎么去选这些W呢？到底哪些W要被我们拿来做分解？这个其实是第二个问题。并且怎么去选他们也很重要。因为W太多了。对于一个一千亿参数的大语言模型来说，就这个W的矩阵可能都非常多。这里就引入了一个很有趣的一个作者的设想。
	首先我们看这个整体性的公式，这个整体性的公式是STWIJ等于ITWIJ乘上UTWIJ，这个公式看着挺简单的。第一我们核心是要对每个模块的参数，或者说它的特征重要性去进行评分。那我们就得有一个分数。那么对于WIJ这样的一个模块参数或者说这个举证来说，我们对它的这个重要性放在SWIJ这样一个分数里面。你可以理解成SWIJ就是一个用来评估任意的一个权重的重要性评分的一个方法。
	那它这个S是怎么来的？通过两个不同的方法相乘得来的。I叫做一个参数的敏感性，U叫做参数的不确定性。这样理解也很好理解。
	然后那么I和U具体怎么计算？对于一个任意的参数，我们都可以去算它的敏感性，怎么算呢？这个是在这篇ADA loa里面，我们的这个作者新提出来的一个观点。这个敏感性就WIJ就是它的输入，这个敏感性计算的输入。然后通过我们的这个WIJ本身它自身和它的梯度进行相乘，然后这个绝对值能得到一个敏感性的一个值，就是它的一个敏感性的平衡。我们先不去管这个T，那么这个就能得到它的敏感性了。
	对于WIJ来说，除了敏感性以外，其实这个地方应该是打错了，这个应该是它的不确定性建模。就是UT这里就是除了它的敏感性以外，它还需要对它的不确定性进行建模。因为这两个值乘起来才会等于这个模型权重重不重要。就相当于我们在训练过程当中，我们要时刻的去知道哪个模型权重是重要的，那么通过S可以得知，那么I是这么去算的那U怎么样去算的呢？其实这个U不确定性是更早时期ADA lara这个团队的一篇论文，所以他就直接引用了这个计算方法。简单来说就是我们整个训练过程，我不知道大家对于这个模型训练有多了解。
	整个训练过程是把模型所有的权重，假设我们这里需要把所有的权重加载到GPU里面，加载到GPU里面之后，我们需要把需要训练的数据一批一批的，因为整个训练数据太大了。会分成一批一批的数据。每一个batch的数据都会把它整合到一起，丢到丢过来进行一次推理。这个推理结束之后得到了一个输出值。这个输出值经过跟这个相对的，你可以叫标签纸，或者说其他的这个可以用来做对比的，做度量函数输入的这个值做了一个差，然后再反向传播去更新它。你每一批其实都是有顺序的，有序列的。
	这个T就是指每一步训练的部署，然后大家都知道这个训练过程当中，我们不一定是非常平整的。所以这个时候会用很经典的滑动平均的一些技术去用于什么呢？用于训练这一步和下一步之间的一些异常值，或者说不同的采样带来的一些问题。所以简单来看，这其实就是一个对于敏感性和对于我们的不确定性，都是在不断的把训练数据喂进来的过程当中去进行迭代，然后这里的I和这里的I是同样的一个I然后这里的T减一是指上一个时刻，就上一个时刻我们会有一个W然后再或者说上一步上一批数据我们会有一个W这一批数据也会有一个W然后上一批数据我们用这个贝塔一，这一批，用一减贝塔一，这个就是一个典型的滑动平均。
	然后这个贝塔一本身也是一个超参数，那对于我们的重要不确定性，它会有一个beta 2，那这个也是它自己的一个超参数。那通过这个方式你可以想象一下这个过程。就是每一批数据加载进来，你都可以对于每一个WIJ去计算出它的敏感性和它的不确定性。这两个乘起来也就得到了它的一个重要性，其实就这么一个过程。
	这个具体展开就要回到刚刚我们说的SVD分解了。因为我们知道这个SVD把我们的这个大元模型的微调分解成了W加上我们的P乘上这个或者我们的这个左期向量，乘上我们的对角矩阵，乘上我们的右期向量。那么这里的这个na和这里的PQ其实就对应着我们刚刚提到的这个奇异值，左奇异向量和右奇异向量。因为具体去做相乘的时候，你不会去拿整个矩阵，你会去做对应的这个相乘。就像我们说不同维度斗到一起才能相乘。然后我们这里把它总结提炼一下，就是到底我们需要对哪些关键的参数去进行关注。
	其实就是什么呢？就是这里我红框框出来的，也就是SVD分解出来的三个重要的矩阵，当然每一个不同的W都对应着这样的一个三元组，也就是它的左期向量，它的奇异值和它的右边向量。ADA罗A的核心，或者说它整个这套理念，它的技术手段，就是用这个SVD奇值分解的这个三元组，去替代原来Laura的这个BA二元组。然后这个部分其实是一个很重要的一个迭代过程。我不知道到这儿大家应该没有听不明白的。有的同学问怎么理解这个敏感性，怎么理解不确定性？其实第一在论文当中对于不确定性没有进行过多的介绍，主要是引用了他自己的一篇文章。
	然后对于敏感性的你可以理解成就是因为我们有不同的下游任务，下游任务都有特定的训练题。然后这个训练题它就是你可以理解成这个训练集，它是一个特定的特定的你需要你自己的特定的某一个模型权重，就是你原来的这个大模型模型权重去做更改，才能更适应他的这个任务。然后我们就得知道，既然都要更改了，那我们就得选一个最有用的，改完之后对他这块更有用的一个权重值去做调整。这个敏感性其实就是针对于这一批训练数据它敏感，所以你看这个公式就能看得出来，就我不同的数据丢过来，如果我的这个值比较大算出来，敏感性的值比较大，那自然它就会被更大的被你可以认为它的权重也会被放大。然后如果它本身这个W无论怎么变，在它的前向推理过程当中都不会有影响的话，那他这个自然也就不会被迭代到。
	重要性高的参数和低的后续的处理有什么区别，这个问的挺好的，那我们就接着往后看啊。刚刚我们知道重要性评分核心其实关注的是我们的期值分解的三元组，这个其值分解的三元组。并且在新的这个目标函数里面增加了它的这个惩罚项，这个正则项RPQ，有我们在前一页前一页看过的这个RPQP和Q是它的左期向量和右期向量，然后会保证这个训练过程尽可能它是正交的，如果不是正交就没有意义了。
	好，那么这个目标函数也去做了一个对应的更新，这个公式可能看的比较多，大家适应一下，我尽可能讲的明白一点。然后目标函数在原来的这个损失函数之上增加了这么一个新的正则项。然后这个新的正则项就会涉及到我们最终其实要训练，我们在训练什么？我们其实在训练的就是这个PQ和这个拉姆达，这是我们真正在训练的东西，也就是这个重要性评分的三元组SKI，还有这个p number所组成，核心主要就是他们。那么我们要训练这些参数，才能让我们最终的这个模型在特定的任务上表现的好啊。所以大家把这个文案给转过来，之前是要训练B和A现在是要训练这个PQ和nmda，然后目标函数就长这样了。那那我们这个目标函数有了训练的过程，就是把一批一批的数据加载进来，然后目标函数输出一个值，然后再去做这个迭代。
	这个其实就是我们迭代的过程，就是我们的第七步，到底要怎么样去更新这个参数，那怎么样去更新这个参数呢？就我们这看到的这个loss值，他会再乘上一个学习率learning rate，这个应该是很经典的，优化器都会去看到的一个值。就是我们在做反向传播的时候，学习率乘上这一次的这个loss，然后再求一个梯度，然后去更新整个模型。它抽象的表达了整个模型。但是这个过程当中我们就会发现，我们刚刚求重要性有一个很重要的原因是想说能不能把把这个模型的一些最关键的模型参数找出来。有一些跟这个任务不相关的模型参数我就不要了，这个其实是我们去做重要性评分的初衷，那怎么样去做呢？
	就涉及到这篇paper的，很重要的这个paper的名字，我不知道刚刚大家有没有关注过，我们回到这篇paper的名字叫什么？Uh叫做adaptive budget uh allocation for perimeter efficiency fine tuning，它是一个自适应的budget预算，对吧？就自适应调节自适应的去分配预算的，那这个预算到底是指什么？这个budget用的就很有意思，它是一个PFT的技术，并且是自适应的去调整预算。对于PFT来说，它到底在调整什么？
	我们终于讲到了这个budget，就是我们这里的这个参数这个B。就在第七步更新的时候，我们有一个小参数叫做B这个B是什么意思呢？其实这个B就是我们的budget，这个budget取了一个分段函数，在不同的训练阶段里面有不同的取值。然后在最开始的就是相当于这个训练刚开始的时候，第零步到第一步的时候，这个budget有一个初始值B0。然后在这个训练过程当中，就当他的这个TI5和他自己有一个超参数叫TF，这个中间的时候它是取得这么一个budget，加上有一个全局的做一个综合调整。然后后面的这个最后剩下的部分是一个稳定在一个状态里面，然后取一个局部最优。
	然后这个过程怎么样去理解，其实挺关键的，你可以理解成什么呢？我们已经在第七步参数更新的时候，我把这个鼠标弄上来。在TT步参数更新的时候，我们要去更新参数。但我们更新参数传统意义上是怎么更新的？就是我算出了很多的delta，然后这些delta都要减到原来对应的那个W上面。但是这里它的这个模型剪枝是怎么处理的？就是我算出来了很多的delta但是我通过这个预算的调整，我去取到底哪些模型参数需要被更新，哪些模型参数不需要被分析。我不知道这个大家理解吗？就是这里的这个取值，这个模型剪枝的核心逻辑就是如此。就是当我需要去更新的时候，我可以选择不更新它，也可以选择用我算出来的这一批数据的去乘上学习率之后的这个delta去更新它。
	然后我到底选择哪些要更新呢？通过我们的上面的这个SKIT，SKIT是什么？是我们的重要性评分，大家还记得吧？就这里重要性评分SKI这个三元组，这通过这个方式来做调整，就是我们去更新什么样的模型参数是由它来确定的。而这个SKIT本身它是更新的top BT的你可以理解成整个重要性里面可以画一道线，超过一些值之后我可以更新，不超过这个职能我就不更新。这个重要性我会去不断的迭代，因为这个SKI是在不断的迭代的，我是能够去对于每一个WIJ，每一个不同的W我都会在不断的去算出它的评分。所以这个地方会调整。同时在整个训练过程当中，我的budget还会去做调整。
	所以在最开始的时候，这个B0会设置的比较大一点，让模型能够快速的达到一个60分相对来说比较好的效果。接下来他通过一个这样的一个算是一个你可以认为是一个衰减过程，进行一个3次方的速度来逐渐减小，让它能够对整个你可以认为其实就是对整个大模型里面需要更新的模型参数在逐步的减值，然后最后锁定在一个值里面。所以你要理解就是一开始可能这个B0取得很大，我的这一道线就是我的重要性，可能重要性大于0，我就需要去更新。所以一开始的时候，所有的被SVD分解的这些W都在更新。然后到了一定步数之后，这个TI到了一定步数之后，他就开始逐步衰减了。我就只更新那些敏感的重要性高的模型权重了。然后到最后就稳定在一个更新频率上，就是我的这个rank。
	大家知道所谓的budget其实是这里有写这个约等于这个可训练的参数量。而可训练的参数量其实是约等于我们的这个rack。不知道大家转过来这个蜗牛，其实就这么个概念。整个Laura它在调整模型的参数，在训练模型。那么这些模型参数到底是怎么样被调的？其实就是这个R，这个R就是我们要去调的东西，R越大训练的参数量越多，这个大家理解。
	然后这里其实我们这个本身是已经变成了SVD里面的这个纳姆达其值的这个数量就对焦矩阵能拉到多大，这个对角矩阵拉的越大。我们再看到这幅图。这个对角矩阵弄得越大，其实你的这个模型也就要训练的这个ADA罗A也就越大。这个就是我们要去自己去找到一个特定的R然后我们还有提到一个点叫什么呢？就是每一个不同的W或者说每一个不同的测试基准，测试下游任务，它会有不同的下游任务。同一个下游任务不同的W它所需要的R都不一样。那么在这种情况下，你的R是不是还要能够自适应呢？
	那当然可以自适应，我们通过这一轮公式给大家逐步讲解，让大家明白其实这里的这个公式是针对每一个W都作用的，它作用于每一个W的那自然它整篇论文还有一个很好的设计，他有一个全局的总预算，全局的总预算还会去把控每一个W他的预算调整。也就是整体的大语言模型里面每一个小的期值分解出来。这些小矩阵在不断的在通过batch的训练数据去调整过程当中，优先的一开始的步骤是大家都训练，都可以去做这个更新。到后面经过一定部署之后，甚至有些W都不太会更新了，或者说有些W它可以有一小部分的参数得到了更新，大部分的参数可能都不太会更新了，这个是一个很有意思的一种自适应的方法，而这个整个思路其实是非常优秀的。然后我们也能看到它的一个结果，它的结果很很有趣。
	第一就是说在两个这stanford的两个很有名的基准测试上，ADA Laura其实是跟原来的这个Laura比起来，在参数量相同的情况下，就每一列参数量相同的情况下，就相对于原本的全量的微调，参数量只有它的8‱，然后这个千分之不到，2‰ 3‰，6‰比起来其实都有提升。这个ADA罗A然后在这个sport 2.0的这个基准测试上也是一样，都是明显优于之前的方法，当然比起普通的adapt方法肯定就更好了。然后同时我们还能看到在不同的参数规模上，比如说整个for的翻译处理是百分之百让他使用，1‰、2‰、1%、2%不同的其实就对应着不同的我们的这个rank，它的我们看到ADA罗A也都是明显好于之前的方法。只有在这个有一个具体的一个基准测试上低了0.03，其他的都是明显更好的。所以ADA罗A其实在某种层面上已经明显比这个，尤其是如果你不是一个对模型网络，对于你要微淘的这个模型网络比较熟的话，那使用ADA的LORA其实是可以让你一个更快速、更ROI更高的方式去达到一个还不错的训练效果。这个也是现在还跟face PFT上面支持它的一个很重要的原因。
	然后在相同的预算的情况下，比如说我们叫budget。用ADA lara的这个术语来看的话，在相同的预算我们能看到AD lora是明显要好于这个罗拉的，然后他的实验结果也能够说明这个情况。这里其实是另一个很有意思的比较，就是我们看到假设我们把这个SVD它其实是一个单独的优化技术，把我们的BA简单的这个BA的一个分解，变成了一个SVD的一个奇异值分解。它找这个rank找的更好。那么单独的这个SVD base的这个option，其实也是比普通Laura要好一些的，就说明SVD的这个引入是肯定有价值的。
	就我们这儿看到的在这幅图里面的SVD短横线lora是有价值的。然后它的正交的这个就我们看到那个RPQ加了一个正焦。这个加了正焦，其实本身是跟原来的比起来，也有一些轻微的提升。没有首先没有下降，就是有轻微的提升，然后同时本身加一个额外的RPQ这个政策项是带来了额外的一些计算量的。但是在论文里面其实他有去放，就加了这个额外的这个正则项之后，计算时间训练时间并没有一个很高的增长，还是一就跟原来时间差不多的那那个实验结构我没有放进来，但大家有兴趣可以去看那篇paper，ADA ora就加了这个新增加的正则项。
	其实SVD本身是有一些开销的，但是没有对他的训练时间造成什么影响。然后在他正常的推理阶段，也没有提升太多的时间，这个是我们想要讲的另一个点，就是十二层不同的layer和6个不同的类型的W它最好的或者说它最终收敛的这个rank其实是不一样的。这也是我们在录最原始的lauda里面有提出来的。不同的layer，不同的模块，它的最佳的rank取值是不同的，那么使用ADA laa至少能够保证它的每一个W因为它都是一个小模块。这里大家能感受到了，其实每一个这里的小方框，它都是我们看到的那幅图，然后它各自在自行的去训练和运转，通过ADA罗A我们可以支持他们是不同的rank。
	但你想象一下，如果你要手动去拍脑袋的给这么多不同的Laura的这个小模块去设置rank，几乎是不太现实的。所以知适应是能让它达到这个好处的。并且我们能发现它的WF1这一层其实它的rank还挺大的。然后下面这一层这个QQV其实到都是到后面相对比较深层次的这个偏top player的这一部分，才需要去做比较高质的分解。前面都是一些低质分解，就rank很低的，这也说明其实前面保留的这个参数并不多，重点是在一些后面的部分然后到这儿看看大家有没有什么问题，就是我们的ADA罗A大家有没有什么问题。
	总预算大约等于所有小的矩阵的rank的几何平均吗？应该不是平均应该是一个总加的值，然后这个总加不一定是线性加和可能是通过一些别的方式去算的那个总预算对。我理解是不是ADA na就是不同优化调整协议值的数量来优化需要更新的参数，直到根据模型剪枝后没有参数可以更新就达到最优的结果。是这样吗？
	我理解一下这个同学问的就是不同优化调整期值的数量来优化。首先ADLA是在不断的调整其值的数量的，这个是肯定的。所以它跟na不一样的点在于NO ra一开始指定了这个rank之后，其实它的BA矩阵的维度就不会变化了。而ADN ora其实在整个迭代过程当中，它一直在做其次分解。你可以理解成他一直在不断的去找很好的P和Q来适应它的。当然找出来的这个P和Q每每一个step不一样。甚至如果你的这个对角矩阵，你的其实对角矩阵维度变了，它也会不一样。那么这个过程当中肯定其值的数量在调整，然后这个对角矩阵的维度再调整，甚至他会因为有一个总预算的设置在这儿，我们放到这篇paper。
	就这篇配方，甚至因为它有一个总盘子，一个总的可以调整的量。所以有的部分它原来可能它的迭代很快速，它的top player可能一开始迭代的很快，到了这个一定阶段，他会把这一部分预算又放到之前没那么重要，但是之前因为重要性不高，给它迭代的量不大的一些参数身上。所以你可以理解它的自适应其实是全方位的一个自适应，既体现在不同权重矩阵，就不同层不同权重矩阵，甚至一个权重矩阵的SVD内部，都在进行这个知识性调整。
	这个总预算是一个超参数，这个总预算没法定需要经验值来调整了。但是他就只需要调一个了，就总预算这个超参数就只需要调一个了，然后你可以通过实验来做了，那他做这个实验的成本就简单多了，比你以前要调成千上万个超参数要好。Adada lora会不会更慢？因为要花时间一直在找最优的参数，几乎对于时间没有太大影响。我记得他当时有有一个实验结果是讲时间的，应该跟之前的Laura比起来，时间是最多增长了5%以内。它的时间消耗是以前的没有超过以前的这个没有多增加5%，就最多百分之几。
	Lora没有总的预算的概念，就是lora是需要你一开始去指定这个rank，这个超参数，然后ADA lora是有这个总预算的这个概念的。这个我们有机会会用PEFT来玩一玩的话，可以大家到时候来实际我们跟着这个实际的实战的代码来玩一玩。现在大家把这个逻辑理明白了，知道可以通过这种方式来优化了，是很重要的。
	总预算和每个小的W的rank是什么关系？你可以认为所有的小的可调整的量加在一起就是我们的这个参数量了。这个是在训练过程当中的一个表述。敏感性和正则化里面的公式。
	有点像敏感性。
	和正则化公式，都是在做的都是在做正则。对，我看一下你说的那个。
	这个同学是在问的一个敏感性和正则化，这应该还是不太一样。这个就是做了一个平滑，做了一个差值，就是这个敏感性建模这个部分算敏感性分数的时候，其实就是一个差值。你可以理解成就上一步在贝塔一下这一步占一减。贝塔一其实就做了一个线性差值，就是做了一个平滑。就上一步占多少，你占一点那么多，重要性也是，这是比较常见的一种处理手段对。我看还有没有问题。Rank在一直更新，同时rank对应的需要修改的对象参数也在变化。那么假设最终rank为R但是考虑中间过程更新的参数可能最后一步并没有更新，那么实际更新的参数个数是不是就大于R了？
	这个同学问的还挺好的。是这样的，首先你要理解每一步都在做HVD的分解，所以每一步替就是都有替步，对吧？每一步每一个batch的数据进来之后，你的每一个W都会去做一遍SVD分解，得到对应的P兰姆达和Q然后这个p nda和Q都会在内部算出一个loss，然后这个loss和他的学习率相乘是我们要更新的这个参数的data。那么我们要更新的时候会去选哪些参数需要更新？就是我们所谓的这个模型训练的参数需要去更新。然后你有提到一个点是说，有可能最终更新的这个参数跟中间过程会不一致，或者说可能中间某个过程更好，这个是会存在的。但是这个没那么重要，为什么？因为他追求的是一个全局最优。
	举个简单例子，就像我们最早学习这个深度学习的时候，咱们都会去看怎么样去做这个深度学习。网络的更新，这个反向传播对吧？它是一个链式求导的过程，然后最后的全连接层算出一个loss求一个一阶导就改变这个W然后这个W每一轮都会去迭代它。然后这个W如果是很简单的一个比如说一乘1024这样的一个最终的一个soft max接的这么一个分类的一个分类器的话，那其实就是一乘1024维的这么一个向量。
	每个值段改，然后有可能他某一次迭代的值是最好的，但你最终改的最后一层的那个W还不好了，但也无所谓，因为你是这一路上每一层的值，每一层的W在一起运算完之后算出的这个loss他是全局最优的。所以有可能你的这个值不是过程当中的一个最好值，这个是存在的一个情况，因为他追求的是全局最优。第二，既然W的值能改，那W的形式是不是也能改？从1乘1024改成1乘5 12，是不是可以？就跟我们这里举的这个例子，但在CNN里面不行，我是特指Laura这个例子，就是你之前是一个rank是十的，后面某一次迭代把rank甚至改成了5，这个也是有可能的。
	参数重要性为什么和不确定性呈正相关？这个同学问的很好，但是你看公式的话，其实不确定性是拿敏感性算出来的。我不知道有没有注意，这个是因为不是这篇论文的重点，我没有讲太多，主要把这个逻辑讲明白，尤其是敏感肌，这个不确定性这个公式我没有展开，直接放在这个第七步里面了。
	你有没有看到这个贝塔2贝塔2T减1WIJ加上一减去贝塔two后面是个什么？是个ITWIJ减去I18T。I18T是当前的这个敏感性建模，IT是上一波，所以其实相当于是上一批数据和上上一批数据的敏感性的差值乘上一减贝塔two这么个逻辑。
	行，那我们接着讲一下q lora，待会儿大家再提问还可以留点时间。Q ora, q Laura是华盛顿大学的一个论文，也是今年的一篇paper，也挺有影响力的。然后很好很有意思的一个思想，它这个贡献点很有意思，也值得大家来学习。并且现在有很多的实际的应用，因为它很好用。
	第一简单来说就是他提出了一个新的训练微调的方法，叫q Laura，就是去训练这个量化的大语言模型。怎么理解量化？然后我们也讲讲，然后通过这个方式，就是可以用一个48G的一个GPU的显卡，就可以微调一个650亿的大模型，然后它训练的这个效果甚至可以跟16字节，这是一个标准的float去微调任务的性能差不多。就是我们看到右边这个图，蓝色的这个是float data time数据类型，然后上面的这个叫n float，其实就是他提出来这个Normal flat，然后还有一个Normal flat加上dq，其实就是他提出来这个double quantization双量化。
	我们来讲讲这些技术术语是怎么回事，其实也没有那么复杂，反正一句话来说就是q ora通过三个技术叠加，一个叫forbid，四个bit的一个NF，它现在叫做NF4，就是Normal float forbid，这是一个新的数据类型。通过这样的一个数据类型，加上双量化的一个量化策略，再加上一个内存管理，内存和减存管理的一个叫做page optimizes这么一个技术。这三个技术叠在一起，然后可以使得它训练出来的量化之后的大语言模型比16字节的微调的大约模型还好，这个是一个简单总结。
	那么具体怎么做呢？其实我们对比一下这三个不同的技术，一个叫FFT，就是full five 20，一个叫Laura，一个叫q ora。我们先忘掉ADA罗A，因为这是另一条技术路线，它跟ADA lura完全没关系，它是跟lora a有关系。那么FFT lora和q ora我们比起来，他们到底额外plus了什么东西，加了什么东西？
	我们先看第一列，这是最简单粗暴的。就是我们有一个16比特的transformer，一个float，一个单精度字节的一个transformer，然后这个transformer它自己有一个优化的一个状态，就是我们的反向传播。大家看这个右下角有一个叫做parameter updates，就是参数更新。也就是我们刚才ADA ora大家在纠结的那个预算，然后选出重要性的模型参数去更新，传统的就是全部更新一遍，反向传播就是你正向传播全算一遍，反向传播也全算一遍。然后如果你参数更新的这个德尔塔太小了，对它没影响。如果比较大的话，那就更新它。那梯度消失，就是说你要更新的参数太深了，然后一堆小于一的数相乘之后，这个精度超过了你现在用来存储这个模型参数，它能表达的精度上限超过它的有效位数了，那他就梯度消失了。
	这个其实就是一个最简单的一个说明，就是我们的这个推理到训练就这么个过程。然后这里的箭头上面就全都是我们的模型参数。Lora跟这个file这个full的这个fine比起来，首先它加了这个adapter，就是我们的这个小的BA小的矩阵，然后他能够去做更低成本的更新，然后这里我们看到这个q lora，他干了什么事情？
	第一他把这个16 bit的transformer压缩了，这个很直观，就是以前你需要16个bat去存的这么一个模型参数，直接只需要四个bit就够了。但它为什么四个bit能存下来，我们待会儿可以去讲一讲，这个是他直接能体现它需要的显存更少，这里更新了。第二个就是它有一个叫做什么呢？叫做所以整个speed的优化技术和双量化技术都是为了把这儿弄小，包括这里的一路上都弄小。
	然后还有一个叫做page optimization的技术是用来干嘛的呢？是这里有一个红色的线，叫做paigning floor，左边的大图里面都是GPU的显存，右边的这个小的虚线框里面是CPU。然后它可以使得一个在我们的GPU显存出现不够用，overflow或者一些异常情况，或者我们预计预先计划好的一些不够用的场景里面，暂时用一下这个内存，就不会使它爆掉，或者出现一些别的情况。这个叫做配置的optimized的技术，这就是完整的全量微调和lora以及q lora的一个不同。
	为什么要提所谓的floor，或者说4 bit，或者说这个16 bit，这个其实是一个基础知识如果咱们不是CS或者相关专业的同学可能不一定知道。我们简单讲一个背景知识，就是刚刚讲的为D一样。简单来说就是我们都知道什么是浮点数，就是有小数点之后几点几这样的数。但这样的数在计算机里是怎么存的，我不清楚有多少人知道这个其实跟一个很很重要的一个机构，就是HFE的一个协议标准有关系。Trie的某一个标准规定了浮点数应该要怎么样去表达，这个在经典的计算机编码里面是很有用的。因为它统一标准之后，大家交换数据的时候不会出错。你们所有人都知道计算机是010101，那么0101就是我们一个01就是一个bat，这一堆领域最终是要被编解码才能够变成我们现在看到的这些数字。
	这个HRE的这个，我忘了具体协议号了，约定了float 32应该是怎么样去表达的，其中最重要的是什么？第一，我们看到比如说标准的float 32，在这个协议里面，它是由32个01组成的。那么最开头的这个叫做sign，就是表达你是正数还是负数，占一个bit，就是一比如说一就是正，零就是负这么一个意思。然后这里还有八位，八个0，表达什么呢？表达指数，就是你的这个指数，就是这几次方幂，然后后面是表达位数，就小数点后的位数。从这个视角你就能知道为什么float 32小数点后，比如说小数点后30位、40位就超过有效精度了。因为它表达小数点的就这么多，你超了就是超了，没办法。然后当然他肯定实际不是23位，它有一个计算公式没有这么粗暴。
	那么float 16也是一样的，就是我们看到现在用的比较多的在大在这个模型训练里面用16位的这个字节，然后再用这个16个bit来表达的这个模型参数，它是五个beats表达它的指数十个beats表达它的小数。然后类似的float 8也有两种不同的编码方式。因为现在大约模型要的这个显存越来越多，然后要的这个模型权重也越来越多，所以大家都在往小了做，那float 8就有两种，一个叫14M3，一个叫15M2，名字也取得很简单粗暴，就是E4就是四个指数三个位数，这个就是五个指数两个位数，那它的特点就不太一样了，对吧？如果是E5M2，其实就相当于它的这个值本身它的range很大。就它的这个值的上限跟下限很大，但它的精度要求小。那E4M3其实就反过来了，就是它的range可能小一点，但它精度要求大一点。
	然后这个是目前我们通常意义上讲多少位或者多少个壁纸，然后他怎么样去存储的时候一个底层的实现。然后这个实现，其实transformer现在有一个库叫engine，就引擎的这个engine已经开始在实现这些库了。并且像这个transformer生态里面的这个accelerate，就是加速的库，包括它跟微软的这个deep speed也有一些集合的集成，也都在也都实现了像float 8这样的一些实现。
	好，这个是一个背景知识。那么有了这么一个背景知识，我们就要来看一看Q罗拉的优秀之处了。最重要最重要的一点是什么？就是我们要理解它的这个新数据类型是怎么运作的，为什么有用？一个数据类型不是指整形或者说浮点数，而是指或者你这么理解也可以。
	简单来说就是我们用这个计算机里面的零一的这个beats去表达一些特定的数字的时候，有很多种不同的表达方法。刚刚说的那些float都是一些很经典的表达方法。但现在q ora这个paper他提出了一种自己的想法，可以用来表达数字，就是用只用四个bit去表达出我要存的这些数，其实就这么一个目的，那具体怎么做的，我们先看一下，之前是怎么去做量化的。就在深度学习阶段，其实就有很多量化技术，包括像什么剪枝的技术、减量化的技术、蒸馏的技术，然后包括像eight bit，然后four bit都是那个时候提出来的。
	像这个8 bit的这个量化，它是一个什么样的一个意思呢？我们可以看到它它具体怎么做的，其他就是用八个字节然后来表达，就八个字节，就八个彼此，并且是表达整数，来表达原来可能需要32位浮点数才能表达的内容。这个8 bit量化其实是最好理解的。我们先放在这儿，第18 bit量化出来的数字，它首先是整数，所以它没有小数点，它的有效精度就是小数点前的这个一，这个是它的有效精度，所以你能看看懂这个的话就是round，其实就是一个四舍五入，对吧？就求出来的就是一个整数。那这个数要怎么除呢？要怎么得来的呢？
	其实就是用原来的就是你量化前，就是精度最高的量化前的那个数，它是一个32位的一个浮点数。然后它去做了一个伸缩变化。简单来说就是把它的这个数，然后取出来之后，再把它的小数点后面砍掉，最后存到了新的用int 8表达的这个X所以如果你做的足够好啊，就是原来的X的32位浮点数砍掉小数点，就是你现在的这个x inter 8是最好的。
	然后通过这个变换，我们要我们理解了什么叫量化，也理解了整形的话是怎么回事了。那么这里这个公式变换出来之后，会发现有一个新的一个值，这个值是什么呢？就是这个round CFP32。因为你看这个公式其实是可以做处理的，就是你你分母有XFP32，然后分子也有XFP32。为什么有个127？是因为inter 8它是整形，他他最多就只能表达这个127。因为28位，八位二的7次方一个符号位，这个大家应该都知道了，我就不再赘述了。
	跟那个flow道理是一样的，它的八位就只能表达这么多个整数，然后你不同的量化它这个常量还不一样，但是这个常量是能被提出来的，因为XFP32这一一处理就得到了一个常量。那么这个常量叫做它的叫做什么呢？叫做量化常数，就是这个C有一个名字叫做量化常数。然后整个这个Normal float他是想要做一个什么事情？第一他想要用足够小的显存，就这个字节数足够小，然后存下表达能力足够强的这个权重，就是我们的模型参数，要用足够小的显存去存下足够大的这个数字，其实就干这么一个事儿。
	然后整个Normal flow这样的一个数据类型，其实它就是建立在所谓的叫做分位数量化这么一个技术上面的。然后这个技术其实是很早以前就是香农提出来的。这个信息论里面有一种最优的数据类型，其实就是这么表达的，为什么可以这样表达？你可以理解成，其实我们设计出这些数据表达，它就像一个一个的桶，有很多的坑位。比如说int 8这种变量，它就是有八个桶，但这个八个桶可以排列组合，所以它可以表达二的7次方，然后下面负数也有负的二的7次方，举个简单例子，如果你是浮点数的话，那它通过一个数学公式，可以把这个小数点后的很多位和比较大的一个数据范围都表达出来。这个其实是尽可能的去把你可以把这个数字想象成一开始就把它想象成这个是离散的，就是所有的数字之间就像自然数一样，就一个萝卜一个坑，0123456中间是没有任何东西的那就一个坑上放一个桶就能够表达好了。
	整形是这么简单，但浮点数的麻烦就在于，我们都知道当你去聊这个浮点数的时候，零和一之间有无数多个数，那么这个无数多个数我们要怎么表达呢？在浮点数里面是通过有效位数，通过精度来表达的。就是当零和一之间，我假设我的有效精度是三十多位，那么三十多位之后的那些数我就表达不了的。因为那个坑太细了，我这个桶塞不进去，我不知道这个表达好不好，其实是这么个逻辑。
	但是我们用这种方式去表达，在这个正常的计算机编码里面是合适的。因为我们的正常的计算机的应用是非常广泛的。你完全不知道它会出现一个什么样的数字，需要被编码之后存下来。所以你要尽可能的去覆盖。比如说当前比如说int 8，就是负的128到正的127。假设什么毒，然后比如说是一个float，它就是十的这个几十次方，负的十的几十次方到正的十的几十次方这块区域，然后到有效精度就相当于框下了一大片树，因为他不知道未来是怎么分布的。但是我们都都清楚的是说，我们搞这个机器学习，搞深度学习，搞大模型。我们需要存的这些模型参数，它有它的特点，就是我们在这儿动一些脑筋，就是这些模型参数本身它不是随机的，它会聚集在一块区，就它的数字的选择会聚集在一块区域。
	而这块区域其实是可以按照这个思想，就叫分位数量化的思想。这就是这一堆数字都摆在这儿了。然后我其实就拿一个桶去装他们，然后无非就是通过装的过程，我可以理解它的有效精度到底需要多少，然后这个精度决定了桶之间的密度。然后还有可能因为我的这个分布有特点，我的桶跟桶之间的密度还不一样。中假设是正态分布，那可能就是正态总体之间的统特别多，这一块需要密集的表达，然后两侧正太这个长尾的部分可能就稀疏表达就够了。
	这个就是整个分位数包括很多量化技术的一个核心，然后相同的这个信息论当中提出的这个最优的数据类型，其实就是指假设你知道这个数据分布，或者说你可以拟合这个数据分布。通过经验的积累，通过历史数据的拟合，你可以去知道你要存的这些数，你要被计算机存下来这些数大概是什么样的一个分布函数。你就可以去估计输入张量的这个分位数，但是这个分位数的估计过程是很比较昂贵的。就是相当于是说你有过去有这么多数需要存，然后你算出这个分布函数是比较贵的，然后那怎么办？所有人都知道一定会有一些近似算法，不能求出全局最优解，但是能求出相对来说这个八九十分的解。比如说这个SRM的这个分位数的方法，其实就这么一个核心思想。
	然后我们再说回来，就是所有的预训练的这个模型，或者说预训练的这些神经网络的权重，他们通常都是可以转换成一个标准差为这么一个有一个标准差，然后有一个零中心的正态分布，你可以去做变换的。如果我们能把权重映射到这样的一个正态分布上，那我们可以把这个缩放指数给记下来，然后我们就能够用一个符合就是能够存下一个正态分布，并且在特定有效精度下面的一个表达方式去存这些权重。我不知道这个有没有说清楚，所以最终就是取巧，这个NF4的核心就是把所有的原来的模型权重，把它转换到一个特定的分布上。然后这个分布我们可以通过一个缩放因子还原它原来的这个值。原来的那个值，然后我们存的是这个正态分布，然后它当然我们也存了它的那个缩放因子的这个值，但是我们就不用存那么大了。这个是一个它的核心思想，F4的核心思想。
	好，然后在这个过程当中我们具体怎么做呢？我们可以看到在这个作者在这个论文当中有提到，首先我们运行内的从这个权重有零中心的这个正态分布标准差，二维码标准差的特点。然后可以缩放这个标准差，使得分布恰好适应这个NF的范围，然后我们再再来想一想，这个NF，其实最终这个NF要把这堆数字给框进来，对吧？
	我们接着把把这个脑洞打开，去想象一下，我们把预训练的这些权重转换成了一个特定的正态分布。这个正态分布需要有一个NF的数据类型把它框住，然后密度也要合适才会不会有东西漏掉，那么就可以搞定了。那么这个范围我们可以设置，比如说我们把这个范围就设置到-1到1这样的一个范围，这个也是我们经常在神经网络的这个网络架构里面看到的，做的各种Normalization，都是往这个区间放，也有放0到1的，也有放-1到1的。
	那把它规划到这个范围里面之后，我们核心要解决的就是说怎么把它存下来了。因为上下限确定了，接着就是怎么去存里面的。所以它跟我们刚刚看到的int 8的这种量化很不一样在哪儿？Int 8的量化很简单，一个量化常数去乘上我们的原来的数字之后，所有的值都是这个整数。整数就一个萝卜一个坑，固定这个密度存下来就好了。
	但是Normal的flat它是能够去存我们的小数的，只不过这个小数是放在-1到1之间的。那么它其实核心就是要去理论上这个零中心的正态分布，然后标准差是一就我们这写的第一步，怎么样去做这个数据类型的计算，就是我们要去模拟一个正态分布，它的中心是0，然后它的标准差是一。这样的一个分位数是2K加一个分位数，那它其实就是要把2K加一个数字存下来，因为这个相当于这个圈圈里面一共就有这么多个分位数，分位数就是孤立的，可量化的一个数字。有2K加一个分位数，然后他得到的这个正态分布的K位的这个分位数量的这个数据类型，其实就是它的。其实就最后这个Normal float k了，简单来说其实就这么一个逻辑。具体要做的时候，其实我刚才也讲到了，就是用这个数据类型将它的值规划到这个分布的范围里面来。
	然后通过一个最大缩放，然后去把它缩放回去，然后还能够得到它的所谓的缩放的值，它叫输入权重张量。一旦这个范围和它的这个范围匹配，就是我框它的这个NFK的范围和我要表达的这个数字的范围能够匹配上，我其实就能够量化了。我不知道这个大家理解清楚没有，因为量化其实不是要说我要把这两个数字之间的这个桶，桶跟桶之间摆的刚好，它的距离等于它的密度。量化就是我有2K加一个坑，然后你有2K加一个数，然后你的顺序我记下来了，我基本上我就能够给你还原回去，只不过成本的高低而已，这个是量化的逻辑。然后它的范围只要不是太离谱，我的这个可复用的这个张量也就能能好搞一些。所以最终我们看到这个很复杂的公式，我相信这么解释应该大家都明白了，就是能够被表达出来，就我们要的模型权重能够被表达出来。
	然后这里我们看得到的是，通过这样的一个理论我们就知道，假设我们用的是NF4，就Normal fat 4个bit的Normal flight，那它就能表达二的4次方加一的这个分位数，其实是这么一个分位数函数，这么一个逻辑。然后这个其实就是整个我们这个NF4它的一个理论的一个逻辑。就这么个方式就能够去存下来了。
	但是这个新的数据类型只是它的一个优化技术，它还提出了什么呢？还提出一个叫做双量化的一个技术，叫做double d quant这么一个技术。这个技术具体怎么实现的？其实我们可以看一下这个图，就我们有了刚刚的这个F4，所以这里出现了一个新的模型权重，用NF4来表达的这么一个三元组。那这个三元组里面还有两个，一个是这个CEFP32，这个是一个我们的量化常数C还有一个C二是这个k beat，就是它要用来表达我们的这个Normal float k的这么一个数。
	然后我们先可以不关注这个，我们看看他怎么去做的。这里其实是有一个公式去表达出双量化跟单量化的区别，我们看这儿有一个很有意思的计算，其实就把整个刚刚我们说有一个NF4存下来了。但是真正计算的时候肯定不能那样存，还得去表达的更更复杂一些，要跟原来的这个16比特的这个，因为除了他以外，其他的都还是16 bit在做计算的。
	怎么样连起来？我们先看这儿这个公式就能比较好去理解了。假设我们有一个double decant这么一个公式，然后它其实把这个double d quant解开之后，是下面的内容。但如果我们不解开它，其实它本身是一个Y等于WX，再加上后面一堆的内容，这个是我们常见的一个形态。然后这里有个叫做BF16的，其实也是一个它的这个小型是一个智能。但这是以前的一些研究成果，你可以理解成这是一个Y等于WX然后这个double discount的成果，最终解出来的双量化的结果是W就是下面这一行WBF16，然后整个上面这个Q罗A的模型就好看懂了，你就把这个WD count，把它改写成WBF16。那整个Q罗拉的这个模型就是一个Y等于WX，加上XL1L2这两个正则，对应的两个loss的一个这应该是它的一个都在BF16，就是他所谓的这个brain float 16位的一个浮点数下面的一个模型的一个表达式。
	然后这个BF16的这个W是怎么算出来的？其实就通过这里的这个解的过程，我们能看得出来这里的k bit对应到里面。首先这个K是我们的这个C2k bit是同一个K然后这个k bit是为了把它，你可以就我们刚才讲的，你的4 bit是存了一个的坑位，然后你的这个BF16这个brain flat才是真正的那个值。那个是分位数，就相当于你一共有这么多的数字，然后这些数字每个有一个大小顺序，然后就大的在前面占了大的坑位，小的占了小的坑位，然后我要变成真正可以用的值。
	需要回到这个16 bit的这个brain flat，这里要怎么回来？其实是需要解压缩的，然后什么时候需要解压缩？需要计算的时候就需要解压缩，所以真正计算的时候模型权重是brain flat的这个16 bit，但是我存在那儿的时候是可以用4B的，这个NF4就能存。然后在这个过程当中，我需要去把这个NF4不断的解压缩成这个BF16，算完之后再重新存成我们的这个speed。所以有的人说Q罗拉能训练特别大的模型，确实能训练。但如果你这个搞得太夸张，那还有可能这个过程就会事实就是经常发生，那就会比较慢。
	然后整个双向的双量化的技术，就是把原来的我们看到的这个量化常数C以及这里看得到的第2个C2的这个k bit，就是我们的Normal flow的一个量化常数，再进行了一次量化，就把那个常数又进行了一次量化，然后又省出来了百分之几十的一个空间。这个是很极致的一个在量化的技术了。然后，这里我就不再细展开了。
	我现在这个逻辑大家已经理解了，就是通过分位数的这么一个函数，可以把真实的数塞到一些桶里面，这些桶其实能通过这个解压缩的方式又回来变成原来的数字。然后这个过程当中这个乘法过程当中会有一些量这个量化常数的产生。而这些量化常数因为它要做解压数，就需要乘上这些数字，也可以被量化的存下来，就这么个逻辑。好，这是他用的用到的最重要的两个技术。
	但这里还有一个事情就是说我们看到在paper里面提到一个叫做page的optimize，就是叫做分页技术。这个其实是什么呢？是它为了防止我们的系统不崩，因为你总有数据要进来，要进行训练。这些数据都会走一遍前向和后向传播的这么一个过程。你的这个数据一旦没选择好，变大了之后，有可能一层显存就崩掉了。所以它可以使用什么呢？它有一个叫做在GPU的运行，偶尔出现不足。就我刚才提到的overflow的情况下的时候，有一个invidia，就英伟达的一个统一内存的功能，可以让你在CPU和GPU之间自动页面到页面的去传输，来进行一些不会报错的GPU的处理，然后能够让你的CPU的RAM，CPU的内存和磁盘之间的一些常规内存去进行一些共享，就跟我们经常看到的虚拟内存技术是一个意思。
	那这个其实就是整个q ora的三个技术点叠加出来，让它能够用48GB的一个GPU去微调这个650亿参数的这么一个逻辑。那它的实际效果怎么样？可以看一下就可以看到他有去比较这个实验去比较16 bit的这个BF16和这个int 8，还有FP4，以及它的NF4在glue。这个大家应该已经看了很多回了，我们的GLUE是一个测试集的集合，包括所谓的这个super natural instruction，这个测试机也是很有名的，大家可以去了解一下。
	那在这个测试集上面，q Laura是表现挺好的。就是它的NF4加上DQ就是这个双量化，double框count双量化。然后可以看到它的性能是不输大家的，跟大家在同一水平线。然后最多在这个T53B稍微大一点的模型上，略输于这个q ora的英特8。
	那这里大家不要看它的值没有超过，这里要比较的是什么呢？就是我的性能跟你们一样，但是我的成本可能只有你们的N分之一，这个N可能是十甚至是一百。这个是一个很夸张的一个优化技术，就相当于我已经有柔软可以去比较低的或者说比较小的模型去训练这个大模型。现在有了Q罗A之后，我这个杠杆又加了一个新的杠杆，我可以把这个模型参数还可以降降的更低。相当于以前这个大模型可能要10万GB的显存来存它的模型参数。那用了Laura之后，可能你可以除以一个除以一个100就可能变成一千或者说几百几百GB的这个显存去存。然后你用了q ora可能就变成你只需要这个计时GB就可以续存。
	这个其实是可以叠加的一个技术，它本身跟ora也不冲突。然后类似的我们可以看到在这个MMLU这个测试集上面去微调的这个lama的650亿的这个参数。我们BF16和我们的flow 4，还有NF4加上这个双量化，明显看到其实它的性能还是很好的，尤其是单纯的跟float 4比起来，Normal flat 4，Normal float 4是有它的这个优势的，并且它的性能表现的拉通了来看，还超过了单纯的BF16。那这个其实就是q lora的这个技术，看大家有什么问题吗？没有的话，我们就先往下面讲。NF4这一块不是很懂，有没有更详细的讲解资料？看你要懂到什么程度。我不知道这个讲大家看明白没有？其实就是用其实按照时髦的说法叫做纯算分离，就是纯跟算是分开的。我再简单回应一下，因为这个NF4确实应该是最难理解的，这个部分也是它的核心价值。
	什么叫存算的分离？就是现在做数据库，做存储的公司都在讲这个概念，就存算一体和存算分离什么概念？就是相当于来说，我们用这个float 32，用float 16去表达这个计算机里面的浮点数的时候，我是这么存的，我也可以直接拿起来就算。我就是因为我其他的数也都这么存的，我可以直接拿来做加减乘除矩阵的运算。但是那个NF4不是的。
	那么NF4它是极致的压缩了我的存储空间。他在什么场景下用？就是在我要存的这个参数已经上100GB甚至上千GB，这么的大的内容需要存的时候，我就把计算和存储分离了。我把我的模型参数存的那个空间极致压缩，压缩之后我需要算的时候我再解压缩。因为我们都知道了q Laura它不是在每一个阶段的每一个参数都需要去算的。它有的时候只需要算一部分，你loa，它只需要算一部分的这个参数。那我算的时候我再把它解压缩，回到这个BF16，回到这个BF16之后再进行运算，运算完之后再进行压缩，就是编码再存下来。这个是它的一个核心思想，就是我用足够多的计算时间去换取我的这个空间，但这个换出来的这个价值是很值得的。因为它让很多以前没法做研究的，就显存不够的，可以做这个研究了，然后他就是会花更多的时间，但他至少可以用了。
	推理的时候需要解压说，但是你不需要全部解压说。我不知道这个说明白没有，就是你不需要全部解压说，你在推理的过程当中，你是一步一步算的，你并没有一次算完，所以你不就可以算的过程当中不断的解压缩再存，解压缩再存。这个大家听明白了吗？
	这个最经济最广泛这个事儿是很主观的对，就是有的人他就做这个这个，他觉得走路很经济，有的人喜欢骑这个小黄车，有的人坐地铁，有的人打车，对吧？有的人还有专车，这个很很主观的对，我们没有办法评价。我们只能讲这个技术它的贡献点是什么？它的价值是什么？它的原理是什么。然后对于我们个人的开发者来说，你想要去碰一个100亿参数的模型，这个是比较可靠并且有技术支撑的一种方法。对，不然你可能就得买很多的显卡才能去用大模型。
	对，就是优化优化你的空间使用，对吧？然后行，我看时间原因，我先把后面的讲完，我们再来聊。这个也算是对PEFT的一个完整，就是大模型高效微调的未来的一个发展趋势。这个图很有意思，其实取自于一个综述文章，我也更新了一些大家可以去参考阅读的这个推荐论文，待会儿也会上传到平台上，估计明天大家就能看到了。我们总结一下，分类一下，就学东西一定要结构化知识，要去做分类才能更好的知道你在哪，还有哪些你没看到的，以及你要怎么样去学才能更高效。我们看一下这个维恩图，其实很直观，中间的这个圈圈叫做adapters，适配器adapter也是我教给大家的第一个论文google的这个adapter tony，整个这个adapter其实到目前为止仍然是PFT的一个很主流的技术。
	然后adapter里面我们除了讲过adapters以外，就是我们反复提过的这个adapters以外，还有一个叫做它旁边的右边叫做parallel adapter。这个挺好的这个方法，并且现在被很多人在用。这个是什么方法呢？就是大家都知道我们在FFN就feed forward network后面会加一个adapter的模块。那parallel adapter就是把这个模块像这个self attention一样，self attention里面是有H个不一样的，最后拼在一起。那你的adapter是不是也可以接在这个FFN后面，接H个adapter或者接N个adapter，然后再拼在一起，就这么个逻辑叫parallel adapter。然后这个ADA mix其实是把一些很多各种add的技术，包括一些我印象当中还把一些adp的选取做了一些优化，这些都叫adapters的技术。
	然后我们还看到左下角这个绿色的部分，叫做软提示的技术，我们的soft prompt里面有我们看到的这个prom tuning，google的一篇文章，还有p prefix tuning，stanford以及这个P20。但这没写，这一类都是属于这个soft proms技术，他们都是在GPT3提出之后，就是in context learning提出来之后，发现不用去调整预训练的这个模型的权重，而直接去对这个大语言模型的响应机制去做训练，就是去调整prompt，就能够去改进大语言模型的生成结果。然后这个proms的训练方法就有很多种。Prompt，在外围，然后训练一个混合的这么一个然后prefix，直接在transformer的一些模块里加一个前缀神经网络，然后去训练这个前缀的神经网络。P20也是类似的，在embedding层甚至是在P20V2里面引入了更深层的一些微调，这些都属于soft prompt技术，然后这个和这个绿色的这两个有一些重合，叫做MAM的这个adapt的技术我们没有讲，但这个技术现在也很火，叫做也不叫很火，也有很多人在提，叫做mix and match。这个是AAMAM就mix and match有混合然后匹配的一个APP技术，它站在这个位置你也就看得很明白了，就是把这个dept的技术和这个soft prompt技术合在一起。
	虽然我们没有讲这个MAM的adapter，但我们会去讲这个uni PELT，它就更这个中心对吧？它是三个圈合在一起。其实整个MAM的adapter和这个uni PELT都是属于一类研究方向，就是怎么样把前人的成果开花结果，然后弄到一个框架下，uni PLT就是想做这么一个事儿，就叫做un y的，这应该是p parameter efficiency的language tuning，应该是这么一个名字。我没记错的话，我们待会讲这篇文章。
	我们再看这里还有一个小角落里有一个叫IA然后括号三的这个3次方。这个IA3也是我们待会讲的一个paper，很有意思，是北卡的一个大学的一个文章。他们也是脑洞大开，但是效果还不错的一个文章。大家可以听一听，但是不用投太多的精力。不过因为hugg ging face的PFT也支持了IA3，并且它这个技术也是一种通用技术，像诺亚一样是一种通用技术，我们可以待会儿简单的了解一下。那么除了我们左边这个a detail就是增量模型以外，这边还有两个圈。但是我们先把增量模型给大家捋一下，有adapt，有adapter适配器，也有这个soft plus s，还有其他的一些技术。
	整个增量模型的核心是什么呢？其实就是我们尽可能针对下游任务的时候，去增量的训练一个小模型这个增量的小模型或者说增量的参数，它也不一定是孤立的小模型。比如说这个adapter就是最原始的data，就是在原来的模型里加了小模块。通过这种增量模型或者增量参数的方式，来高效微调我们的大模型，然后使它能够适应下游任务。这个是所有的additive的这一类方法的一个模式，包括IA3，IA3就直接加了个参数，就可以就就有用了，就很有意思。
	然后我们看到这个右上角这里有一个叫做selective，叫做选择性的方法。Selective measure, 这个我们没有讲，但这个方法其实主要是因为他的从我的视角来看，他的这个方法比较孤立和这个单点，就是各种灵感的集合，有点八仙过海的意思。就是他的方法跟这个增量可以正好对比着来看。增量是说我不去不去动原来的我去增加一些这个参数来训练。选择性方法是说我有一些慧眼，我能够挑出一些原来大模型里我应该去调整的参数或者模块。我挑的准，我挑的好啊。
	这个其实就是一个selective的方法，这一个流派或者说这一个技术路线的重点，大家如果要去看的话可以去理解的那右下角还有这个reparation based，基于重参数化的一些方法，这个就是我们待会儿要讲的这个uni PLT也会去讲。包括像我们开始提到的lora都是这种方法。他就是说原来你有一个这个模型，然后这个模型我可以把它的参数重新甚至是把它的网络结构都会做一些调整，把它的参数全部重新做一遍，或者说部分重新做一遍，然后来把这个单元模型的能力给它复刻出来，这个叫repetition ation based，这就是这几类方法，刚刚做了动画我都忘了，就这几类我们已经讲过了。
	那么接下来讲一讲这个uni PLT，2022年的一个paper是UIUC，和meta一起发的一篇文章。他他他的核心思想其实就是怎么样把我们前面介绍的这些不同的PEFT的方法模块化，像乐高一样模块化。然后通过这个门控机制来学习激活最适合当前数据或者任务的一些方法，尤其是我这列出来的最常见的这应该是个拼写错误这个地方应该是从参数化的方法，这应该打错了。最常见的三大类PFT的技术，一个是这个适配器，一个是这个软提示，一个是从参数化。然后他试图能够把这些已经被广泛证明有效的技术整合到一个统一的微调框架里面。然后针对不同的任务可以去做一些学习和配置。
	我们就展开来看一下，这个框架很有意思，这框架似曾相识。就是大家如果看过门控机制，学过这个RNN的都会知道这个LSTM，还有这个GRU这两个经典的RNN的改进单元，都干过这个门控机制的活儿。那门控机制的核心是什么呢？就是我可以把这个几百家之长，有像天龙八部里面讲到的我什么都能学，或者像那个乾坤大挪移一样的，我什么都能学，我可以直连，我也可以跳过。那我把我的这个adapter和我的这个prefix tuning，或者这一类soft prince和这个Laura都接到原来的transformer里面。大家看把这三个有这个get a，就是门、get AP和这个L三块移走之后，剩下的是一个原来的transformer的这个结经典结构，然后做Normalization，做feed forward做Normalization，然后后面再去接，就以这个方式引入进来了。所以你说这个算是统一框架吗？算。
	但其实他在探讨一个什么问题？就是PFT发展到今天已经有三四年的时间了，大语言模型也有五六年的五年的时间了。那么到今天这个节点，我们一直都在想把大语言模型的这个高效微调做好。但是好像没有一个抓手，大家都在瞎猫碰死耗子似的。就是我我去改一改结构，我去改一改这个嵌入的方法，我去改一改这个加到前缀里面的什么位置，什么成什么模块。
	Lora的话就是说我去把大的变小，我甚至把大的变小的过程当中，我还是能够自适应的去调整。但是这些技术本身都有它自己的应用场景，也都找到它自己的价值。但它不系统，它不系统的话就很难规模化和工程化。所以这一类研究这个PFT的这个统一框架一定也是一个研究方向。因为只有框架统一了，我们的这个就是我们learning的框架统一了。我们写这些network的这个框架才会统一。就像只有等深度学习的这个backbone相对统一了，它的这个神经网络相对统一了，这个TensorFlow py touch他就不用去大改，不用天天去变，因为没有那么多算子需要去更新了，模块都定下来了，然后我们回过头来看，其实现在PFT这三类主流的PFT的技术已经有一些趋于开始收敛的状态。
	比如说adapter，我们看到adaptor其实是什么？Adapter其实主要它它有哪些变的东西，有哪些不变的东西。变的东西从最早的这个adapter tuning我们就知道，它有一个接入位置是很重要的。包括在这个uni PELT里面，都是把它放到了这个fit forward的后面，放到了这里。所以它的接入位置是比较重要的，并且通常也都放在这里。然后在这个fate forward就是一个全连接网络，然后它的接入方我们只在第三节课里面讲过，它有简单的一个adapt layer接在这里的方式。
	那我们刚刚也提过，还有像这种parallel adapter，就是并行的adapter。就像这里的mark head tension一样，是可以有多个并行的adapter模块放在这里的。并且这个模式也是我们下节课要讲到的MOE，就是混合专家模型。
	有很多的这个模式其实就是学的这个parallel adapter的模式，就是在FFN的后面接多个adapter模块，每个dept又有自己的特定能力，然后去做这么一个设定，然后包括它这个MLP的设计，这里本身这个德耳塔H要怎么设计，其实这里有一些变化，可以去做一些调整。但是整个uni PLT的核心就是adapter，它自己有一个门控的这个参数，这个参数是可以被激活的，当我去选择特定任务的时候，我的这个GA是不一样的，就是相当于我有一个统一框架，大家都在这个框架下去学。我有十种不同的任务，十种不同的任务学出来的门控的参数是不一样的。有的任务是需要adapter的，我就把这个门框打开，有的任务是不需要adapter的，我就把他们的门控给关掉。这个是它的一个核心。
	类似的像这个soft prompt这一类技术，它的嵌入方式是一个可以被调整的部分。然后它的微调方法，我们通过比较P20和非P20，或者说这个是否是手工生成的，还是它是连续可微可以去优化这个prompt生成的过程，这个其实也有两种不同的方式，但他们都可以被统一的被放在这个soft proms下面。然后在这个uni PPLT这个框架下也都可以放在这个位置，不太会影响别的什么内容。然后LORA也是一样的，Laura的核心可能就是它的缩放因子，以及我到底要选哪些模型参数和模型模块的类型去做Laura来进行微调。其实就是这些东西，那么整个这三类主流技术其实现在放在这个uni PLT里面是看着不怪的。
	但是你会发现这个Laura它针对的是我们的transformer和整个transformer模块输出的映射的向量。包括这个feed forward，它论文里面没有写，都是可以去做lora a的，在ADA Laura里面。然后GP其实是在这个外部去做了一些，比如说在embedding层做了一些内容。然后adapter是在这个结果后面和这个输出映射之前做了一些内容。但他们能同时训练吗？就是我们都提出来过prefix 2，它带来的问题是难以训练，然后我们用这个uni PRT的框架的思路是好的，就这个想法是好的。用一个框架然后把所有的技术装进来。
	然后我能够用门控在什么时候去用什么样的技术，可以训练出一个好的门控的一个参数。但是把它们全部都放在一起之后，第一训练成本变大了。第二可能prefix通灵的难训练问题仍然没有解决。但prefix 2本身还在迭代，或者说soft prompt的技术还在迭代。但把多种不同的PFT模块放到一个框架里面去训练，还有很长的路要走。但这条路应该是一条对的路，我们可以做这样的一个评价和疏解。
	那么它的一个实验结果也很有趣，我们看到这个UIPLT，当然它的这个实验结果我们能看到这个比较复杂。我们分成几块来看。第一个就是它只用100个示例，这个K是它的示例，就可以在这个相对低数据的场景当中去展示。单个模块的技术和它整合到一起的技术比起来，整合到一起是有优势的。但我们不知道它的训练资源用了多少，他这有对比，完整的find me和这种selective的method bit fit，单独的adapter和单独的prefix 20以及Laura，然后比起来和他自己的这个unip LT比起来，我们能看到最终这个uni PLT，它的这个表现是很好的。
	在这个SST2上面几乎是有将近一个点的提升。右下角这个是它的提升点，右下角这个应该不是提升点，是一个什么参数？这个可以，这可能我得再看一下论文去关注这个细节。但我们这个大的数字能看得到，是它的这个bench bug上面的一个跑分一个performance是还蛮不错的。但我们关注细节的话会发现在这个罗A81.56和这个uni PLT比起来，其实并没有太大的一个提升。然后类似的我们看到这里的QNLI，这个测试集73.52也是高于这个uni PLT的。所以怎么样在一个比较有限的资源或者说固定的资源环境下，去统一我们的PFT的这些训练技术，还是一个比较难的。
	这个路还有很长的路要走，但是往这个方向去发展，现在已经是一个比较明显的一个状态了。包括我们看到还有一些其他的做训练微调的库，也都在朝这个方向去发展。但这个K等于100，K等于500，K等于1000，从K等于1000的数据来看，好像相对来说比小数据的样本好一点。因为本身它用的模块更多，所以需要的数据相对来说也更多，但无论如何没有一个非常质的提升，这个是很明显的。
	然后这个是它的一个实验结果，大家有兴趣可以在论文里面再细看一看，然后在glue这个很有名的这个benchmark上，我们看到其实unp LT1样的，也是使用了所有的训练样本，就是这K等于2，刚刚是100 500 1000。当他使用所有的这个训练样本的时候，在这个benchmark上，然后整体上还是不错的。就是看最终的这个average还是不错的，但是优势没有那么明显，就相对来说在小数据上面去精雕细琢它的这个uni PLT的门控，还优势更大一些。在这个全量的数据上，也就83.5和这个83.12 lora的这个区别不大，和这个82.2区别不大。然后甚至还没有使用这个ADA罗A作为它的benchmark作为比较。所以这个核心GPN paper我认为可以分享给大家。
	就是说怎么样去理解所有的不同的PFT的技术。它在一个transformer的大模型里的，它在像former这个网络架构里面，它应该处于什么样的一个位置。然后他训练的特点是什么样的？他这一类技术有哪些横向的不同的论文实现可以去做对比，这个是uni PLT这篇paper想要分享给大家的一个重点。然后我们看到在这个不同的训练参数上面，uni PLT它的时间没有增加太多，这个是118%，这个是指它的就相当于他开了更多的这个模块。我们都知道你新增加的这些模块，最后计算的时候都要去算一遍。
	那这个新增加的这一部分其实是会消耗你的时间的。但比起来它的推理的时间增加了27%，27%比之前我们看到的这个都要多，但是不是一个数量级，甚至不是成倍的增长，但我们反过来看，它的收益，其实也没有变得特别高就它的这个性能提升。所以整整个统一的框架，一定不是通过一个简单的门控，就是我开不开，还关不关，像这个电灯开关一样来做处理。未来应该还会有迭代，但是用一个框架去把这些模块的能力承载起来，应该是一个比较好的状态了。当然它的模型参数也会变得很多。
	变了，变成原来的这个多了26%，就是因为加了这么多不同的模块，然后我们再看一篇跟他反过来的文章，就是简化的，就是把简单做到极致的，叫IA3。这个paper也很有意思，叫few short parameter efficiency fine tuning，就是小样本的或者说叫小样本的PFT，是更叫做什么多快好省又又好啊又便宜。相比于这个ICL，这篇文章是是这个北卡罗来，北卡罗北卡罗莱纳，简称北卡的一个分校的研究人员提出来一个方法，他怎么做的？就是他有点类似于这种我直观的感觉是一个灵感的萌发，想出来的一个paper。然后他这边paper其实要对比的碧池是GPT3。
	因为我们知道GPT3当时提出来的文章是说，in context就在上下文中学习，在上下文中学习最大的问题是什么呢？其实soft prompt就是从google的这个prompt I开始，包括我们学到的这个P20都是在回应ICL的问题，就是在上下文中学习的问题。因为最简单最原始的SAL就跟我们去使用ChatGPT1样，需要人去想到底要怎么用这些prompt。我们现在学过了就知道那个叫hard prompts。
	后来出现了soft prompt，soft prompt里面有各种learning的办法。I3是说假设我就是跟ICL去做对比，然后我也不去跟你的这个soft prompt去做对比。因为你的soft prompt虽然效果好，但是很麻烦。就是你也得去搞各种各样的。你们还是属于ICL，只不过一个是人工的，一个是用机器用算力去用数据驱动出来的那这些方法都有一个问题，就是我得去设计各种各样的这个，要么就设计模板，要么就设计网络，要么就是去训练神经网络。无论如何都很麻烦，就是Laura吐槽过的他都可以再吐槽一遍，他想说我能不能以一个极其简单的方式就来把下游的任务给学习了。
	然后样本还用的不多，然后这个团队的这个作者，其实之前也在这个T0的这个大模型上面做过一些工作，然后具体怎么做的，简单来说就是他给每一个激活层，就我们知道神经网络里面有个激活层，是activation这一层，学习一个向量一个系数，一个新的系数。然后在训练就是我们在用小样本去学习它的时候，以及我们正向做推理的时候，这个激活单元和向量当中对应的一些分量去相乘，然后这样算下来，就只需要用1‱的参数，因为激活层本身就没啥参数，就可以达到一个微调的效果了。所以从这个角度来看，它很巧妙，他找了一个激活层，这是一个参数最少的一层，其他都不用动。然后用激活层的这些参数的微调来适应小样本，然后实验结果很好。但是而且这个方式是可以用到任何地方的，因为你就改激活层。然后在他的这个实验结果里面，他在在这个RAFT这个bench mark上面，甚至超过了人类的一些基准水平，取得了一个全新的一个sota。
	然后他自己的这个套路就是他自己在T0上改了这个激活层，然后训练出来的这一套套路的这个模型，他们改名叫t fu，就对应的这个few shot，就相当于在T0上面的few shot，然后用这个方式训练出来一个模型，然后这个套路叫做IA三这个IA3就是用这个相对来说在内部激活层上用非常少的方式，能够让它adaptive去做提升。然后具体怎么做的呢？我们可以，具体怎么做的，其实也比较简单。刚刚说过了就激活层里面去做调整，然后简单做一个总结。因为这个模型它没有什么太复杂的微调这个手段。然后hugin face的PFT也是支持的，也只需要两三行代码就能够load进来，那我就不花太多时间。
	但这个思想很有意思，就想告诉大家，有人在往统一框架做努力搞门控。门控也很简单，但有很多同学他是拍脑袋想，你们搞的那些确实都太复杂了，就有点over design了。是不是我在某一层就随便改一改1‱的参数，就可以有不错的效果了，这个也是一个脑洞和思维方法和这个不同的方像。并且它也足够的低成本，你可以快速去做试验，它就是一个简单的对比，就是刚刚提到的差不多1‱的这个训练参数，lora最少也要1‰，也相比lora都只要10分之1的显存，就能够开始做训练。然后这个训练过程，其实就是一开始的训练，你其实最终训练的是一个很轻的IA3的模型。
	然后它可以用来做下游，甚至还可以基于这个新的I3的小模型去做下游的翻译通知，去对接一些新的，这个也是可以的。然后在它的测试集上，这个性能跟一些负的find tree的模型差不多，然后它也不太会增加推理的延迟，然后这个适配器的权重也可以跟基础模型合并，也可以像adapters一样甩出去，然后也可以用到任何神经网络的这个架构上。因为它其实跟adapter一样，它本身就是一个adapter，刚才有提到。但这个adapter，它不会去改太多的参数，也不会像我们的adapt里面内部还会有这个down project的up project这样的一个映射。然后整体的结构也很简单，就是在我们看到在K和这个V这两侧增加了一个小的向量，然后这个向量可以被用新小样本去做调整，然后得出来的这个结果也是很夸张。
	我们看到这个五角星是这个IA3，然后他训练的这个参数是1‱，然后这个1‱这个柱子上其他的一些方法，比如说这个prom 20。以及我们看到的这个layer Normalization，还有这个fish mask，这些都是一些selective的一些方法起来，它是有优势的。然后它的在不同的每个例子上面的算量也是有不同的。
	然后还有一些优势，但这个我建议是介绍给大家。第一是他的思路很有意思。第二就是说这个东西是很好让我们自己去做试验的。如果你自己想要去试验一些大语言模型，比如说几百亿参数的，然后你又不知道怎么样去微调。刚刚那个q loa还有的同学看着也很劝退，对吧？那AA3是PFT已经支持的一种low的方式，同时它也能够让你比较快的去微调一个大模型，然后用一个很小的adapter就能去撬动这个大模型。然后在他自己的这个比较上，T6和T0就一开始的这个模型以及这个T5和这个GPT3的小中大三个版本去做了一些对比，它的需要新增加的这个推理没有增加太多，然后占的这个磁盘也是不大，然后它的准确率是有一些提升的，然后整个推理的计算消耗也没有增加太多。
	然后右边这个，是我们刚刚提到他自己说的RFT的这个数据集。对IFT的数据集是一些比较偏现实意义的一些实际任务有11个，在这11个任务里面，它有跟当时最好的模型以及人类的基线去做对比。我们看到第二行有一个叫做human，this night, 就有一个人类的极限水平是73.5%。
	它在11个测试里面是比人类还要高2.3个点，然后跟当时的最好的sota比起来有高六个点，这个很很有趣。如果效果真如他所说的话，大家可以都去试一试。并且我觉得他这个思维方式很有意思，就是你在激活方面去做这么一个事情，然后我看看应该最后还有一页总结，然后我们可以再提提问题。
	就是整个大元模型的PFT，我觉得可能我们长期来看，除了刚刚说的一些内容，未来还有一些什么样的发展趋势？第一个就是说我们都看得到，大语言模型微调是一个刚需，因为大模型太大了，我们不可能针对特定任务要用这么多的资源才能去训练出一个我们满意的模型。所以更高效的一些参数优化就是PE的这个PE一定还会有更多的新技术出来。然后目前一些我们看到的Laura，包括ADA罗A都是2023年，ADA Laura q lora都2023年。今年这几个月的一些成果，未来肯定还会有一些更高效的一些lauda，或者说其他的一些参数共享之类的一些技术出来，这个是毋庸置疑的，包括我们后面看到一些终端设备上还要去跑一些领域模型，然后适应性和灵活性未来也一定会去提升。
	这个其实就是我们看到ChatGPT以及instruction tuning也是google当年的一篇paper，开启了instruction tuning的研究热潮，后面OpenAI发布了instruct的GPT，这个也是下节课的预热。我们下节课会把ChatGPT的这一套instruction tuning的技术，包括INHM给大家讲一讲。因为那个又是一个大坑，可能放到今天就讲不完了。所以我们还不如都讲细一点，让大家把这个理论部分稍微学的扎实一点。然后后面去动手，会不会一脸懵？所以适应性和灵活性的提升一定是一个重点。
	因为我们讲了PFT有一个大坑，深坑是什么？就PFT现在还是局限在一个任务或一种任务，我得去做一次PFT。那有没有可能是把他的这个PEFT的这个事儿从一种任务变成一大类任务，或者说把它的这个能力再往上拔一层，就是把会做做这个微积分考试的题，数学的题和只会做加减乘除三个层次。那现在可能PFT聚焦在我会做加减乘除了，但是我要做微积分是个天坑，做不来。我只能再把微积分分解成好多个不同的小的加减乘除，要去单独去学。那么有没有可能真的把微积分学会了，或者把数学学会了，就是更高层次的能力去学习。像这个instruction其实是一个很好的思路，在指引着大家怎么样去学，然后跨模态和多任务这个也是很热的，就是包括在这个prom tony里面还有multi task的这个prop 2IP20V2也有讲到这个多任务学习，跨模态怎么样能够把多个模态的这个PFT能够做高效，微调就更难了，但这一定是未来的发展趋势。
	然后模型压缩和加速我刚有提到了，还有一些低资源的一些语言和任务的支持。就是一些特定领域能不能提供一些支持，包括一些小众的语言能不能支持更广泛的去覆盖他们，这个就涉及到一些小样本的一些PFT了，这个可能就是我认为PFT for这个LLM未来的一些发展趋势。好，那看大家现在有什么问题，我们再回答下大家问题对。
	Q lora是算量化技术，q lora加上Laura AQ lora包含了ora某种层面上这个同学提了一个问题。我再回到kora的paper，这篇paper里面的截图。
	大家看一下，q lora是某种层面上是这个我不是我只是应该这么讲，就是看这幅图应该比较好能够理解。第一就是你的transformer本身这个transformer的base model是可以通过q ora能够让它存的就更小。这个是可以直接收到他的好处的。收到他的好处。
	然后Laura的核心是什么？Laura的核心是把大的模型变小，然后去做低成本的训练。这个其实跟Q罗A是不冲突的，我不清楚大家理不理解，这个其实是不冲突的，并且这个在他的公式里面最终是能够体现出来的。就在这里。其实他曾他可以在这个地方用他的这个Laura来做表达，就不一定是要用它的delta w。
	然后我不知道有没有回答这个问题，然后大模型微调一定可以解决数据隐私问题吗？我理解大模型微调和数据隐私是没有关系的。这个同学你的点是什么？就是数据隐私跟大模型微调没有直接关系，对q lora有精度损失，任何微调都有精度损失。就算你不用量化，你的float也有精度的，你只要超过float的精度就会有精度损失的。然后q lora的精度肯定没有float 32高，也没有这个flow 16高。这个没办法，也没有float 32高，这个没有办法的。
	Lora的分数很高。是的，因为lora这个技术手段就是好啊。因为Laura的核心就是说大语言模型，为了学知识需要那么大，但是大元模型做具体任务就不需要那么大了。那我可以找到它的本身的这个本就是跟它相关的这个本质的这个模型。这个本质的模型可以通过这个low rank adaptation把它找到。然后找的过程当中，这个low rank可以用SVD再来找，就是用其质分解再来找。
	这个同学我再解释一下，我跟你的回复就是数据隐私问题，跟大模型微调没有任何关系。然后你理解的数据隐私，其实也看你理解到哪个层次。就比如说你在你的这个训练过程是怎么发生的，你的训练过程有没有用到晕，你的训练过程用到了云，那你云厂商算不算数据隐私泄露问题，或者你的没有用到公有云，你用的是私有云，你的私有云是在你自己家里面吗？还是在电信的机房，在移动的机房。那你的电脑都你的服务器都在电信的机房，你觉得有没有数据隐私问题？
	然后我们再来就是你需要用公有的这个网络，你需要连接中国电信、中国移动的骨干网络，然后连接这个网络才能访问你远程的服务器。那你在访问这些网络的时候，有没有数据隐私问题？所以数据隐私是一个没有底线的。没有就是你不谈条件谈数据隐私是没有意义的。然后大模型微调跟数据隐私没有关系。
	我不知道我这么讲你明白没有？什么时候能带着练一遍na和IA3的微调？这个同学不要急，我们下周三应该会讲ChatGPT的这些技术。我们先把这些技术讲完，然后我们在用的时候我们就直接开始用这些库了。因为我们要是用了这些库再来讲这些技术，大家也没有太大意义了。对。然后我们把这些库的熟练，包括在用这些库的时候，才会具体涉及到一些数据处理的技术。因为你不是动手再去讲那个数据处理也很空，但你如果没有这个理论的铺垫，直接一上来在用的时候给你说了很多的技术名词，可能就都对不上号，那就会很麻烦对。
	运行在手机端。我的建议是如果你要运行在手机端，你比如说最近google发布的这个germany经理有一个运行在pixel的上面的版本。运行在这个pixel上面的版本就是一个大模型的，这个应该是它的量化版本。压缩其实是一个模型的压缩是一种诉求。模型的量化是一个解决模型压缩这个诉求的方法，除非我用自己的局域网，用自己的服务器训练才可以，对吗？这个同学还在问，这个同学你没有理解我说的point了，就是你就算用了自己的服务器，服务器是你生产的吗？会不会有后面就你问这个问题没有意义，就是数据隐私，没有条件谈这个数据也是没有意义的。
	能欠跟这些微调lora ISMP20之间是什么关系？能欠是实现吗？不是的。南茜是一个大模型应用的开发框架，然后能欠的作用或者说能欠这个框架的价值，是你现在有一个已经可以直接拿来用的大模型了。我用蓝线来调用你这个大模型很方便。
	这里的大模型主要是指大模型的服务，或者说开源的大模型。如果你自己是基于开源的大模型，然后你做了微调，微调之后你又把它部署成了一个你自己是这个局域网里的服务。那你把门嵌down下来，抠下来，然后用能欠去调用你的这个大模型会很方便，是干这个活的。并且他跟这个大模型周边的生态做了很多的集成。比如说大模型需要向量数据库，大模型需要对接搜索引擎等等。南茜是干这个活的，然后这些微调的技术是为了让你自己把你的大模型微调到可以适应于你的特定任务，但就算它适应了特定任务，它也只是一个大模型。大模型跟服务还有差距，那能倩可以帮帮你把它更方便的变成服务。
	Lora和ADA lura分别是在哪一层微调的？这个同学可能没有get到，就是罗A和ADA罗A它的微调。其实我再给大家看看那个图，我就知道这几节课可能干货有点多，需要大家多问问才能把这个事儿讲透。
	是这样的，就我们看啊这transformer，这个是transformer，那么Laura是干什么呢？Laura是把这个transformer里面的这些W变成这个小的矩阵，它是干这个活的。然后变成这个小的矩阵的过程当中，它有很多超参数需要去调。所以它到底要去调哪些矩阵，以及它调出来的这个小矩阵是长什么样的，其实都是超参数。这个东西通过ADA罗A就能看得特别清楚了。就比如说这幅图，我们不同的层的W它的rank都不一样，然后我在调整它的时候也不一样。所以这个完全取决于你的训练过程，你的训练参数和和你的这个设置的一些超参数了。
	Flash attention后面会提到的，在实战的时候可以再回忆一遍高精度数据映射到小范围的量化过程吗？正态分布的部分，这个同学建议去看一下这个基础的一些量化的技术，这个是很很朴素的一个过程。量化就是指把你现在有100个坑，你就只能表达100个数，然后这一百个数字只要按照你的这个坑一个萝卜一个坑的放进去，你最终就能够把它还原回来。只不过这个还原过程需要有一个函数记下来，或者说需要有一个权重或者说别的方式能够还原回去，能够挤压缩回去就可以了。现在这100个数字如果能够符合某个分布那就更好了。这个就是NF4其实核心就是这是把神经网络的权重变到一个特定的分布里面来。然后把这个表达它的NF4的数据类型，能够做到可以表达足够多的不同的值。然后最终通过一个解压缩的过程，把NF4里存的顺序的值还原回那个BF16，就brain float 16能够表达的这个具体的值。
	现在说的PFT技术是适合现在所有的大模型吗？还是只有transformer架构的，绝大部分是transformer架构le的。这个也是我要给大家讲uni PLT这个图的原因。就我们看uni PELT，这个更大一点，uni PELT这幅图，这三类基本都是围绕着transformer base的这个大模型来的。但是你像IA三这种套路，其实也能往别的地方放，lora也是可以往别的地方放的，只不过他们一开始提出来可能都是为了大模型。就像Laura一开始那个paper写的很清楚。微软它是为了大模型来做的，但是也有同学把它做到了stable diffusion，用到这种扩散模型上，它也能用。因为罗A的核心是说你有一个高维的权重矩阵，我可以把它用low rank拟合出来，这样其实是很好的。
	量化技术有相关的入门资料吗？这个同学想研究多深入，量化技术没必要花那么多时间学。这个是我的一个建议，就是量化技术用就好了。量化技术就那几条路线，有的是去直接解决这个存储本身的数据类型，就像我们看到的int 8之类的，但它一定会降低精度。而有的模型它对精度要求高就哪怕我们看这个float 8，它都会有E5和E4两种区别，它精度不一样。然后还有一些量化技术可能是或者说一些模型压缩的技术，可能是在剪枝、蒸馏等等方面。
	对，但我理解你问这个是不是想说怎么样用更低的成本去运行一个大模型，这个我们后面会去讲的。因为我们其实就是不是要求只有16GB，就充满了这样的一些没没折中，就是用小显存跑大模型的场景。但是它会有这个性能上，这个性能就是指比如说它跑得慢，或者说它跑的没有那么大的显存跑出来的效果好。但是要能跑起来，那你如果有更好的机器，更多的资源，那你去复刻这个技术，去改一改参数，就应该能跑得更好对。
	IA3其实是也加了一个adapter，这个同学你要理解IFIIA3也是一个adapter，只不过他是改的这个激活层。激活层是什么？就是这这个应该激活成，你应该可以搜一搜就知道了。
	6B的模型用今天的微调方式至少用多少GPU显存？这个同学你是这样的，第一这个取决于这个模型本身它实现的怎么样。就比如说你的今天的微调方式具体指的是哪一个我还不太清楚。然后你说这个比如说ChatGLM的6B那你用16GB用它的量化版本就可以了。微微调了，就是16GB就可以用q ora来微调chat GM6B了。我不知道这个有没有回答你的问题，对。实际使用上是不是10B以下基本没法用，就看你实际是用在什么地方了。有很多公司是在用chat GM3的6B或者2的6B在做一些应用的。
	几十亿的参数不小了。我觉得大家可能被大模型给这一波给搞得太就是就千亿大模型这个宣传太那啥了。你要知道以前的internet，以前的微基金，哪怕以前的retina net这些主流的工业广泛使用的CV的网络，也就是几千万小几千万参数。然后大语言模型它的知识要学下来确实需要很大。但是你实际用到一些小的具体场景任务上，其实是有很多空间可以优化的这也是刚有同学在问量化技术，我觉得可能核心是把这个大元模型怎么样能够蒸馏也好，压缩也好，然后这个lora也好，用到这个小场景里面。非常重要的未来的一个发展方向。因为只有这样，这个AI的大模型，AI的agents才有可能像移动互联网一样的有1万个APP出来。
	哪些场景使用提示词微调无法解决？要引入开源模型的微调，首先要看你用什么模型，这个很关键。如果你已经有办法在你的场景里使用GPT4，甚至GPT4的付费的版本，就比如说它的各种assistant API你也可以用。那么你可能优先要搞定的是把你的提示词写好，把你的提示词模板写好，把能切入好。如果你一来你用不了GT4，你在国内只能用ChatGLM的6B那这个基础模型就差了N倍，就差的太多了。那这个时候你又不可能去从0到1训练一个大模型，你就在这种场景下你就得去微调了。就是在这种几十亿的级别的开源模型上，你可能就得微调了。因为它不像千亿或者上万亿的GT4那么强。
	看大家还有什么问题吗？
	商用模型太贵，也是自己微调模型的原因。是因为大模型运型的成本就是高。想微调模型实战，我们后面的实战会用hugin face上面的开源数据集的。Hugin face上面的开源数据集有几万个了，有数据集的这个大家不用那啥量化是不是有损的？量化肯定是有损的，我们也没承诺过他没有损，量化是有损的对。
	看大家还有什么问题。对，有一个同学在回答开始数据隐私的问题，这个确实是我我不能在那个有也有录播的去承诺什么什么就没问题了。但是有的同学可以作为一些建议去去举例子。对，因为数据隐私是一个非常敏感的问题，然后不同的场景下有不同的要求，然后你做不同的业务也有不同的要求，你做to c的应用更是有一些监管机构的要求。还有同学问chat GM6B用今天的q lora的方式，至少用多少GBGPU显存？这些应该都是有有可以查到的。咱们可能之前都没有实际去跑过，这里都写的有的，上次的给大家看一眼。
	这里有支持这个量化的版本。对，然后我记得他的by two应该是。不过我们会在实战课的时候给大家都都开始去做这些事情，大家倒也不用急。
	他可能改了一下文档，对，这里有一个参考的显存容量。它的P20v two如果不加这个量化，是21GB，那加了量化会好一点，只需要7.6，这些都是官方的数字。但是你去实际试的时候，会有一些这个可能不同，因为这个是指加载模型，你还要根据你的训练数据，这一次训练数据加载了多少的这个训练数据进来，有一定关系。
	IA三这么好用，那些为模型为什么不用IA3去微调？是这样的，这个同学就是IA3也有很多人在用，就是我没有我没有任何主观的评价，I3也是hugin face上支持的一个还蛮好的。蛮好的一个方法。对，I3的这个adapter，然后这个ADA的Laura，对，就我们讲的大部分都是可以直接在hugin face的PFT上库库上面直接用的，这样也方便大家后面因为实在不会把每一个PFT技术都给你来一遍，但是会告诉你其他的应该怎么样去用，那你其实就能够举一反三的去用了对。
	会讲自己用ChatGLM生成数据集的方法吗？这个应该不会。数据集我们应该都会去使用这个哈根face上面的开源的数据集。然后之前的应用开发课用GPT4生成过数据集，那个是因为本来基础模型要好很多对。
	好，我们今天的课程就先到这里，看大家有什么问题我们可以在群里面再来问，然后我们在群里面再来沟通。好，谢谢大家。