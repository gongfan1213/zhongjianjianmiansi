	大家能看到了吗？我这边好像没刷出来。
	好，我这边显示也正常了。行，那咱就正式开始，不好意思，耽误了十多分钟。今天要讲的这个内容，混合专家模型其实是一个既是老技术又是新花样的一个技术门类。我们今天一起来学习和探讨探今天主要内容是分成两部分。一部分是讲这个混合专家模型MOE它的一个最关键的几个里程碑，就是跟我们现在的要讲的大模型的MOE结合起来的话，有几个关键的里程碑。然后后面是讲MOE跟大模型结合之后的一个技术发展。
	当然我们能很明显的看到google的技术统治力是非常强的，尤其是在大模型这一块，虽然没有做出一个很成功的杀手级的大模型应用，没有做出ChatGPT。但是他们在大模型的技术积淀这一块是非常有话语权的。我相信前面我们的一些理论课的内容大家也能感受到。
	在正式开始之前，回应一下下午群里的一些问题。就是咱们其实把之前做的一个预览的课程内容，当时初步规划了一个八周时间要讲什么，然后大概的顺序。但实际的这个讲课的过程当中，我发现其实大家还是希望做一会儿这个实操的内容，然后学一些理论的东西，然后交叉着来。所以我们把整个课程的理论部分就拆解成了好几次来做这个授课。包括后面的录播课，我们也都是这样交叉交叉着来进行的。
	然后关于华为的升腾和deep speed，我们应该会在这周日有3个小时的时间。我们会去讲一讲怎么样用华为的升腾来部署我们的ChatGLM。然后deep speed我们也会做一个比较全面的一个介绍。然后在下周三，也就是我们的最后一节课，我们会去讲妈妈2。但其实我们在规划nama two在规划nama two作为我们课程的一个重要内容的时候，今天要讲的这个mro他还没有发布，就那会儿还没有发布这个8乘7B的混合专家模型的版本。但是在上个月底的时候发表出来之后，包括几天前他们应该是在1月10号左右才公开了这个8乘7B的这个混合专家模型的一个论文。在论文里面有一些介绍，去讲他们怎么做的这个模型，包括他们做的一个instruct模型，所以这些其实都是最近几十天的一些新内容，我们也都在快速的去做跟进的。
	课表会及时的去做这个调整，包括我们有提过后续有一些新的模型出现，也会放到加餐里面，所以这个大家不用太过担心。我们今天来首先来看看这个混合专家模型。他最早的发展其实是从我们的1991年的时候就已经提过这样的一篇paper了，去讲自适应的局部专家混合这么一篇paper。然后我们能看到前面这个阶段，其实都是一些很有名的人在推动，像这篇91年的论文是Michael Jordan和hinton一起发布的。91年的时候，我们都知道那会儿是一个什么样的AI的状态。在八九年90年初，乐坤刚刚提出了CNN可以用来做这个m list手写体数字识别，然后再再往前几年出现了反向传播的这个方法。但是整个深度学习都还没有出现，机器学习在整个90年代是具有统治地位的。所以在提出这个自适应的局部专家混合模型的时候，并不像今天这么火热的。大家都在讨论MOE，只是认为它可能是另一种形态上的集成学习。
	这个in simple learning属于机器学习里面的一个范畴，我们待会儿会大概给大家提一提，所以当时并没有引起特别多的关注，直到2013年，伊利非常高频的出现在我们的课程当中。伊利亚本身也是hinton的博士生，现在也是OpenAI的首席科学家。他其实是在2013年的时候，提出了一篇paper。这篇paper是他跟这个深度学习，跟我们的这个叫MOE叫混合专家模型第一次能够结合起来，能够把深度神经网络和MOE结合起来，是一个非常有开创性的想法。包括在这个过程当中，提到了这个就相当于有点类似于这个distributed的representation。
	我们之前学过的分布式的这个表示学习的一些思路，也引入到了这篇paper里面来，然后接着出现了稀疏门控，就hinton和jeff dean在google其实也是在google的研究成果提出来了，就是能不能通过这种稀疏门控的方式。因为整个MOE其实都强调这个门控网络，那么能不能通过稀疏性的门控网络去支持一个超大网络的一个MOE？那会儿其实就已经做到了1700多亿了，2017年的时候还没有发布这个GPT1，应该是同年发布的是transformer。所以那会儿在稀疏门控的这个网络上，是在RNN的一个变种LSTM上面去做了一个1700多亿的一个超大网络，然后在这个超大网络上成功完成了训练。
	接着到了transformer发布之后，我们才会发现MOE跟大模型也可以结合，其中以google为首的，因为他们又有资源，又有算力，又有人，提出来了，连续提出来两篇文，分别是G72和GLAM。G72的这篇文章重点是提出了一种扩展技术，当时在2020年的时候，那一年应该是GPT3刚刚发布，然后接下的提出来了一种去用MOE来训练巨型transformer网络的一种方法。当时应该是做到了将近6000亿的一个参数规模，然后还训练成功了，可以训练，当然他也用了一些在硬件方面的一些优化，然后GLAM其实是google在自己发布的T5这个sequence to sequence的网络上，去做了扩展。一直做到了应该是万亿级别，做到了万亿级别的一个语言模型，并且还成功训练了使用MOE。然后它的这个性能也非常不错，在二十多个自然语言处理的任务上都达都达到了很好的sota的成绩。所以大家可能因为OpenAI ChatGPT的火热，让所有同学都以为这个只有GPT，但其实不是整个大模型技术。为什么这个mixture出来之后大家觉得好牛逼。好像这个MOE又是一个可以让我们以一个比较小的模型去达到10倍这个模型参数规模性能的一种做法，这个其实是也有它的历史演进的一个脉络，这个技术其实一直在迭代，只是它不在这个舞台中央，不在这个聚光灯下面。
	然后switch transformer是2022年提出来的一个很有趣的技术，通过这个稀疏技术也能实现这个万亿规模的模型。至于这个稀疏技术，这个稀疏性，我们待会儿可以一起来探讨，了解。最后就是2023年12月底，也就是应该不到一个月前发布的这个模型，有myr ai公司发布的这个模型。然后在今年1月初24年的一月初公开了他们的一些论文，讲了他们是怎么做的这个8乘7B的专家模型。然后这个专家模型在跟lama two的比较上，尤其是跟娜娜兔的70B这个比较上面，是取得了更好的成绩。所以现在整个大语言模型，都趋向于一个在几十亿到几百亿这个规模上，是一个白热化拼刺刀的一个一个模型规模。因为大家都希望说能发布一个模型到开源社区，然后这个模型规模是50%左右的人，至少是或者说左右的公司是能够部署起来的。而不是说一来就发布一个千亿的第1000亿的基座模型，它不一定发布。你看GM4130B就没有发布，然后这个GPT3.5GPT4都没有发布，百亿的又更偏向于更落地，能够到应用层面上去公开的一个还不错的一个规模，所以这个是目前真的比较火热的一个赛道。
	像我们个人的开发，或者说我们自己要做一个demo，在几十亿的规模上，应该接下来24年还会有很多几十亿规模的模型会不断的去迭代和刷新榜单。因为不管是训练技术也好，还是说MOE的引入也好，都会有一些新的进展，这个是我们能够期待并且看到的。好，我们接着就来讲一讲，首先我们看一下这个mystery这个8乘7B的模型，它有什么样的特点，为什么他这么引起关注？就是所有的人为什么好像至少我身边，包括外网的很多的朋友都在关注这个模型，一个最直接的原因就是性能比较好，就是跑分比较高。或者说在一些我们已经比较熟悉的基准测试上，像这个GSM8K，做数学题的，hello sweat MMLU这些基准测试上，我们能看到它是一个8乘7B的模型。
	这个8乘7B的模型是什么概念呢？就是这个顺序我们先讲最好的，然后再看一路怎么过来的。8乘7B的意思就是说它有8个7B的模型，然后这8个7B的模型它并不是完全独立的，不然就变成八模型对吧？它并不是完全独立的，它共享了一部分的这个权重，也是attention部分的权重，还要去做一定的共享。然后它的FFN就我们还记得这个transformer这个网络结构，一个self attention，后面接了这个FFN。
	然后我们通过前面的学习也知道我们的lora adapter加到哪一个模块效果最好，加到FFN的效果最好。这个是在应该叫阿达Laura这篇paper的时候做过一个系统性的对比。就在不同的模块加上这个lora adapter，哪个模块效果最好。整个专家模型其实后面都在替换这一部分，就是替换我们的FFN的这个网络。然后myra它就会把这个注意力权重这一层是共享的，然后FFN这一部分做了八个不一样的专家模型。然后这八个专家模型它并不是说我只能用一个，我也可以决定我到底用几个。在他最新发的这个论文里面，他其实有提，他可以选2到8个，就他至少会用两个，这个是他公开的一些信息，简单来说就是来了一个问题，这个问题他会至少请两个专家来问诊，然后问诊之后给出一个答案。当然它有一个门控网络，这个门控网络也是一个神经网络。
	然后他也会去训练什么样的问题有什么样的专家来回答，然后要几个专家来回答，是这样的一个运行机制，背后然后通过这样的一个机制我们能看到它比这个拉玛2的70B首先它比70B的模型尺寸肯定要小，因为它并不需要这个七八五十六B就是560亿这么多的模型参数。因为它会共享一些权重，所以会更少。我印象当中应该是将近500G500亿的这么一个参数，总量将近。然后70B就是妥妥的比他还要多个20%多30，那GPT3.5肯定就更大了，GPT3.5是一个千亿级别的模型，至少从描述上来看是这样的。但不确定GPT3.5内部有没有使用，MOE大概率会去用。那我们从这个结果来看，它跟这个纳马2的70B和GPT3.5在好几个基准测试上，其实性能是相当的，甚至是略微领先的。
	这个是大家没有想到的，就是原来7B的专家搞八个都能达到这样的一个水平。这件事情其实会引发很多的思考，就是我们是不是需要一个那么大的模型。因为这个mixture它还是相对通用的一个模型，这个MOE做出来是一个相对通用的。但如果我们就像咱们讲微调的逻辑一样，我们只需要聚焦到一个特定的领域的时候，那是不是一个mix 7B就够了。一个7B的模型就能在一个特定领域上做到特别好啊，这个是引发后续很多思考的一个点。
	然后第二个就是在大模型的训练当中，很严重的幻觉和偏见的问题，mixture它的这个训练在这方面其实是比lama 2的70B还有进步的。通过实际的基准测试我们也能看得出来，在两个基准测试上，它取得了更好的一个结果。更详细的信息也可以去看miss tro AI这家公司他们做出来的这个详细的一个blog。
	除了刚刚提到的这些优点以外，它还有一个好处，当然通过这个模型的尺寸我们也能想得到，就它的这个我叫质效并举，就它的质量提升了，然后它的预算或者说效率其实并没有下降。在这个inference budget就相当于它推理的这个成本或者预算。从这个视角来看，六个不同的基准测试上面，然后mixture都取得了更好的表现，并且用了更低的成本，这个是一个。
	从第二个维度来说，因为lama本身也是一个开源的模型，它跟这个lama的竞争就是在开源的赛道里面去竞争。谁是在几十亿和几百亿这个规模上最好的模型，显然现在显然娜娜二是遇到了一些挑战。不过乐坤最近刚刚拿了一个全球AI的大奖，也在憋着这个大招去做这个新的lama的模型。所以大家去想为什么前两天facebook的扎克伯格又说要买新的卡，对吧？这些都是背后有原因的，也感受到了这个威胁。
	好，我们大概知道最好的这个部分是啥了，最好的或者说也不叫最好的，是最热的当下最热的这个mixture。它的这个模型，它现在在benchmark上面超过了nama two，超过nama 270B那么这个MOE到底是个什么神仙技术，为什么他好像很神奇的打开了一个开关一样，让我们几十亿的模型队了八个之后就能达到更好的效果，这个是我们要去研究它背后的一个发展的。然后所有的这些神秘的新技术，其实你都能找到它技术的一个发的脉络。然后你抓住它关键的几个技术点之后，你就会发现其实他也没有那么神奇，就是太阳底下没有什么新鲜事情。
	这个大模型发展之前，或者说跟大模型结合之前的MOE有3篇比较重要的paper，我觉得是值得一读的。就我列在这里的三篇文章可能左边这个移动的有一点问题，应该再往右移一点，显示的有点问题。第一篇就叫做adaptive mixture of local experts，就是自适应的局部的专家混合，它也不叫模型，或者你也可以叫自适应的局部的专家混合模型都可以。是当时mike Jordan还没有去berkeley，还在MIT，然后hinton还在多伦多，还没有去google，还没有自己创业，也还没有遇到A91年的时候一篇paper非常早，然后也是非常有预见性的，开了这个呃我们叫什么？开创了MOE的一些关键技术点。
	第二个就是咱们这里看到的这个learning factor的repents tions in a deep MOE，这个是A列，当时还在NYU做的这么一篇文章，第一次把深度学习和MOE结合起来。然后第三篇文章其实就已经开始尝试，能不能把深度学习的训练问题通过稀疏门控给他解决好。就相当于大家都知道学这个深度学习的这个训练有好多问题。你把这个网络变得特别深之后，梯度会消失。然后你的数据不够多的时候会过拟合。然后如果你的无限的去增加这个中间的神经网络的数量，包括这个神经元的数量，layer的数量的训练成本还会变得非常高，可能你是支撑不了的那怎么样去解决这些问题，这个是第三篇文章，hinton和jd一起去解决的一个问题。好，我们来一起看一看，这里有一幅图怎么没了？这里有一幅图应该是被盖掉了，比较尴尬，我把这篇paper打开。
	在我们的adaptive这里。就这个。就这一幅图是很重要的一个影子挪进来。
	这篇paper里面其实关键就是提出了这样的一个架构，然后这个架构叫做自适应局部的，所谓的局部的专家，他的这个概念要怎么样去理解？我们就拆解一下来理解它就好了。首先从结构的角度上来说，他提了两个东西，一个叫experts，一个叫门控网络experts。其实这个experts就已经跟我们现在要聊的这个专家有一点相像了。因为他的这个思路是延承着91年的这篇北本一起来的，他提出来了一个叫做能够有效减弱这个干扰效应的一种模型的结构。这个模型的结构它的核心就是多个独立的子网络。这已经是用的这个neural network，用的这个神经网络的一些结构来进行数据的处理和训练了。然后简单来看就是我们在这幅图里看到有四个不同的网络不同的这个network。
	我们在这幅图里看到有四个不同的网络，不同的network。然后输入就是下面的这个input，然后这里有三个不同的专家网络，然后旁边有一个getting network，有一个门控网络。这个门控网络输出的P1、P2、P3其实比较好理解。这个P1P2P3你可以简单理解，就像这门控的意思，就跟这个开关一样。他的训练就是为了去让我们后续的这个输出，这个是一个output，让我们的输出的这一部分到底要用哪一个专家网络的这个结果作为输出，就这么一个意思，就像我们的这个电路开关一样，它现在有三条电路。但我的门户网络，是只选择其中的某一个，专家把他的这个电路接通，剩下的两个专家，是不需要去做计算，他的计算结果也给不到下游的，这个是门控网络的作用，然后这个门控网络它会根据输入来判断我当前这个输入应该给到哪位专家。
	你比较好的理解就是你就像医院挂号，你现在你感觉有一个症状，这个症状只会到某一个特定的专科门诊。那每一个expert的network就是属于一个特定的专科门诊，背后有一个老专家，这个老专家处理的是自己擅长的这个子集或者说这个任务。所谓的自己的任务其实本质上是什么？我们搞了这么多的模型训练和微调了，大家应该能够理解。所谓的这个任务通过不管是free train find 2的这种范式也好，还是通过其他的各种learning为主的这种机器学习方法也好。我们都会发现一类任务其实就是一类特定的数据分布。那么getting network要做的事情就是判断当前的input，它可能是属于哪一类数据分布。
	举个最简单的例子，就是我们在这个微调的第一第一个微调的这个实战里面，有一个叫做bert的模型。这个bert的模型既可以在下游去做文本分类，也可以在下游去做QA的问答。那么这个时候如果来的这个input明显是一个做分类的，就是描述了一堆正文，也没有也不像一个question和context的结构。那他可能就直接去问第一个expert，让他去判断，他其实是需要让你去做一个分类，这个地方应该打一个四星的好评。第二个input，它的形式可能就是我问你叫什么名字？然后还给了一个context，那可能就会走这个expert next word，next network two. 
	当然我举的这个例子是简化了这个过程。实际的input不可能有这么大的差异，连这个问问题的形式和格式都有这么大的差异，它更多的可能是根据数据的一些特点，在训练过程当中不断的去调整。而这个门控网络本身它也是一个神经网络，然后背后是一个典型的激活。整个架构的提出，其实有一个大的背景。这个大的背景就是我们要用一个整体的模型，然后这个模型具备多种能力，能处理多种任务。这个其实也是我们对于AI的追求，他能处理很多事情，而不是一个事情一个AI的模型。所以在这样的一个大的背景下面，希望做一个模型能解决多种任务。而这个多种任务最终实际解决它的这个网络是背后的不同的专家网络。
	但是要达成这样的一个效果，在提出这篇paper之前是有难度的，难度是什么呢？就是这里提到的一个词叫做干扰效应，包括我记得好像是在前几天给某一个同学答疑的时候，他有提到到今天为止，我们讲P20v two的时候也提过这个marty task from tuning，就是多任务的这个from the tuning也会互相有影响。包括这个P20V2论文里面自己都提了。我在学习任务A的时候，我同时也在学习任务B那如果我要同时学习任务A和任务B可能会导致这俩分数都不是最高的，就没有我单独学习任务A的时候，我们得到的这个分数高。这个现象其实本身就叫做干扰效应。
	就是当我在一个模型要在执行多种任务的时候，不同任务的你说叫它的训练方式也好，对应的模型权重也好，都会互相影响。它不是那种一刀切的状态。所以怎么办？就是我的模型是切不开的那通过门控网络是想说，那好，你切不开我来帮你切。我这专门训练一个网络来确定什么样的任务该由谁来干，不要出现有个经典的预言叫什么三个和尚没水吃，对吧？就不要出现三个和尚都在这指手画脚，没人干活，我就只让同时尽可能只有一个人来干活，剩下的人你就别干活，当然这是比较极端的一种做法，但它是有效的，当然实际情况下，它可能不一定是说P1设置为一，P2P3就完全设置为零。
	它可能也会去做一个调整。就像我们的这个注意力机制一样，它的注意力权重也不可能说我只对一个输入响应，那其他的可能也有值，但是比较低。实际的这个值应该是多少，就是体现这个adaptive的思想。就这个自适应的学习。它会根据我们实际的这个数据，我们的这个训练数据来去调整getting network的这个权重，相当于他会去动态的根据我们的数据去调，而不是写死的。
	这个现在看起来大家就觉得理所当然。但是其实是在1991年的时候，这个自适应学习是挺先进的一种技术。因为在那会儿大家都在研究什么呢？研究这个决策树，研究随机森林，我写好了一堆判断条件。大家如果了解这个决策树和随机申请的话，那就写好一堆判断条件给一个输入。然后这个输入如果是这个呃比如说第一个大家知道这个二叉树树状结构或者N叉树。一开始input就是我的这个问题，然后根据我的每一个叶子节点，或者说我的中间的这个节点的条件，我去判断我会走哪一条逻辑。那个是当时很多机器学习任务能够做到的一个实际水平。
	但是在91年的那会儿，能够通过learning的方式去自适应调整一个模型的权重，确实是一个比较新的一个技术手段。我们把91年的这篇paper大概理解它的逻辑，就提出了两种重要的结构。一个是单独的expert network，然后他能自己训练自己的这个数据集上面去做任务。然后有一个gating network能够去做这个分拆。就是我应该让哪个专家网络的权重大一点，哪个小一点，根据我的输入来做判断。
	这个是当时提出来的最重要的两个模块，我们来回顾这篇paper它最重要的这个贡献点的话，第一就是他刚刚提到这个结构。能够把一个复杂的问题，复杂的场景，用分治的思想处理成更小更易于管理的一些子问题或者子任务。然后不同的这个问题由不同的专家网络来进行处理，并且不是一个写死的，像随机神经一样写死的这么一个判断条件，比如说年龄或者说男性女性，或者巴拉巴拉这样的一些条件。第二个就是说它能够去引入这个learning的这种机制。第三个就是对于后续的这个研究有很多的启发。
	这里有一个可视化，因为这边paper因为太老了，没有这种叫什么电子化的版本，全是影印的版本。如果有同学找到非常清晰的论文的版本，也可以提出来我们更新一下。我在这里不管是在这个阿凯还其他的一些平台上都只能找这种扫描的版本。
	这里其实就是针对三个不同的network去做了一个可视化，就相当于它有一个门控网络。然后这个门控网络，它能去判断什么样的输入，去找什么样的专家网络来进行处理。现在明显看得到这是两类不同的问题，它被不同的两个专家网络在进行处理。像上面左上角这一类问题，就由network一就expert network一来进行处理。下面的由这个net 0和net 2来并行的去做处理。这个get是它的这个门控网络的一个比值。
	整个思想就是大家记下来这个门控加专家的这个思想几乎就已经定调了。就是到今天为止，我们的这个mixture的这个逻辑，其实也都还是这样的一个大的逻辑。无非就是说大家想象一下有哪些改进的点，第一就是说我的权重怎么样分配更合理。然后我的这个专家网络要在神经网络里面怎么样去安放，怎么样去布置，怎么样去替换更合理，无非就是这样的一些改造了。所以其实91年这篇文章是非常值得读的，这个思想也非常的深远，怪不得是两个图灵奖的作者写的这篇paper。
	开始还提到了一个机器学习的概念，叫集成学习，集成学习是机器学习里面的一个子门类，或者说一个小的分类。这个集成学习有很多不同的方法，像刚刚提到的这个boosting申请都是这样的一些方法。它跟这个MOE在早期的就MOOE的这个早期阶段或者说甚至有的地方会把它认为这俩是一样的，我这里专门做了一个对比，让大家有一个理解，就集成学习它更强调什么呢？如果硬要说他们有关系，可能91年的那篇paper还有一定关系。因为91年的这个专家网络专家模型也好，专家网络也好，它更强调的是我是独立的一个子模型处理的一个子任务。这个模型就是一个神经网络，我就拿数据来训练就好了。而集成学习大部分时候也都是独立的模型。然后这些独立的模型最好的理解，你看这个transformer这篇paper里面就有提过。
	就是这个in simple的方法，就把mark head你也可以当成不同的head是不同的这个模块，他们也在学习不同的内容。这个Martin head就是一种典型的in simple的方法，因为它的输入几乎是一样的，在input是一样的。集成学习的核心是我就是三个不同的模型，我看到的都是全量的数据。你可以理解成大家都针对所有的训练集都会进行一波训练。训练完了之后大家来进行投票，然后这个投票是最终比如说我有三个不同的模型，这个投票是由最终的有一个你可以认为也有一个分配权重的。比如说三个模型ABC，然后我会让A的这个模型它的权重可能是0.1B的这个是0.2，然后这个C的这个可能是0.7，这个玩意儿通常是训练完之后写死的，这个过程叫joint叫joint model。就是我把三个模型的这个结果以一定的权重加权之后，得到了最终的这个结果。
	这个是典型的集成学习的方法，所以它的这个权重相对来说是固定的当他学习好之后，但是MOE在91年的时候，其实跟他就有不同了，他的专家模型在那会儿确实是独立的去针对这个一个问题来进行处理。但是它的这个权重其实是可调整的，就根据数据的不同，它是可以训练去调整的。然后我甚至可以做到在一个input进来之后，我的这个input的数据可能是当时我训练的时候，我的专家A他他更多的子问题里面的数据。所以我在当前的这个input可能几乎就把A的权重拉满了。然后当我给了另一个数据，可能它是C的这个数据集的分布的时候，就像我们刚刚问的这个QA和文本分类的这个例子，虽然比较极端，但比较好去解释，这个比较形象。
	那么他可能就会完全去使用C的这条链路了，因为它的整个门控是动态算出来的一个权重。所以从这个视角来看，其实跟上他们还是不同的，尤其是越往后发展就更加的不同。因为MOE的核心就是要把相当于把一堆乱麻复杂的问题通过分支的方法切出来，切成一些子问题。然后子问题让每个专家去管理他们。所以MOE的极端或者说MOE的最大的障碍就是过拟合。
	有可能你会变成为什么我拿医院举例子就比较形象，就你现在去看病，你去医院挂号，我经常会遇到的一些问题就是我遇到了一个症状。然后我问了这个医生，医生做完一堆检查之后说好了我这儿没问题了，我这科室已经检查出来没有问题了。但是我说我还是不舒服，他说那我就不管了，就正常情况应该是其他的科室他也是懂一点的，你应该推荐给我说，我这个比如说我这个外科，检查出来就没问题了，你要不去这个什么消化科或者什么神经内科，你再去看看。这个是比较正常的逻辑。但如果MOE的专家模型做到过拟合之后，你就可以理解成他不会管你的这个其他就他认为不属于他的问题，他就完全不会管。因为这个是由门控决定的，甚至可能会出现一个极端，就是来了一个输入，所有的专家都觉得不该管。
	然后这个门控网络它是可以输出成大家都不用管，然后这个输入就被放弃掉了，这都是有可能的，这个是MOE的这个极端情况。而这个instance learning它是一定会去算一遍的，算完之后再去做这个集成投票，所以门控是一个巨大的区别。针对咱们的这两种方法比起来的话，然后他们后续的发展自然也就不同了。集成学习是一个更通用的一种更更偏方法论的东西。而MOE它可以跟各种各样的模型去做结合，就包括后面我们会讲的这个switch transformer，以及我们还能看到包括像mixture这样的一些模型，其实都是在利用这个思想，一个门控加各种各样的小模型这样的一个思想。
	好，那我们接着看一下这个第二篇paper。第二篇paper是alia这个OPI的首席科学家，他当时在咱们的91年的这篇paper基础上，过了22年，做了一个改进，终于又被捡起来了。2013年其实是深度学习已经开始火热了，2013年s net应该都已经发出来了，那会儿我们看到这两边的这个图有一个非常明显的对比。然后这个图其实说的也很清楚了。我们简单看一下，就是第一左边的经典的这个专家或者叫什么混合专家模型。混合专家模型MOE，就用英语来说，MOE它是一个门控网络，左边GEX加上F1F2FN，这是N个不同的专家网络，是这样的一个模块，这样的一种模型的架构。然后我们所有的这个输入，其实针对不同的专家网络就相当于一干到底就解决完了。
	但是对于13年的这篇paper伊利亚来说，他想到了第一那会儿神经网络已经开始起来了。然后我们会发现神经网络有很多经典的特点，是会被广泛的采纳的。比如说隐藏层的概念，你说隐藏层在学什么？你说这个卷积核，它的这个不同层的卷积核，这些feature map是什么东西，对吧？然后我们之前学过的注意力机制里面，我这个不同的序列里面的这些黑这个hidden state是什么东西不知道，我也不知道他到底学到了什么，但是他是能表达信息的，他是能把一些输入的信息被抽象出来的。
	那么是不是可以理解成这个专家网络也是一样，就是我的这个专家网络不能不一定要做成一个一一干到底的这么一个模式。而是说不同层的深度。因为他已经在用神经网络了。不同层的这个神经网络里面，我每一层都可以有不同的专家网络来进行决策。然后每一层都会有一个门控网络来决定什么样的输入来给到我们的哪一个专家网络来做处理。
	这个其实是挺好的一种思考方式。大家去想象一下，以前我们人的角度就会说X是人看得懂的。然后人看得懂的这个信息由一个门控网络交给一个特定的模型去处理，处理完了之后就拿到结果。但是神经网络它内部怎么处理的？包括它内部的那些值，那些高维的向量是什么意思？没人知道。但是没人知道神经网络他自己知道。
	因为这个getting network它就是个神经网络，它就是个模型。所以你只需要把中间层的这些结果，就比如说这里的这个Z1，它是属于这一层输出的一个结果。至于这个结果是由哪一个专家网络输出的，他不关心，他只知道他是中间层的一个输出。就我们的经过第一层的神经网络的输出，那这个输出给到下一层去做计算的时候，它就变成跟X一样的一个输入了。这个输入由第二层的这些专家网络来进行处理，然后谁处理由第二层的门控网络来决定。看起来好像不是很很高级，就这个思路。但其实能把这个事儿想明白，并且能付出实践，还能做实验，这个是很厉害的。
	然后这边paper其实就彻底把这个MOE跟神经网络结合起来了。因为大家发现原来MOE可以模块化的，它不是像机器学习一样。大家知道机器学习都是形式化的一些方法，我通常一个模型是很难模块化的，但神经网络天然就具备这个模块化的一个结构，我可以搞一个全连接网络，它是两层的，三层的。我可以搞一个卷积网络，它是五层的、十层的、十二层的、15层的、1000层的都可以。只要你能训练出来，然后你的数据跟它的这个规模是匹配的，不要出现过拟合就可以。也是因为这样，把MOE就模块化了。
	模块化之后可以把这个MOE当成一个神经网络里面的一个小模块来单独训练。这样他相当于就像我刚刚提到的，两层也可以，十层也可以，100层也可以了。这个是第一个重要的贡献点，就是能够去把MOE和深度学习结合起来。第二个其实刚刚就表达就这个意思，我们学习这个输入数据的表示，除了学习这个原始的输入input以外，其实中间过程的一些隐藏层的这些状态，按照这个论文当中的说法叫做分解表示，就是factor的representation。因为他把X其实是做了升维或者降维的，然后在内部的隐藏层里面，他把这个叫做他的factor的representations，就是这里的Z1，甚至包括这里的Z2。
	还可以继续去按类似的MOE的逻辑来进行处理，那这样的话，每个专家网络它其实就可以处理数据的一部分特征了，这个是一个也是一个巨大的贡献点，大家要去细想，就左边这幅图，就算我有不同的专家网络，它处理的是一个完整的输入数据。但是右边这幅图的结构就是我把一个输入数据本身就拆成了很多层了。这个拆成很多层如果是transformer就是做了不同的多头去进行处理。然后如果是这个经典的CNN，就是不同的卷积核，它在进行不同的处理，拿到的都是输入的部分信息，而这个部分信息可以叫做它的分解表示，我叫这个factor representation都可以。
	这个思路打开之后，其实MOE本身就不再像是机器学习时代的那种固定的模型了。而是它就是一个可以在表示学习里面跟其他网络一样的一种处理手段，那它就变成了神经网络里面的一种模块了。然后第三就是因为它本身可以去做模块化的处理，那他就可以去更高效的去处理一些复杂数据了。因为他不需要就跟你买东西一样，不需要买一整整整块了，我可以只放一部分的这个模块，甚至我在不同的层，我的专家数量都可以不一样。以前的逻辑可能就是我要八个专家，八个专家都得是就是都得这么大的。
	那现在我的这篇paper，我可能第一层layer，我可能专家不需要这么多，可能我到越到后面专家越多，或者反过来都行，就灵活度也变高了。实际情况就是，他在这个paper里面也做了大量的实验，就包括这个专家是被怎么分配的，就是我们的这个门控网络是怎么样去分配专家的。然后还去做了单个的single expert，这个单个的专家和混合的专家，以及把这个专家和第二层的输出直接去做连接，去做了各种各样的对比。
	通过对比发现首先如果我们不去做任何处理，原先的这种处理手段，可能你把layer变身，它的效果还会变差。但是通过你把这个因为原来的整个深度很深的时候，你只有一个门控网络，那现在因为你每一层你都可以有单独的门控网络，它可以自行的去判断每一层要给谁，甚至给谁的小的模块的网络，它也都是可学习的，而不是一整套去做训练的。所以它的整个对于数据的适应性也会更强。通过这个对比，他就会发现，其实拆散模块化的去做这个MOE在神经网络里面是行得通的。然后在不管是在图像任务上，还是在这个语音识别任务上都是可行的。这个DMOE就是它的deep MOE的意思。所以这个是在当时13年提出之后，非常有意思的一个进展只不过当时也是受限了资源的原因，没有大量的GPU，13年没有去做非常大规模的实验。
	在m list和一个比较小的语音识别的数据集上面去做做了这样的一个对比。接着，我们就会看到17年的一个大的进展了，因为17年是transformer提出来的这一点，但这篇paper肯定跟他是并行研究的了。然后17年的时候已经有比较多的GPU了，然后大家也都在探讨一个问题，就是是不是就应该这么讲，首先在17年的时候，renee t已经被提出来了。
	在CNN的这个领域里面，我们会发现计算机视觉通过残差网络可以把我们的神经网络或者说卷积神经网络的对于知识的容量，对于下游任务的处理能力再提升一个高度。因为它能学得动，就最大的问题是学不动。通过残差网络，我可以让我的模型学到50层、100层、1000层，那我就能给更多的数据让他去学。这个是所有的learning的模型，learning的方法一个很重要的逻辑，也是我们一直在讲的这个scaling law。就是你的数据越多，然后你被有效的学习了，你的模型的能力就会越强。这个图应该是放到后面了，因为后面有一个更大的模型，就是我们一直在提的这个逻辑。这也是伊利亚在搞OpenAI的GPT的时候，也一直坚信的一个理念，就是从12层的到100多层的，现在可能就更多了。
	这个逻辑是行得通的，如果我们不考虑它的经济性的话，只要你能够把这些知识灌到这个神经网络里，那它就能够把这些知识成功的存下来，或者说表达出来。下次如果你要问一些类似的问题，只要是在他的学习的范畴内，他就能够给你很好的回应。这个是一个很大的逻辑，这个大的逻辑在CNN上面验证成功了，但是在RNN上面一直没有。我们知道要去做这个RNN的训练，有一个天然的问题跟cn一样，就梯度消失。然后这个梯度消失没有特别好的办法去解决。一个大家现在都不用这个传统的IN了，都用的是STM，这个网络也起了很久了。但是LSTM即使被提出来了，他也没有办法说彻底解决这个梯度消失的问题，它也一定程度上可以解决，就相当于可能以前这个序列长度是十几，到20就学不动了。那现在可能有了STM可以做到30、40，今天了可能那你要再往长了去训练就不行了。
	那这个怎么办？在transformer提出来之前，这个应该是并研究的这篇paper发表的时候是看不见transformer的。所以我把它放到了前半段，没有放到跟大模型结合那半段。那怎么办呢？当时就在想，不能用transformer来学，能不能用MOE来学，这个思路其实他要解决的问题就是我用MOE来解决我的不能把知识学进去的问题，就提升我的神经网络能够接受的上限。而这个神经网络是一个INN的神经网络，在当时其实是非常非常重要的一个举措。
	第一是因为有前沿研究的动力和实际的实践的水平，怎么样把一个1370亿的RNN给训练出来，就是一个到现在为止，其实也就只能这么干了。我们现在说的千亿模型，万亿模型都不是IN了，都是transformer。这个大家应该知道。
	然后因为transformer它天然不需要这个序列了，它通过位置编码来解决了。但是如果是RNN，它还是有这个序列性的，所以它首先要去解决的就是千亿的RN怎么训练怎么训练呢？引入了MOE来进行训练。然后这个MOE能够实现训练有一个条件，或者说有一个假设，就是这个稀疏性的假设，我们其实又回到了91年的那个逻辑里，只不过他实现方式不一样，简单来说就是所谓的稀疏性的概念就是指以前我是说针对一个完整的输入数据，它会有不同的子任务。不同的子任务由不同的专家来进行解决。现在大家就是一个整体，是一个1370亿的整体。这个整体里面专家无处不在，因为它都是一些小模块。大家想象一下上一篇paper我们讲的它是一些小模块，专家无处不在。
	那什么叫一个专家呢？什么叫一个专家模型呢？其实就是你从输入的X到输出的Y那条链路上，被你选中的那些MOE的模块，构成了解决这个问题的专家。
	这个逻辑大家去捋一捋，相当于你针对一个特定的X然后就像dropout一样，就是大家训练的时候用过dropout就理解了，做炮的也是伊利亚的paper，你在这个训练过程当中，不是每一个神经元都需要参与计算的。或者说他的贡献，有的神经元的贡献几乎为零，因为它那条线乘出来就是一个无限接近于零的，或者说经过那次激活之后就没了的。那么这条线路上就会发现，真正再去响应一个特定的X的时候，假设这个1370亿的参数里面。可能某一个特定的X实际有作用的参数可能只有10分之1甚至更少。这个10分之1甚至更少就牵扯出了稀疏性的概念。包括咱们今天最后要讲的这个metro也是这个逻辑，它叫稀疏的MOE。
	SMOE其实从2017年这篇hinton和jeff定的这这篇非常有名的SLR的文章开始，就把稀疏性概念给引入进来了。然后这个稀疏性的概念核心就是我们刚刚提到的哪些专家，就是哪些模块。就像我们这儿看到MOE layer一样，哪些模块被激活了。这些被激活的模块是通过gating的network门口网络来进行激活的那它的权重就会在这个特定的输入里面去被放大，它那条线路就会真的被选中。这样这里的右边这个被展开的expert 2和这个expert的N减1，它俩的结果被成功的放上去了，剩下的专家都没参与，这个叫稀疏性的逻辑。
	这个稀疏性要是一个是一个设想。这个设想在这篇paper里面取了一个特定的名字，叫条件计算compute叫什么？我忘了他的这个英语单词当时取了一个什么样的词，条件计算它的核心就是刚刚讲的这个逻辑，就是针对一个特定的输入或者叫特定的样本。真正对它的这个样本进行响应的其实只有网络的很少一部分，而这一部分组合起来的就叫做响应这个输入的experts或者专家。
	如果是这样的话，那就又又变得很有意思了。是你会发现如果是这样，我们其实要做的事情就是原来的这个ASTM你在训练它，你训练的是什么呢？训练的是这些隐藏层，就是这里的红色方块，红色方块里面的这些内容，隐藏层的状态。现在因为我们有了这个MOE的layer，那这个MOE的layer其实就可以变得很长了，就跟就跟我们去搞这个CNN也好，搞其他的深度神经网络的时候一样，我可以把它做的非常多。因为把它做的非常多并不影响我的这个，应该这么讲，因为整个安它就不太可能做的层很深，所以他只能去做横向的这个扩展。然后这个横向的扩展不会增加它太大的计算量。因为它实际去算的时候，并不是每一个experts都会参与计算，大部分的experts都会被这个权重给刷掉。所以它的实际计算量也不大，就相当于实际在做训练的时候，你把做port设置成了0.5甚至更低，那它就有很多的生均就不参与计算了。
	那么这种方式是OK的，并且在实验上面也取得了成果，然后这个Y其实这个Y的这个公式，其实就是刚刚表达这个意思。这个GX是指它的门控，这个EX是指它的专家网络。然后GX针对每一个EX都会有一个权重，然后大部分时候这个GX它都是不需要背后的EX去参与计算的，因为它只会响应一部分的experts，这个是从训练结果来的。但这里又带来了一个别的问题，因为我们刚刚那个描述里面已经想象得到了，就是针对一大批的数据，可能有的专家就特别忙，有的专家就几乎就是闲着的。这个也比较符合实际的情况，不管是任务的总数的分布也好，还是说这个expert的训练过程导致的强者越来越强，而弱者就不会被挑出来，因为他输出的效果不好。
	这里其实就引发了另一个挑战，就是在训练阶段的时候，怎么样能够保证大家的token是尽量平权的。就相当于我们不考虑实际已经训练好的模型，是考虑怎么样给MOE的这个神经网络去做训练。因为他要训练1000亿的这个IN怎么样去做训练。这里其实当时就引入了一些很有意思的技术，其中一个最大的技术就跟我们做互联网的这个web server去做负载均衡一个逻辑。就我尽量让我的token或者说我的输入法X能够比较平均的给到每一个expert，不然的话就会导致我们刚才说的强者越来越强弱者可能在刚刚训练的前几个x apple这个airports之后就不会再参与了，就永远都不会再参与了，他就已经下桌子了，他可能几几乎不会被激活了。有这样的一种技术手段来做相对的标准的一种多专家的这种训练，这个是咱们能看到的。
	然后在实际的这个叫做结果上面来看的话，17年的时候，这种各种各样的机器翻译，包括transformer提出来的时候，也在机器翻译的这个任务上去做对比。因为我们也提供在17年的时候，自然语言处理里面最重要的任务就是机器翻译。只不过大模型出现之后，包括我们看到的这个后面的一篇paper，GLAM这篇paper出现之后，机器发现这个任务就不再成为大家去刷榜做比较的一个任务了。因为多语言学习变成一个可解决的问题了，什么意思呢？
	就是我们看到整个论文里面的benchmark，通常都会比较的是两个语言之间的翻译。比如说英语到法语，英语到德语，之前其实也提过一回，就是整个不管是神经网络也好，还是说早期的transformer GPT e也好。他们去训练这个机器翻译，都是训练特定的两个语言对儿之间，而且是英译中是一个模型，可能中译英又是另一个模型，没有办法去做到一个语言模型完成任意多个特定语言之间的翻译，这个是做不到的，或者说做到效果是极差的。
	在17年这边paper里面也是同样，它是比较的在WMT这个比赛上面的这个数据集里面，用英语到法语和英语到德语的比较，训练了2048个experts，就是我们刚刚看到的横向可扩展的非常多。然后2048个experts在跟其他的一些当时的sota的模型比起来，其实是成绩非常好。甚至在BLEO上面是超过之前的这个模型的，这个其实是非常强。
	就事实说话，就相当于之前的这些网络，不管是用了这个简单的注意力机制，还是用这个强化学习，这里有这个deep attention，就是啊因为已经17年了，那会儿是有attention的这个network。但是直接用这个专家网络，然后在标准的STM上面去做训练，也算是一种以力破巧的方式，能够做到非常好的一个成绩。并且其实他并没有增加额外的太多的计算量，它的计算量还是保持在同一个水平，甚至还花的更少，在英语到法语的这个数据集上面。所以这个是google其实在MOE方面一直在推进的一些研究进展从我们最后看到的17年的这篇片，包括不同的experts他的表现，我们看到在experts的数量增加的过程当中，其实它的训练还是相对比较稳定的。并且增加了experts的数量之后，它的效果还会变得更好。这些就是具体怎么训练的，MOE几千个experts在STM上面怎么训练的？这个需要的资源相对来说还是比较多的，可能咱们不一定能去做复现，但是这套训练的逻辑被后面的大模型也都逐步的把他的这个方法论和这个思想也都继承下来。
	对，有个同学问专家模型需要人手写出来吗？不用，就是我我们从91年的时候就不用了这个东西他是用的神经网络，他不需要写，他就只需要把这个数据往里丢就好了。然后你做好这个数据，就所有的这些深度学习的方法，大模型的方法。为什么说数据很重要？质量数据的质量很高。因为你去做数据打label打标记的过程，就跟以前那些老专家写规则是一样的。但是老专家写规则会有偏见，做数据的偏见相对少一点，但也会有然后learning的方法。就是说你只要数据做得好，标注做的是不是太大的这个偏差，它自动会把它学出来的。这个是所有的learning方法的逻辑。
	然后从最开始91年的整个模型是一个神经网络作为一个专家模型，到13年我们发现其实每一层都可以有自己的这个专家网络。一直到17年发现我因为有稀疏性，有条件计算的这个逻辑，我把它扩展到了2048个，几千个experts它同样是有效的。然后17年的时候能做到这件事儿，是因为GPU已经出来了。这篇paper应该是在GPU集群上面去做的这个训练，然后这个模型容量提升了1000倍，这就是我们开始提到的他的初衷就是怎么样让RNN系列的这些模型能够把模型容量提升，能够学更多的知识。这个其实是我们看到的17年这篇paper，用稀疏门控的MOE来做超大模型一个非常好的一个实验验证，并且这个方式是在后面的大模型也是有继承的，接着我们就来看看MOE在transformer出现之前，大家都在搞研究。从一个独立的模型到深度神经网络当中一个小的模块，再到可以通过在RNN里面做到千亿级别，仍然是有效的，因为有稀疏性的知识。可以实现条件计算，那再下一步怎么做对吧？我们的大模型GPT3走起来就是1750亿，然后后面还出现了万亿的模型。
	对于一个google有这么多GPU集群和TPU的一个公司，也一直在研究前沿的技术。他们又是怎么样做的这个大模型和MOE的一个结合。我们来看一看，就第一篇要讲的，就这幅图，叫做这个机芯儿的，这边paper名字就很恐怖。然后这个基线的和后面的这个g language model，GLAMG language model这两篇paper都是由陈志峰老板去主导的项目，他都是通讯作者在工作内部，然后一篇是去侧重讲怎么样去把模型扩大，有一套技术方法论和框架。那一篇是直接做出来了一个超级大的模型，做了一个一点几万亿的一个一万多亿的模，应该叫一点几万亿的一个模型，就一T的模型。怎么做到的？其实也是都通过这个MOE来实现的。
	然后整个去做这件事的一个背后的动机，其实就是这个scale store。就是只要你的模型能够有效的去学习这个数据，那么你把模型的尺寸做大，它的性能是会提升的。而其中有一个能够支撑它的背后的原因就是这个稀疏性，就是包括像现在这个ministry也是一样，我们的模型并不是说每一个输入的数据都需要那么大，甚至这个稀疏性也能支撑我们做QLA。就是我们并不需要在一个特定领域的应用或者任务上的应用，需要全量的参数，需要一个密集型的全量的参数，而只需要一部分的参数就好了。而q Laura也好，或者lauda也好，这一类做微调的方式都是去找出那些参数。不知道这个逻辑大家现在能不能理解，其实这些技术背后有很多理论支撑，都是一回事儿。
	对你只要整明白了，那么MOE也是一个逻辑。只是说MOE，你可以想象成MOE就是把咱们的这个当然不能完全这么讲，但是可以做个类比，就是MOE就是内部有一堆的专家模型，然后这些专家模型某种程度上就是一个的lw rea的adapter，但是不完全一样，因为Laura apt是旁木做出来一个，但是是这么个逻辑，就是它的一组MOE的模块就能完成一定的任务，而且这个比例并不高。但是也是因为这样，所以我就可以不断的把新的任务，新的数据集丢进去。它就可以再找一些新的参数来做训练。这个训练的参数它会自动的去做去做这个挑选，这个挑选的这个事情由门控网络来学习，因为有一个单独的门控网络在训练哪些数据应该交给哪些experts。
	听起来很神奇，很玄学，这就是为什么越来越像炼丹了好，我们接着看这个GC2的那G72的这篇paper，其实它就是展示了进一步展示了在20年的时候增加参数量就能提高性能。然后这个机器人的整个项目组，它其实是一个项目组。后面发生了一篇配本，就是在干这个事儿，就是能不能把transformer这个模型扩展到超过6000亿，实际上他们后面的palm也做到了，就是那个PALM google的第一代大模型应该是5400亿。整个G72的这个组就是想说现在说former的结构已经出来了，我们就不折腾这个STM了。The transformer这个self attention的机制，使得它能更好的去学习语义。通过GPT1、GPT2、GPT3 bert等等这些大模型基于transformer的结构都证明了这一点。这个transformer的网络结构是比单纯使用LSTM好的。那我们就用transformer，然后在transformer里面，transformer也是一个神经网络，能不能把MOE加进去，然后让我们的transformer变得更大，这个是第一个命题，就是让transformer的模型参数能不能干到6000亿。
	第二个就是说干到6000亿之后，我能不能尽可能少用一些资源去实际去运算。因为我们现在都知道，你要去跑一个GPT3，这个1750亿的模型，你需要消耗的资源就巨多。你要实际去生成这个内容的时候还会非常慢，因为它的计算量很大。
	那你如果做到6000亿，那你这个资源消耗会不会特别离谱，能不能尽可能的不要这么高的资源开销？一个最直观的感受就是你虽然模型这么大，但它是极其稀疏的。所以针对每一个特定的输入问题，它都可能只需要计算，比如说600亿甚至更少的参数，就能完成整个的计算过程了。这个是背后的一个逻辑支撑，所以要去做这样的一个事儿，这个GHR的确实也搞了各种各样的对比，他这里最多就是做到了6000亿，我们在这幅图里面看到600亿。最小是12.5B，然后基础的这个模型应该就是它的这个T1还是T5，我这里有点不确定了，这个T应该是他自己做的那个encoder decoder的transformer结构的那样的一个模型。
	然后在这个模型里面我们能看到针对这个不同的大的模型都去做机器翻译的任务。然后在这个BLEU是一个机器翻译上面的一个指标。随着这个规模的增长，从125亿或者说从最开始的23亿一直增长到6000亿，它的这个翻译质量是有提升的。就这里的这条线是有提升的。这个Y轴是一个机器翻译的指标，叫BLEU是有提升的。
	你可以简单理解成模型的性能或者准确率。所以你增加模型，它是就我们提的第一个假设条件，就是你的模型变大了，它是真的把知识学进去了，而不是说单纯浪费了一堆的能源或者资源。所以通过这个指标，我们能看到模型在变大。然后下面这条线呢是数据的样例，就example每一个语言的1 example，这里是能看到通过更少的单个语言上面的样例，或者单个语言上面投入的资源。其实模型学习到后面的阶段反而还效率提升了。
	我不知道这个逻辑大家能不能理解，就是他要同时学习好多种不同的语言，又要做机器翻译。以前可能小模型的时候，我学会一门语言，它需要10亿个数据样例才能学会。然后大模型反而需要1万个就能学会了。这个其实你可以理解成这个6000亿的这个模型，除了学习这个语言本身以外，语言和语言之间的一些语义信息。他可能在某些MOE的layer也好，或者说MOE的模块也好，共享的一些权重或者参数，所以他不用学那么多遍了。举个最简单的例子，就是以前的小模型它可能没法学习那么多种语言，它的模型容量有限。你现在变成了一个超大的模型，相当于你变成一个很聪明的人，你学会了这个中文，学会了这个英文，然后你再去学德语跟法语的时候，因为你大概了解了，比如说英语跟法语有一些相似的地方，你就不用从头再去学了。有好多词其实是博来词，或者说这个可以演化的，然后有些语法上是可参考的，所以他反而学习效率是提升了，这个是极限的带给我们的一个非常有意思的一个结论。
	就说明首先大模型是有效的，然后大模型的学习效率还是提升的。也因为学习效率提升了，所以综合来看，反而单个语言的学习成本还下降了。它的这个实验里面有一个统计，就它达到所谓的6000亿模型的最佳翻译质量的需要的时间是他在当然这个资源很恐怖，因为是6000亿是在2048个TPUV3上面训练了四天的时间，然后他他应该有一幅图我没有截出来，就是他有一个训练的成本的图。花费的这个成本，其实相当于22个TPUV3的一年的使用量。这个现在已经成了一个很很很什么很标准化的一种对比成本的方法了让大家去聊GPT4GPT3.5的训练成本，会聊是等于在多少个A100项训练一年的成本。
	他的这个772的这个大模型，在做语言翻译的这个大模型是22个TPUV3用一年的成本。它的一个实际对比是说，如果我要做这个小模型，因为小模型没法做这种通用的语言翻译，就是没法说我掌握这么多门语言，他就得训练100个双语模型。你包括像ChatGLM的早期版本，我不知道他们现在怎么样了，都只强调是中英模型，它没有强调其他的语言。就是因为模型容量的原因或者其他的原因，它只能做双语模型，他不能做这个全语种模型。而这个6000亿的模型，它是做了一个全语种翻译的模型。所以你相当于拿任意的两个语言给他，让他去做翻译，他是可以做到的。所以他要去对比的就是小模型，那小模型就得训练，因为它的选的这个语种的数量，它对比的是需要训练100个双语模型。
	100个双语模型需要29个GPU训练一年。所以它这样对比下来，它反而不管是单位的这个语言需要的数据集的成本，还是总体需要的算力的成本是都下降了。这个是一个很好的探索，相当于为什么讲现在去搞这个翻译问题，机器翻译问题已经不再成为至少第一梯队这些AI公司的一个探索方向了。大家也都不会拿翻译作为benchmark了。其实就是从这个阶段就开始了，就从2020年左右就开始了。大家发现再去讲我在什么英法翻译，然后英德翻译、中英翻译上面做的多好。意义不大了。因为我可以用另一个方式，用这个足够大的模型就能够去打败你了，并且我的效果还更好。刚刚也看到那个数据了，所以现在大家就做的更偏向于所谓的语义的部分或者智能的部分了，而不再只是一个纯翻译的活了，OK那么72的它具体是怎么做到的对吧？刚刚讲的都是效果很牛逼，6000亿，然后是在transformer上面去搞的，搞了一个6000亿的transformer。
	但具体怎么做的呢？我们可以看它这个经典的演化过程。首先我们左边这幅图应该是所有同学都烂熟于心才对，因为这是基础中的基础了。然后左边这幅图是transformer的encoder结构，包括我们后面的几篇paper都是要在这上面做文章的，所以要这个transformer的网络结构不熟的话就麻烦了，然后transformer的这个encoder结构非常经典，一个对这个输入是两个in bedding，一个是对文本的embedding，它叫做input embedding，一个叫positional embedding，就是它的位置编码。
	这位置编码我们其实在课程当中也有讲过，就包括我们用transformers的这个库去find to这个bert模型的时候，其中有一个去截断，把这个QA问题大家不记得可以回去看啊。因为我最近正在录播，刚好又把那个讲了一遍，代码也有一些注释的增加，把一个长超长的输入截断成两个输入，截断成两个数的时候，有这个offset mapping去找它的位置，就类似于position in干的事情，就知道原来一段话里面的每一个token，它的起始位置是什么，它在原文当中的位置是什么。这两个编码一起给到transformer的encoder，然后给到encoder之后，通过mark head attention，就head 1、head 2、head三多个head attention。然后可以变成就是有一个注意力层，这个注意力层给到这个FFN这样的一个全连接的网络。最后输出的这个结果，就是我们的一个经典的transformer encoder的结果。而这个结果会给它下一层的这个transformer encoder，一直到最后一层的transformer的encoder，会给到我们的这个decoder的每一层。
	大家还记得transformer那个经典的网络结构，这个就是啥也不干的17年那篇paper attention就是OUD的里面提出来的结构，之下的做了一个什么事情呢？我们看他在这个MOE transformer encoder，就这里做了一些调整。第一个调整就是在我们的这个encoder里面，输入这个地方不变，注意力层都不变。但是针对我们的FFN去做了调整。大家看啊还是一个门控网络加上多个这个FFN。为什么说刚刚我要把这个FFN讲成全连接网络呢？因为其实所谓的刚刚提到那些MOE那些专家网络，他们在LSTM里面也好，他们在伊利亚的提到的这个DMOE里面也好，其实就是一些全连接的神经网络。而绝大部分的专家模型，现在在大模型的这个语境下面提到的专家模型也都是在FFN这一层去做了多个多个FF。
	然后这多个FFN，你就可以理解成，首先它不用做多个transformer了，对吧？因为按照1991年的做法，就是我得搞多个transformer encoder，那个多多浪费资源，对吧？你其实就通过一这个伊利亚那篇paper大家就能理解了。其实在里面去换一个模块，让他去学习这个过程当中的部分特征，他也是能够去解决问题的。
	然后这个就是伊利亚带给后面的人的一些你说这个贡献也好，思考也好，启发也好，那么MOE其实它的在机器2的里面结构就这样的，甚至后续的很多大模型的结构也都是基于这个来的。所以google是在整个大模型属于深藏功与名的一个状态，现在产品上做的一般，但是技术上确实做了非常多的前沿探索和启发，所以整个MOE的这个transformer encoder其就这样的一个结构，把它的一个FFN做成了多个FFN和门控网络的一个结合。而这个结合你就只看见一个被替换掉的FF的模块，就跟91年的那篇paper也是一样的一个思想。
	然后有个同学问ad和norm表示什么？会不会变化？不会的，这个ad和norm就是经典的全连接层输出，要做激活，要做规范化Normalization，这个是一个标准操作，是一个基操。如果不清楚的话，回头可以发一些参考资料给大家。
	这个属于一些神经网络的基础部分了。这个神经网络大家会去看啊，它的这个MOE设置的还挺巧妙的，它不是说我把所有的FFN都换成这个MOE了。大家去看它的这个变化其实是隔着来的，就相当于135的FFN被换成了这个专家网络，246的还保留。所以你们看这个细节，这里有一个二分之N乘就相当于按照标准的transformer网络的话，就是一个六层的encoder。这个六层的encoder里面的135的FFN被我换掉。好啊，26的继续保留，所以这个地方展开，就是它会把这个FFN换成这个专家网络之后，再直接输入一个没有经过改变的encoder的下一个模块。再下一个模块就会被调整过来。
	这个是在单机上面的一个改进，或者说在单个机器上的一个改进。但是我们都知道6000亿肯定不是一张卡，一个服务器能训练得了的，那怎么办？对吧？怎么样去能够做这个训练？所以这里就涉及到了其实TensorFlow的分布式，或者说在这里面也有一些分布式训练的方法，包括像用这个大模型，后面微软也做了deep speed，就这些分布式的这些方法里面，把分布式训练也用到了这个绩效的里面来。
	就各种buff加成怎么训练的呢？这里其实是做了模型并行，后面我们再讲这个sweet transformer的时候也会提到各种各样的并行。这里先提这个模型并行，所谓的模型并行是什么呢？就是分布式训练模型的时候，有两种并行方法是最常见的，而其中又以数据并行更为常见数据并行更为常见。但是它也只存在了这个深度学习的那几年，现在的大模型时代，这个数据并行也搞不定了，为什么呢？
	就是数据并行的意思就是比如说我有十张卡，每张卡里都完整的加载了我的一个模型，一个深度神经网络，然后我要去做训练。比如说我的best size，现在单张卡的上限就是best size等于8。那十张卡其实是可以做到80的。但是这个80我希望做的事情是这80是放到一个bach里面，相当于我这个十张卡里的权，这个权重都是完全一样的。然后我把这十份不同的数据，每一份都是八个，然后给到我的这10个GPU。10个GPU算完之后都会算出一个delta的W，都会算出一个需要去迭代的梯度，然后这个需要迭代的梯度就会再统一汇总到一台机器。
	在经典的分布式里面，这个就叫做参数服务器PS和这个worker的这样的一个角色，然后通常来说为了不浪费资源，这台参数服务器就在那十张卡当中的某一张他会把这个剩下九个汇总，汇总之后再去做一个计算，加权平均做一个计算再去迭代。这个迭代就会同时下发给剩下的这9台，相当于这十台上面的GPU的权重又一起被刷新了。如果是这样去操作，就相当于是同时一次性把big size这个设置到了80，这个是数据编辑。
	然后这种方式在度学习阶段是尤为常见的，但现在行不通了，最大的一个问题就是模这个一个模型它装不下，就是一张卡里是装不下整个模型的，尤其是我们使用MOE之后，那他要怎么办呢？那他就把这个MOE每一个的FFN又拆到对应的这个机器上，这个就叫做模型运行。这个时候大家用的数据是一样的，相当于besides就只能设置成八了。然后这个十台机器可能有10个FF，大家算完一遍之后，然后再去把这个数据发到对方那儿，然后再接着计算。那显然这个效率就会比之前要低一些。但是这也是没有办法的，因为确实太大了。
	然后这个下的为什么叫机下对吧？这个馅儿的意思叫分片，分片分的就是模型，就是模型太大了，然后我只能把这些模型的一个就跟我们经常做。做很多的计算机经典的分布式系统都会讲分片，这里的分片的对象就是这个模型。然后这个模型被分发到不同的GPU或者GPU上之后，通过这样的一个方式来完成分布式的模型并行的训练。然后这些专家模型也是通过这样的方式来进行更新，不知道这个有没有讲明白，就是它的第三幅图叫做MOE transformer encoder with device placement，这是典型的一个分布式训练的一个逻辑，然后这里应该大家没有问没有什么问题了，对吧？
	讲的应该还是比较清楚的那接着就有一个很核心的问题了，就是刚刚都提到了那种方式就比较慢。如果我的FFN是1000，或者我比较坑爹，搞得跟那个ASTM1样，搞2048个对吧？那我就得要2048张卡那我每一次迭代就要跟2048个服务器去做数据交换，这个显然是不靠谱的对吧？就这样就没法训练了，我在这个网络上的开销的带宽拉满了也搞不定。因为我一次分发就一份数据要copy 2048次，传过去再传回来又是2048次，这个效率是无法忍受的。所以这个时候门控网络就开始迭代了，刚刚我们都在迭代什么呢？迭代的是这个MOE，我们都在迭代MOE。从大了拆小小了拆模块，模块拆到这个INA的网络里，又拆到transformer的网络里，拆到这个FFFN里面。
	现在我们发现模型太大了，只去改MOE不行，我们得去改这个gating network，就这个门控机制得去改一改，他怎么改呢？他其实简单一点就是那个门控getting它最差的情况就是所有的MOE，所有的experts我都需要去参与计算。但是这个一拍脑袋就能想到一个搞法，就是这个top 1、top 2、P3对吧？就top k的experts去参与计算就好了。有太多这种算了可能都在有效进度外了，或者说算了也影响不了什么大局的，你就别来了。
	所以有一个叫做top two的门控机制被提了出来，在机器二里面，然后这个里面的这个top 2就是字面意思，就是他选了一个最优的，就是按照我们的这个权重getting network。现在排出来的这个就比如说top 10或者top 1000选了一个最优的。这个experts我肯定要用。第二个，我就不像贪心策略了。因为你要是用贪心策略，就会出现最开始我们说的强者越强，好多experts就直接老早就下桌了，他不会参与了。
	第二个选择，它会根据这个权重，就像我们去做这个learning的时候，它不会固定的这个learning rate去做这个优化，偶尔会稍微跳一下，这个就是这个意思。第二个专家会根据这个权重，可能设置一个range来选它。这里有很多可以调的，就这个随机路由选择的机制，包括这个mixture o的这个新版本的模型，也是在路由这一块做了一些改造。与你能想象的transformer的网络现在也很难创新了。然后这个专家的模型的这些设计也很难创新了，现在不就得去折腾这个路由了，对吧？那么现在G12的20年就提了这个top two的一个路由设置，然后以及这个专家处理能力的一个设置。这个也很有意思，就是应该是昨天还有一个同学在答疑的时候问过，就我们提的所谓的大模型的处理能力，上线32K184K64K还是2K还是4K这个token上线是怎么来的？
	它跟我们的这个transformer网络里面用到的这个alignment function。大家还记得网络里面用的element function是scale dot product，可伸缩的点击它是怎么表达的？它是用Q乘上这个K的转制，再去除以一个根号下面的DK对吧？作为它的这个模型的维度在开发。然后这样做的一个方法就相当于是我有几个头。比如说我有三个hit，我就把这个原来的这个比如说原来的这个输入的维度是1000，一千不好，整数是九百，我就把它变成300，300之后就给到一个case，另一个除完的300给到另一个case，第三个就给到第三个case。就跟我们经典的CNN去做了三个不同的通道做卷积是一样的。
	但那个DK并不是等于我们模型处理的上限，就那个2K4K32K跟注意力机制里面的那个DK没关系。注意力机制里面那个DK是一个超参数，也是可以调的。你可以调成三个头，调成十个头都行。
	根据你要处理的数据总量，然后某个大模型它能处理的token上限是由什么决定的呢？往根儿上说是由你的，就你是你地主家的余粮多不多决定的就是你有多少资源。因为我们就算忘记这个MOE，就单个transformer来说，它如果要处理更多的token，它的计算量就是得变得更大，并且token数量的N的增长。假设这个token的长度叫N这个计算量就是N方的一个增长。那么你的token如果设置的太大，你训练不了，你训练不过来，对吧？所以这个到底是支持2K还是4K本身它是个超参数，是我们人为去决定的。然后这个决定是一旦决定了这个训练开始之后就不能改的，这个大家应该也能get到。因为你你这个处理上限定下来之后，我们都已经学过前面的课程了，再给大家回顾一下。
	定下来之后，相当于这个中间隐藏层的这个宽这个长度就定了，这个token的这个sequence的长度就定了，那么你是不能改的。然后你如果输入数据超过了你还要截断，输入数据不够的话还要pending。所以这个处理上线这个是一个超参数，然后定下来之后就定了这模型就这个能力了。超过了就超过他没法处理了，不够的话他会去补全你的输入数据。所以这里就有第二个重要的限制，又拉回来，这个需要的知识点还是有点多。
	但大家只要动手，然后深入理解了这些理论部分，应该大模型的这些技术也没那么复杂。然后这个处理能力的上限就相当于我们说的2K4K这个上线会被限制，会被限制住。然后如果超过了，就是相当于我现在处理的这个我选了两个专家，top 2这个能力都已经达到上限了，那么这个token就处理不了了。就相当于截断之后，我前面第一句话第一个sentence给了这个experts 1，截完之后第二个sentence给了expert 2。然后我们因为做了top 2的这个门控，就没有别的专家来处理这个token了，那你丢了好像也不行。这个时候就记下来，很聪明就做了renee的一个设计上的参考。
	就是我们怎么理解这个话？就是我还是用这个击鼓传花的这个逻辑来跟大家讲，什么意思？就相当于就跟大家看这个王牌对王牌的这个综艺一样。我不知道大家看过没有，相当于前面有个人在给后面一个人说话，说说这个说一段话，后面一个人任务是直接把它记下来，然后再讲给再下一个人。但现在因为时间到了，他说不完了。说不完之后，后面的那个人他就听不到你没说的这个话。但没关系，你就跳过他，你再给他的下面那个人去讲，然后这样也是OK的。因为绝大多数时候再隔了一个人，下面那个人得到的信息也是受损的，并且中间又多了一环，它传递的信息也是有损的。
	所以他是能够去get到你这个信息的。我们先不考虑下一个人时间有限的情况，因为整个在神经网络里面去传递的这些信息，它都是被压缩表达的，都是一些隐藏层的状态。只要你把信息传过去了，你怎么知道他是不是就学不到呢？事实上是可以学到的，rez NET就是这样去处理的，就renee t整个短连接，就这个skype的跳转的核心就是我最差也是把它背下来，复制了一遍。这个大家有兴趣可以再再去看一看。
	Rennet其实之前我忘了是在哪个课里也讲过，就整个renee t的这个结构，所以整个机器的能够被训练出来，做了这样两个最大的改进。一个就是把MOE在FFN去做了隔层的一个取代，然后做了模型并行。然后在模型并行的实际训练上再做了一些创新。比如说top two的门控设计和这个expert的这个token数量上限的一个设置，就可以训练了，训练出来的结果也都挺好的。就在咱们的各种各样的叫做多语言的翻译任务上，它有就是我们开始看到那幅图的一个BLEU的一个平均值。这两个超参数分别代表它的layer和他的咱们的这个专家数量。那么可以看到它的超大的模型其实是表现挺好的，这儿我们就不再详细赘述了，所以从这一天开始。Top 2的门控设计就开始很吊诡了。
	再多说一句，就是我们看这个mixture的这篇paper，你会发现它的这个路由设计就是一共八个专家，不是2048个，就八个专家，八个专家这个路由激活就是2到8。那这个二怎么来的？我觉得大概率都是参考这个极限二的这个top 2门控设计来的。他这样是可训练的，这种训练方法是有效的。不管你是两个专家，还是2018个专家的训练方法是有效的。它只不过因为它的模型太大了，所以它固定只激活两个参数。
	然后在这个GLMGLAM这个paper里面，就把咱们的这个大力出奇迹就更进一步的就做出来了。就把整个语言模型，它这个GLM跟刚刚说的绩效的是两个东西，我们还有一些专门去做对比。它跟我们刚刚的这个g sharp比起来，它本身是一个模型。G sharp是一种训练模型的方法，这个大家得理解一下。就机下部本身是一种分片加上top 2门控路由来训练模型的方法。他可以干到2000个专家，也可以是一百多个专家都可以。那么GLAM它就类似于像GPT31样的，它是一种语言模型，所以这个语言模型当时提出来对标GPT3，然后也去做了zero shot、one shot、few shot的一些比较。然后在不同的一些任务上，就我们竖向看到的这个是不同的benchmark，然后这个benchmark里面橙色的是GPT3，绿色的是GLAM。
	然后我们能看到在前三个图表里面，不管是zero还是few shot，还是one shot，GLAM都比这个GPT3表现要好。然后在训练和推理的成本方面，他还比GPT3更便宜。就是他花更少的资源就能训练出来，为什么呢？这个很很有意思，核心还是因为GPT3没有用MOE，简单一点就是为什么MOE现在这么多受欢迎？就是大家发现原来大力出奇迹也还是要讲一点技巧的，你整个都可以归为大模型预训练技术。我认为MOE就是我有足够多的资源了，我有足够多的训练数据了，然后我要怎么样降低我的cos的情况下，还能提升我的训练效果。MOE是一个在这方面有价值的方法。这样去总结。
	然后GRAM它的这个训练出来之后，它的实际比较就是在他自己找了29个NLP相关的benchmark，在各种刚刚提到的zero one和few上面都比GT3要好啊。所以这个是M当时提出来很有意思的一个结论，他怎么做的呢？第一就是他用这个稀疏激活的这种专家混合专家模型，在降低这个成本，具体我们看一下在这幅图里面，大家看这幅图现在就有感觉了，跟刚刚我们深入讲过jp的思路，这里也是一样，它不需要这么多的FFN。我不知道这里大家能不能get到，就我们刚刚用了top two的门控设计之后，他发现原来不需要这么多的FFN就能搞定，就少1点FFN。然后这样做的好处是啥？
	就是其实跟我们的CNN网络的迭代有点像，就整个transformer的网络迭代，首先大的价值变不了了，因为muti had tension。的这个注意机制确实好用，然后前面的这个文本加位置编码也确实好用。那现在无非就是说怎么样降成本的同时还能提升模型的能力。
	是到了这样的一个阶段，2021年，那么怎么办？稀疏激活是一个方式，然后这个稀疏激活就配合着这个专家模型，在用到这个阶段，这个FFN就已经开始变小，并且变少了，就是我们的GLM。但是这个变少可能是在特定层变少有一些特定层还是做的很多的，这个就要他去这个就相当于在深度学习阶段，有一个时期叫做auto ML和这个NAS比较盛行的阶段。就是关于这个神经网络架构应该怎么样去设计，已经不知道怎么办了。然后这个架构的设计，能不能让AI自己设计出一个神经网络的架构？这个其实就是现在transformer在类似于深度学习那个阶段做的事情，就怎么样智能的去设计和优化一些架构。所以这个GLM其实是一系列模型，我们看到这里先有一个对比，应该没有把那个一系列模型都列出来了，但是paper都传上来，大家可以看一看。
	为了尽量不让咱们讲的特别枯燥，那么这个一系列的模型里面最大的是有1.2万亿的参数。但是因为它不是一个稠密的密集的一个模型，它并不需要在实际计算的时候把1.2万亿参数都拉过来做计算。所以它相对来说反而比起这个GPT3，GPT3其实是它的将近10分之1的规模。他反而还训练的训练消耗的资源还更低，计算资源消耗也要更低一些。
	这个是为什么他会单独拎出来说这个点，就是因为我比你大十倍，但是我的运行成本、训练成本、推理成本都比你低，为什么？因为我足够的稀疏，就我不需要在每一次计算的时候，都去把所有的这个模型里面的参数都拉进来计算，然后我还在29种任务上都比你表现的好，这个是叫做generalist language model，就google提的这个GRAM带来的一个很有意义的价值。这个就更进一步的被ms mixture给捡走了，就这个稀疏的激活，其实整个这一路过来，大家会发现MOE的技术演变，就是在翻来覆去的螺旋式的迭代。然后这个螺旋式的迭代到21年22年的主要的一个探讨就是稀疏性要怎么样去解决，因为前面的这个top two的门控已经做出来了一种实际可操作的训练方法了，包括experts的这个token上限的一种超参数设定。
	最重要的最近很火的这个switch transformer，就不得不提了。这个switch transformer这个是也是google发布的一篇paper，这篇paper是更进一步的去把这个绩效的做了一个迭代。因为我们知道这个绩效的里面，他把这个FFN换成了好几个FFN再加上一个gating一个门控，然后switch transformer他要做的事情就是。就咱们刚刚想象的都是挺挺理想化的，就是6000亿的参数或者1.2万亿的这个参数，然后你加载进来了，然后你去训练。训练过程当中应该没啥问题，但肯定没有这么顺利，对吧？就大家自己训练过几十亿的模型都会知道，数据不对或者这个超参数设置的不对，有时候压根就收敛不了，训练就没法很稳定的进行，loss肯定都会跳来跳去的。咱们现在看到的这些paper也没有几个单位可以复现，所以也很少有chAllenge，就很少去挑战。
	他说你这个模型实操性不好，他可能十次只能训练出来一次。虽然一次的成本是22个TPUV3的一年的成本，所以既然他自己提了switch transformer，说明单纯的据线了肯定有问题。不管是哪个部分的问题，我们很难去实测验证。但是他能够从结果的角度去看到他的训练，稳定性肯定是比较差的，就很难去稳定的训练。然后如果是这样的话，他一定会有解决进一步的解决方法。
	这里的解决方法就是提出了这个switch transformer的paper，然后进一步的去做实。我们用sweet t transformer是可以去训练万亿级别的这个大模型的。并且这个万亿级别的大模型也不是一个密集模型，不是一个极其稠密的模型。它本身内部就是一个稀疏的MOE的一个模型。
	然后这个模型具体怎么做的呢？这里第一就是原来的FFN被他替换了，替换成了叫做这个switching FFN。就是我们这儿看到的左边是经典的transformer的结构。如果我们不看这个变化的这个部分的话，它加了一个叫做switching这个FFN layer，把原来的这个换掉了。换掉之后，这里他还做了额外的一个处理。比如说我们看到这里这个最下面这部分，鼠标看到最下面这部分是输入叫做这个more parameters，有对应的文本和位置编码，然后给到注意力层。
	然后注意力层上去之后，有两个rotor，两个路由不再是简单的门控了。然后这两个路由各自有自己的，你可以认为就是不再是原来的一个gating在处理我到底要把这个给谁了，而是两个rotor来决定我要把这个输入给到谁。很很像套娃对吧？就是大家想象一下，就以前是一个门控网络决定，反正你给我这个输入，我这个门控拿到这个输入，我去找应该找谁。现在不是了，现在是说门控网络变两个了，叫rotor了。因为我不止一个门控了，我变成路由器一样的变成两个rotor。然后这两个rota r都拿到了我的输入，然后都要进行处理。然后这个都要进行处理之后，都会去自己的这个专家团队里面选出我要用哪一个专家，其实就是要用哪一个FFN。
	但是这个地方有一个更大的一个比较，就是一开始的这个门控网络，它有它的就是它肯定不完美，它有它的缺。而这个switch的former肯定也都不是最完美的。但是从形式上来说，它比这个拍脑袋的top 2要好一点。
	怎么个好法呢？就是第一他的这个单专家的这个策略是由两个不搭嘎的rotor来训练的，就不会出现说这个top two的那个老二老是选的不对于他随机按照一个range去选的，就随机选的肯定没有自适应的learning好啊。那么现在它相当于我还是知道只用两个专家，但是我这两个专家不是随机选的，我是自适应学出来的。我不知道这个描述清楚没有，在这个过程当中，两个router就会这个叫什么许贤对吧？就是我知道这个问题，我下面的这个诸葛亮出战对吧？这边是说我这边的周瑜出战，他俩一起来解决问题，而不是说我这边诸葛亮出战。然后我这边是带了什么一个完全不搭嘎的一个人来出战，即使这个不搭嘎的人也许在这个概率上还行，就这么一个逻辑。这个是switch transformer一个蛮大的一个改造，然后通过这样的一个改造它使得这个训练过程，或者说它的这个模型容量进一步的提升了。这其实也比较好理解，你可以这么想象，我举个例子让大家理解。
	然后我们再来讲这个容专家容量的一个概念就是第一每一个刚刚看到的这里的要被选出来的FFN，就是我们所谓的这个专家他自己有一个容量上限的问题。刚刚我们提到了，然后这个容量上限是要我们来定的，由我们训练的人来定的。然后第二个就是说咱们的这个训练过程当中，一开始的这个门控机制是选出一个最有最该来解决这个输入的这个FFN1个专家，然后剩下的那个老二是随便选的。但是这个过程当中，这个随机性会造成一定的问题，我不确定会选到谁，甚至我同样的一个输入，因为我的随机性，他可能找到了不同的专家都有可能结果也都不一样，训练的稳定性就会受到挑战，这个是底层的一些很多的原因。同时也因为选出来这个专家，可能本身他就不擅长做这个任务，所以他的这个结果也不够好啊，他能处理的这个token上限也不够好，模型的质量也会不够好啊，都有可能。甚至这个路由应该怎么样去算，我还得把选出来的这个老二再通知给我们。刚刚说的模型并行，通知给那个对应的就写分布式计算里面这个参数服务器，它在每次都得去迭代等等。现在的好处是说，我就是用两个rotor。
	他们专职就是在每一次输入的时候，给我找出自己下面的哪个FN是合适的，然后这些FAN是不一样的，然后这些FN被找出来之后，他们就是专职干这一类输入的。然后这一类输入因为是专职干的，所以它甚至这个模型质量会比随机的要好。同时我还可以把它的token做一定的调整。因为每一个输入数据都是专门去做自适应去调整的，这个是一个应该大家能能转得过来的这样的一个逻辑。它的实际情况就是预训练速度还提升了。所谓的预训练速度提升了，就它收敛的更快了。
	然后这里这个专家容量的概念其实就是我的模型。比如说同样的1000亿的模型，我能够学多少知识的意思就是你可以理解成就是跟他是一个等价的关系。那这个专家容量具体怎么算，其实就是一个非常简单的公式公式。就是这个EC expert的这个capacity是指专家的容量。
	这个专家容量首先它还有一个因子，叫做容量因子。然后我们先不管这个容量因子的话，其实要去理解这个专家的容量。就是我有多少个专家，然后我每一个best size每一个batch可以放多少的token。那么理解这个就是在专家个数不变的情况下，我如果能够调整每个专家处理token的上限，我的专家容量就变大了。好理解对吧？就是我们不管这个专家的数量，也不管咳这个容量因子的话，我每个专家能够处理的上下文变长了，那肯定我的专家容量就变强了。
	然后如果我的这个token不变，然后我的专家个数变少了，那其实某种程度上也是专家容量变厉害了，为什么呢？你可以想象成我所有的这个模型的跑分什么的能力都没变。但以前需要十个专家，现在只需要五个专家了。那你模型的参数总量不就只有一半了吗？那你不就是单个专家变强了吗？他变成一个多面手吗？它能处理的任务更强，这个分子分母关系好理解。
	然后还有一个东西叫做它的容量因子，这个容量因子其实内部有一个调整，这个容量因子写的比较复杂，反正它内部是取了一个各种各样的值来做计算。这我还没有看的特别像，是看到这个分布式的通信有一定的关系。但这个后面看清楚了，可以争取下一次再跟大家去做这个解释，但核心其实就是单个专家的处理上限和总体的专家数量来做的这个设定。
	然后这个capacity的这个factor就是它的容量因子。我大概当时看下来是每一个设备上可以放的FFN的这个小小的神经网络的个数。但我不确定，等我回头查证之后，再跟大家去做这个比较确定性的同步然后刚刚都提到了这个模型并行，在switch transformer的这个附录里面也就提到了各种各样的并行方式了。就是我们像我们刚刚提到模型并行，下面这个叫数据并行。而且都是典型的做分布式训练的时候会处理到的一些方法。然后在这个MOE的这个架构里面，因为它是分层的，就是大家想象一下transformer是一层一层的，然后那个AMOE那些专家其实也就是每一层里面的那些全连接网络，然后这些全连接网络也都是串行去计算的。所以一整个encoder它甚至都可以被切被切成几个。
	所以整个模型甚至一张卡都不用存，整个模型一张卡存的是一个encode的一层，这个都是有可能的。这幅图其实就是想给大家去展示，整个分布式训练里面有各种各样的并行的模式，并行的方法。然后刚刚说的那个容量因子，其实在他的那个论文里面，我的理解是指这个呃同样的硬件资源，然后我能够装下的这个小方块的数量，有有数据的，也有这个模型的但不能单纯说他装的这个模型FFN多，它容量因子就大，所以我说不能直接武断的下这个结论。更多的是它在整个分布式训练的过程当中，我的数据和模型要怎么分配显存，然后这个分出来的一个小格子，就是他的一个可以分配的GPU单元，所以这样去理解，然后他为了去加速这个MOE，就会去做各种各样的调度了，这也是为什么现在有一些公司在创业做大模型的info，包括像这个one floor。不是老袁最近重新做了这个硅基智能，去做大模型的加速一个逻辑，就这个调度这一侧确实还是有需求，但是跟上一段深度学习创业的时候一样，真正有资源去做这个分布式并行训练的公司没几家。然后大概率会自己去做对，就像TensorFlow像OpenAI他们open I可能找了edge等等，会是这样的一个情况。然后容量因子就来源于这幅图里面的效率，做的高不高效率高那个容量因子就大，效率低的那个容量因子就低。
	然后我们接着就到这个mixture，8乘7B这种模型这个就前面如果大家都都跟下来讲明白了的话，这儿应该就好理解了。首先mixture是一个像GPT1样的decoder warning的一个模型，然后每一层每一层就是这个rotor，我们刚刚看到rotor，但是它不像switch transformer一样搞了两个rotor。因为他专家不多，他就八个专家，八个前馈快。现在大家再去理解专家，就是我们经典的FFN全连接网络，八个不同的全连接网络。然后这个rotor会去输出getting with，就是我们的这个权重，你可以理解成概率也行，权重也行，或者用专家的这个值的权重。
	然后在整个前馈，在我们不讲训练的话，在它使用过程当中，对于每一个输进去的token，然后在每一层他都会选择两个。就像我们开始提到的这个top two门控机制一样，会去选择两个专家来处理当前这个token，然后组合输出。这组合输出就跟咱们之前讲marty head一样，这都是类似的思路来解决问题。然后每个token都只会经过两个专家的处理。
	最终我们看得到这个8乘7B的这个大的模型，它是每一个专家构成的一个或者说每一个专家他都是一个7B的一个模型。然后推理过程当中，因为有一些共享的权重。比如说我们刚刚提到的这个激活，或者说刚刚提到的这个注意力这部分会共享。
	所以实际上它的计算只会跟13B就130亿个参数去做激活。但是它可以访问到所有的这个参数，所以我这个数字没记错，就七八五十六对吧？但其实不需要整个的这个8乘7B加载到这个显存里面，其实只有47B的参数。因为有这个9B的这个参数是共享的，就是有这个差值是共享的。然后你在这个实际推理过程当中，只会涉及到两个expert，这两个expert也有一臂的参数是共享的，这个是它的一个大的逻辑。至于它的expert是怎么训练出来的，这可以看他的这个训练集，它也都有公布。它使用的这个方式方法其实就是一个稀疏的MOE，并没有特别的不一样。
	所以说到这个地方，大家就能get到在咱们的现阶段，2024年在几十亿或者说一两百亿的这个模型参数规模上面，主要就比两个东西，一个是比你的数据质量。这也是为什么前两天我们看到唐杰老师有一个圆桌，请了清华的四位教授都在聊，其中有一两个老师都在强调现在做高质量的数据是当下最紧迫的事情。然后前天我还看到一个很有意思的一个twitter上面发的一篇文章，就是在讲，我忘了是哪个模型，是lama还是哪一个模型开源的。然后他使用了中国区的就是说中国人写的github的项目这些代码去训练之后，明显感受到生成质量降低了。
	对你也不能说是中国人写的代码不行，所以导致模型变傻了，对吧？这个结论也太武断了。有没有可能是本来对中文的支持不够好，或者说中文的这个代码可能写的风格跟这个预训练的不一样，都有可能。这里没有结论，这里只抛一个结果和现象，但是我们能从这些事情背后发现现阶段的这个模型，就是模型技术本身，它是一套方法论和技术栈。
	咱们现在其实都都已经知道怎么样去做反应痛，怎么样去做lora，怎么样去微调，甚至如果你有资源，也可以去做MOE做预训练。但是到根儿上就是你有没有去准备数据，还是说咱们觉得这个数据不重要。就数据的这个权重意识，数据的这个优先级，一定要提到一个非常高的高度。然后如果这个做不好，你去折腾很多事情都没有意义。包括你看lama出来没多久，但是这个mixture这么一个模型又迭代了很多，然后超过它了。那你也不确定他有没有针对这些benchmark本身去做各种各样的预训练和微调，这个我们说不清楚，但现在实际情况就是数据占的比例肯定是一半甚至一半还要多。因为我们一路看过来，这个MOE的技术发展没有开开天辟地的变化。非要说的话就是前面那三篇文章是有一些很有意思的改进的。
	后面这几篇文章其实就是TensorFlow在深度学习分布式阶段就已经有的东西了，只不过现移植到大模型来了。OK那我们再看一下这个更全的一个比分，其实能看得到，mixture。他自己有跟这个，我就说我老搞这个，搞名字搞的不对，他自己取名就这样的，他的这个7B的模型叫miss o，它的8乘7B应该是意思是mix了，mix叫mixture OK。好的，然后好，我待会儿把课件里面的这个单个的模型给它改改过来。我就说好像这俩都看到过，他公司名字好像叫这个MAI。好，anyway然后我们能看到他自己的这个active parameter，就是我们刚刚前面提到的那个数字。因为他用了MOE，所以它即使是一个47B的这个参数的模型，但是实际被激活的这个parameter针对一次的推理只有13B。这个active perimeter也能体现出它是一个稀疏的MOE，也就是SMOE。因为它不需要全量的参数参与某一次特定的这个token的前向推理，然后他的目前支持的预训练的上下文的长度是32K。
	然后在各类的benchmark上，主要是数学代码生成和多语言的理解能力上面。还不错，并且比lama 2的70B还要高，这个地方你看active permeate就能看出来，nama 1和ma one和nama 2其实是没有用MOE的。但是你想象一下，我我至少我能想一下，可能最快Q1，最慢Q2，lama一定也会出一个MOE的版本，然后再来刷。
	这个是24年的几十亿到百亿的这个阶段的模型，一定是天天刷的。你看国内的这个富盛，然后天天看他发朋友圈，在参加各种活动，然后讲他们自己的这个大模型又怎么样了。就是不管国内国外的，大家都会去疯狂干这个事儿。因为这个事儿还没卷出一个输赢来，大家还在疯狂的去去去去打榜，就跟上一轮大家疯狂的灌水发论文去去刷榜一个逻辑最后额外提一点，就是这个mixture的这个模型，它的协议是很友好的，它用的是阿帕奇2.0的协议。这个协议就意味着你可以拿它去做学术方面的工作，肯定是可以的。你也可以拿它去做一些商业的研发，并且它有一个instruct这样的一个聊天模型，然后有一些benchmark去讲。他的这个偏见比较少。
	我们开始看过BBQ和这个board这个基准，就是主要是去评价这个幻觉和胡思乱想的这个维度的比较少的偏见和幻觉。然后在这个有一些人类评估，包括我们看这个scored就斯坦福的这个question answering dataset这个基准的human performance，它也有一些基准比分还不错，超过了cloud 2.1，包括这个金民i pro。当然这个版我相信很快又会迭代了这个其实就是咱们如果说要揭秘的话，最新的这个mixture 8乘7B就主要做了这样的一个，你说迭代也好，说进步也好，但从结果的角度来看，确实它的各项表现都还不错。
	不管是聊天能力，还是代码生成能力，还是多语言的理解能力。但是以后他的技术用了多少黑科技呢？也谈不上，因为这个里面大部分的技术我们一路看过来这。
	十年的发展。从13年的伊利亚的这篇文章提出了D这个deep MOE，就是把神经网络跟MOE结合起来。到今天为止我们再来看这个mixture o的这个8乘7B没有质的变化没有质的变化，只能说大家走了很多弯路，知道这个门控网络应该怎么设计了，然后top 2就够了。尤其是在1.6万亿的这个模型上都只需要用top 2。你这个几百亿用top够的了，不需要用那么多，所以就是这么个逻辑。对今天我们可能要分享给大家的这个关于MOE的技术揭秘，就是这些内容看大家有什么问题我们一起探讨。
	8乘7B也可以lora微调吗？好问题，我想想。理论上是可以的，这个同学问的问题就是理论上是可以的。但是你得知道他的几这个同学问了一个很好问题，就是首先你要知道他的active parameter是13B对吧？但是你要把这个8乘7B的模型整体加载进去，它只是说我你可以这么理解，就是显存是一个资源的指标，然后生成一个答案是有一个速度，有一个性能的指标。然后它表面上看起来是13B对吧？但其实是一个47B的，这里有写47B的一个参数。所以你首先要确保你有这个47B的资源，就是你能加载一个47B的模型到你的GPU里面，然后能加载进去之后，你再去判断这些具体的每一个专家网络要不要去做LORA。
	因为这个MOE本身其实已经跟lora的这个逻辑已经很像了。你去想象一下，其实对于一个几十亿的模型，每次只用其中两个专家模型。它本质上跟lora的训练，你要往高维上去讲，其实一个意思其实是一个意思。
	再细讲一下怎么提高数据质量，这个我感觉没有一个通用说法。咱们可以最简单的就是就问这个同学问题问的很好。首先但是你得理解，就首先问这个同学问题，你得想一想你自己有没有去看过一个数据集，它到底是怎么样的，怎么样去看一个数据集？就dataset review上面都有，就比如说我们看一个。
	那个是浏览器，这个是浏览器。比如说我们随便找一个hugin face上面的数据集。比如说就我们都用过的这个问答数据集，斯坦福的问答数据集，咱们自己对于这个数据集的理解现在是什么样的？数据集不是我们对数据集的理解，千万不能是说我有一个example，一定是首先有一个宏观的理解，有一个整体性的理解。就比如说我需要多少条example对吧？然后我的这个example里面，我的这个。稍等一下，这个VPN有点崩溃。对。
	简单来说就是你对于数据集整体有一个理解。比如说他需要多少条，对于问答的数据来说，可能你是要至少准备个10万条这样的一个数量。如果是一个特定的领域知识，那可能也是要准备上万条这样的一个数据。然后假设你是要做这个问答，那么问答类的数据集已经很多了。无非就是说现在都是英文的，没有中文的那一个最挫的方式是不是可以把这堆英文的数据集拿GPT4洗几遍之后，是不是变中文的数据集了。就比如说这个SQAD能不能去做一个SQAD中文版，然后这个中文版是不是可以拿来去提升你的中文大模型的质量，这是一种思路，就洗数据。
	第二种就是去学习它做数据集的方式。比如说咱们的这个数据集，要做一个GSM8K这样的数据集。它是一个应用题，是人手工编制的，这个就跟我们的场景很像是一个手工编制的数据集。那我们要去怎么编一个自己的数据集？不管是请标注人员还是自己去手工，那我们可能就得去看一下它的这个数据集，它的这个格式是什么样的。
	我们先来整理一遍，因为你选的那个数据集一定是要么就是语言不行，你是为了增加另一个语言能力，要么就是说他原来里面的内容不行，你要去做这个point，要去做扩张，那你肯定是要有一些对标参考的数据集。而这些经典的benchmark它都有数据集。所以你可以去参考一下这些数据集是不是适合你的任务。因为每个数据其实它都是针对一些特定任务的，千万不要你还不知道你要拿这个模型干嘛。然后那这个时候是永远找不到一个适合你要做的模型和数据集的，我不知道这样表述清不清楚。可以量化吗？肯定是可以量化的，这个不用考虑，这个肯定是可以量化的这个东西。
	自己构造数据集，lora微调不知道多少条数据集合适，对模型问答的影响程度多大。就咱们咱们比如说咱们去用lora微调对吧？如果你的问答是多轮对话，那你可能就得基于聊天模型来做微调，都不是基于这个6B来做微调。就像这个mixture，它也出了instruct这个聊天模型是可以支持多轮对话的。
	首先咱们要对这个模型的能力有一个认知和划分，聊天是聊天的能力，然后知识是知识，对吧？聊天是额外的一项能力，是聊天的一个训练优化。但那个那个是有基础模型的，就像咱们的这个mixture，他有聊天的模型，chat GM也有聊天的模型，基座的模型更多的是我预训练了一堆知识。然后无非就是说你现在如果是领域知识没有你要去补充知识。然后你的这个知识假设你要补充的就是今天有一款新产品叫apple vision pro，就它的一个新产品。这个新产品以前的模型你就是没有，那你就应该首先把市面上能找到的关于这个产品的官方介绍的信息都全部给它理出来，对吧？然后按照一个他能够接受的token上线去做切分，切分之后我能想到的就是直接用GPT4去把这个第一你可以构造这种问答型的。第二你也可以直接去把这个内容直接去按照像这个fill mask一样的方式去训练一些对应的这种encoder，就auto encoder的模型或者T5的模型。
	这个完全取决于你最后用什么模型，因为你的模型决定了你要把训练集做成什么样的样式。然后如果你的这个任务都不清楚，那这个就比较头疼了。你还是得聚焦一下，你要解决一个什么具体问题，然后再来回过头来看准备数据和模型应该怎么弄。
	看大家还有什么问题吗？没问题了。
	数据并行的时候，十个卡上的模型更新的参数都是一样的吗？是的，以前听说GPT4在各个模型的结果稍有不同，现在看是的。GPT4至少从各种暴露出来的叫什么八卦，或者一些北美一些经常有科技八卦博主讲到的这个GPT4，应该是用了八个甚至16个专家模型。哪里有门控网络的训练代码？我今天也试了一下mixture o的一些代码，没有跑的特别顺畅。这玩意儿才刚刚出来了十几天，我估计社区应该还会做一波迭代。
	我看到transformers最近更新了一次，宣称支持了mixture了，但是还有一些小问题。对，然后关键是这个T4加载不起来，它可能需要大几十GB甚至大家其实可以去算的，这个也好算的47B然后47B就是470亿需要的显存，哪怕我们是float 32精度的话，就是将近100GB的显存。那如果你用这个单精度，就是47GB的显存，显然你至少需要一个A100的这个多卡的A100才能够把它加载起来。但是它也有单纯的mistral 7B的版本，这个就跟lama的7b chat GM6B是类似的了。
	看大家还有什么问题吗？没了。只剩五分钟了。行，那我们今天就先到这儿，大家有什么问题可以群里我们再沟通。好，谢谢各位。