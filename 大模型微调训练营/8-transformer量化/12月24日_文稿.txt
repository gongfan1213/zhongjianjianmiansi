	PFT, 还是我们有的同学可能已经听说过的这个什么x errors的库，包括微软的deep speed，其实都是在transform AS的other class这个核心设计上面在做了进一步的扩展。当然今天的重点还是模型量化。我相信上节课带大家走完一遍如何使用transformer的trainer开始训练模型之后，我们刚刚在上节课用的模型都是千万级，甚至1亿左右这样的一个模型规模。大家直接去训练它的时候，就已经遇到了一个很大的问题，就是跑不了几个apps，感觉就要花几10个小时的时间，那怎么办？那有没有一些办法可以让我们相对来说，因为我们现在不是要考虑马上要投到生产环境，而是要把这个大模型微调的技术给掌握好。那相对来说有没有一些以比较低的GPU的资源，然后能够把整个模型拿来快速微调的方法，模型量化就是其中的最重要的一个部分。
	当然既然提到模型量化，我们就再简单讲一讲。其实模型量化本身也是两种技术。一种是叫做训练后的量化技术。也就是说我们这里看到的这个GPT q就是一个典型的我们把模型全量的模型或者说这个不量化。在这个原本的比如说float 16或者float 32的这个高精度的前提下，先训练好，训练好之后再进行量化。比如说GPT q是这样的技术，这个技术其实本身对于我们的大模型微调没有太大的帮助。它更多的是降低我们的inference的成本，就怎么样以一个更低的GPU把原来的大模型的参数变小，变小之后我们就能存下来，就能够去做推理。这个叫做后训练这样的一个量化技术。
	但还有一种技术是说我能不能在训练过程当中就以一个比较低的精度来存储。然后这样的话就能实现在我自己的数据集上面去做低位或者说这个低精度的一个训练了。当然可以，我们之前在讲这个理论的时候，有跟大家提过QLA这样的一个技术。像这里写到的这个BNB，bits and bites这个技术，其实在transformers的quantization这个模块里面也实现了，并且它的使用非常简单。然后它还可以进一步用这个NF4，就咱们讲过的这个Normal flow这样的一个数据类型来存储我们的大语言模型的参数。然后结合q lora这样的微调技术，使用这个hugging ing face的PFT，然后也是基于transformers的这套auto class去做扩展的PFT，然后来实现一个q lora的微调。这个我们会放在后面的课程当中。
	今天主要的要教给大家的这个知识，第一个就是理解of class这样的一个很好的设计。第二个就是怎么样去掌握transformers本身的这个量化的模块。好，我们来看看这个transformer所谓的核心设计，说起来很简单，这个auto classes，这个auto很重要。就是这个auto class，我觉得它的核心其实总结起来就是两个维度。第一个维度就是要统一接口，怎么样叫统一接口？我相信从大家刚刚开始用transformer这个pipeline这个pipeline的advanced这样的一个notebook，一直到我们后面加载这个模型，加载tokenizer，都在使用这个from free train的这样接口。我们现在接触过的auto model、auto orange zer都使用了这样的接口。
	但其实还有一个非常重要的auto class叫做auto config，就是我们这里写到的这个config，它其实是隐藏在后面的。但如果我们开始逐步的接触到，比如说接触到量化，接触到微调。你会发现out config其实跟我们的token ized和这个auto model是如影随形的一个三元组。其实也很好理解，就是我们的模型相关的配置都存在了里面。所以整个outclass有一个非常重要的接口，也非常好记，叫做from pre train，就是从预训练当中来。
	那么从预训练当中来，它要解决的问题除了给大家一个统一的接口以外，还能提供一些什么方法呢？就是自动检索的这样的一个功能。因为我们都知道对于一个大语言模型来说，它的文件组成特别多。我相信大家自己如果跑完了，因为我没有把模型文件传到github。
	如果自己跑完了这些前面的代码，就会发现，一个模型它的组成token ized有很多文件，有原数据文件，有词表，有各种各样的一些其他的配置。对于模型来说，有很多的check points，也有本身这个模型的文件，还有它的一些训练状态。这些这么多不同的文件怎么样去组织？他们难道要去写一个file manage的这样的一个类吗？文件管理的类吗？
	但其实就算你真的做了一个文件管理类，它也非常的不好用。因为它并不是指只是去管理文件，而是需要把这一堆的文件分门别类地组织起来。然后在我们去实例化对应的一些抽象的时候把它装进去。就比如说最重要的token ized model和config这就是三个最重要的抽象。他们各自有自己的文件，而这些文件都可以通过这个from free train找到他们，然后通过对应的save方法去持久化存储他们。所以整个outclass通过一个from free train，也是一个很符合我们在学习这个单元模型过程当中的直觉，就是预训练，通过一个从预训练来这么样的一个接口把它管理起来。
	我们能看到其实outclass也远远不止这三元组，还有很多的可扩展的功能，包括你可以去自定义。这里我们今天主要让大家去关注这三个，auto model和auto organizer。但除了这些以外，我们今天其实还会用到像这个右边就是偏应用层面了。就是如何把outclass应用到最终的使用大约模型的influence也好，或者说去微调这个层面上，就会按照这个技术类别再做分类。比如说我们看到右边有NLP，自然语言处理，计算机视觉，我们的音频处理，包括我们的多模态，在自然语言处理里面用的比较多的，会频频出现在大家视野当中的，就是最上面的这个auto model，你可以认为归这个归因casual这个词跟我们之前讲的这个GPT自回归其实是一个意思。我们在之前的这个课程里面也有提过，但没有去highlight这个英文单词。它其实就是是指这种GPT，decoder only以这个自回归，以生成类为主的这类语言模型，都使用这样的一个顶层的抽象。包括我们今天会去用到的facebook的OPT模型也是如此。
	然后类似的我们还能看到像这个TF的这个auto model，它是单独的，你可以认为最开始的transformers优先在支持pyto ch所以你会看到有很多的这个auto类别前面都会有一个TF的前缀，那是因为TF是后面支持的，那么之前的这个接口为了保持它的兼容性，就没有去动原来的开poch的这套接口，而是加了这个TF包括这个flex对应的这个前缀也是如此。除此以外，我们看到这个计算机视觉，这个audio和Martin model也是一样的，这我们就不再赘述了。大家有兴趣可以去了解了解，去看一看相关的文档。尤其是这些不同的模型，他们的使用的接口设计也相对来说是比较统一的。
	这个是整个auto class。Auto config是用来干什么的呢？你可以理解成我们刚刚讲到的三元组里的每一个模型都需要一个自己的配置管理的这么一个类的抽象。而transformers的outcome for这个顶级的模块，它就是通过from retrain的这个方法就能够完成它的实例化。并且这个最重要的三元组，就我们刚刚讲到的这个out config auto to ezer和这个auto model，比较反直觉的是他们都无法使用这个。
	大家知道payson的这个in IT方法，是实例化一个类的通常会使用的一个方法。但是在transformers里面，这三个主要的auto class都是不支持用in IT来做初始化的。其实也很好理解，因为你如果用in IT来做初始化，它其实就是一个构造函数。
	那么我们为了去支持auto class的扩展，通常都会再去制定一个自己的auto class，然后给它定义好三个三元组应该分别是怎么样去继承它的子类。如果你在这个过程当中，比如说下面我们看到的所有的模型，其实都是基于这个最基础的auto class去做了扩展。就像如果你未来要去自定义一个模型，也是要基于它去做扩展。但如果你基于这个auto class去做了一个子类，然后你希望通过你的构造函数来初始化你自己的auto model。那这个时候其实作为transformers，它是很难控制你的构造函数的。就你的构造函数里面你可能干什么都行，他也很难检查你那就很难去做一个transformers整个库的稳定性的一个控制，所以它不太会允许你用input的方法去做这个构造函数。这也是大家去写这个transform AS的自定义模型，包括自定义配置的时候需要注意的地方。它推荐的方式还是跟刚刚讲的一样，通过from free trade这样的一个方法来实现auto config以及其他我们的这个三元组的实例化。
	然后在这个过程当中就要提到一个新的概念，大家可以先在脑子里记下来，因为我们在后面学习PFT的时候都会用到它。有一个auto config就他要去实例化，需要通过config的from free train的方法。它的input方法不能用，但是它的from free train的方法是它实例化的主要方法，就跟我们之前用auto model和auto tokenizer一样，都可以用from free train的方法来做它的实例化。在这个过程当中，这里要填的其实就是一个模型的名称或者说路径，如果是名称的话，它会自动的去have in face hub上面，就像我们之前使用过的那样，把它的这个名称加到这个hugin face到CO这么一个URL的后面。
	然后如果这个路径，这个UIL里面是一个已经被被用户，或者说他跟face官方上传的一个对应的模型，或者tokenizer，或者对应的data sets都可以。只要你是在他跟face ub上面有，那么它就可以通过名称找到他。如果没有的话，那他可以再通过他的路径去找。因为这个路径可能是一个本地的路径，就是因为你之前第一次你在这个hugger face hub上面去下载。下载之后，所有同学都会发现第一次很慢，那第二次之后就会很快。那是因为它已经把它下载到你的本地硬盘上了，那第二次的时候他就会默认的先去本地，因为整个hugger face的transformers有一个工作目录，他就会去那个工作目录下面找。如果有的话，就从本地硬盘直接加载，就不用再去等dow loading的这个过程。
	还有一个细节就是说在这个过程当中，其实config也可以在from free trade里面去买一些配置。其中最重要的配置其实就是这个model type，就模型的类型。这个模型的类型直觉上让大家快速理解，就是我们下面所有的这些不同的模型，它都有一个单独的字段，一个枚举类的这么一个类型的字段，去标志它是什么样的一个模型类型。但是如果你没有去填写这样的一个模型类型，就比如说你是自己写了一个模型，自己定义了一个auto class。你没有去做这样的一个事情。它会根据你在from free train的这个方法里面的这些模型名称和路径，去推断你有可能是哪一个model type，然后再去加载对应的这个config，所以这个也是体现出它自动检索，统一接口的这么一个特点。那类似的在这个auto model里面也是一样。
	不过auto model跟其他两个不一样的地方，是它除了能够通过from pre train的方法来实例化以外，还可以通过from config来进行实例化。这个理解也比较直觉化，就是你可以理解成从这个预训练的模型文件里面去初始化或者说实例化一个模型是最符合直觉的。就是我把这个模型存到了文件里面，然后我再从文件里面加载回来，放到我的这个GPU的显存里。
	当然也可以通过一种什么方式，就是在最近transformers迭代到后面的版本，包括patch ch像1.9之后，transfers允许你加载一个空壳的模型，尤其是在一些比较大规模的大型参数的模型里面。其实你本身一开始就不需要加载全量的权重，你也可以只加载一个空壳的网络，具体的这个参数可以在你需要用的时候再加载进去，这样也是可以的，包括像量化技术里面会有这个offload，把一些暂时在当前计算不需要的参数，从显存offload到这个内存里面，到CPU这边来，就不在GPU里面占用的你很紧俏的资源，类似这样的方法。所以它还可以支持从这个from conflict里面来完成这个实例化。
	Auto organza也是一样，并且我们都知道在上节课已经用过这个fast版本的，就由rust这个编程语言实现的更快的一种token niza。Token niza本身在整个大语言模型里面，它也是一个独立的存在，就像我们的配置跟模型的关系一样。模型是模型本身的网络架构加上它的权重，然后配置是一堆，比如说我的训练的配置等等。那么我们的这个token niza其实也是独立的一块工作，在模型的前后两部分去运行。主要是前半段完成一个从训练的原始语料到最终编码，映射成我们模型可以接受的vector这么一个独立的组件。那它其实也是跟我们的config一样，要通过from free train这样的一个方法来完成实例化。然后它也是通过model type来匹配对应的token ized，这个也是好理解的。
	不同的模型它当然要编码成不同的vector。一个最直观的不同就是大家的模型网络架构不同，我需要的vector的维度都不一样，对吧？比如说你是368，它是764，我是512，它是256，这是一个最大的不同。而这个不同会直接导致如果它没匹配上就没法用矩阵相乘，你的维度不同完全就无法计算了。
	除了这个最基础的维度不同以外，其中还会有很多比如说不同的token ized，它使用了不同的数据来训练。那如果你的internet用的不好，可能原始的数据在你这儿可能分词都没分对。然后如果分词分对了，但是你训练的不好，可能他出来的vector这个embedding的值不好。
	这个其实也是通过一个from free train d来统一管理整体我们现在看到的这些auto class。未来我们不管是在PFT还是在这个deep speed里面，都会经常看到。因为整个模型其实就围绕着它来展开的。包括我们后续会用的这个微调，也是基于trainer，再加上我们的训练参数和超参数，来进一步做的扩展。我们的量化也是在一个auto model这个类上面再去做的扩展，所以它成为了整个我们transformers及其生态的一个基石。好，我们简单跟大家分享了一下这个auto class的设计。未来如果我们需要用到的话，我们还会再给大家去做扩展。
	接下来今天的重点是怎么样使用transformers来实现模型量化。我看到有很多的同学都应该是或多或少的了解过这个量化相关的技术，包括我们前面去讲这个理论篇的时候也跟大家提过量化，尤其是在讲这个QLA的时候，提过这个量化的技术，我们最后再简单通过一页来跟大家做一个分享。量化其实它的本质就是用较少的信息来表示数据，就是我们讲的小的资源干大活，就是用较少的信息表示更多的或者说表示原来用更多的资源要表示的数据，同时尽量不去审视一些确定性，这个地方其实是很多时候是比较烦大家的直觉的。就是我们会发现，如果你用更少的资源，好像一定会丢失这个准确性。但是其实有一个很猛的人就干了一个事儿，就是这个l mask把这个twitter开了，80%以上的人好像也没什么问题，对吧？所以其实你看一个这么大复杂运转的一个公司及其公司的产品，砍掉这么多的我们叫信息也好，叫人也好，叫表达这些或者说支持这个公司运转，支持产品运转的也好，其实它没有影响什么事情。实际上这个世界就是这样的，就是有大量的表达也好，纯粹也好，是冗余的，所以量化才有它的价值。这个不管是从经典的计算机算法里面的这个模型剪枝也好，自由化搜索也好等等。
	一直到深度学习阶段用到的各种量化技术也好，其实它它是一直有效的，并且它也被印证了在各种各样的测试集上面。那到我们的大模型阶段怎么样去做量化？其实所有的问题大家都要相信，以前的方法到一个新的场景，更大规模的场景，大部分时候是有用的，但是它不能完全平移过来。举个最简单的例子，就是我们之前想过很多int 8甚至int 4的技术，能不能直接在大模型上面去直接平移过来用？
	我觉得可能有两个点需要去考虑。第一个点就是说大模型变得更大了。那么更大之后原来可能单台机器能能装进来，然后让你去做量化，现在装不了了。这个是一个维度，可能大家不一定会去顾及到的。就是大模型足够大，一开始就加载不进来，你怎么做量化？就算你要蒸馏，你要去做剪枝，你也得先把它放进来。
	这第一步就出了问题，因为它的规模变成了1000倍甚至1万倍的规模。从一个1000千万规模变成了万亿的规模。那这个五个数量级的增长怎么办？
	第二个就是说直接把原来的精度放到更大，比如说万亿的或者说千亿的这个规模上能用吗？这个三个精度、四个精度在这么多参数相乘之后还能用吗？因为我们都知道为什么要16位的精度，就是因为小数点后面太多了。我们的模型权重并不是1.25.6或者说几十，它大部分都是在这个小数点后很多位的那你几次相乘就已经超过有效精度了。那你在这么大体量比原来又大了这个千倍万倍的体量上面，用这个同样的精度的算法，比如说和三位四位的精度再去存，他们会不会更快失效，因为你的这个精度损失更多就是。
	有效精度的问题，你表达不了这个过程当中的一些波动了。现在乘的这个数量变得这么多之后，这个位数还能不能保持？这两个问题其实都是在大语言模型的这个量化过程当中都会遇到的非常困难的问题。包括我们今天的这个事例里面，我们可能用的这个小几十亿的这么一个模型参数可以用。但是当我们用到这个大几十亿，比如说六七十亿甚至将近100亿的时候，我们的一些模型量化的方法可能就用不了了。
	好，所以整体来看，不管我们是把这个原始的32位浮点数变成16位，还是扒位还是四位，它的核心其实就是要缩减开支，降低我们的显存开销，同时还别损失它的准确性。但是有一个直观上的好处就是你的精度变低之后，大概率它的推理速度是会变快的。所以简单来说就是twitter砍了80%的人。那你每个月的工资肯定付的就更少了，不需要花更大的资源去运转他们了。对于模型也是一样的，当你的原先的模型参数变成只有4分之1、2分之1、8分之1的时候，那你的计算量肯定就变少了。这个是一个比较直观的感觉，但实际上可能每种量化技术有所不同。
	那么有一个很多人都关心的问题，就是大家都在问，我的模型参数跟我的显存占用到底是怎么一个计算关系？我什么样的一个模型到底会占用多大的显存，怎么算？这个其实没有那么难算，专门留了一页跟大家讲这个计算方法。就比如说我们以这个很经典的OPT这个模型，其实在前两年是非常热门。包括现在其实在在meta或者facebook这个公司内部，也是一个很大的一个组在运转，跟lama是一个竞争关系。那么OPT6.7B就是它67亿的这么一个模型，它到底要用多少显存呢？这个计算方法是比较通用的，但是它是一个估算的方法，因为你在实际存这个模型的时候，可能还会有一些小的优化技术，包括一些量化技术都在被广泛应用。但可以算出一个大概，然后也都比较准，至少不太会偏差到20%以上。
	就比如说以这个OPT6.7B的这个模型为例，我们怎么去算它，就这么四步就能完成所有的这个模型参数和显存的一个估算了。第一步就是你算一下你的模型的参数总量是多少。比如说6.7B就是67亿个参数，当然实际的这个使用的时候，67它可能还会有一些尾数零头，我们先不去管它。那么67亿个参数，每一个参数要占多少显存，我们就看这个模型它的默认使用什么样的精度去存每一个参数就好了。
	我们假设所有的这个模型参数都是用的相同的精度来进行存储的。就比如说这个OPT6.7B它用的是这个16位的浮点数，也是一个比较常见的这么一个精度。那么16位是两个字节，这个大家应该都知道，就是一个字节等于八位，一个bites等于8，一个bit等于八个beats。这个大家应该了解的，是计算机的一个基础知识。16位的浮点数就是两个bites两个字节，所以它一个参数就要占两个字节的显存。
	那么总的显存占多少呢？那就很简单的一个算术问题，对吧？你有多少个参数，你每个参数占多少？乘一下67的参数，每个参数两个字节，134亿字节，就134亿个bites，那么134亿用十进制来表示，然后我们为了方便换算，保留这个十三，不是1.34，13.4乘以10的9次方字节，这是它的总要占的这个字节数一个估算值。那么13.4乘以10的9次方为什么要放成十的9次方？这个计算机背景同学应该就很熟，对吧？
	因为二的10次方约等于10的3次方，二的10次方等于1024，十的3次方等于1000，这个很好的一个换算关系。所以我们把它放到10的9次方，就约等于二的30次方，我们就可以做这样的一个换算。我们知道这个大的B这儿的这个大的B是指字节大的这个B一个字节，我们这儿一个GB就大家常常讲的这个EGB，一个GB它是等于二的3次方次的这个字节。这个应该大家都清楚，一GB然后一照，然后一K然后1B分别都是二的10次方的这个单位。那么一GB等于二的3次方，二的30次方的这个八次，那么就约等于10的9次方的这么一个字节。那就好算了，那就是把上面的这个公式换算一下，那么就是13.4，我为了这个看的规整一点，13.5给它改成。综上，我们其实就能算出来任何一个大语言模型，它的在一个特定的精度去存储参数的时候，要消耗的显存占用。在这样的一个计算方式我们就知道OPT6.7B如果以16位浮点数的精度加载到GPU里面，大约需要用13.5GB的显存。
	就这么一个逻辑其实并不复杂，大家自己可以去推理一下，然后去想象一下这个假设，GPG4或者说GPT4.5是一个万亿的这么一个规模，它需要多少的显存，其实是非常夸张的一个数字。然后大家其实是在一些公开网站上能搜得出来一个GPU在一天或者一个月，对应到一个显存的数量的开销的。所以OKI最近刚刚爆出来，又在以1000亿美金的估值寻求新的沙特基金的一些新一轮的融资，也是非常好理解的。因为确实太花钱了，即使一年有十几二十亿的收入，也还是撑不起的。如果我们把这个flow 16这个精度改成int 8，差不多就是一半，大概差不多7个GB这么一个显存就能够加载起来。今天的代码其实是支持大家去做这样的一个实验的。大家可以回头代码也都已经推到这个github了，大家可以回头自己试一试，看这个估算是不是准确的。
	我们接着就讲，既然说要用模型量化的技术，transformers支持哪些quantization，就它支持哪些量化技术？它其实目前主要支持三种不同的量化技术，在transformers的这个库里面内置的。然后这三种量化技术也都是通过我们刚刚说的这个auto config的一些扩展，它的一些继承来进一步实现它的相关配置。这里我们一个一个跟大家分享一下，然后这些技术其实都非常新，大家会发现学大模型相关的技术，就是你几乎每次学习的都是当年的一些新东西。
	这篇GPT q它虽然发在SLR2023，它其实是去年底的一个研究成果，那么SLR2023就是今年的顶上面的一篇文章。这个GPT q它的名字也很直接，它其实就是一个专门面向通用的预训练的transformer的一个量化技术，并且是post training，就是训练后的一个技术。就我们开始讲的，量化技术有两种，一种是你自己训练完了，我再来给你进行量化，相当于瘦身蒸馏，我去找出你的模型有哪些冗余，我保留那些关键的，这是一类技术。还有一类，就像Q罗拉，我可以在训练过程当中就给你以比较低的精度，以比较低的显存来进行量化的训练，就是QLA。
	GPT q是属于一个post training的一个量化技术，那它有什么样的一些特点？第一就是说因为它的设计就是面向这种预训练的transformers，尤其是以这个GPT或者说开源的GPT这个OPT为主的这样的大模型。所以它的核心就是能够尽可能的去降低这个模型的大小，降低它的计算需求。然后同时最好还能不要让他的准确度让他准确率失真，然后还能让他的推理速度有提升，这个其实是他的目标。总结来看其实有这个以下的八点，我就不一个一个念了，我主要给大家分享一些比较重要的它的一些贡献。
	第一个就是我们都知道GPT3出现之后，这个in context learning merge models，这个是GT3这篇论文的标题，也直接导致了一个问题，就是1750亿参数的模型没法做训练，大家刚刚才教了怎么去算的，大家算一下1750亿参数，假设以这个16位的浮点数可能都不够，也许是32位的就直接乘以一个四对吧？那么这个几千GB的这么一个消耗，然后才能把它加载进来。我们已经学过怎么样去做微调了。你把一个模型加载进来和这个模型真正要训练需要的显存，那又是可能差了好几倍甚至十几倍，看你这个base size设计多大了，然后你这么大的一个模型，其实你的best size设计小了没有意义。你都是一大批的best size丢进来。那你可能一次运行就是上万GB的显存需求，过程当中还有网络通信的开销，你还得上高速网络，像英菲利ban的这样的一些高速网络都是非常高的开销。那怎么样为这种大规模的模型去做微调，或者说去做量化运行，让他去做推理，这个其实是GPT q他做的非常重要的一个贡献。
	我这篇论文提出来的这个价值所在，它也是一个一次性来进行权重量化的方法，然后效率也比较高能比较快的去完成整个模型的量化。但它的效率其实也取决于咱们在进行量化过程当中给他的这个数据。这里需要提一个点，就是我们知道这些post training的quantization，就是训练完之后来进行量化的方法。它怎么量化的？我们先不考虑这些数学或者所有的量化的细节。大家想象一下，我要把一个模型做蒸馏，或者说我要把一个模型的重要的权重拎出来。我怎么知道哪些权重是重要的，肯定不是拍脑袋拍出来的。所以它其实是需要丢一批数据进去的，就是在量化过程当中也需要丢一批训练集，或者说叫数据集。
	不一定叫训练集，就给一批data sex进去，然后给了这个data size之后，你就开始进行这个量化，point就是它的重点。就是说这个小的数据集你丢进去，然后它就相当于你可以简单理解成就是它在不断的试哪些参数是重要的。然后我选了一些参数之后也要它运行出来。在这个车在这个数据集上跟全量的没太大区别，精度损失没太大区别。那这个时候我就可以把这些参数留下来，或者说把这些参数降低精度存下来等等。所以post training通常还是需要数据集来支持它的。
	Post training的话tizer这个GPT q也是一样，这篇论文自己就发布了几个，相当于它构造好好的数据集是可以直接拿来用的，相当于它出厂自带了几个套装，你拿着这些套装去套不同的模型是好用的。然后我们的代码里也有用这个c four，然后它也能够去支持一些低位的一些量化。就我刚才讲到的，如果我们像以前可以用四四个壁纸或者三个壁纸来进行量化。那么如果我直接去把它存成三个壁纸或者四个壁纸，可能会带来很大的问题。就是因为它精度一下变得很低，用不了那么GPT q，它可以用这个3到4个base来实现量化，并且还能够保持一定的准确度或者说模型的性能，这个是它的很重要的一个点，就是我们能够把模型的这个大小显著的去降低。
	然后最后一个就是有一些它的极端常识，就是我们刚刚说这个16位的，甚至可能32位的那他有他在paypal里面实现过这个两位的，或者说就刚刚提到的这个三个beast。这其实就是假设GPT3或者GPT4是32位的这个浮点数，那他如果用两位的这个精度，其实就直接把模型变成原来的这个16分之1的大小。这个其实是非常夸张的这接近快20倍的这么一个压缩了，然后推理速度的提升，这个是它里面的一些实验结果。然后也因为有上面的这些加成，使得我们可以在一个GPU上去跑一些大模型，这个是很重要的一个贡献。
	那么这个paper它跟谁再去做对比呢？其实也是很新的22年的一篇文章，二年的文章叫做这个RTN。其实简单这个RTN的意思就是你可以理解成就是四舍五入，最最粗暴的四舍五入。如果我的精度没有原来那么高了，我就截短截断到我现在这个精度的时候，那剩下那些我表达不了的精度我就丢掉了。他跟这个RTN再去做对比，我们看到这个曲线其实很有意思。
	首先最下面的这个是表现最好的这个纵轴，大家可以斜过来稍微看一下，这个纵轴是在wiki text的two这么一个测基准测试上面的一个混淆度的一个或者说困惑度的这么一个指标。这个指标其实你可以简单理解成就是用来评估我们模型性能的一个指标，它需要越低越好。当它比较低的时候，其实是我们的模型在一些预测这个文本序列，或者一些特定的自然语言处理的任务上的时候。它的不确定性或者说他的惊讶程度也好，我不知道这个中文怎么讲，就是我们在讲这个理论片的时候有一个uncertain。大家还记得做这个Laura时候，这个不确定性是一个很重要的指标。它的不确定性如果比较好的话，那这个模型是没有像一个傻瓜一样指挥部模板的。所以PPL这个困惑度是尽可能要低一点。
	现在我们在这两幅图里面能看得到，不管是在OPT这个模型上，还是在bloom这个模型上，其实这个FP16就是没有量化的模型，肯定是表现最好的。但是在这幅图的这个X轴我们能看得到，其实越来越大。就是当我们的模型变得越来越大的时候，我们使用这个四位的就for bat的这个GPT q来进行量化，几乎没有太大的精度损失了，就是小模型，就是一亿的这个是一亿的或者说这个1000万，这十的0次方，这个其实是还好，到这儿其实我们能看到没啥变化了。到这儿为止那是这个10亿的就更没啥变化了，那这个100亿的几乎就一样了。
	但是还有一个虚线，就是这个RTN被吊打的这个基线baseline，他是比较夸张的，因为他没去做这个对应的一些优化，他就只是把16位的给他砍砍成四位存下来，这样肯定不太会好的，然后也不稳定。你在不同的这个规模上也可能就不太稳定，就会出现这样的问题。视为如此，在bloom上面的3 bit的优化也是一样。所以整个实验结果其实我们能看得到，通过这个GPT q这样的post training quantization去量化之后，在越大规模的大元模型上，其实它的精度损失越少。这个也好理解，越大的公司越臃肿，越有闲散人员，对吧？
	那我们就大概过一下，这个是它的一个基础的一个实验结果，后面还会有一些详细对比。这个是在我们刚刚看到的，我们有这个大的模型。从小到大就刚刚我们看到有十的负1次方，从这个125M应该是1.25亿、3.5亿，13亿，一直到1750亿，然后这个PPL这个指标，GPT q都是一直低于这个RTN的。尤其是当我们看到从这个13亿开始，明显就有一个下降。然后这个4B、3 bits差不多都是这么一个情况，下面的这个bloom也是一样的，所以我们就看这个实验结果，具体讲一下他怎么做的。大家也不用太太纠结这个数学公式或者怎么样去实现的。我大概把这个逻辑给大家讲明白，然后我们能用就行了。并且这个方法现在也有被后面的一个AWQ要被超过的这么一个趋势但是我们了解一下这个GPT q的思路，也是很有名的一个团队做的作品。
	GBQ它的这个量化算法的核心流程，其实很数学，但是也很有意思。首先他在这个paper里面有引入了一些数学上的公式，就跟我们看ADA Laura和这个q lora一样，尤其是ADA罗A引入了这个SVD的分解，那这里也一样，它引入了这个切尔斯基的这么一个分解，大家先忘掉这个切尔斯基，简单来讲就是整个paper里面通过做实验发现了几个特点。第一个特点就是我们直觉上来说，我们要让这个模型以最小的显存，能够保留它最大的信息。我们通常是想说把一些有价值的权重给他拿出来，然后把那些所谓的没有价值的权重给它就不要了，那么有价值的权重如何被量化，这个其实是一个问题。但假设我们有一个方法去量化，就是相当于我每位一批数据，我们就知道哪个权重对产生这个结果影响比较大。我们可以用一种贪心的策略，大家想象一下，可以用一种贪心的策略去把每一批数据喂过来的这个比较重要的权重都拎出来。然后相当于每次都是取以贪心的方法来取当前的这个局部的最优，然后最终看能不能获取一个全局最优，这样的一种思路，这个是比较符合直觉的做法。但这个做法在大语言模型，就是在这个百亿千亿这个规模上的时候失效了。
	这个在论文里面也有提，他们对比了一下，不管我是以这个贪心的顺序来进行量化，还是我任意的顺序来进行量化，其实没什么大的改进，几几乎没有改进。然后这些顺序对整个我们的，就是我去去取我们的模型权重的这个顺序，就没有啥对这个量化带来什么样的帮助了，所以顺序就不再重要了。然后我们要去训练这个模型，要去训练一个量化的模型，其实也需要去通过反向传播来更新这个参数。但是我们去像以前做做训练也好，大家都知道一次正向的传播，前向的传播。接着就需要把这个delta做一次反向的传播，然后加回到这个模型权重里面去更新一次模型，然后接着我们再开启下一次的正向传播，但是在这个GBTQ里面他他发现第一反向传播需要求导，这个大家都知道。然后你的模型越复杂，它求的导的层数越多，求一阶导、二阶导、N阶导。那这个求导的过程，这个微分的过程本身就是非常耗算力的那能不能在这个过程当中，因为它要提速，能不能这个过程当中我不要去每一次都去更新这个模型的参数，可以延迟，然后批量的去更新这个模型参数，从high lever层面上，这是两个很重要的洞察，顺序不重要，因为我无法一次找到一个最佳的顺序，然后乱序对我的结果也没有影响。
	然后就每次从我的模型参数里找一部分来做这个量化就好了。然后我的更新也不需要每一次都更新，我可以延迟的批量的去更新，然后这个也是分布式的计算里面有时候会用到的一些技术。第三个就是就从high level层面上。
	第三个引入的很重要的一个数学上的方法，就是切尔斯基分解，这个又是一个很很讨厌的数学的词。简单来说，其实是这样，就是我们在做这个量化过程当中，其实参数量是非常大的，是千亿级到万亿级的这么一个参数量。然后如果我们要在这个量化的用新的数据集去测试它的过程当中，不能保持它的稳定，就是我们整个秋季过程不稳定，比如说你东一榔头西一棒子的去找这个参数，然后去做量化求解，那可能最后会导致整个结果没有收敛到一个比较好的状态。
	通过这个切尔斯基分解，其实本质上他想解决的一个问题就是能不能更稳定，然后更高效的去求解一些这个线性方程组。然后通过求解它减少一些数值的不稳定性。这个是一个纯数学的方法来解决这个问题，让我们整个求解过程当中的这个算法的鲁棒性、准确性更高，尤其是在这个模型规模比较大的时候，这个数值稳定性是非常重要的。这个大家如果上过对应的课程应该知道，就是我们在本身现在这个量化求解过程就是超过有效精度表达了。那如果你的这个数值计算还不稳定，就会导致中间过程就有问题。那你怎么样去求解，都达不到一个最优解的。
	所以在这个过程当中，切尔斯基的引入是为了让我们的中间过程尽可能是数值稳定的，不要再搞到一半的时候，你就已经超过这个有效精度了。然后你再怎么努力，其实用处也不大了。这个是一个非常棒的一个引入，就是从high level层面上有这三个很重要的洞察。
	具体来看，我们通过这个图解能理解他是怎么做的。其实他要做的事情，我们跳出这个数学来看，就是我要在一个千亿级的大矩阵里面，其实就是我的模型在一个千亿级的大矩阵里面摘一点，摘一个子集出来，摘这个子集出来，我去算一算，算完之后我看看这一批大的大的千亿级的模型里面一个小的子集。比如说是1000万或者说一个亿，1000万或者一个亿的模型参数里面，我要去算出一个，在我的给你的这个数据集上算出一个数，这个数里面也能够反映出我这个一个亿的这个参数里面谁比较重要，是这么一个逻辑。那么在这个过程当中我们怎么去算，其实就跟这里一样，我们如果能通过这个切尔斯基分解，第一步就解决了，我不需要去算1000亿了。因为其实整个就跟SVD的分解的逻辑一样。
	如果我一开头就把这个千亿级的矩阵变成了一个小一点的矩阵，是不是就很爽，对吧？那现在假设整个这个模型是用一个矩阵来存的，那么切尔斯基分解其实就是可以通过一个经典的数学方法，把这个矩阵分解成一个下三角矩阵和它的一个转置的一个矩阵的层级。你可以理解成就把这个大的模型切成了一个下三角矩阵，和它的转值乘起来就等于原来的一个矩阵。那这样就很爽，就模型参数少了很多。
	再用一个类似于这个滑动窗口一样的方法，其实没有这么简单，就像我们这儿的这个黑色的这个框左边有一个图，然后图里面有一个黑色的框。然后这个锯齿一样是因为它已经变成两个三角矩阵了，这个框框然后去选取，就是我们刚刚举的例子，这一个亿的参数，然后这一个亿的参数里面目前我们正在算的是哪儿呢？其实是这个白色的这个中间有一个白色的这个条，这一条就我现在鼠标滑的这个，我不知道能不能看见鼠标，就白色条滑的这个。那这部分是真正我们在进行量化的这些参数，这可能就是一个亿了，因为整个是千亿甚至万亿。
	这个白色条里面的这些参数被我们拎出来，然后算完之后它也可以不用每个都去更新。它可以分批次的延迟的去做更新。就比如说我们这个白色条的这个参数，其实你再展开，就会像我们右边看到的这个权重的矩阵。然后你可以colum by column，就是一列一列的去把它规整好，变成一个batch，然后再一批一批的去更新，那这样的话其实就有很多好处。第一个就是总的参数要算的变少了。第二，更新的时候不用每次都去更新，实时的更新，一批一批的去更新，然后这个过程还可以递归，计算机编程就比较方便了，并且因为他用了这些方式，比较更贴近GPU底层的一些实现，所以还能够进行一些加速。那也因为这一堆的整个逻辑捋下来，这一堆的这些，技术的叠加，使得GPT q让我们能够用一个GPU，当然不是这个16GB的，开始能够去玩一些比较大规模的语言模型了。
	好，那么这个是GPT q的一个实现，我们这就不再去装太多的细节了，大家理解它的这个核心逻辑就好啊。然后在这个文档中我们看到GBTQ，它其实它的基线还比较简单，就是跟RTN去做对比。跟RTN的这个对比，除了刚刚我们看到的VK two的这个text，然后也有一些包括他自己有一些不同的超参数可以去做设计。这个group size就是它的一个很重要的一个超参数这里我们能看到最后一行有一个三斜线，G1024和G128，分别就是指它在3B然后1024的这个group size和128的这个group size的情况下，去做的对应的量化，得到了一些不同的值128是它的一个可以算是一个经验值了，也是transformers这个库里面推荐使用它的一个经验值。他在我们刚刚看到的叫困惑度，上面有一个比分。我们都知道这个ACC准确率可能是另一个大家都更熟悉的一个参或者说指标matrix。那么在这个ACC上面，我们看到GPT q其实不管是这个4 bit还是3 bit，在对应的两个大模型上，都几乎快接近于我们量化前的这么一个模型的。
	准确率了。
	好，那么刚刚讲的这个GPT q是很数学化的一个表达，我们再分享一个也是transformers s知识的一个量化技术。这个方法其实是22年，就是去年下旬的时候发到了这个archive上面。然后我看今年又更新了一版，并且今年的更新进一步的去提升了它的效果。我们待会看到一些实验数据很有意思，它这个方法叫激活感知的一个权重量化activation aware wait quantization。它比起我们刚刚看到的这个GPT q的方法，其实会数学方面的一些能耗，数学方面的一些背景知识要求会更低一点，会更直觉会更让你有直觉上的一些感觉第一就是他提出来我们对于一个模型的量化。当然所有人都知道不需要对所有权重都进行量化，只需要保留一小部分在他的paper里面提出来1%就够了，99%都是废物，都是不重要的，1%就够了。然后对这个1%的权重去进行操作，进行量化。
	那具体怎么做呢？第一就是这个AWQ它本身在设计的时候，就是专门为LLM去做的设计。然后这个也是出自韩松老师之手。大家如果了解MIT的这个韩松老师，其实一直在模型量化这方面有很多年的造诣，在深度学习阶段就发过很多代表性的论文，那么AWQ这篇paper就是第一专门为大禹语言模型设计的一个量化。然后聚焦在什么地方呢？
	它不再聚焦于我们的整个大语言模型的权重，而是对我们的这个激活后的一个分布去进行了一些研究，所以我们能看到它这写到了观察激活而非权重。就是我们激活之后，就所谓的激活就是大家都知道，我们的这个神经网络里面最后会有一个激活层，或者说叫激活函数。然后这个激活函数值激活函数之后的这个值，我们会看它的分布，而不再是直接看那个with本身的一个分布，这个是跟之前的很多方法都不太一样的。
	然后第二个，就是它不用去搞很复杂的反向传播和重构。像我们刚刚看的GPT q它其实在优化这个反向传播的这个过程。包括这个重构的过程，包括刚刚讲的这个切尔斯基分解，其实也是在优化这个重构的过程。因为重构的过程一定会遇到刚刚说的这个数值稳定性问题。然后你只要上了反向传播，就会有一些很大的性能开销，这些都是这个问题。那么AWQ它没有太依赖这个第四第五个就是它其实不只是适用于我们看到的GPT，刚刚GPT q一来就写的很清楚，它是为它是一个叫做加速post training quantization for generative，我们的pre train transformers。但是AWQ不是它，其实它更通用，待会儿我们也能看到它的实验结果，不只是在大语言模型上，在一些视觉任务上，包括最近比较火的叫做视觉语言模型上面，也都很表现不错，就有点像我们之前讲过这个IA3那篇paper，就是在大语言模型在transformer的这个激活层上面去做文章。它它freed所有的参数，除了这个激活层，然后在上面去做这个呃微调，还能达到一些不错的效果。
	那AWQ也有一种直觉上了，就是simple is best。它很简单，他他直觉上就让你很喜欢，结果它的效果还特别好。然后最后还有两个他在paper当中的贡献，一个就是说他自己给这个AWQ这个量化方法开源了一个对应的推理的框架，就跟我们在深度学习阶段早期的时候都在这个TF serving上面跑服务器上面跑。后来出了这个TF light，像英特尔除了open VIO，现在大语言模型也都在往端侧上面去赶。那你要在端侧跑一定要做量化，深度学习阶段也是一样的，要做量化才能部署。你像这个包括后面出的这些mobile net，在框架层面上去做了瘦身，那么现在大模型框架层面上还很难做瘦身，那就把每一个相当于总量很难做瘦身，那就把每一个参数的存储空间更小一点。然后AWQ其实就这么回事儿，并且他也在总量上做了瘦身，因为它只要1%的权重就够了。然后对应适配的这个框架，能够去提升它的推理速度。
	适用于一些桌面的设备和一些移动的一些设备，也支持一些边缘设备的部署，比如说下面的这块边缘设备，就是invidia的一样的模型，这儿我们就大概总结一下这它的一些特点和它的一些价值贡献。那怎么做的对吧？他具体是怎么做的？但RTN肯定是会被大家拿来比较的一个baseline，经常会被拿来比较。我们看这幅图其实是三种不同的方法的一个比较，就paper当中的第一幅图，这幅图其实我们细看一下ABC3种方法，其中每一个方法的下面都会有一个PPL，这PPL就是我们刚刚讲这个困惑度这么一个指标，这个指标是月越好啊，那么这个指标其实我们能看到，显然第一种方法最差baseline对吧？后面两种都好啊，但是这两种方法里面，这个C方法第三种方法更好。
	那分别是什么方法呢？第一种方法就跟我们在GPT q里面看到的一样，就是用RTN直接去量化我们的模型权重，它的这个配图也很形象，就比如说原来的这个float，16 16位精度的这么一个浮点数。当然就有小数点了，那你直接给它干成这个int 3，那它不就变成了我们右边看到数字加一，然后-0.2变成0，负二变成这个负，-2.4变成这个-2，就做这么一个RTN。Round to nest就这么一个很粗暴的一个方法，那它精度必然不会好，对吧？大家能想象到你的损失太多了，那中间的这个方法是干什么事情？
	就是我们看到中间这个B的这个方法，它其实是我完成了他想要的第一个目标，就是我只找到1%最重要的模型权重把它找出来，然后把这1%保留它的精度，剩下的给它干到这个inter 3。这个逻辑其实能理解，就相当于我举个不恰当的例子，这个大环境就是假设推特公司没有开人，但他只把那1%的人保持原样，剩下99%的人工资打到1折，那么它消耗的这个开销也会减不少。你算个数，就相当于原来99%的这个开销变成9.9%，再加上那1%，相当于你砍了90%的开销了，其实这个数也很好算的，那么中间这个方法其实就这样，我把百分分之99的参数我都给他做了int 3的这个RTN，然后我1%我认为最重要的参数留下来给他一个FP16原来的这个精度。
	但是首先这个方法它是很好的，从结果上来说它跟最后那个方法是一样的。但是它对硬件很不友好，就是它上面红色加粗的部分写的它是一个bad hardware efficiency，因为它是一个混合精度的表达，实际上那1%肯定不会这么规整，对吧？就是你想那个不是二者一个同学是很低的，因为只有1%，不是10%。实际上这1%就像我刚才举这个例子一样。你如果这堆人他是一个独立的BU或者一个独立的BG然后他干了这事儿，所有人都不知道，剩下99%的人以为，其实百分之百的人都是int 3也没事儿，对吧？
	就大家都都破罐子破摔。都是只有原来的这个一折的钱了。原来可能一个月拿这个1万年只拿1000了，都只有这点钱了。大家都这样，我也无所谓了。但是实际情况是剩下那1%，它是无处不在。它可能每个组里面都有一个1%，那这个就很麻烦了，对吧？你想你的工作怎么开展，你身边九个兄弟，甚至99个兄弟都都匠心了，你还挺挺开心的，你这个沟通不就不顺畅了。在这个正常的硬件的数据交换里面也是一样的，所以他对硬件非常不友好，那怎么办？
	有一个非常好的思路，就是我们看到最右边的这个方法，做了一个什么样的思路呢？就是他在这个量化之前，我们这个整体就不做这个混合进度了，在量化之前，然后先去做一个scale，去保护这些纸，你可以理解成什么个意思呢？就是假设我这个3.5不去做，就我鼠标中间这个3.5不去做任何的操作，我去做了一个inter 3的量化，肯定就把尾巴直接砍掉了。但如果我去做一个scale，我去给它一个乘上一个放大的一个值，然后我再去做最后的量化，那它其实它的这个分布还是保留下来的，我不知道这个大家能理解吗？就是你的原来的3.5可能变成了35，但是这个随便说个数，就可能放大十倍或者放大几倍。然后放大到一个就我们之前讲Q罗A也讲过的，放大到一个合适的range，一个合适的尺度，这个合适的尺度我通过这个裁剪到inter 3。仍然保留了它的这个区分度，那么它就还能保持这样的一个他原来表达的这个价值。
	因为最终还是沉潜的，去算一个总数，那这个就还是有用的，那么这个就挺好，就相当于什么呢？就是双十一？这个商品说给你打一折，其实先涨了十倍打了一折。
	然后就这个商家知道这事儿，其他人都不知道，但是所有的人看起来都是我们打了一折，但其实其中有1%的商家是先涨了十倍，再打了一折，所以他的收入没有任何变化，然后他还享受了平台的促销福利，但是其他的99%就硬贴，所以就很很恼火，大概这么一个意思，大家去理解一下，这个思路其实是很简单很直接的，并且结果效果还特别有效。我们看到他在OPT的，就是我们这个facebook这个OPT上面有13亿的，有130亿的，差了十倍这个规模上。然后不同的这个的方法比起来的话，我们看到绿色的这几个值是非常好的这么一个比值了。因为是没有做过，这个就是在FP16的这么一个我们说的这16位浮点数这个精度上，怎么去看这几列，就是在16位的这个浮点数的精度上，首先看的是RTN跟FP16，RTN肯定不行，对吧？RTN把它这个group size搞到128的三位的这么一个量化进度都飙到119了，这个非常夸张。
	首先baseline肯定被干趴下了。好，那么当baseline被干趴下之后，我们再对比这个方法内部，我们开始有提过，它是聚焦这个激活的分布，而不是这个权重的分布。当我们去聚焦这个激活的分布的时候，它的表现是明显优于右边这个权重的分布的。我们看这有一个base on这个activation，这个ACT是activations，这个W是为此是基于权重的分布。
	明显看到我们基于这个激活值，其实就是算完内部激活之后的。这个也很好理解。因为本来那个激活，大家看那signal ID的，就是用来区分这个东西重不重要的。这个东西不重要它就没法激活。所以你在激活上面去弄这个，效果确实很不错。在激活上面去做了这样的一个，你可以说采样也好，或者说群众的寻找也好。我们能看到它还有三个超参数，在激活上比在权重分布上好。
	然后我们到底选多少？1%是拍出来的对吧？是不是3%也可以，1‰也可以。实际比起来，其实1‰就是又小了一个数量级。那么3%其实是大了三个三倍。从这个往上往下来看，其实1%算是一个还比较折中的一个值，1‰感觉有点夸张了。然后3%也没必要让渡这么一个，相当于你要多花多少的显存，也没必要。1%是挺好的，不会太差。像16.91跟16.68的区别，11.39和11.36的区别，43跟12的区别，都已经是这个小数点后的一两位的困惑度的差异了。所以其实很小了，因为大家比的时候都比的是前面这个小数点前的这个值，已经是非常小的一个差异了，所以包括在这个random上面去取，显然肯定是没有在激活上面去取好的。
	所以最终得出一组比较好的超参数，就是在激活的这个分布上面，使用1%的这个参数，然后先做scale up，或者说也不只是scare up，因为它复数的话可能是变得更小。先去做scale，然后再去做这个量化，把它变成低精度的同时还能保持非常好的困惑度。因为我们第一列就是没有做量化的，其实没有增加多少。大家去细看的话，尤其是越大的这个模型，它的效果越好啊。13B130亿的这个OPT上面，10.13和10.43的一个区别，6.7B是一个10.86和11.39的区别，就是已经很好了这个效果。那么刚是OPT那么如果是在lama上面，并且我们把刚刚讲到的GPT q拉进来比我们看到在llama和lama 2上面也分别做了对应的这个benchmark，看得到其实不管是nama还是nama two，它的效果肯定都是比RTN要好的，然后也比GPT q就我们刚刚讲的那个各种操作要好的。
	GPT q还有两种版本，一个就是裸露的这个版本，还有一个就是在我们刚刚讲的那个流程里面再加一次重新排序，一个remark就这个R重新排序的这么一个版本，或者叫real adding都可以。这个版本这两个版本比起来重新排序有一点点的提升，但是也不如AWQ，并且AWQ的资源消耗和后续的通用性还会更好一些。所以整体来看，其实AWQ是在不管是在int 3还是在int 4的这个优化上面都是非常出色的。并且在这个70B就700亿和650亿的这个规模上，也是最接近于原始的这个模型的。
	除了我们刚刚讲到的语言模型以外，最厉害的点就在于AWQ它居然在视觉语言模型上表现也还不错。我们看到这个是paper里面举的两个事例，都是在coco这么一个字幕的数据集上面。对，这个一个叫做视视觉的语言模型视觉的语言模型，然后一个90亿的一个视觉语言模型进行量化，就相当于有一个已经发布的视觉的语言模型，叫open fly ingo 91。我们再用一个coco的字幕的数据集来对这个视觉语言模型来进行量化，大家来对比好了，从这个结果来看，不管是reera shot，就我们没有任何的零样本的一个提示，大家应该都现在很熟悉这些的这些概念了，或者是这个few shot这样的一些city比起来，明显看得到我们的AWQ在这个表格里面是比我们的这个RTN和GPT q都要出色的，甚至在某些这个地方已经快接近于我们的原始模型的一个表现了。
	然后下面其实是它的一个就是我们在这个字幕数据集上面有一些相对于基线方法的一个提高，怎么样去理解？你可以理解成在这个RTN它是上下是两个，一个是对应的这个机械模型RTN，一个是这个AWQ。然后它的这个视觉语言模型就是看图说话。就所有的小学生都会做应用题，也都会看图说话。那么这个四位精度的低精度的这么一个RTN的量化模型，针对我们看到做这三幅图都是他配的一个相当于对文字的一个理解说明。明显看得到AWQ这个绿色的部分是相对图的这个理解，或者说我们说话是说对了的，他没有乱说，是一个正确的这么一个描述。那相对于基线来说，其实这个还挺明显的，比如说我们看到左边这个第一幅机械模型描述的是这飞机肯定不是在天上，对吧？是在这个草坪上，然后包括写的一个小朋友、小宝贝、baby, 他怎么可能抓着这个大象，举着这个大象只是跟这个大象在一起拍照，摆一些拍照的姿势类似的这里明明是这个两只小狗，被类别识别错误为一个人和一只狗，这些其实都是量化的效果不好导致的。
	你可以理解成在原来的flamingo上面肯定是比。AWQ还要好的。但如果你的量化只是一味的把模型变小了，但效果变得贼差，那肯定会有很多的问题。
	就比如说我们今天还举了一个反例，就是GPT q如果你用自定义的数据集去给它做量化，但是你这个数据集没走心，随便乱弄的那你可能本来有一个很好的原始模型，经过你的这个量化之后就变得没法用了，就这么一个意思。同样的，它还跟类似的描述做了一些更多的解释。在这个nova就是最新的视觉语言模型上面的一些对比，模特就不再赘述了，反正大家应该能看得明白。这个GT4当时也做了很有意思的炸鸡的这么一个图的一个认知。
	那么AWQ和GPT q这两个技术能不能结合在一起用呢？其实是可以的，这个应该也比较符合。如果大家把它捋明白了，其实是能理解的。他俩没有冲突，就是GPT q和这个AWQ大家想象一下他们是一个什么逻辑。就是GPT q是把这个原来的模型，然后我能够用这个切实G分解，把这个大矩阵做成俩。就是假设原来是一个大的矩阵，然后我能把它的下三角矩阵找出来，然后在这个过程当中还能保证它的数值稳定性。
	那么AWQ是一个什么量化方法？它就是找原来这个大模型里面，我从它的激活里面的分布，去找出一些比较重要的值，然后去做scale。这俩其实一个是带卷积去变小，一个是怎么样去选它的方法比较巧妙。所以这俩其实结合在一起本身是OK的。
	实际来看，刚刚我们看到AWQ在很多的模型上，跟GPT q比起来是有一定的差距。通过两个技术的叠加，尤其是在小模型，这个AWQ居然会比这个GBTQ还有更强的一个差距拉开了，相当于之前他们有差距，但可能是一个90分和80分的差距。然后在各个阶段的大模型上都一样，或者说在各个尺度的大模型上都一样。但是通过这个吸收GPT q的长处，在一些像13亿、27亿这样的模型上，居然还把差距给拉开了。这个是很有意思的一个表现。
	然后当然整个AWQ还提供了一些对于硬件的一些友好，包括在边缘设备，在消费级的显卡，我们看到的这个桌面端的RTX4090和笔记本端的4090，还有一些边缘设备上，都还能够跑起来。并且能跑一些70亿甚至是130亿这样的一些大的大语模型。这个其实是很有意思的一个发展方向，大家如果经历过这个深度学习阶段就会发现，就是这么一个流程。就所有的技术发展，这个模型本身，然后框架，然后设备厂商，往边缘端走，最后大家再跳出来，我要统一一下这个模型的架构实现transformers干的这个活就已经有这个趋势了。Hugin face通过这样的一个核心设计，把很多生态的一些资源方都绑定在了这个auto model auto class这一套体系下，就很强。对，大家待会看代码就能明白了。
	好，那么简单的对比一下这个AWQ和GBTQ。首先GPT q它是专门为GPT，或者说专门为pre train的transformer设计的这么一个量化技术，然后核心是高效然后降本。AWQ其实更通用，因为它针对的是聚焦于去激活上面的分布之后，去按照那个超参数1%也是可以去人为设置的，然后通过这样的一个手段，能够去把原来的大模型找到一些最关键的权重。然后把它最关键的权重通过scale再量化的方式保留它的这个分布之间。你可以认为把把它的这个差异这个distribution还是留下来了的。那通过这个方式，我们能够进一步的既减少误差还足够通用。所以它就不只是适用于这个GPT模型，也适用于其他的多种模型和多种任务，包括未来的这个多模态可能也是合适的。因为它其实说到头，它是适用于神经网络这样的一个架构的，然后基于激活而不是权重来进行选择。
	而GPT q是一次性进行权重的量化。像我们刚刚看到的那个那个矩阵进行分解之后，然后有一个滑动窗口，从头到尾到底，然后一路捋下来，有一个像滑动窗口一样的，然后再一批一批更新这样的方式。然后在这个精度和效率上面GPT q有一个非常极端的设计，甚至可以支持两个两位，就two beats来进行这个模型的量化，在他的实验里面还有一定的合理的准确度，就不是很高的准确度，但还能用。就相当于没把那个视觉模型变成瞎子，或者说没把一个大学生弄成这个幼儿园，至少他是个什么接受过教育的对，是这么个意思，但是也不可能达到原来那个水平。但是AWQ相对来说我们看到了，即使把参数改到1‰也还行，没有特别夸张。
	然后硬件的适应性上面，AWQ他的自己推出的这套推理的框架，可以广泛适用于这个桌面端、移动端、边缘端。所以这个其实是更有野心的一个一个思路，然后在推理性能上都还可以有一些提升。然后这两篇paper我也放在这儿了，回头应该今天也会上传到这个平台，然后大家应该明天就能在平台上访问到这些paper了。这两篇配件AWQ是MIT的韩松老师，然后GBTQ是苏黎世理工。我们也看到苏黎世理工他们之前也有一些工作做的一些相关内容，主要就是为GPT和OPT做的。
	最后再讲一个，就刚刚我们看到AWQ，其实这个量化方法很美，还有一个懒人的量化方法是transformer，是实现的一个很很傻瓜式的一个量化方法。放到最后也没几页跟大家讲，其实这一页就能讲明白，特别好用。然后它是干嘛的呢？叫做best and bites。这个其实就是硬件级别的一些GPU本身的库大库里面有一些你可以认为有一些buff可以加到这个transformers上面来。然后beat and bite这个简写叫BNB本身它就是一个自定义的一个酷大函数的轻量级的包装，一个CUDA的一个rapper。然后它的这个八位的就是8 base的优化器，包括它的这个矩阵乘法和它的这个量化函数，这都是我们整个大语言模型量化需要用到的。这一套都是他酷大本身就去支持的。
	当然他也对酷大的这个版本什么的就会有一些要求，它能支持混合精度的分解，就比如说我们刚才看到的整个量化过程当中，有大量的方法是需要去用数据集，然后不断的去做反向传播和这个分解的。分解就重构这个矩阵的。那么它能支持混合精度，所以如果你有混合精度，它有一些原生的硬件支持，然后它支持inter 8的推理，在硬件层面上支持这个英特八的推理。如果你是用的巴比特的这样的一个整数进度来进行存储模型的话，可以直接用这个来推理，不用再去内部做转化了。这个可能在咱们课没讲过，应用开发课的时候跟大家讲过，扩大的内部到底是怎么回事。
	有一个就是一个批量的一个流处理器，然后里面有对应的这个算子，有这个tensor的这个算子，有32位的算子，16位的算子，int 8的算子。然后现在的这些A系列已经开始把很多不必要的精度就直接拿走了，然后就保留下来的这个芯片上面能烧的就全是一些浮点数的一些运算。但tensor的矩阵相乘单独一块大的，它支持原生的into推理。
	然后也有一些8 bit的一些优化器。你可以理解成就是直接在芭比的这个精度下不用做转换，就可以直接去做反向传播。那就我们讲的这个很好很耗时又耗这个显存的这么一个优化过程，就是我们求反向传播的这个优化过程。Of optimized这个玩意儿它是有8 bit的，然后它的嵌入层也更加的稳定，然后也支持这个babbit的量化分位数。我们讲这个q lora的这个NF4的时候有提过，然后能够支持快速的分位数的统计。
	因为整个Q2的过程当中需要用这些，然后在transformers的这个量化方案当中，我们开始讲的那两个都是transformers的原生支持的，通过它对应的config就可以去做配置。当然我们也讲了config是在模型上的config，所以你配置了这个模型的conflict，这模型就能被量化了，然后在transformers里面有两种量化的方案是最简单的，一个就是这个八位的量化。我们刚才讲了，BNB在扩大层面上全是英特八的各种各样的优化，然后还有四位的量化。然后四位的量化里面，就我们刚刚看到paper里面有大量的int 4，包括int 3。那我们在讲QLA的那一节课的时候，应该有跟大家看过，现在的我们做大语言模型的时候，这些不同精度的浮点数是大概什么样的。包括八位的浮点数和8位的整数都有对应的表达，尤其是八位的浮点数，还跟大家看过有两种方式，一种是四位都是表达小数的，还有是这个五位表达小数的，它的指数分别是三和2，不太一样。类似的这个四位的这个transformer里面，这个BNB也支持两种。
	一种是我们很显然就看得懂的，在论文里面有的这个int 4，就是直接支持这个四位的整形。还有一种就是我们写到的这个q lora，在q lora里面提出的这个NF4Normal float for，也是这个transformers原生支持的。并且我们在QLA里面还讲到了它有三大法宝用来做这个量化。NF4是首当其冲的，提出了新的数据类型，进一步降低了这个模型存储的开销。第二个就是它的这个双精度double count，它也支持，并且也就是一个参数就能支持。这些其实都是transformers在跟进一些新的量化技术方面非常好的一些特点最后一个预告就是这个PFT，然后我们在这个PFT这应该是下节课就会讲到PFT了，这节课基本上就把transformer最重要的几个部件模块都给大家覆盖到了。然后我们刚刚讲到了QLA，其实QLA就是PFTD，可以直接去在trainer的基础上再去改造的一个，或者说再去赋能增强的一种高效微调的方法。
	Hugin face自己也加了一个像radio，这是一个radio做的一个前端，就是PFT当前支持哪些模型，哪些方法，大家可以自己去通过这个lead world去看一下，就在PFT的这个主页上，就能看到这么一个类比。像我们之前讲过的这些技术，这些PFT的技术在这儿都是大部分支持的。比如说Laura，prefix 2，IP2i prom，tony IA三等等，然后对那最后这个transformers他还贴了一个很有意思的对比，就是他现在支持的三种不同的量化方案的一个基准测试，这个基准测试是在他的NVIDIA的1000上面运行了。然后在这个两个不同的模型上进行了测试。然后也与这个BNB的量化方法和原始的这个16位浮点数的方法进行了一个测试。然后我们看得到使用到的使用到的资源的角度来说，明显肯定三个都大大的降低了这个资源的开销。因为上面这根线是没有去做量化的，这个不用说他们都能很好的去节省资源，然后便秘的这个方法在越大的比赛上面越能够优化，那几乎是一种有跟这个量化过程无关，一定破晓直接去做的这么一个在数学上面的优化。所以有这么的一个特点。
	然后在吞吐量这个角度上面来看，我们看得到之前的竖向的这个值是每秒钟多少个token。然后从吞吐量的这个视角来看，跟AWQ的这个paper里面讲的是比较像的。就是我们在不同的这个比赛上面，它的这个时延居然还拉大了，那这个可能还有点时间变居然变长了。这个可能我还得回头再细看一下他的这个benchmark具体为什么这个AWQ还拉长了。但这几个benchmark其实还是挺说明问题的。就是我们在不管是在这个模型的量化的效果上，首先上一幅图很明显跟没有量化比起来是资源开销大大的减小。然后在这个吞吐量和它的食盐的角度来看，其实这个AWQ它的吞吐量显然是增加了。但它有可能在增加吞吐量的同时，它的时延也变大了，这个是有可能出现的。
	Anyway这几个方法是我们目前看下来，在2023年看下来，第一如果你要去优化一个大模型的微调过程，然后你又想尽可能用比较少的资源来做量化的微调，那么两种方式。一种就是用post training，就是我已经训练好了。Maybe你在某一种情况下，你能接触到一个已经训练好的大模型，然后你把它进行量化，量化之后有一个小一点的模型，然后这个小一点的模型你可以再接着去用这个QL run或者一些其他的方法来进行量化。这个是一个比较目前来看比较成熟，然后也是有大量的开发者和学术界的人在往前进行持续研究的一条技术路线。如果咱们能把这三个技术都能够首先能接触，然后能自己去了解，然后还能去跑一跑相关的这个实验。最后根据你自己的实际的情况，再去选择不同的技术方案，是比较好的一种做量化和做微调的一种手段。然后BNB的这个方法就是我们的the best这个方法。我们下节课会紧接着讲怎么样把这个q Laura和它的这个NF4结合起来，然后在在训练的过程当中就能够用低精度来进行微调。好，我们接下来就是偏实战的部分，跟大家看一看怎么样去做这个模型量化。
	我们先看一下这个项目本身，有一些更新。我去接杯水，大家稍等一分钟，有什么问题可以提一下。
	上节课的实战算PFT吗？上节课我们上节课有讲PFT吗？上节课没有，我们上节课不讲。的transformer吗？直接用的这个transformer，the trainer来进行反应错误的，就是来进行训练的。然后不还没有用到PFT，对。很奇怪，这个代码居然没有被推上来。No yah. 
	Remote end hang up, unexpected, fail to push some reference. 所以这个代码还没推上去，没事，大家可以待会儿在这个视频看一下，然后待会儿把再同步到这个gipp上好，然后我就我们接着讲，然后待会儿有问题再提。这个项目里面应该是更新了一些内容。然后第一是有两个同学提交了PR，然后也都合进来了。一个是跟这个两个应该都是跟文档相关的，然后其中有一个还是补充了一下怎么用本地的模型的路径来加载这个预训练的权重。然后这里面我们看到有一个详细安装说明。大家如果不太习惯去看这些原始的官方网站的话，有一个同学加了一个transformers基础的一个开发环境搭建的内容。把咱们的这个官方的文档的一些重要信息也都整理到这里了。大家有兴趣可以看一下这个新的read me，里面有一些更新。然后在这里我们确实。
	没同步。
	上来，我在本季跟大家看这个。
	渲染一下。
	好，然后。这个文档里面新加了一个安装的python依赖包，这个我会再持续去迭代。所以大家如果每次新拉取了代码，不知道就看得见吗？再放大一点，每次新拉取了代码之后，是可以直接通过这个pipe install杠r reference来安装python相关的所有依赖，然后就比如说我们今天可能会用到的一些依赖也都放到了这里。比如说这个auto AWQ，实现了这个AWQ这么一个量化的方法。包括这个GPT q，包括这个BNB，也都放在了这里。这个应该很快会推上去，推上去之后大家就能拉取到最新的这个代码。
	好，我们回到这个项目本身，今天要讲的，我们首先来看一下这个GPCQ这么一个量化方法是怎么玩的。在这里还加了一些相关的一些信息，大家如果在平台上没有找到怎么去下论文，这里应该是能直接跳转过去的，能访问到这个r cap的网站，就是他们的这个paper。然后这个方法具体怎么做的，我这就不再细说了。
	刚刚已经给大家都讲过一遍了。我们具体来看一下怎么用transformers来实现这个GPT q的量化。在paper里面其实有提，它是有一个默认的数据集的，我看来还是得给大家打开一下。
	我记得在在。
	see . 
	phone上了。有这个c ball，你看。
	怎么样。
	能够两个页面，就我们看到这个量化器支持这个VK text two，three four three four new PDBPDB new这几个其实就对应着paper里面写的这几个很有名的这个数据集是默认的，它已经知识的。所以我们可以去使用这个论文里面介绍过的这些。大家如果有兴趣可以去详细读论文。但如果没兴趣，就可以直接看看它默认的数据集，包括这些，然后我们可以使用这些默认的数据集来直接进行量化，然后支持的精度我们也可以自己调，就是对应的这两个参数，beats和这个data set。这个大家能看见吗？还要再放大一点吗？再放大一个然后我们还是重启一下把之前的都关掉。
	重启一下看了。
	好，这个地方是它的量化精度。
	这是它的group size，也是它的一个超参数。论文里面默认是推荐128，然后这个GPT q的这个config默认也是128这么一个值，这个是他使用的这个数据集，我们使用这个C4的1个数据集。然后是不是要regarding，这个是另一个超参数。
	在我们刚刚讲的这个理论原理解读的时候，应该都跟大家涉及到了。大家可以再去看一看，这里需要提的是GPT q的config就这么一个玩意儿。其实它就是跟我们大家再看这些代码就逐步找到感觉了。模型tota ized config，这configure就是在普通的config上面再去封装了一下，具体怎么封装的，我们可以看看。
	大家看到这个右边右边这个部分，在hugin face的这个quantization就支持这3种config应该能看得清楚，这三种config然后这三种config，比如说这个AWQ的config和我们刚刚看的这个GPT q的config。那它其实，是一个什么呢？它其实就是一个基础的config，然后进一步的一个封装，然后支持了更多的各种各样的参数，包括像刚刚看到的是否重新排序对吧，然后用什么样的一个token set，这个data set。所以大家其实可以通过这个能大概捋得清楚这个重要的三元组。他们其实构成了你要去做模型的推理模型的微调，非常必备的几个要素。
	好，那么GPGQ的这个config我们主要设置的就这几个参数，使用这个facebook的OPT2.7B这么一个模型。27再大就容易比较慢，然后演示的也不是很顺畅，然后这个模型现在大家应该很熟练，怎么样去找到我们使用的模型？就是把这个model的name加到最后面，就可以找到对应的这个模型。然后这个open这个或者说叫OPT，其实就是对标着OpenAI的GPT，当年的这个facebook meta AI做的对应的一个开放的GPT的版本它也有很多的不同的模型尺寸。除了2.7B以外，像比较常用的应该还有一个一亿多的一个，还是应该有一亿多的版本，差不多它的20分之1的一个版本，还有一个1.3B还有一个6.7B13B都是一些它比较常见的一些模型参数的规模。好，我们这边也没有重启成功。
	好，重启成功了。好，这边已经变成新的这个加载了。一这儿我们开始大家看到这里，其实我们就开始去做这个模型的量化了，这应该会跑一段时间。就我们从这儿定义好了一个config定义好了一个config之后，我们通过两个from free train的方法实例化对应的toc iza和模型。我们刚刚讲过of class的设计，就为什么量化微调都会照着这一套auto class的设计来做，通过这个model ID，这其实就是给了一个model的名称，我们的from free train给到这个auto token ither，它就能找出facebook OPT2.7B对应的tokenizer。然后这个模型也是一样的，我们用的这个from free train的给了这个model ID，然后同时还给了这个config，然后这个device设备的这个map device map设置为这个AUTO，其实是一个quantization里面一个比较重要的参数，这边正好跟大家讲一下。可以。
	应该不是，是在这个文档。
	里是一个。
	底层的支持，需要咱们可以大概了解一下。对，就这个device map其实是在我们加载模型，用from free trade去实例化一个模型的时候，会经常会去设置的一个参数。然后这个参数可以比如说你有这个多个GPU，你可以通过直接指定这个CUDA，然后冒号林去标志你现在这个模型想要加载到哪一个GPU上面。然后有的像AWQ做完这个量化，还需要从CPU挪到这个GPU上面来。还有一些比如说像我们要用的这个BNB，或者说这个GPT q他都会用到底层，这应该没写了。
	我们可以直接到这儿了，就直接就看这个地方没有写苦的，直接写的这个AOTO auto。当你写auto的时候，其实本质上你在用一个新的依赖库，这个依赖库叫accelerate，写到了这个新的requirements里面，看看在这里。这个库就是当当你是把这个DVS map设置为auto之后，其实是用了底层的这样的一个加速库。这个加速库其实就是在用酷大的一些底层的加速，所以如果咱们要用这个参数，需要去装上那个依赖库这边其实已经开始在进行量化了，我们看到有一个进度条，它会大概的完成时间二十多分钟，我们这儿就不去等了，待会儿等这个讲完，我可能就跳到下一个了，之前有一些下午的一些执行结果。
	然后我们这个量化过程当中，其实也都可以通过像这个invidia SMI去看一下大概用了多少的显存，那像这个2.7B27亿的这个参数，然后默认应该是用的16位的这个浮点数。我没有记错的话，其实大概是这么样的一个数字，大家有兴趣可以再去实际的去算一算。然后整个GPT q这个方法有一个非常简单，去检验他是不是量化正确了。这个量化正确是指没有出现这种异常的错误，而不是指它的参数值被量化的特别好。这个是两个概念，就相当于是他没有残缺，他没有中间包一些错误，但是它里面的值到底量化的好不好？就比如说它的困惑度到底有没有到达一个比较好的指标，这个是检查不了的。这样他只能检查你的维度正不正确，这么一个意思。
	然后因为你看到咱们在量化的整个这个decoder layer，所以它只需要去判断这个线性层的这个decoder layer，然后随便找一层，比如说这个layers 0，然后去判断它的这个self attention。其中self attention大家都应该讲了无数回了，self attention里面的QQV，比如说Q然后query他的这个with和他的这个zero，是不是比如说我们这里提到的PITOCH的这个int 32的这么一个数据类型，如果是的话，那么它就是量化正确的那我们通过这样的一个检查能看到这个q wait就这里queried这个位置，和它的这个zero s都是torch into 32的，所以这个上下部的这个量化过程是没有问题的。所以这个是一个比较简单粗暴的一个检查，他是不是出错的办法。然后在这下面会有一些它的超参数，就包括他用的这个四位的这么一个量化。
	Group set设置的是128，然后它的device自己去找到了库大林，因为这个演示的设备上面只有一台提示，只有一张提示的GPU卡。然后量化之后，可以把这个模型加载到我们的这个显存里面，加载起来。加载之后我们不好意思，这我们加载起来之后在这儿加载起来之后，可以使用我们量化后的这个quent model，是我们上面训练出来的，把这搜一搜。这个是我们量化出来的一个模型，就基于我们本来有的这个OPT2.7B，量化出来了一个quent model，然后这个模型就是小一点的这个模型。这个quant model是也是一个model。
	我为什么要讲统一接口？就是你通过这个GPT q量化出来的模型，它也是符合这个auto model的一些基础的接口的。其中最重要的就包括这个generate这样的一个接口。就像你不同的token nier，你都得有这个decode这么一个接口，它有这个generate接口。然后我们可以把把现在的这些混，你自己构造了一些特定的文本去做这个测试。比如说今天是平安夜，我们写这个merry Christmas，I was glad to怎么样，要怎么用呢？也是一个很简单的一个套路，大家理解了这个套路就可以了。
	如果不想深究拍套期的一些底层实现，就是我们在第一节课就讲过，我们人说的这些原始的数据，这些自然语言叫做row text，row text经过这个toka、ized、分词、编码、映射，就变成了对应的内串vector，有它的inputs ID有attention mask等等等等。其中还可以在token net里面加一个参数，叫做return tensor，就是返回它的这个vector，然后这个返回的tensor。因为它本身底层transformers支持PITOCH，也支持TensorFlow，也支持一些其他的那你写PT就是指返回的tensor，是这个python的这个数据类型的样式。
	把这个玩意儿再输到这个模型的generate这个方法里，然后可以限制一个输出，就是这个模型输出的token的数量，就是我们经常调GPT API也好，或者看到的这个所谓的模型的生成的长度的限制也好，就是叫做max new tokens，generate的一个参数，通过这个就能生成，或者说让我们的模型去去generate。其实就生成一些内容，然后生成的这个内容，我们也看不懂，所以还需要通过这个decode。这个其实在前面的课程里都讲过，就是in code在干嘛，decode在干嘛，然后还讲过特殊的编码。以这个bert模型为举例，然后skip这个speciaal token。就是因为如果你有多句话，你不把这个speciaal tokens给跳过。那么像之前讲的这个什么COS之类的，这些用来做句与句之间分隔的这些token也就会都在里面，那它解码出来也就在这儿，会有有碍观瞻，不影响你读这个内容。
	我们看到量化之后的这个OPT2.7B，它的这个生成基于前文生成下文。这其实就是一个小版本的一个二几小几十亿的一个GPT，其实就这么一个模型，它基于我的前文。Merry Christmas. 
	I'm glad to see you still here. Thank you。I'm glad to be here, you too. 因为他的训练语料是这个对话，大家可以去看一看他的训练集OPT的2.7B，还是这个pretrail ed with english text，small amounts of no english data still present with一大堆，然后对这个训练级大概是这么一个情况，所以以英语为主，但你也可以试一试中文，效果不一定好。
	我们举一个反例，就是GPT q这个量化方法，就大家总总希望深度玩家希望玩，对吧？那怎么玩呢？第一我们知道有默认的，他帮你准备好的几个数据集，就是我们看到的这个VKX2、c four、PDB等等。如果你就是想自己造一个数据集，相当于你做了一个data set，然后这个data set设计出来之后，通只要在你这个data set表现好，相当于你知道你的这个数据集的分布大概是什么样的，是适用于什么样的任务的。只要在你的数据集上面表现的好，它就能够完成你要的这个任务，那也行，甚至你可以用更少的数据集，甚至你还可以去提升它的这个量化的效果，量化的速度。因为我们知道整个量化GPT q是一亿波流，就一次量化完这个权重就一次性量化好了。然后如果你的数据集搞得少一点，那么这个过程当中其实它的计算量还小一些，那当然会更快一些。
	这里就是随便举了一个例子，就比如说他就是差生典范，叫做worst practice，最差实践，就是讲这个就最差的就是这样去搞，肯定会把模型搞坏了，就量化不出来好的模型，什么意思呢？就比如说我们为什么样本数最好不要少于128个，因为group size就是128，对吧？你都不够一个group就很蛋疼，还得在你的group里面再去默认的填充一些内容。然后精度四位，然后这个要不要rewarding？可以不要。然后group site 128都是一些精简值，然后最关键是这个data size，你可以自己去折腾一些，比如说also GPT q easy to use model clancys ation meary with other friendly FPS based GPT q aleem。这可能就不好，这对他说的不好，然后丢进去同样的方法去训练，很快大家可以看到下午应该是五分钟就训练完了。
	上面的这个在C4的数据集上的训练，我印象中是用了二十七八分钟，在这个完整的量化完，然后关键是量化之后的结果，它就很奇怪，就相比去你去使用这种默认的数据集，没有经过这种精心准备的自定义的数据集，确实会明显的去降低模型性能。就比如说我们同样这段话丢给他merry Christmas，i'm glad to他就疯了一样对吧？就点点点，并没有输出任何的内容，也很正常。因为你这data set的就做的太奇怪了，里面也没有任何什么merry Christmas，i'm glad to相关的内容。然后他自然可能就尽可能的就把模型的参数整的就很奇怪了，至于具体怎么个奇怪法，我们不去深究。但是这样的一个操作肯定对模型是不行的。大家如果想要自己去准备数据集，也得慎重。
	我这边有写，有一个GPT q相关的一个homework，也是希望大家去试一试。这一套代码是可复用的，然后它其实也不复杂。大家仔细去看啊，整个大模型时代的代码都不复杂。你就用这个GPT q的config，这个甚至都不用变。可能你需要改的是这个model ID，把它从2.7B换成这个6.7B，是一个比较也是一个比较经典的一个模型。我们看看。它的表现肯定比2.7B要好一些，是一个67亿的一个meta AI的OPT模型，它能写一些更长的一些文章和上下文，就不只是跟你一两句话就聊完了，然后这个67亿的模型，我希望大家能在上面去做一做这个量化他也支持这个fast took niza。这个大家可以下来自己试一试，这个是第一个关于GPT q那我们再看看AWQ是怎么用的，我们把这个就停掉了，把这个停掉了。
	好，把GPU释放一下。
	好。
	好，然后再在这个事例里面大家可以看到它的套路第一个就是准备了2个AWQ的，因为我感觉这个方法其实还挺值得大家去试的。如果大家想要去试的话，那可以你可以保留一个，然后在另一个里面随便玩，或者你拷贝一份随便玩都可以。然后这两个不同的AWQ，一个是125M这个也是它非常经典的一个尺寸，就是1.25亿1个规模，还有一个是2.7B，跟其他几个都是齐平的一样规模的一个OPT的一个模型。
	27亿把这边。
	先关掉了。好，然后我们看一下怎么样去去研究这个AWQ。它跟咱们刚刚看到的auto GPT q不一样，但它也是基于一个开源库来实现的，并且现在这个实现AWQ的库还挺多的。然后在transformers里面也有不同的AWQ的知识，其中就有这个auto MQ，这个是其中的一个实现，也都是一些很新的一些代码库。然后这个奥tmq我们在示例里面用的也是这个auto MQ，没有使用这个LLM AWQ。这边也有做解释，是因为它不支持我的这个GPU。就是我们现在the book连接的这个GPU是一个NVIDIA t4，然后LLM AWQ它不支持，所以我们就用这个auto AWQ。大家如果想要去尝试这个LLM的实现版本也可以，然后就可能你得注意一下他对GPU的一些要求。这我就做一个说明，然后我们接着看一下怎么玩这个玩意儿。
	我们刚刚加载的那个是2.7B27亿，这个是1.25亿的一个模型，其实两个模型还是差距挺远的，差了20倍。在这个1.25亿的模型上，我们看看他的GPU用了多少。这个是下午的一个实测，我们可以看看后台。这里有一个窗口，这个是watch杠一直在观察的一个数字。大家应该能看见，这里使用了630兆的一个规模。为什么是630兆？其实理论上它应该只用更少的，用三四百兆。是因为PYTRCH本身会有一些预分配的显存，以防止它崩溃。这个大家如果之前不了解的话，这次应该做实验就会发现pyt CH本身为了维持框架有一些缓存区，它会有一些显存。
	然后我们再来试一下，就假设我们在量化前，我们模拟这个流程，就量化前它会针对一些输入有一些质量相对来说不太差的一些质量的生成。然后如果我们量化之后，看它还是是不是能达到这个水准。就比如说他官方给的这个示例，然后这个也是一个非常容易引起歧义的一个示例，也是在他们在努力的消除这种歧义，比如说偏见的一个事例，就是说男性和女性，就男人和女人的工作描述，就比如说对于这个woman。对于woman来说，通常都是一些什么类型呢？比如说学校学校的老师、护士、然后服务员，包括一些motel里面的一些security guard，就感觉不是不是属于比较有社会地位的一些工作。然后这个男性delivery boy、taxi driver、security guard也还行。
	其实偏见并没有那么大，我们经过量化之后，可以看一眼，这个可以理解。因为新版本的六点这个OPT肯定是在不断优化这个事情的。之前说的这个偏见的问题，应该是可能通过一些新版本在逐步的解决。
	好，我们来实际操作一下这个量化。然后这里跟刚刚有一些不同是什么呢？我们看到把刚刚的也拎过来GPT q放在这。
	刚刚我们的量化是用的是from free trade，然后quantization config，config device map等于auto，那就开始量化了。然后这里我们用的是count的这个config，然后这两个不同的config，其实大家去查这个文档的话，是跟他自己的这个时间有关系。大家看这个名字非常相像，一个是从transformers里面导入了一个叫做auto model for carrol LM，这个叫做auto AWQ或Carol LLM。这俩其实是不一样的两个类，所以他们的这个参数是不同的。如果大家在实际操作的时候，这个coach face或者怎么样的时候，需要注意一下这些细节。其实整个这些类的名字像这个是非常相像的对，然后中间只是穿插了一个AWQ他这里其实已经量化完成了。
	对于这个1亿2000万的这个，然后在量化过程当中，刚刚忘了看，大家回头可以自己观察一下。在量化过程当中，它其实是有一些额外的一些显存的占用的。这个是它的一个量化的配置，然后。
	保存一下，这个比较小的，比较方便给大家做这个live demo就存在了。OPT125M，就是我们的一这个模型就放在这里，这些文件大家应该都比较熟了，我就不再展开了。有些是tokenizer的，有的是咱们的模型的。有一个新的count config这个大家点开看一眼，这个是我们的，就我们刚刚定义的这config，就我们说所有的文件管理，这些auto class即使到最后会落到这个文件的持久化上面来，那这个就是我们刚刚说的这个auto config，这个count的这个config。然后它本质上就是用的auto config来实现的。底层在我们上面存的这几个值，就在这里面，再拎过来对应。
	着我们的这个。
	zero point group size w bat和它的GEMM的这种AWQ的实现，其实就是在这儿把它存起来了。这个AWQ configure就是transformers里面的这个config了，就是我们在这个API文档里会看这一页AWQ的config其实就这个玩意儿，那它本质上也就是我们的一个auto config。所以如果未来会实现一些其他的量化，大概率也是以这样的一个方式来进行初始化和具体的量化。
	然后保存下来之后，我们同样也可以使用这个GPU把它加载回来。我们可以再把刚刚说的这个套路定义成一个函数，然后把这个generate text就放到这个函数里面。比如说这个小一点的OPT，同样的我们给它一个上文叫merriston，as I am glad to, 他会写对应的这个回复，然后像这个the woman works as他也会写，然后他写了这么多个不同的事例，主要是因为我们这个new token设置的相对来说比较长，就跟你的这个上下文使用有关，然后直观的感觉就是感觉一点都不智能。这个模型为什么还会话说一半，然后还不换行？是因为一个最裸露的，几十亿或者说几亿规模的大模型，它就只是一个概率模型。所以它就只能做到这个程度，跟大家想象的这个ChatGPT是有很大的差距的，因为本身ChatGPT就比它要强太多了。
	但如果我们要去做这些应用，其实也不是不能做的，也会有一些办法去做，后面我们也会去基于这个几十亿的再去做RAG，但这个应用的搭建就是这么样的一个过程，好吧？然后下面也有一个homework是跟这个AWQ相关的。同样也是希望大家能够去实际操作一下，使用这个AWQ来量化一下facebook OPT6.7B这个模型，这留了一个索引的链接是去访问这个hugin face上面，这个是facebook的这个主页，在hugin face上面的一个主页，它有哪些OPT的模型？就我们刚刚看到的125M，1.3B，我们用到的2.7B. 
	包括它的6.7B13B. 
	这个都是在它的各种paper里面广泛出现的，也都是下载量比较高的，应该可以通过mos当mos排一个序。
	对，我们还有1.3B125M这俩应该是下载最多的。分别是前两个，在论文里面经常会出现的前两个规模都是60万以上的下载的量，然后350M和2.7B也是比较多的访问，包括这个6.7B这个相对就会大一点。跟大部分的大语言模型，要做到普通的个人开发者或者小B小的团队能用的话，通常都会在这个67B这么一个尺寸，六七十亿这么一个规模。
	好，我们讲了这个AWQ，其实很简单，就是活学活用。就是学会用AWQ的config把对应的几个参数设置好啊，然后同时它的model要使用auto AWQ的这个model。但是这个auto AWQ的model它也保持了接口的统一，使用这个model的这个path或者model的ID，然后传进去就可以了。对应的这个2.7B其实是一样的操作，用来给大家做这个试验的我这就不再过多的展示了，也会传上去。接着最后有一个关掉了，不小心有一个这个BNB，怎么样用量化是还蛮方便的，刚刚其实整个都很方便，刚刚看到这两个量化方案也很方便。
	怎么样用这个transformer的beat and vice的量化技术，前面有讲过这些，包括这个异常值是什么，其实就是其实就超过了这个range了，超过这个range之后就不太好，这个就是我们开始说的，我要先scale，然后才能够把它做到这个低精度量化之后它的差异，它contribution尽可能不受影响。但如果我不论怎么去做这个调整，它都大于我上下两个阈值，那这些值就是属于range以外的值，就像我们正态分布的小概率也会被丢掉。但是这些值它又是比较重要的权重。它因为超过了这个阈值，所以它的精度就会有损失。因为它被量化的时候表达无法尽可能的保持原来的distribution，就这么个意思。
	让我们看一下，如果要在这个transformers里面去使用这个BS and bite这个BNB的这个技术，其实它做的更直接，在我们的transformers标准的auto model里面，这几个类别大家还有印象的话是不同的NLP模型，是不同的子类。我们在课件里面都有题，再给大家看一眼。这里有写N1p，natural guage processing，我们刚刚用的这一系列的OPT模型都是属于这个序列的。好，然后像这个birth就是另一种，然后就这个模型本身的from retrain的方法里面，当然这个device map刚才讲过了，就是它能用这个底层的加速。显然这个lora或者说我们现在用的这个BNB都是扩大层面上的一些硬件加速，所以你要用这个auto accelerate。然后同时我们要把这个模型以低精度来进行量化。
	刚刚也提到了要么是八位的，要么是四位的。那这个八位和四位的其实就对应着模型里面本身就已经原生支持的这个load in eight bit或者node in forbid。就这俩参数，你把这个参数打开，它就会以forbid的方式来加载这个模型，所以这个其实是非常简单，但这个forbid是int 4。
	这个需要稍微加载一点时间。好，我们可以看一眼。
	这个应该没法直接看到每一层的数据，得打开每一层。我们下午有在这个实测上面去看，能看到它的一个实际的一个占用，是一点这边可以看到这个是我们通过NVIDIA的SMI，可以看到后台这个2.7B占了1.7GB其实相当于1.7GB的这么一个显存。大家会发现其实这样看很烦，对吧？如果你不是为了实时监控这个GPU的数据，那么其实这个model类本身也可以有一个方法叫get memory footprint，就是去获取你当前这个调查的这个模型。就是这个模型有一个方法，哪个模型调这个方法他就会去查有一个get memory footprint，然后我们再做一个单位的一个换算，就能看到其实这个forbid的这个OPT2.7B占用的显存是1457，跟我们实际看到的1779还有差距。我可以实际。
	去大家。
	能看到这个屏幕，848是开始的AWQ的占用的那这个1774是现在的OPT2.7B的int 4量化占用的。然后我们都知道酷达现在都很智能的，它是能够支持。如果你的显存不够，包括pyto ch本身也做了支持，它是能够支持你把一个GPU的显存掰开给几个进程去用。所以这是针对两个不同的notebook，两个不同的模型分别加载到同一个GPU的这个提示里面。然后我们可以看到这个是这样，就可以用int 4的量化，是不是很很神奇？其实就这一行，假设我还想要用这个NF4，当然你可以去执行一下生成的这个实例，merry Christmas，今天跟大家说很多回，平安夜。
	当然它int 4直接很粗暴的，int 4的量化之后大家看到这。I'm glad to see you are still around. I'm still around, just not position as much. I'm still here, does not position as much. 
	开始重复自己了，所以直接int式的量化还是有很大的风险的，毕竟这个效果有限。那么用NF4的量化，从这个效果的角度不会变得更好。因为F4更多的是为了让大家接下来下节课能够去跟PFT的q lora去做对接，然后我们可以用F4再加载一下。然后它的这个调整也很简单，就是在这个beats and beats config新增加一个config，前面这个是直接一行，在from free trade里面，如果你只是做8 bit或者这个format，它直接有一个对应的参数。但如果你要做NF4，它是一种新的技术q ora的那它一样的需要有一个config，就跟我们前面看到的GPT q和我们的AWQ1样，有自己的一个config。这config也是在quantization里面预定义的，我们在这儿能看到它的。
	这个config，这是它预定义的一个config。好，然后我们回到这里，那么这个configure里面同样的node in forbid，但是这有一个quant，NF4就是你forbid的量化类型是什么？就同样是四位，你可以有很多种不同的数据类型，现在我们用NF4。
	好，加载完，它的NF4的这个显存占用1457 1457.52。但是大家看这儿，这些细节还是都在这个魔鬼都在细节里。大家看到这里没有3414，简单来说，不管你是用这个int 4还是NF4，这个模型本身的尺寸它应该是一样的。因为它都是format，只不过int 4它是直接给它消成了四位的整数，而NF4是变成了这个Normal flow 4，但是他们占的模型的大小是一样的。
	这里我们也能看到同样一个进程，刚刚是1700对吧？然后现在变成了3400。因为他现在在这个进程里加载了两个四位精度量化的OPT2.7B，那假设我们还可以进行双量化，对吧？我们看过QLA这个paper，这里有写forbid的这个transformer还需要还可以双量化，那个图里面看不出来，在我们的这个理论课里的slight这里有讲过，那双量化其实也是就额外的一行代码就能够实现。
	我们在这个地方大家看到有一个use for BNB forbid you double quant，这里是可以去直接去实现这个双量化的，我们可以在演示一下怎么样去实现这个，这个其实就是一个嵌套的量化。当时跟大家讲过，就是怎么样把它的，因为第一次这个量化就是把一些量化常数可以再量化，可以类似的方式可以嵌套一下。然后1457，这个大小是完全一样的。但这4910，因为每一个虽然都是1400，但是py touch在这个模型加载到显存里面的时候，它本身就会预留一些。这个刚才也讲过了，所以会有一些差距。所以大家可以通过这个方式来严格的更精准的去去计算你的这个模型到底是占用了多少。
	然后这个四个bits的量化明显是降低了模型的开销的。因为我们刚刚知道2.7B，它的这个实际需要多少是很好算的。刚刚其实是用6.7B做的，比较几乎是乘2。
	那正常的2.7B16位浮点数应该是5.45.5GB，就五千多兆。那现在因为变成四位精度了，所以就4分之1，就从5.5个G变成1.5个G差不多。好，这个其实就是今天要跟大家分享的主要的一些实战的内容，就量化的部分。然后剩下的这个时间，因为今天的这个时间关系，我们到九点半结束，然后下节课我们再准备这个PFT的这个内容。看今天这些关于模型量化的部分大家有哪些问题。其实总结一下就是transformers它的outclass三元组、auto model、auto config、auto kani、zer这个三元组共同把一个模型所需要的所有的配置，然后权重，然后他的这个token ither就分词相关的一些内容都做好了。然后做好之后，我们的量化也好，后面要讲的PFT也好，都是在这套机制上面去做的扩展。包括我们上节课讲过的这个trainer就是要去训练微调的这个trainer就是PFT能够在上面去做的一个扩展，这样我们可以再给大家录一下这个trainer。
	这个玩意儿就这个trainer，其实PFT就是扩展了这个trainer。但它能够扩展的前提是大家去看这个trainer里面核心还是有这个model。然后model其实就链接了这些，包括像token ized，包括像这个config都在里面。好，看看大家有什么问题。
	这些量化参数能用在别的模型，比如lama two吗？当然可以。这个同学你都忘了我开始讲GPT q的时候，讲GPT q的时候不是还对比了llama和拉玛兔吗？包括AWQ。Chat GM6B能用aw q的方式来量化吗？可以，没问题的。
	工程上怎么评估量化后的模型与原模型的指标？你要看你最终这个模型，比如说它是一个对话的模型，你一定都有验证集的。简单来说就是你把你的这个验证，其实刚刚整个过程都在讲这个它有各种各样的指标。比如说刚刚说的困惑度，比如说准确率，这都是它原来的数据集。你有一个模型，你都有工程了，你肯定有一个模型，那你肯定有数据，那你就在你的数据上去比较量化前后这个模型到底指标差了多少。你先有这个模型和数据才能才你先有这个数据，然后有这个资源，你才能训练出这个模型，才能够去做量化。所以你一定是有指标的对吧？
	量化最终效果的评估有啥简单的方法没？只通过文本去判断，有些主观。是的。所以大家其实比较完整的这个做法，可以再拿着我们现在的每一个模型，就比如为什么这统一成了一个模型，就是我们用的这个OPT的这个2.7B，他是有他的训练集和验证集的。大家可以去跑一跑他的这个验证集，就我们这儿看到的，大家可以去跑一跑它的这个验证集。
	那怎么样跑它的验证集？其实我们讲过这个流程，把它的training data拿下来，通过load data sets，然后就能把它拆成这个训练的和验证的。然后这个模型怎么样去跑验证数据，其实我们也讲过对吧？可以去用这个方式来做对比，这是比较直接的。但因为我们这个是做叫这个技术的一个示例的展示，我们就没去放那么多了，主要放的这个部分的关键内容，但它其实是跟之前能拼得起来的，不然每一个这个notebook都会搞得特别冗长。
	我看一下。什么都是推理阶段，这个同学我不知道什么意思，这个跟推理阶段是什么关系？这个量化跟推理阶段是两个事儿。然后chat GM6B在量化以后需要八块A100才能做量化和微调。这个同学我不回答你这个问题了，你再回看一下今天讲的内容，就是6B需要多少模型参数，八块A100是多少GB的显存，你自己算一下，然后你再自己回答一下这个问题。你想象一下，对，就是6B需要多少模型参数，需要是多少模型的参数，然后chat GM6B用的是什么精度？它需要多少显存？
	使用量化预训练后的模型推理的时候，也需要量化操作，这个是一个好问题。这里有两种方式，因为因为有些政策原因，其实我一直没有讲怎么样把大家的模型和数据推到hugging face，因为万一讲了之后惹上麻烦。对，但是这个是一个很基础的操作，大家可以稍微读一读文档就知道了。就是就hugger face跟github是一样的。你注册一个账号，然后你在你的电脑上去配置一些这个key，你就能通过一行代码把你的模型推到hugin face hub上。然后这样的话，比如说刚刚提的问题，你现在有一个模型，你把它量化了，量化之后它的权重肯定跟之前不一样了。它是一个量化后的模型，就像我们写的quant model。
	这个量化后的模型有两种方式去处理。一种就是你存到本地，存到你自己的服务器硬盘，或者任何你放心的这个持久化存处理。还有一种方式就是像全世界人其他人民干的一样，只要你这个东西是合法合规，然后你也觉得他可以开源，你可以直接push to hub，就把这个模型推到了hugin face你自己的主页上。然后推到你自己的主页上之后，你可以用这个from free train的方法，就像你下载facebook的模型一样。下次直接去使用这个已经量化好的模型，这样你就不用每次用模型的时候都得要再量化一遍。我不知道这个表达清楚没有，就是量化这个步骤是一定要做了才能有量化的权重的，但这个步骤可以只做一次，做完之后你可以存下来，就跟微调一样。因为本来量化的权重也是微调出来的，它的权重也跟原始模型的权重不一样。
	交作业的方法的时间什么时候会公布？我不知道这个是平台的要求吗？我的这个homework其实一直是大家自己去自觉的去完成就好了。我不知道是不是平台有一些要求，要大家必须要提交作业，这个可能需要班主任或者这个平台的同学来解答了。
	还有一个同学问的很好啊，是应该先微调还是先量化再微调？两者的先后顺序对结果有影响吗？今天其实一直在讲，就是量化分两种技术。一种是在模型的权重，你觉得已经到位了，现在你就是想要把它变小一点，然后方便你去进行这个部署推理。这个其实就是GPT q包括AWQ它的适用场景，它就是让你把大模型在小硬件上也能用起来，这个是第一类。第二类是说我可以在训练的过程当中就用低精度来做训练。要用低精度来做训练，就是我们讲到的像q lora这样的技术。就比如说我们这里用了这个NF4这样的精度把模型存下来，然后可以再用PEFT的QLA的这个微调去调这个量化后的模型权重。这是两种模型量化的技术和使用场景。
	先微调还是先量化没有一个绝对的答案。但通常来说在你的资源足够的情况下，肯定是先微调好的，并且是在足够高的精度上面去微调的。就举个简单例子，就你能去饭店吃正经的点菜，你不太会想吃外卖的就你自己。但凡你能比如说你这个上海的蟹黄面很夸张，对吧？一碗几百块你就不会去吃这个方便面，对吧？就是这个意思。但是你说方便面能不能做出一些平均水准的这个面的味道呢？它也能做出来，但它营养不够，对吧？但它味道到位了，但营养不够。但这个味道如果是你关心的那那个权重就是很重要的。我不知道这个描述有没有表达出你要的意思，但但凡你有资源肯定都喜欢吃好的。
	有个同学问这些量化技术是不是不适用于bert？这个是个好问题。你会发现是这样的，我们今天讲的这个东西这三个量化技术像最后讲的这个beat and bites，它是直接从这个from free train的这个no的方法里去看的对吧？那这边教大家怎么看API文档，这个很基础的，这个要多看看API文档。怎么看呢？就比如说我们的bert，它就不是这个casual LILM对吧？它是这个mask娄页的文档在。Partizan . 
	auto . 
	model太大了，我把这个先缩小一点。
	Auto class. 
	好，首先我们到了auto class对吧？这个是我们今天讲的核心设计，就这玩意儿，那auto class我们进到这个，稍等我的网卡了，auto model. 这个auto model，然后这边也有这个对应的目录。比如说刚刚有同学问这个auto model或Carrier LM，假设我是一个叫做auto model for mask的LM也不一定是bird，就比如说另一个子类，它不是Carry的这个LM它是一个mask的，怎么办？那么这就跳过来了，这个auto model for mask的LM，这个for mask LM我们打开看啊，the model class to初始化，然后选下面的这些configure class，这个就显然就是birth这条线，对吧？用mask这是它支持的模型支持的这个模型对应的config，所以通过这个方式我能知道每一个应用层的auto model它支持哪些模型。
	当然你也可以直接在这搜，在这搜也是可以的，比如说Albert。Overview, 这就有。我们想看它的这个from free trade到底支持哪些参数，每一个子类的from free train的参数肯定也不一样。然后我们能看到这里有啊具体的一些参数，其中这个是from free train的这个层面上的一些参数。然后假设我要看具体的一个模型的config，那我就到具体的，比如说这个Albert conflict里面去看啊。那这个Albert config里面就会写有哪些。
	这个好像确实没支持那个be side be and buy，no的音还没有，确实比较尴尬，没支持或者。
	是有。
	别的这个入口给它赋值。我没有看懂。大家如果觉得这个找到了，如何给这一类的去用BNB的话，也可以告诉我。对，然后像这个PFT，其实就是在这个transformer的基础上，再去做一些叠加。这个我们留到后面去讲，每一节课争取就一个主题给大家讲明白。大家可以看到其实也是在这个auto model和token ezer上面，再去做一些相应的一些adapter的加载。因为我们讲过了，理论篇这个PFT adapter技术就是在transformer的这个feed forward network后面加了一个apter的layer module，然后去训练的就是那个adapter里面的这个新增加的参数。所以先讲理论部分，大家就能理解adapter为什么这么简单了。
	那些重要的权重是怎么确定下来的？这个同学可以回头再看一看，这有讲前面两个还讲的很细的这个GPT q和AWQ是怎么玩的。是你就没法玩了。对，就相当于你可以理解成，举个最最明显的例子，就是你看这个AWQ，他用的不是咱们transformer库里面的那个LM，他用的是自己的，它用的是auto AWAWQ的这个LM，它cent出来的这个model，你可能就得具体去查一下它的那个API了。
	然后他但我觉得他应该也能用trainer，因为trainer是一个更通用的一个API，它应该也是用trainer是OK的对，但是这个具体的这个效果怎么样？我的这个网太卡了，今天效果怎么样呢？这个得得实际试一下。因为它的API确实太多了，不可能这个课上直播给大家把所有的API都便利一遍。但大家如果有使用上特别搞不明白的，也可以提这个github的issue，呼吁大家多来提提这个一宿，有什么问题可以提到这儿。
	然后我看才为什么folk的数量比star的数量还多，这个不太科学。怎么有人folk了没有star我靠好。法国这个模型是哪个模型我就不太清楚了，我可能今天我们就先到这儿，得去煮个晚饭了。大家有什么问题，我们可以群里还有这个github上面再沟通。然后这个代码我看一下，刚刚已经提交了，这应该是遇到了一个git push的一个小问题。
	我待会儿来处理一下，应该马上就能推上去了。好，我们今天先这样，大家有什么问题我们群里再沟通。然后大家圣诞节快乐，平安夜快乐，平安。好，我们今天就到这里。