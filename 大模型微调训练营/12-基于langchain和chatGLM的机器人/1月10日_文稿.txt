	还没有评论，大家能看到吗？hello. 
	wish. 
	现在没问题了。
	我再刷新一下试试。
	刚断了一下，好像我自己手机进不来了。稍等，我重新进一下这个直播间。这断了重进吗？
	好了，这下可以了。好，行行，那现在应该正常了。昨天在给这个应用开发实战营的同学做直播答疑的时候，还有同学在问，好像一直没把这个能欠去调用私有化部署的，比如说chat GM这条路给跑通，有很多问题。那么今天正好本来我们这个课就是要搞私有化部署的，正好也要讲这个。那么就把这部分内容到时候也会同步更新到这个开发课的同学那边。所以今天我们会更聚焦到就是现在这个题目里面的内容。
	主要两个重点。第一个就是在明白对于一个ChatGLM6B这样的一个模型，几十亿规模怎么样去完成它的私有化部署。然后它主要的私有化部署的形式有哪些，这第一个。第二个就是说有了TIGM这个模型了，我们要把这些模型用起来，有一个很好的抓手和框架，就是能欠。那么能欠现阶段跟ChatGLM生态集成到底怎么样了，目前来看其实就两条线，一条线就是这个智浦AI这家公司提供了类似于OpenAI的API1样的服务，通过APIK可以来付费使用公有云的大模型。还有一条就是我们要去重点讲的怎么样去私有化部署的这个chat GM6B上，用南茜来调用它，那么今天我们也会完成这部分的实战，所以今天我们重点是把这条路路径给跑通。
	然后对于咱们这个微调的同学来说，可能之前没有太多能劝的使用经验。但如果今天咱们有课上的同学已经学习过这个应用开发，可能对南茜就会比较熟悉一些。主要就是把这条通路跑通。至于怎么样在ChatGLM6B上进一步去优化，我留了一些悬念。可以在下节课的时候，我们再用一个优化后的ChatGLM6B来替换这节课里面的模型。这节课主要是把这个端到端跑通，这个是我们这节课的重点。我们先来看第一部分，就是chat GM36B这个模型，它的私有化部署到底有哪些常见的形态，这些基本就全是动手的这个部分，而且6B的这个微调不在我们这节课的这个课程演示环节当中，终于能够很顺畅的完整的给大家去做演示了。
	主要对于chat GM36B这个去年10月份十月底发布的模型，它有几种常见的形态。这一种可能是咱们微调课最熟悉的，就是怎么样通过transformers的token ized和auto model来加载一个hugin face上面的模型。这里就是官方推荐的，使用这个transformers这个库就可以了。当然这种模式我们之前也试过，然后在GPU，在训练状态，在微调的状态下，它是可用的。在交互式的环境里，比如说就pat LAB里面，我们可以很方便的把它这个模型一次性加载到我们的显存里，然后反复的去调整我们的prompt，然后来获得一些这个结果。这也是咱们之前的几节课里用过的方式。但是今天我们重点是要讲怎么样把这个模型变成一个服务。然后这个服务能够被客户端去调用，甚至变成一个有web UI界面的一个应用，所以接着我们重点来了，就是怎么样去用web UI，然后用棉签来使用chat GM6B。
	第一个要讲的是比较简单的，就是radio这么一个web UI。Radio这个web UI其实是hugin face开源的一个项目，就是跟咱们的transformers PFT都是同一家公司，也是开源的。不过最近我有注意到，应该是在16号。我没有记错的话，这个月16号制服好像要开一个技术开放日，去对标OpenAI的dev day，据说要把基座模型调到第四代，我也不知道一年不到的时间发了四代的模型，到底它的改进在哪儿？就像很多同学对我的灵魂拷问，就是这个模型微调前后，它到底有没有什么可量化的结果去证明它变好了？这个问题也可以同样去问一下质谱，就它的GLM这几代到底变化在哪儿？
	当然官方的回答是在一些特定的benchmark上，它的跑分增加了。但是我们学过前面的理论部分，知道这些benchmark其实也可以作为模型微调的一部分。比就像我们在chat GM36B上面去微调这个ATGN1样。因为它可能本身就在它的预训练的数据，所以他即使不去微调它对于这个ADG的数据的生成的响应格式也还是比较像的。Anyway我们静观其变，看看16号的这个GMGLM第四代发布之后，会不会又有一个同步的6B的模型，如果有的话，我们争取就把它加到我们这个微调的课程里面来一起讲。
	毕竟这个GM36B刚刚发布这两个多两个月，其实有很多的适配的工作也都还没做完，包括它整个代码的结构等等，我们看到其实我很有意思在这周去看这个chat GM36B的在github项目的时候，大家可以自己去看一看，有一条新的PR pull request叫2024年的第一个更新，这个更新其实就打破了他的read me文件文档。就他的read me文档里面其实关于这个规模的启动是也用了它的第一代还是第二代的这个文件目录。但是它的第三代的这个版本，其实加了一大堆的demo，其中就包括它的radio和它的命令行启动的demo。它的demo主要放在了我这里写到的以这个demo下划线demo作为后缀的各种各样的目录下面。使用radio的这个web UI启动放在了basic，应该就我们这个图片当中写到的这个basic这个目录下面。当然我们今天的演示也都需要大家去get clone对应的这个代码在github上，待会儿我们会演示一下。
	所以你clone下来了这个chat GL3 chat GM3的这个目录之后，在basic demo里面会发现有一个叫做web demo radio这么一个文件，当然也有这个streaming，那么我们使用这个python web demo radio，点PY就能够启动对应的这个radio的界面了，它在8501端口，但你也可以去改变这个端口文件。然后启动起来之后，就是一个经典的radio的chatbot的界面。然后里面也加了一些参数，这些参数都是它的模型支持的一些参数，包括它，生成的这个上下文的总的长度，然后它的关于概率的预值，包括它的这个temperature之类的这些值。
	那这里我其实是按照他的这个官方示例提了一些问题，比如说上海明天的天气如何，你提你可以提供哪些城市的天气。这两个问题其实是首先关于天气预报这个事儿，是最早在OpenAI的function calling发布的时候提出来的，他的提出是为了表达大模型本身无法得知天气预报这样的一些最新信息。但是通过function call去调用外部的第三方的API，它就可以去查询到这个天气信息了，是一个这样的方式。
	其实重点是体现的这个大模型有调用外部工具的能力，但是这里需要跟大家着重重点提出来就是说这种简要的chat GM3的6B的这种启动方法。它是没有注册工具，也没有去做这个function call的对应的这些知识。所以这个demo是无法通过simple的演示的。但是在复杂的demo，就我们接下来要去演示的另一个demo里面，它是可以像open a一样去注册一些工具，然后实现类似于这个方向call的功能。但是它注册的工具的种类也有限，因为它不像这个OKI有这么多人给他做八个一，所以这里看起来这个问题就会比较奇怪，就我无法提供关于上海明天天气预报的信息。跟大家在在read MI里面看到的那个事例可能会略有不同，当然我们同样的在事例里面也有问，巴黎的天气如何？这里显然就是他因为预训练里面可能关于一些城市然后的这个气温有过一些相关的语料，所以他能大概回答一些。但是这个不能算作天气预报了，最多只能算是一个统计信息。
	那么对应的在后台启动的这个日志，就是我们在用radio启动的这个日志里面，其实也能看得到它的radio其实本身是有对话的这个历史记录的能力的。就是我们看到的这里的每一轮对话其实都会被存下来。而这个存下来的能力是由radio这个库提供的。
	这个后续在我们今天的最后一部分，就是我们要去自己做一个这样的聊天机器人的时候也会看得到。当然除了radio以外，能劝的conversation券也有类似的能力。就是我我们提到的能欠的memory相关的这些功能，那我们现在就来实际看一看，这个demo要如何去玩。
	把这个放大一点。
	这个大小大家能看得到吗？还是需要在这个再调整一下。
	关掉了VPN之后，好像这githa特别卡。
	我们看这边的好了，这个服务器的这。
	里面启动。
	了一个GLM2，先关掉。在我的本地其实有四个不同的项目目录，分别代表着这个ChatGLM26b、chat GM36B它第三代的时候把6B给去掉了，因为它里面还有一些别的模型什么的。然后我们的这个微调课的项目，111m quick star和我们的实战的这个项目OPI的这个quick star。
	我们先看到chat GM3这个目录里面，其实有几个细节和重点需要跟大家讲。第一就是当然你需要去把这个项目代码给抠下来，克隆下来之后我把这个先关掉。克隆下来之后，咱们能看到在这个项目目录里面有很多部分都会出现这个requirements。当然对于这个基础的这个requirements，这个是一定需要去做安装的。大家如果担心跟咱们的课程项目有一些冲突的话，也可以使用这个隔离的环境。就是我们用这个康达的creative去创建一个新的环境，这样也是OK的。好，然后下面他有写到for OpenAI的demo和for no change的demo，这个是为了它实现不同的，像这里写到的OpenAI API可能欠的这个agency，它有一个对应的demo所需要的一些额外的依赖。好，那么我们现在要展示的这个radio其实是在basic这个demo里面，跟他自己的read me文档其实是有一定冲突的。我做一个渲染。
	我们看得到其实在GM3，它已经不再只是一个单这个github项目，已经不再只是一个单独的所谓的6B的主页。而是说他还在在这儿放了一些像这个base这个长上下文版本，包括相关的一些索引都有放，包括一些其他的一些介绍。然后在他的这个demo的使用上，它其实这里有提到，有综合的demo，也有这个简单的demo，网页版对话的demo。这俩网页版对话的这个demo和streams的这个demo，其实我们能看到现在已经被移到了basic和composition这两个目录下面。
	好像我还是得打开我的这个VPN。
	把这个海外的服务器连接太有问题了。还算比较快。我这边的直播没有任何的断掉，对吧？应该正常的。好啊，这边终于加载出来了。
	对，这就对应着我右边看到的在服务器上渲染出来的这个read me，就这两幅图刚刚就加载不出来，在这里我们能看得到它的read me，这个我们待会会去讲，就怎么样去启动这个带有这个chat to和code interpreter的这个demo。但现在除了这个综合demo以外，是早期从第一代开始的时候都会有这个网页版，这个对话的这个demo，这就是我刚刚讲到的，已经被移到了对应的这个basic demo的目录里面。大家如果找不到的话，稍微注意一下，有这么一个调整，那我们具体来看怎么样去启动。
	现在我是处于这个也在稍微放大一点，应该能看到命令行了。现在我是处于这个chat GLGLM3这个目录下面，但是我这儿有一个隔离的这个环境，chat GM3 demo，然后这个环境是有别于我们做高效微调hugging face的这个环境，然后我们需要进到这个basic demo的目录下面去。
	在这儿启动之后，我们也看一下GPU的情况。
	这边GPU开始加载了。对。这边正在加载，它在加载整个6B模型的这个，因为整个6B的模型没有放到一个模型文件里，它分成了七个chek points做了这个分片。所以我们能看到它的显存在逐步的增长，这个过程大家应该很熟悉了。我操。
	我还需要加载个30秒，42秒。加载起来之后，其实我们大家如果不熟悉这个radio这个库，可以简单的去看一看。其实是很很实用的一个工具。而且代码都不复杂，是模块化的。这个web demo radio点PI文件。其实我们启动的这个文件，里面使用的仍然是transformers来加载，但是用radio做了图形化的一个封装，包括我们刚刚看到的这个conversation，其实也是一样的。
	这里它已经自动的。
	给我跳转。
	过来了8501端口，启动在了8501端口。因为我这边装了插件，所以它能够直接是是否是否在浏览器当中这个参数，它是能够起效的。好，这个就是我们刚刚看到的这个例子，就跟chat GM306B的简单的事例，我们也可以简单问一问你。
	在后台我们可以看到conversation已经及时的输出了这个role user content你是谁啊？有什么功能？这个是我向他提的问题，然后他在过程当中会输出这部分的内容，然后我再去向他提问的时候，其实这个hugging face的radio，就是你能看到这个radio，他会帮我去维护这个聊天记录。就像我们讲到的这个memory的功能，其radio的实现了这样的一个功能，并且并不复杂。使用它的chatbot这个模块，比如说你是他说我是人工智能助手，可以提供各种信息和帮助，对吧？问你的作者是谁？
	我们看到这儿，其实他把conversation进行了第二段的输出。并且这个road assistant content这里把上一轮他的回复也记录到了这里。当然大家看到这个日志输出需要明确是谁在维护这个conversation，因为待会儿我们还会看到南茜在维护的时候有不一样的一些操作我问了我的作者是谁谁谁，我的模型的参数量约为13.5亿。这是他的一个一个回复。然后通过这样的一个模式，其实我们也可以去调整这些参数，然后来达到一些效果。这个跟OpenAI的这这几组参数设置是完全类似的。然后它使用了一个radio的chatbot，能让我们比较轻松的去实现一个web端的这么一个，相当于有这个web界面的一个聊天框。对然后有个同学。
	问。
	13.5亿这个参数量是咋回事儿？我也觉得他这个回答的有一点点问题，因为我印象当中应该是62亿的一个参数anyway这个有问题也正常，毕竟现在GM36B，但我这个不重点，这个一定会有问题，他不一定能回答出完美的这个答案，但我们不局限在这儿，我们回头会再去看看怎么样去解决这些问题。60 60多亿对，66.2B62亿对。
	然后这个其实就是一个最简单的示例，通过这个方式我们能去启动。但是这个示例它是没有这些功能的。就是我们看到的各种各样的关于我们的这些代码解释器to chat对话之类的，这个都没有。然后现在你看到的这个对话模式其实是由radio在维护，他自己其实没有在维护太多的这个的模式，或者说他以比较巧妙的方式，借助radio帮他把这个历史信息给他传进来了。并且从这儿如果大家用过OpenAI的API就会发现，其实它的这跟第二代跟第三代的一个变化，就是它开始能够支持不同的肉了，就不同的角色，这个是一个变化。但是你要去细抠的话会发现它目前支持的是user，assistant，我印象当中好像不支持，也支持system，支持这个系统。但是google发布的这个gym I pro，好像目前还不支持这个system，还只支持user和这个assistant，不支持这个系统级的角色。好，我们大概了解其实以这样一个方式就能启动一个图形化的界面。
	那我们再来看一下这个，这是它的后台日志，我们刚刚接触的一些图。那么stret这个YBUI去启动它的时候，有一些什么额外的好处呢？这个就是它相对来说比较健全的另一个web UI的版本。然后我也我也在这儿贴了一个图，就是如果大家对康da的这个环境不太熟的话，有这样的一个命令，用康da的create杠N这个杠N就是name的意思，给你的这个新创建的康大虚拟环境的名称叫做chat GM3 demo，使用的python版本是3.10。然后创建好了之后，他会把3.10 python需要的一些依赖都装好。然后你激活之后，再去安装这个streaming，这个composition demo需要的requirements，我们待会儿可以看一看。然后接着因为它需要使用这个code interpreter，所以还需要额外的去装一个jupiter的内核，这个我们待会可以去实际看看。那装好之后，其实去启动它，我们在今天贴的这个图比较多，就担心大家在看视频的时候不一定看得清楚，把一些关键的内容放在这儿。
	我们首先安装好之后，需要激活这个环境，激活之后还需要install对应的这个requirements。然后从basic demo去到我们的composition demo，去安装这些requirements。这个requirements是在composition demo上面有一些新的，然后使用这个streamlet这样的一个启动的命令，这是一个python的依赖，它也是一个图形化的界面，比radio看起来会更丰富一些。待会儿我们可以实际去展示一下。然后它能支持外部的这个UIL在公网上去进行访问，也支持局域网的这个埃批进行访问，然后在这儿我们能看到一些细节，就是加载完模型之后，相比于刚刚的图形化界面的加载，这里有一些新的信息输出。
	这个注册了的工具，description，generate a random number，包括这个下面有一个get the current weather for city name，如果上过咱们应用开发课的同学很熟，我讲function calling的时候就第一次举的这个实例就是查询当前城市的天气以及这个get weather forecast。就是不只是给天这个城市的天气，还要给未来几天，就是相当于天气预报。那现在的这个stream里面其实只能查城市的天气，这个forecast还做不到，就未来几天的天气还做不到。这个我们在实际测试的时候也发现了，看代码也都看得出来，这个是stream net的web ui启动起来之后，我们会发现相比radio已有的功能以外，它开放了system print。
	就是我们这儿看到的only for chat model，就是对于南庆来说，我们上节课讲过，它有LLM的这个形式和chat这样的一个模型模式mod模式。这两个模式最大的区别就在于chat mode，它是支持跟大语言模型进行聊天，然后有不同的角色，不同的肉。尤其是这个支持system role之后，能使它有一些常驻的命令，常驻的提示词，让他知道他是谁，他在干嘛。就像我们现在在看到的左下角有一个system prompt写的you are chat GM three，a large language model trained by trip AI follow the use users instruction carefully. 
	然后有一个细节写的respond using markdown，就返回要用markdown的形式。但是这就出现了一个很尴尬的事情，就问他你有什么看家本领？这里的markdown渲染出现了问题，而且大家熟悉markdown语法就知道summarizing这个地方应该要没有空格，才能使用两个心来进行加粗的渲染，并且他在第四条这里出现了英文，这个应该是一些小的问题了。它的一些bug，或者说这个还不够到位的地方。然后除了开始的这些些system role以外，其实在上面还多了三个按钮，就是我们看到的tool和code interpreter，分别是三个不同的功能，也是对标着这个OpenAI的GPT3.5，还有后续版本。
	Chat其实顾名思义就是支持对话的形式，主要是因为这个ChatGLM2还不支持这个chat，这个是我们待会会去讲的。Chat GM2它更像是GPT3的模式，就是我们达芬奇003这个模式。它也不支持什么你是用户说的还是assistant，就是我们的大模型说的，还是这个系统级的常驻命令，或者是function call都不支持。它就是一个单纯的条件概率，一个自回归的模型，你给我什么样的上文，我就给你生成对应的下文，一个概率上的一个自回归。但是这个M3相比于GM2，它做了对应的chat相关的一些学习。也就是我们后面会去讲的，怎么样去用这个一些训练的指令微调的方法，包括RLHF，让大模型具备一定的对话能力，这个其实是所谓的代差了。如果硬要说有什么样的新的本领，那这个支持不同的肉，包括这个two其实也是function call的一个体现，都算是代差，这个是在训练方法上有不同的层次的。然后code interpreter其实也是一个新的子功能，就是把大语言模型当成一个代码解释器来使用，这个也是一个子功能，待会儿我们会去看。
	然后在这个部分，我们首先去使用了这个对话的模式，可以跟它进行交流。然后第二个，这个工具模式就很有意思了。因为我们刚刚看到了后台他注册了这个get weather。然后get weather这个tool是一个查询天气的函数，这个函数目前只支持一个参数，就是CT name。然后我们通过给他提问上海的天气，明天的天气如何，这个是一个很tRicky的提问。
	那这个提问有刁难的意思。第一就是首先通过这句话，咱们的大语言模型需要去进行判断，就是判断我们需不需要调用工具，就是function call引领了这个工具模式。Function call的核心逻辑就是对于我的大语言模型，我可以注册一堆的工具，然后这注册的这一堆工具都可以有一个描述，就相当于我这个工具是用来干嘛的，它的输入参数有哪些？Require的参数分别是什么含义？然后由大语言模型根据用户的对话内容去判断当前的这这个问题需不需要通过某个工具来进行回应。
	现在我提的这个问题显然我们的chat GM36B本身是回答不了的，因为在radio的这个示例里，它就已经回复了，它是一个AI助手，它无法查询天气。但因为现在注册了一个工具，所以他通过这个get weather这个工具的描述，他知道了这个get weather是可以查询天气的。但是他通过解析我的输入发现我不仅要查询的是weather，我还要查询的是tomorrow weather，就是明天的天气，是一个天气预报，而不是今天当天的天气。所以它的to call解析其实是这个to call的含义，就是它需要去调用一个注册的工具。然后这个注册工具的名称叫做get weather。然后这个工具的参数有两个需要传入。这是从我的输入，我的自然语言解析出来的。
	但是到下一步他真正去实行实践去调用的时候，这个是7IGM36B有新的一点，就是用过function call的同学会发现function call这个OPI把它的能力抽象是做的很清晰的，就是符合react这个经典的agent的抽象大语言模型。它作为一个agent。他能够理解你的需求，然后知道你要干什么。在这儿是要查询天气，但是真正要查询天气的时候，其实是需要一个类似于跑腿的角色，它是一个提供真正查询天气API的一个平台。你查天气如此，查天气预报如此，查车票机票都是如此。
	所以OPI认为这些实际跑腿的活他是不干的，但是切了GS36B，他为了把这个demo做健全，他把这些一部分跑腿的活也内置到了他的这个demo里面。但是没有无法穷举所有的跑腿的活，所以他就只有get weather。所以当我去提了这个方法之后，他真的实际去执行了get weather背后的这个查天气的这一套。但是超出了他已支持的这个范围，不支持时间这么一个参数。所以你看它最后底层是报了一个错，报错是报的这个注册的工具里面不支持查天气的，有这个参数，所以最后他catch了这个错误，相当于捕捉到了这个错误，由机器人在告诉我无法获取明天上海的天气预报，尝试其他的查询会尽力帮你，然后同时把这个错误信息也抛出来了。这个observation就是典型的react这种范式里面通过action操作之后得到的一个结果，我们通常把它称作observation，这个是很有意思的一个挑战。
	如果大家想要去扩展，其实简单一点就类似于我们实战课里讲的，把get weather forecast的这个工具以chat GM36B支持的方式也注册起来，那么它就能够查询明天的天气了。那么在后台的日志里面其实也能看得到，它调用了这个get weather，可能我没有画红框，他这边有写这个system的信息，包括注册的工具，有这个random number generator，就我鼠标在在晃动的这个地方，random number generator生成随机数。还有这个get weather，有一个查天气，还有什么？还有查这个什么，get shell就相当于它实现了能够用大语言模型来执行shell脚本的这个命令。就下面有一个get share scription you share to run command，包括这个query，在linux shell里面去实际执行他，包括之前给他的这个聊天记录，他也都维护在这里，但是，最后查询的这个天气这个事儿确实超出他能力了，这个tomorrow是它不支持的。
	在function calling里面，我们会发现如果真的写清楚了，有一个tool叫做get weather forecast。其实我们会把这个forecast需要的这个参数命名为number day，就是我到底需要查询的是从今天开始后面几天的天气，这个更像是天气的查询平台给出来的接口。但因为这儿他本身没有提供对应的to o注册，所以他只能抽出这个信息叫tomorrow，等你真正注册的时候，tomorrow是会被大比模型转译成这个。比如说number day写成一，可能就代表的是tomorrow这么样一个意思好吧？这个observation也是典型的agent的输出结果。
	然后这里还有一个很有意思的，就是我们用这个代码解释器的功能我已经用这个ipad on的内行依赖注入了一个新的kernel，能够去在这个stream net里面加载这个jupiter的kernel，然后来执行这些mp t lab输出的这个图形这儿我没有画爱心，因为他给的这个示例是一个爱心，我让他画一个葫芦很难的。为什么很难呢？因为他能画出爱心，是因为整个画爱心这事儿，其实是画了一个函数。咱们经常我们在数学或者说高等数学上理解的这个函数。一个二维的函数，这个函数刚好它的Y和X的关系可以画出一个I型，但是很少有这个Y和X的关系能画成一个葫芦的，大家想象一下，那就不合法了，相当于同样的X有两个Y就很难受。
	那么我让他画一个葫芦，他就画了一个画了一个啥呢？画了一个应该是正弦，然后这个正弦函数然后的平方，这个比较像是葫芦，它下面还有一行解释，待会儿我们可以再实际展示一下。这个它是确实能画出来，并且能用代码解释器去执行，还不错这一点好啊，那我们来实际实操一下，带大家都过一过。
	我们来到这个综合demo，按照他的说法叫做在composition demo里面有这些，其中比较重要的是这个requirement，这些是需要安装的。然后在外层有一个requirements，这个是我刚才已经关掉了的，就是整个前的GM3都需要装的。这些大家可以在下来之后进行一个安装，然后不要把它克隆到咱们的项目目录里了。这个是我一开始就提到了，最好是并行的其他的目录。因为待会儿我们会讲，咱们的这个目录里面会放它的客户端，而这些GOM的模型其实是属于服务端。
	好，然后我们打开这个requirements。好，其中有一个文件，就我刚刚提到的这个tool registry，就是我们注册工具有哪些工具现在我们看得到这个。看一下，主要是这一行，它实现了真正去查询天气。但是正如这个代码的这这部分所言，就是目前只支持一个参数get weather，只支持一个参数就是CT name。
	如果咱们想要去扩展，其实也很简单。大家想象一下，在我的open I que star对应的这个代码里面也有类似的。你就在这儿新增加一个参数number day，然后描述清楚它是一个int型的变量，然后意思是说the number of days to be to forecast类似的。然后有个默认值没比是零，就是指当前的这个天气。这里的这个python的函数解释就可以写get the什么weather forecast for city name in number days，这里就可以写一些判断，然后这个request get，这个是他实际去实现天气查询的一个第三方的API的调用，然后最终实现了这个过程。包括去执行这个shell，也都是一样。所有的function call类似的这样的agents几乎都是这个模式，然后使用了这样的一个python的装饰器去完成了注册真实最后去启动的是这个命令文件。在这个命令文件里面，其实是启动了我们的整个图形化的界面，我们来实际执行一下。
	大家如果有问题可以随时提问，因为今天会有很多交互性的演示，主要是为咱们很多没有上过如果你没有上过应用开发课，你也没有实际去跑过GM3的这些模型，然后你把这些重要的这些demo也好，部署也好，能过一遍，然后能端到端的把这个聊天机器人跑起来。这儿我们看到启动很快，streamed启动很快，它没有像我们刚刚这个radio花了很长的时间，这个主要是因为它用的是懒加载，它并没有真实的把模型加载起来。它这会儿只是把这个前端页面记起来了。你可以看到真实的去访问他的时候。
	有点慢，稍等一下。
	最近的墙这么高吗？
	这个已经关掉了。
	还没打开，打开的话，这里应该就会触发加载了。
	哎呀。
	应该不会这么叫。靠。有个同学问暖加载的出发时机，其实就是我访问他开始，现在就是还没没访问到，对我的网有一点点慢。
	应该。
	大家看有什么问题可以提一提。这边稍微需要网络解决一下。
	启动成功了，这个肯定是启动成功了，因为服务端这边有写，对。
	我本地有另外一个服务占用的这个8501端口，我靠，把这个关掉，稍等一下，看来问题好像在这儿。
	咦。这么神奇的吗？
	去拿瓶水。
	不应该。
	看着好像是有点网络问题。
	比较尴尬，那我们先把这个料在这儿先往后讲一点，看待会儿能不能加载好啊。理论上这个应该没啥问题，因为刚刚才跑过网络都是正常的，这会儿墙格外的高。我们先往后讲，不耽误时间。待会儿我们再回过头来看这个GOI的这个demo。现在我们知道就是除了。
	就是除了这个GUI以外，更重要的部署形式其实是API。就咱们所有的这个OpenAI的这些用户，基本上也都是在使用这个GPT的API在进行调用。那么chat GM3其实也是一样，他现在本身这家公司其实也需要有这个商业化的支持了，所以他也一直在做这样的一个推进。包括我们后面会看到的能这边的对接，他也是用的像OpenAI的API1样的方式在进行对接，就是KOAPIK。
	那具体其实怎么做的呢？我们可以看到在新第三代的这个chat GM的6B的模型里，它使用了叫做OpenAI的API demo这么一个目录来进行对应的启动。就我们类似从composition demo里面出来，走到OpenAI的API demo里面，然后启动了一个服务器。这个服务器其实就是把chat GM36B这样的一个模型变成就像你在调用这个TPT的模型一样。它变成了一个服务器。那你去访问这个服务器的时候，你需要走一些API的请求。
	已经有很多的大模型的应用，其实都是基于GBT的API进行构建的那到了后期，大家如果有一些各种各样的原因，必须要从OpenAI迁移回国内的这些模型的时候，或者说迁移回私有化的模型的时候，大部分的应用还是不希望去改变它的代码。在这样的一个大背景下，你就会发现这个chat GM36B它去实现一个类似于OpenAI的API server是有一定的价值的。所以这里我们也是把它成功启动起来，然后会发现它的request，就他客户端请求的这段代码其实很有意思。首先他用的这个from open I input open，其实就是把这个OpenAI的python SDK下载好，然后用python的SDK就OpenAI person SDK，它相当于就模拟了一个客户端，然后这个客户端里面的APIK直接写成空，就我们把这个APIK直接写成空，这个base UIL写成咱们部署的这个路径，就比如说127.0.1就本地，就在我那台远端的服务器上的8001端口，然后走的是这个V一的版本，这个是API的版本。然后V一的版本下面，OPI最重要的几个，这个API1个是chat completion，就是能够去实现这个chat mode，就是有角色，然后还有function call类似的一些功能。那么这部分就完全使用OpenAI的python SDK就可以了。然后模型我们就直接使用自己部署的这个chat GM36P然后发送请求的方式也都完全一样。
	然后唯一的变化就是在构建这个client的时候，变成了一个本地的UIL。或者说你自己部署的在不管是那个局域网里面，还是在你的这个受限制的这个overlay的network里面也行。但是你部署的一个私有化的一个121，然后就用OpenAI的原来的这套代码就能运行。
	然后实际去请求的时候，我们看到，我们在这儿构建了一个用python SDKOPI python SDK的check completions，去构建了这么一个对应的请求。这个请求其实就是有一个system的角色，告诉他他要根据用户的指令来解决问题。然后真正的user，真正的用户，其实给他的这个prom的是用生动的话语讲一个小故事。
	然后实际去请求这个chat GM36B的时候，使用了这个u stream这样的一个或者说叫做streaming这样的一个参数。使用这个参数的时候，就是指当我要生成相对长的内容的时候，我用stream它是能够就像我们说话一样一个一个往外吐的，而不是生成完之后给你完整的结果。然后对应的像这个max token temperature等等这样的一些参数，都能够去有效的去使用。
	好，我们实际来看一看怎么去玩的。不会我的8501端口真有问题吧。我们先来看看这个OpenAI的这个API的demo。
	好，这边再加载。
	好，8001端口。我们可以。再起一个。命令行，然后进到projects。
	我们打开一下这个。
	把这两个文件都给大家展示一下。还有一个是客户端。是对应的这两个OpenAI server，类似OpenAI server的这个客户端和服务端的代码。首先服务端我们可以看得到，这个是我印象当中它应该默认是8000。我们可以看一下，对，默认是8000。然后我的8000因为启动了这个就patter lab，如果大家有类似的情况，需要注意一下这些端口，这也是比较小细节的地方，但容易出错。就因为我的8000端口已经启动了，这个就拍ter lab了。这里能看得到，有一个pt lab占用了我的800端口，所以我的8000端口是不会再给到他了，他会用8001端口。然后8001端口启动之后，这个workers是启动一个，如果你有多张卡，这个地方可以启动多个。
	然后启动起来之后，整个chat GM的这个OpenAI的API demo，其实就是模拟了OpenAI的经典的API设计，也不经典，就是这广泛使用的这个设计，然后包括它的这个路由的响应，包括function call的一个处理等等，assistant的一些处理，包括它的这个key，就是我发送请求的时候要用哪些key。然后这个路由使用的是V1 chat completions。然后如果我要查看我有哪些model，v1 models，这个也都是OpenAI支持的这个接口，只不过OpenAI可能会有几十个模型，这个chat GM3的这个demo的server，只有这一个模型叫chat GM36B。所以如果我们在客户端发送请求的时候，我们给的这个模型不对，就比如说我们这儿传的不是GM36B那肯定这个就会报错，没事的。好，这个是客户端的代码，非常简单，就是构造了一个simple chat，然后最后实际调用的就这个simple chat，当然也可以调用其他的一些demo，就比如说不使用这个stream，现在就是不使用stream，然后使用这个in bedding，embedding这个部分，然后用这个方形扣，对应的我们来实际跑一跑。
	当我们去调用这个OpenAI API request的时候，它这只有这一个是会被启动的这个逻辑，所以它会一次性输出诊断这个故事，当然我们也可以试一下这个stream的这个参数。这里就会以这样的方式来处理，这个就取决于咱们会怎么样用这个demo了。因为实际上如果你构建的是一个APP或者一个网页应用，然后你又不想你的用户一直在等，然后你又是用的流式处理的数据，那么stream是一个非常好用的。但是stream因为它是不断的，它建立了一个socket，它不断的在给你吐这个数据。如果你不太会处理这样的东西，那你就乖乖的用这个完整的这个输出结果，也是OK的。
	好，这个是用OpenAI的这个API的形式去启动一个咱们的chat GM36B对应的这个模型。我还是争取能把这个stream net给大家跑一跑，看看到底是不是端口问题。8501，可能我确实防火墙没有开这个透传。
	The import age. 
	最后再试一下，不行的话就只有改天再给大家展示了。
	讲道理应该是能够运行的，之前运行的很顺畅。
	实在不行就没办法了，那我们就只能往后走了，等到待会儿结束了再回来看啊。我们接着看啊，既然都已经可以把这个CIGM当成OpenAI的API来用了，那么我们就会进一步就像咱们这个大模型的应用开发实战可一样。其实大有上过这个课程的同学就会发现，其实整个技术发展脉络就跟咱们设计的这个课程是一样的。
	基础篇的时候再教大家怎么用OpenAI的API。就是你不会冷却，你会用OpenAI的API，然后你也能做点小活。就像刚刚我们演示的，你不会用能欠，但是你可以把ChatGLM，像刚刚的类似于OpenAI的API的形式去部署起来。那部署起来之后，你原来那些小活就可以直接迁移过来。因为chat GM它的API形式是跟OpenAI的API1样的。但是如果你想要更进一步的去做一些复杂一点的应用，甚至想跟这个向量数据库什么的再去搭上一些集成的一些生态。那么南迁就是不得不提的一个很重要的应用开发框架。
	然后南茜本身和chat GM的生态也有一些集成，第一个就是这个质谱AI，我们一直在看这家公司质谱AI它自己提供了你可以认为类似于OpenAI的这个API的服务，公寓的。然后在这个地方有一个细节，首先这个来自于南茜的官方文档，最近刚刚更新了南茜的V0.1版本。第一个0.1版本一年了，然后还是有一些API的变化的。然后我们后面也会把这些代码的更新体现到项目项就是咱们课程项目里面来。目前还都可以用，它也有提示，要到0.2版本的时候才会遗弃我们现在用的一些实现，所以大家也不用太担心。
	在质谱AI的公寓的API服务里，我们看到它其实是实现在什么位置的呢？在这里第一有一个叫做南茜community，它不是属于南茜的整个核心库，或者说不是属于它最主要的这个库，这部分其实也一直在变化。最早我们看到在零点几版本的时候，一直都是from能欠的models，或者是LLLLM或者是chat models里面再去import具体的一些模型，这个我们应用开发课都细讲过，它的代码设计和实现，包括基础类它的派生类。现在它有一个明显的趋势变化，就是因为大模型的数量很多，包括像OpenAI的模型，有一个叫做南茜下划线OpenAI这样的一个包，他没有把所有的代码都集中到这个南茜的包下面，开始做一些分类细分了，然后像一些不算他认为世界上最重要的公司，他就放到了南茜的community下面。像OpenAI，它有一个独立的目录，因为太重要了，在南京的community下面也有很多的chat model，质谱的这个AI就是其中一个。然后制服AI的这个API，因为它是支持对话模式的，所以它是在南茜的这个chat models下面有不同的角色，然后对应的这个类的名字叫做chat智谱AI然后你要去使用它，就跟我们要去使用OpenAI一样，也需要有一个质谱AI的APIK，那这个key去哪里拿呢？
	质谱AI有自己的这个网站，这个左下角我现在都放左下角了，我上次放以前放右下角，发现我的直播的时候人脸会挡住左下角大家能看到都是有一些关键的URL的。这个左下角提供了对应的这些关键的URL。然后在智能AI的这个开发者文档里面，其实明确有写他一直在这个做对标。
	然后国内的java生态也比较好，所以它首先实现了python和java两种SDK，用来去调用质朴的大模型服务，然后调用方式也是多种，跟open是完全类似对标的一个状态，同步异步和这个stream的一个状态。然后这部分大家有兴趣可以去了解，但因为这个不是我们重点，因为讲这个就跟讲OPI的API服务一样，应用开发的同学应该都知道，我们就不去过多的讲这个公寓上的质朴AI的大模型服务。我们看一看具体如果我们要用私有化的chat CM应该怎么办？这就又有一个很很割裂，也是很很有意思的现象，就是我们看到刚刚的第三代的私有化部署也好，还是说咱们的这个质朴AI的公有的大模型服务也好，它都是chat model，都是属于有角色的，可以去做这个多角色的prompt的一个大模型。但是第一代和第二代的ChatGLM才是现在南茜上面官方支持的主流的这个chat GM的6B模型。我们看得到在components在年前的components里面，它是放到了LLM这个下面的，而不是chat model下面的。
	我们刚有讲到LLM下面其实对标的就是GPT3，然后chat model是对标的3.5和4。所以这里就会有一个问题，就是咱们现在用的这个chat GM36B他是没有跟任倩去做官方对接的，这个是一个比较现实的问题。所以我们今天的这个事例，我们为了尽量跟官方保持一致，不用一些容易出现长期不可支持的代码。我们后面使用chat GM2作为咱们的这个聊天机器人的端到端的这个代码示例。然后后续如果16号他们出了这个新的GM4，然后也跟连线未来做了官方的支持，我们可以再去做对应的代码扩展。就像现在这个应用的开发科的同学搞不定这个GLM的对接，我们也会去做对接一样，那怎么样去部署ChatGLM26B呢？这个就类似是一个完全跟第三代类似的一个策略。只不过我们看得出来在第二代的时候，它的功能还没那么强，它没有多角色。
	然后它的API形式上也更加的简单，有一个API点POI文件里面使用的fast API的方式，实现了咱们的一个API server。回头我们待会儿可以细看一下。那么通过这个方式，我们可以启动一个chat GM26B的这个API server。这个API server是比较简单的一个请求体。大家可以细看一下，这个请求体其实主要就包含什么呢？包含一个prompt，然后它返回的其实就是这个生成的结果。
	在这个里面我们在南南茜发挥的一个最大的价值。上节课我们讲过一些南茜的基础，就是用一个最简化的LM chain LM chain就包含一个模型的本身，然后模型的输入，也就是我们的front和模型的这个输出，输出部分是可选的，这个输出是它的output power。好，这三部分加起来可以构成一个最基础的欠叫AM茜。
	然后这个AM茜在咱们去调OpenAI的API的时候，可能大家没都有感触。这个OpenAI可能是一家独立的公司，然后他有他自己的大模型服务。我们通过能欠这个客户端去调取了这个OPI的这个服。那现在其实是一样的，只不过这个服务是由咱们自己来部署的。比如说就以chat GM26B的API server为例，我们部署了一个第二代的6B的模型，它是一个服务端，是一个API server。然后我们要去再启动一个客户端，一个南茜的AM券来调用它，然后发送这个prompt获取结果。
	代码实现其实也不复杂。我们可以看到在这个代码实现里面，我们同样的在8001端口上去启动了一个第二代的6B的模型，然后它的这个请求的方式是比较简单的。就是跟我们之前一样有一个AMT，这个AM茜只需要传入一个提示词模板和对应的大模型。其实此模板是极简，啥也没有，直接把用户输入的就当成它的输入。然后唯一的区别是可能还额外放了一个历史记录，有个history。同样的，我们的大模型使用的不是OpenAI使用的，而是叫做ChatGLM这样的一个大模型。这个我们刚刚看过了，在人群里面是社区官方支持的，支持第一代和第二代的6B的chat GM。然后它要传入的参数end point UIL，max token是指我最大的上下文长度，88万，然后history是我的历史记录，那top等等，这个是能够去直接运行的一个代码，待会儿我也会把这个代码传上去。
	但是这个部分就有一个问题了，就是我们知道radio帮我们实现了这个聊天记录的维护，那南茜怎么办？南倩有没有办法帮我们实现这个聊天记录的维护？可以，对吧？我不知道大家有没有去跑我上节课的那些代码虽然他需要一个OpenAI的APIT，但是如果咱们没有这节课，直接使用你自己部署的M6B也可以，不用OpenAI也可以，就是调用我们的第二代的6B模型就可以了。然后在南茜里面有一个conversation chain，就是类似于自动的帮你去维护咱们的聊天记录的。所以咱们注意看这个变化。第一左边变成了conversation圈，调用的内容增加了history，它的内部实现其实就是嵌入了一个AM chain，加上一个conversation buffer memory，然后除了conversation button memory，conversation券还支持一些其他的内置的memory。大家可以回头去翻一翻我上节课给的一些这个notebook，然后在这个代码实现上也很简单，我们就不用LM chain，而是使用这个conversation chain a conversation的buffer memory，然后去实例化这个线的时候，memory这个地方就使用预定义的这个buffer memory，它就会帮我去记录这些过去的对话，就跟咱们看到的radio是一样的，这部分我们来实操一下。这端口看来今天是访问不了了。Anyway我们来这儿实际的运行一下。
	重启一下这个notebook，然后我们的这个目录，大家再再确认一下。这个chat GM-6B的这个目录，对应的是是第三代的GM对吧？这第二代的这个GM都在THODM这个group下面。在这个部分是我们刚刚看到的南迁的文档，明确写了确实目前只支持第一代和第二代。因为第三代它的请求体格式发生了变化，包括它的网络也发生了一些变化，就相当于3.5和三的这个区别。在训练方式上。那么GM6B第二代没有这么多demo，比较简单，整个目录结构上把它的demo都放在了最外层，没有做这么多花里胡哨的demo。
	然后由P20，P20使用的就是我们上节课，不是上节课上上节课介绍的这个ADG这个广告生成的数据集来进行的P20这样的一个高效微调，当然我能想象得到他这个P20在GM two上面的这个微调。在第三代的6B模型里，应该这个AD线已经变成它的训练语料了。所以上节课大家也会发现，前后在第三代变化上不是特别大，但也有一定的倾向性的变化。同时我们现在要启动的这个API点PI文件，和这个OpenAI的API点PI文件是不太一样的。我们到这边的代码目录上来看，API点PUI和这个OpenAI。
	那么APAI点PUI文件它要求的这个请求格式比较简单，就是prompt history maxim，top p temperature就构造这么一个就好了，然后OpenAI的这个API仍然是跟咱们的这个chat model是类似的，就跟我们刚刚讲的这个第三代的这个APS server的启动方式是类似的。他在第二代的时候，去模拟造了这么一个对应的请求的UIL，这个不重点，我们看这个API，因为南茜其实对接的是这一套关于python怎么样去起APS server，包括fast API和request这些代码，大家如果不想去深入了解就不了解，因为这些代码你就可以直接用了，你也不需要再去做扩展。我们后续的课程也都是不会去改这些文件，而是去改我们训练好的这个文件，我们前面的课程几节课大家都知道，改完了之后，其实就这儿from free train加载的这个模型权重变了，仅此而已。从THODM变成我们自己的，所以大家不用担心，那我们这儿一样把它改成了8001这个端口，进到第二代的。two. 
	GPU没有占用了。启动起来。
	有个同学问chat GM3竟然能用OpenAI的SDK调用，不能用南茜调用GPT那样去调用chat GM3吗？可以，这个同学问的很好，可以就是需要人去做，需要有人去做这个适配和对接。本来我今天最想去做了，但是看到新闻稿说马上出JM4了，这不挺气人的，可能马上GM4又要改，对吧？那你对接了半天，最后所以要马上去对接下一代的。然后我们在这个地方，南茜去去调用咱们的第二代的6B模型。稍等一下，还差一个分片。
	好，这里写好了，用的这个UVCN。然后在8001端口，就是我们这儿的8001端口，代码其实挺简单，重点就是你的end point UIL一定得搞对了，这再放大一点，in the point的UIL一定得搞对了别别搞错，然后实际执行一下。
	好。这儿其实最关键的一步就是横这个history这的处理是啥也不写的。我的经验是说使用相机第三或者说切GM的第一代第二代这样的模型，纯或也不叫纯，就是以自回归为主的这样的模型。
	没有做过和人类价值观对齐和这个对话训练的，还是给他一个预设的一个概念，这个history就相当于第一轮对话，你给了他一个角色，因为他也没有这个system role，他不知道他是谁，你给他一个这样的角色他不会迷失。他不知道他不知道要跟你聊什么的时候，他就会给你发散的去聊。但是如果你给一个这样的启动的命令一样的，我说叫做启动的front一样的东西，他会知道他是个销售，那么现在我们就实例化这个LM，这个question就是咱们前面都讲过的这个提示词模板，然后构造了一个AM券，就可以开始运行了。大家可以先忽略这个部分，因为这个AM chain的用这个南茜的表达式语法去构造的这部分他们还没做完，就是南茜官方也还没做完，这就运行出来了，这其实已经调用成功了，就是我们看到后面这部分，你们衣服怎么卖？Response，我们的衣服是通过品牌授权巴拉巴拉说了一大堆，这个是LM券直接去调用它。但这个很麻烦，大家一眼就能看出来对吧？
	就你执行完之后拿到了一个结果，这个结果你还得手动的去维护这个history，这个是很烦的，就像我们一开始去学习用OpenAI的API的时候是类似的，你会很很很麻烦的，需要得自己维护这个history。所以比较推荐的就是如果你要去做这个对话，然后又不想绑定在radio上面，可以去了解一下这个memory相关的功能。其中这个conversation chain就是一个最佳实践，然后这个conversation chain就在能chain的chains里面，是一个预定义的conversation的buff er memory，关于buffer memory的介绍，在上节课的南茜的memory里面有大家有兴趣可以去再看一看。就关于这个memory system，然后我介绍了几个基础的memory，包括今天用到的这个buffer memory，以及buffer window memory。就是我不会无限长的去记录，可以记录最近的K词对话，以及下面我印象中还有一个summary bubble memory，就是我边记边总结，就不用原封不动的记了。这是比较常用的三个典型的memory。因为我们现在这个还是有余粮的，我们就用buff er memory原封不动的给它记下来，然后我们去构造一个conversation chain里面使用的这个memory，然后我们再去问他，你们衣服怎么卖？这个问题其实是刚开始问的一样，我们可以看下服务端这边。
	大家我不知道大家有没有注意这边的变化。大家看见这部分我高亮了，我不知道看不看得清楚，我在这个VS code的命令行这里高亮了。这个部分其实就有意思了，就是能欠能给大家带来的一部分价值。我们看到这里之前我们直接使用AM chain的时候，用户写什么就是什么。现在使用了conversation chain，那么我们能看得到，其实conversation自己实现了它的prompt template ate，我们的conversation chain不需要我们传prompt，大家对比一下看啊，是不需要的，AM券是需要的，UM chain不管prompt，只帮你做一个整合组装，那conversation change的prompt是这部分，他说了什么呢？他写了一个the following is a friendly conversation between a human and AI。AI is a talk to then and provides lots of specific details from this context, then I does not know that. 
	堆，然后有current conversation，就是当前的这个对话。这个其实就是南茜的conversation线conversation chain自己实现的。它的因为有前面这一段prompt，所以即使我们啥也没干，chat GM26B的回复也显得相对聚焦一点了，并且没有生成这种乱七八糟的这些内容。
	我们的衣服是通过品牌授权和直销营销两种方式进行销售。品牌授权是指我们与一些知名品牌合作，包括说的很奇怪，然后你看这一段的这个回复就好多了。我们的衣服都是自己生产的，然后拿到市场上卖。主要的销售渠道线上线下线上是我们的官方网站。是这是这样的对，然后我们再问有哪些款式？当然我在这边高亮的这部分，其实这里也有。因为我们把word boss给打开了，只是在服务端给大家高亮一下，让大家看到在这部分其实就是我刚刚在服务端这边给大家展示的这部分，由conversation chain的prom template的实现。
	然后重点就来了，这里有这部分不变。然后current conversation其实它自动记录了上一轮对话，就是这一轮对话。然后我们问他有哪些款式，不好意思，多划了一个字儿，有哪些款式？让他回答道我们有很多款式，不同的衣服涵盖了不同的风格和场合。产品线主要包括T恤衫、衬衫、连衣裙、牛仔裤、休闲裤之类。这个其实是我下午的时候执行的，他的回复有点不同，但是核心都还是往这个方向上在引导，就是产品线有哪些，包括这个什么休闲裤对吧？那么就问他休闲裤的男款，休闲装的这个男款有哪些？他这边会拿着过去的这个聊天记录，继续去访问我们的这个CGM two 6B，这里面他就会写休闲装的男款，包括我说了一大堆，这个还会持续去你还可以持续的去提问他会把这个呃过去的聊天记录明确写好啊，哪些是人说的，哪些是AI说的，然后让这个第二代的6B模型有一个大概的了解。
	但是需要注意的是，第二代的6B模型你给了这样的一些提示词，human冒号、AI冒号、human冒号它是能有一定的理解的，但是比起像这个第三代的6B或者说像这个更强的GPT3.5GBT4，它本身是有肉的。它在训练的语料里，就他的训练数据里就大量的存在这样的角色。加上内容的训练数据比起来，第二代就算有这样的提示，它它的这个生成的结果肯定还是有差距的。那么通过这个方式，其实我们可以用南线来比较方便的调用我们的ChatGLM了，然后当然还有第二代的这个chat GM是跟南茜的官方对接石先生了。那假设我们现在还需要去做这个，更进一步，把我们的这个CGM6B也弄一个图形化的界面，然后这个图形化的界面还是跟南茜去做了对接的，甚至可能我们下节课要去训练一个新的chat GM2的6B甚至再去对接一些这个RAG，怎么做呢？
	我们一步一步来，先假设模型不用换，也不用接这个向量数据库，先把能券和我们的radio接上。因为之前的这些代码其实就是一个简单的radio的封装。我们把能欠的这个conversation chain和我们的radio先接上，看看是怎么样做的。首先这个是最终的gradual的界面，问了一些很有意思的问题。第一就是问他是不是卖电器的？是的，就是你理论上他的训练语料会主动的积极响应人，所以你问他是卖电器的，卖什么的，他通常都会说，是的，我是卖这个。
	然后对于这个GT4，在dev day之前，就是21年的那个版本，我有问过三体这个事儿，GT4当时是不知道的，那么显然对于一个擅长中文的大语言模型来说，他要是不知道三体是什么说不过去。所以他肯定知道三体是什么，这个是他比GPT3点GPT3GPT3，因为它对标的就GPT3，或者说其实是更更小的模型要好一点，因为它有这部分的语料。但是既然是中文的这个大模型，那肯定就得考一点更难的对吧？就我问的周一当中有1 64卦，有一块叫滴水石，这个怎么解读？这就搞错了对吧？大家稍微有一点医学基础的就知道，这个六十四卦也是只有六爻，不可能有九瑶。他这边写到地水师，有六个阳爻和三个阴爻组成，根据这个结果可以得到不同的解读，然后后面就开始扯闲篇了，就是说什么每个都有象征意义和自然属性，然后什么水一大堆，显然就是不懂，开始在这个在编。这种情况下就是留给大家思考的一个问题，也是我们下节课需要去解决的一些问题。
	如果是在这种情况下，用RAG能不能解决问题呢？就是我们假设有一个RAG的向量数据库，这个向量数据库里面存的都是关于特定垂直领域的数据。比如说周易也好，或者说医学也好，任何我这只是去想了一个他大概率可能会不懂的这个领域大家也可以想到一些自己的这个领域。那用向量数据库去存各种各样的QA这种对就是问题和答案的这个对QA能不能解决我解决好它的这一类似的领域问题，我觉得这个是很有意思。
	像我现在提的这个医学这个领域，我觉得就很难，我的直观感觉是很难。因为它有很多的概念词是跟我们平时的生活当中的语料不太一样的。它的内涵更丰富，或者说它的词汇更独特一点，不一定在平时用得到，就像这个什么滴水诗？你平时哪会听到这样的一个词呢？那他就不太会有一些条件概率，能够让你猜出来它是个什么意思。
	针对这种数据，我认为可能用微调去做训练，会对几十亿这个级别的大模型更有帮助，而不是去使用这个RAG。但我们可以做成一个家庭作业，大家可以自己去先试一试。因为这些经典的著作，网上都有啊，应该可以自己去构造这个数据，如果构造不来，那我们下节课会来教大家怎么去构造。
	那我们先看假设不去改这个模型，就用原来的这个GM26B要怎么样去启动。其实很简单，上面这部分代码就是我们开始notebook里面的代码。用这个conversation chain去启动了一个ChatGLM的这个也不叫启动。用这个conversation chain去启动了一个去去创建了一个client，创建一个客户端，然后这个客户端可以访问到这个ChatGLM URL这个特定的服务器端的这个呃GIM的模型。
	然后radio其实它的极简版本就是用一个chat interface，然后里面有一些特定参数。第一个参数是这个响应函数，就是我每一次发送提交的时候调用的一个函数，这里有一个ChatGLM的chat，是它的响应函数。第二个就是我的这个聊天框，title名字是什么。第三个就是我的整体要用一个什么样的一个模块。在我的这个interface里面，我们用的是chatbot，就聊天的这个模块可以指定高度接着去把这个radio启动起来，两个重要的参数，我们接着就可以直接来看一看怎么玩。现在假设我们。
	回到。
	我们的课程项目里，LLM quick star。
	有一个叫ChatGLM的目录。然后我们这个叫做chatbot web UI，我把它打开。
	这个目录，就是我们看得到的刚刚的代码截屏，相比刚刚截屏的代码就多了几个import，下面有一个in IT chatbot和net radio对应的启动。那我们现在需要把G2m two 6B给启动起来，然后应该是把它关掉。
	稍等，网络有一点点卡。
	好。加载起来了，然后我们在这儿启动这个radio这个服务器。
	稍等，我们大家有注意一个小细节，就是我们使用这个康达唑环境管理的时候。大家可以看得到，其实这个是今天为了演示这个GM3的各种seminar demo，新创建的一个环境。现在我们其实是想用整个我们大语言模型的微调课的这个环境，我们可以因为它是base环境，所以d active就可以回到这个base环境，去做这个环境的隔离。
	那我们启动起来之后，应该会再占用一个端口，7860。这个7860就是我们的radio的这个端口，就这个。
	那一会儿我们会看到，我们在这儿提的问题。在这儿我们在这儿提的这个问题，我把这个放小一点，还是六百。
	这样可以放大这个页面。
	好比如说我们问他。
	这个提问能在这儿首先看到这个是radio的这个页面。
	触发了这个conversation券。这里触发了这个conversation券。同时在我们的这个8001端口，也就是我们的真正部署chat GM two 6B的这个API server，同样也能收到这样的一个请求，就是很直观的很简洁的。如果大家希望radio这边的日志少一点，把这个地方。设置为false就好了。所以能看到他这个写的，我是一个大型语言模型，可以回答各种问题。没有实体也不能卖东西，对吧？然后。
	这个就是它的缺陷，我们看得到如果我们使用了conversation，这些都是故意找一些错误的点。让大家知道真正要去做这个大语言模型应用的时候，还是有很多AI工程需要去体现的。就比如说我们这儿，因为一开始引导的是你是卖什么的，他回答的是大语言模型。然后同时我们又使用了conversation chain去记录他的什么之前的内容。所以他现在当我在问你是卖电器的吗？他其实已经知道他前面有历史记录，有memory，说我是一个大型语言模型。所以我不是我不是卖这些的，然后。那就没有办法了，那就没法去做这个处理了，那他可能就绕不过去了。
	通常这个时候有两种解决方法。一种解决方法就是你自己后台这边得做大量的工程。你可以想象就是ChatGPT刚刚上线的时候，就是22年底刚刚上线的时候，应该有将近一个季度的时间。就是他在跟全世界人民玩游戏，因为全世界的人都在类似于我现在做的这个操作，去绕过ChatGPT设置的各种各样的prompt memory的这些设计，让他去说一些违反公序良俗的内容。那现在其实如果你要做一个真正的像chat GP1样这么健壮的大模型应用，其实这个过程是无法避免的。因为大语言模型本身它是没有特定的公序良俗的概念的。甚至说他也不知道，他回答说我是一个语言模型师好还是不好，是这么一回事儿。
	那那就说回来，我们既然要去做这些对应的AI工程的内容，那从哪些地方入手？第一个可以入手的点就是我们的conversation chain本身可以去进行处理的，就conversation chain的这个buffer memory是可以去进行操作的。之所以会出现记录的这些历史的信息，就是因为它因为bubble memory，那你可以去进行这个memory相关的一些设计。第二个就是如果你绑定的使用了radio作为你的图形化界面，那radio它本身有一些额外的功能，比如说retry undo和care。
	这个Clair点击之后，其实对应的是这个部分，就我们说有一个响应的函数function，chat GM chat，这个chat GM chat在这儿去做响应的时候，其实是传入了两个参数，这两个参数我们目前只用了一个就是message。这个message具体指的就是当前我们提交的这一段话，叫做message。这个history，其实是radio，它也维护了这个history，就他也维护了这个对话的，只不过这个对话没打出来。咱们要是在这儿加一行代码打出history，他也把这个都维护起来了。如果我们想要去跟radio进行绑定的时候，那也可以用这个history来做处理。那你就可以不用这个conversation的这个buffer，你就直接用radio的这个history也行。但是它就没有能欠的这一堆memory的可扩展性了。
	因为radio的history它就只介入而已，不太会说还会去帮你做这个什么conversation的，什么summer的memory之类的，所以这是两个可以去对历史记录最近的进行的这么一个操作，这个是一个思路，大家可以去进一步去做扩展。那这样我们简单一点，我们就不浪费时间在这改这个代码了，我们重新启动。一开始的引导就不问这个卖什么的，我们就试一试你是卖电器的，看他能不能走到这个正确的路径里。
	大家可以看到这就很tRicky，很有意思。就是当我们一开始没有去做引导的时候，他就会说我是一个语言模型。正如我们在这个地方想给大家展示的这个含义是一样的，对于大语言模型这个出厂设置是很重要的。如果我们在哪怕使用LM茜这种最原始的操作的时候，你没有去做这样的出厂设置，它会给你瞎聊。但是如果你做了这样的一个出厂设置型的引导，这就相当于你是一个什么销售顾问，你是卖电器的嘛？啊，是的，我们出售各种类型的电器，包括这个智能手机，巴拉巴拉一大堆。
	然后右上角是它的一个处理速度。这里他有写，目前我们销售的是这个iphone安卓，包括其他一些手机。可以想象到这些其实都在训练集里，咱们现在看到这些数据其实都是来源于这个训练数据。就训练数据里面有这些东西，我们也几乎能通过一些，就是你玩的多了，你就大概能通过一些特定的问题裁出来它的训练数据大概是在什么样的一个截止日期。就比如说通过这个问题，你就能去猜一猜，搜一搜小米11、iphone 12、三星S21、mate 50的发布时间。然后大概率就是最近那个发布时间前后几个月的数据，就是他的截止训练数据了。当然他可能只是取了那部分的这个语料，是那个截止数据这个是一个很有意思的问题。我们可以再再。
	话锋一转。
	问一个别的问题。大家想象一下，一个如果是真的一个销售机器人，那么他这边有写这是截止到2023年。
	对，通过这个过程我们能看到这个conversation的这个buffer是一个相对来说比较好上手的一种memory。但是这个memory它的缺点就是它会一直记录，所以大家可以实际去先用这个去用到超过它的边界的时候，什么时候会超过边界呢？就是你的对话太长了。这个太长的对话超过了背后真正提供支持的这个chat GM two，它能接受的这个token上线，那就会出现问题。那这个时候你就会对memory再去做一些调整了大家可以看到，其实他不是完全不懂的。他对于这个乾卦，显然就是一个所有人都懂的，但不是每个卦象他都懂的那就说明在这个领域知识里面，因为这个钱挂本身也是清华的这个校训的去来源之一，所以他当然知道。但是有一些64个里面有可能有五十多个都是不是很有名的那他就不知道了。像这种特定领域的知识，大概率我们就得通过这个微调的方式去注入整个流程。
	其实就给大家展示完了，我们再来回顾一下这张图去理解一下。其实这个就是咱们整体就radio，就是在conversation chain外面再套了一层YBUI，大家去想象一下，然后YBUI里面也有一个他自己的很很粗暴的一个history的一个记录方法，它都不能叫memory，它叫history。所以整个使用南茜去调用ChatGLM，然后的这个pipeline其实就这样的一个pipeline。然后它能支持着我们做各种各样的聊天类型的机器人，其实都可以在这个价值上面去完成。无非就是换零件，你换了新的模型权重，相当于更新了这个API server里面的模型，或者说你换了这个新的prom template，换了这个模板，你换了memory的管理方法，甚至在这个流程之外，我们还有一个东西叫向量数据库，你换了一个新的检索器或者检索方法，或者相应的数据库里换了新的数据。但是基于大模型的RAG或者这种chatbot，基本上就是咱们现在看到的一个完整的形态了。好，大家有什么问题我们来进行这个交流。
	我前面提过一些问题，我看一眼。
	模型项目有ctp项目，实际部署的时候最好用这个CPP。我不是特别同学可以举个具体例子，但实际执行的时候，py touch和TensorFlow最终实现都是用C加加来实现的。所以你不用这么纠结，就是这个下面的深度学习框架层会帮你解决你关心的这个问题。调用开源的大模型需要付费购买key吗？不需要，有个同学问，调用开源的大模型不需要付费购买key，因为你调用的是你自己服务器上的模型。
	对，有个流式输出和非流输出还专门讲了的哟。对，别的语言都能看懂，工程化还有点差距。这个就是其实你多去跑跑，就是大模型的代码已经非常简单了，你看我们今天演示的就有多少行代码，才这不才二十行代码吗？是吧？已经高度浓缩了，大模型的轮子造的挺好的。
	还有同学问ChatGLM3，这个我刚刚回答过了，对，需要去做对应的实现和适配。然后比较尴尬的是，我不知道你们有没有注意，就是南茜的这个LLM这条线是对应的一二代的模型，它的chat model这条线对应的是质谱AI的公有云服务。所以如果它它的第三代和第四代的模型也是chat model的话，某种层面上其实跟它公有云商卖的API服务是有冲突的。不知道各位有没有想过这个问题，简单来说就是如果他的chat model实现的很好，大家就为什么还要去用字符AI的这个API呢？对吧？这个地方很tRicky，也是他没有再进一步迭代的，我觉得可能背后有一些原因。
	我再看文茜是不是只适合纹身纹场景，在纹身图场景以及纹身纹加纹身图场景的结合的应用，应该不太合适。这位同学你的合适是或你的适合是指啥？就是合适这个事，这是一个主观问题。我给你看一眼，我印象当中林倩好像也支持调用纹身图模型。
	对，在在tools里面它也有纹身图的这个模型，包括你说的纹身纹，再用生出来的纹去生图。这条pipeline是通的，并且还蛮早就有这样的一条pipelines。就在大理一出来的时候，达理E大理就有一大理一第一代的时候就有这条pipelines。
	如果我想部署比较大的模型，需要多个GPU的算力才能支持，应该怎么部署？这个同学可以举一个具体的例子，就是哪个模型你可以说出来是哪个模型，我们来讨论。对，就算是几百亿的这个参数的模型，它也能支持在多张卡上就单机多卡来进行部署的。这个就跟我今天演示的这么多个不同的启动的demo文件一样，他自己往往就会提供这个文件。就这个同学问怎么部署这个模型的提供方就会干这个事儿。对自己部署要GPU的算力，调API不需要付费。是的，就跟你买台电脑在家玩和去网吧一样，去网吧就交费呗，你不想去网吧，你就买个电脑呗，自己买一张卡。
	Chat GM3本身是不支持function calling的吗？为什么这个stream net就支持了呢？是在外面包了一层的吗？怎么实现的？好问题，我们来看一眼。
	问题的关键在哪儿，在这个文件。
	我们看到这个是它这个前端页面的一些代码，就三个按钮，chat to和这个code interpreter。然后这个是它的win点修改文件的实现，chat input巴拉巴拉都是一些前端的代码，然后这部分没什么滑头，关键是开始给大家展示的另一部分的这个代码，就是我们的register to这部分的代码。这个其实是它实现所谓的function call的一个关键。
	就是有个同学问，本身不支持为什么就支持了？是因为在这个例子里面，他注册了这堆工具，就是function call。它本身的核心是你要有工具给它call，然后这个basic demo里面，它没有东西给它扣，它只是起了一个对应的这个chat GM3，然后它的prompt也很简单，就是一个对话的prompt，一个chat的模式，给的这个prompt的都是这个roll。然后这个content这样的一个模式，你可以理解成在在GM3训练的过程当中，它的偏向对话的训练语料就是raw user content内容，然后role assistant content内容。还有一类是function call的训练语料。然后function call的训练语料里面就是给了一堆prompt，prompt里面有各种各样的tools，tools里面写了我支持哪些不同的巴拉巴拉就像我们这个tools里面写的这些内容，我们开始也打印出来的是这个逻辑。
	所以chat GM36B它原则上是支持的，只不过那个简单的demo没有注册工具，所以他没法去调用，相当于他没开启这个功能。自己部署应该不能用130B这种大规模的模型。你这个问题我回答不了。就是借用最近一个电视剧很火的说法，这个繁花的这个说法就是咱们自己也不太会吃88个霸王别姬，对吧？吃不下去，但是魏总他就能点88个霸王别姬。这个你也不管为什么，但人家就是用了，那你说他能不能用呢？应不应该用呢？也不知道。这完全取决于你的需求。
	对，就比如说如果你是在一个超级大的有资源的团队或者公司里面，1300亿的规模的模型你能部署的，对吧？因为1300亿在int 4精度下面，其实也就是几十GB，几十GB你也是有的。对，解决大模型的幻读问题，常用向量数据库，刚才老师说使用微调处理问答，使用微调大模型做智能客服难道更准确？如何评估？不是，我没有说做智能客服，我是指他对领域知识的理解。我刚刚举的例子是说，比如说这个特定的卦象他是搞不懂的，比如说我们举个例子。这个他是总的，对吧？然后。我确认一下，我别搞错了。
	应该是叫这个挂水三件，应该有这个卦象。对，比如说这个卦它就很少见，我们再问问。这个领域知识是他之前的训练语料没有的，而不是说智能客服这个领域知识，我是想让它类比什么呢？类比你的产品。你看这个就错了，水三件是六个音瑶组成，这个明显就是乱讲，对吧？六个音瑶是坤卦，水三件怎么会是这个六个冰瑶组成，对吧？所以这个其实就是相当于你把水三件换成你公司的某个产品一模一样。他不知道这个东西，他就只能猜猜了，就容易猜错。
	所以对于智能客服的场景，如果你卖的这个东西，它不是一个就是你不是一个纯销售岗，你是一个产研岗，你在研发的东西你研发的东西他不知道，那你需要把这个东西，把这个产品相关的概念给它注入进去。而我们举这个例子，因为我不知道你是个什么产品，我也不知道，所以我只能找这样的一个领域数据来打这个比方，UI的按钮可以自定义吗？可以可以去了解一下radio这个库，其实不复杂的，这个同学问的就radio这个库其实不是那么难用的对，应该也有各种各样的中文资料。这个是一个纯UI的，没有花太多时间给大家花时间在这儿。
	Function call和agent的任务分解工具有啥本质区别？首先function call它本身就是一种agent，这个我相信之前听过开发课的同学都都理解。Function call本身就是一种agent，因为agent就等于APP。你相当于是在问这个，我去想了想个比喻，就是你相当于在问微信和APP的聊天功能有什么区别。就是因为function call是OpenAI做的，然后它的名声很响，所以function call现在就像是微信一样，它成了一个特定的名词概念了，因为他把这个词给占住了。Agent的概念更像是一个APP的概念，然后APP当然就有各种功能了，所以是这样的一个概念关系。
	然后regent的任务分解，agent本身不能做任务分解，大模型才能做任务分解。大模型怎么样才能做任务分解呢？需要你给他合适的提示词模板，它才能做任务分解。所以从这个角度来说，你可以再解构一下function call是通过给大模型一堆tools的名称参数描述这样的提示字母板，使得大模型知道我有哪些函数工具可以去调用，让他能够去走开，让他能够去做这个函数调用。然后我们的agent去做任务分解，不管是auto GPT还是react还是其他的这种agent，是因为我们的提示词模板里面给了一些特定的提示词，它去响应他才能去做对应的任务分解，是这个逻辑。
	微调导致的遗忘问题怎么解决？这个微调导致的遗忘问题只能通过微调解决。对，就是你得就相当于这个事儿是你我举个例子，就是我刚刚开始学习怎么炒菜，我老容易炒菜的时候忘放盐或者忘放酱油怎么办？那只能通过你多炒几遍，你炒个十遍，炒个100遍，每次炒的时候都告诉自己要放酱油或者放盐才能解决。落到我们微调上，就是比如说你在微调的时候，某些超参数没有设置，对，那你就得调很多遍。并且通过这个模型的evaluation过程知道那儿没搞对才可以解决。
	下节课会讲怎么调用微调之后的模型，下节课我们会讲去微调模型，调用不就是一行，你就换一下这个同学没有上我们之前的课程吗？我今天这么多个demo都是为了强调一点，就是我们看看这些demo都是怎么在调用我们的模型。在这儿，这个radio它不是真正的服务端，对吧？真正的服务端是什么呢？真正的服务端是我们的chat GM26B这个API点PI文件，这是真正的服务端。
	它在加载模型，这个加载模型用的是什么？用的是transformers的库，这个我们是第三周，第四周就讲过的哟，就是我们用transformers的trainer训练完了之后，我们save pre trained model，save free trade, 我们就训练出一个模型了。所以至少流程上你应该能够加载回来这个模型的。至于这个模型有没有训练到位，简单来说就是之前教的这些东西。如果学会了这盘菜，你是无论如何你都已经炒到这个盘子里了。甚至你也学会把青椒换成木耳，把青椒换成洋葱了。至于这个盐、味精、火候顺序，就是我们微调的水平高低了。这个是要不断学习的，但是这个流程你肯定是应该会的。
	就是换这个微调之后的模型，就是这儿改成你本地的。所以下节课我们会尝试做出一个数据集出来，把这个数据集去在chat GM two 6B上面去做微调。然后换一下这个数据集，然后把这个流程再给它跑通。就相当于在今天的流程上，我们端到端跑通了。但是我们换成一个自定义的6B的模型，然后再对接上向量数据库。
	看大家还有什么问题吗？
	Save model之后不是把微调的模型单独保存了吗？原始模型没变。这个同学没没好好听讲，那个讲过的。首先你说的这个问题，是所有的lora的微调会有这样的情况，因为lora a微调本来就不会改变原始模型。Na的微调是指在原始模型之前，在这个原始模型旁边增加了一个旁路网络，对吧？增加了一个小小的矩阵。真正使用的时候，它就是需要先加载原始的模型，再加载Laura的adapter，然后把它变成一个PEFT的模型。这个我们都有代码的，这个同学再再复习。
	我们的PFT我还专门弄了一个notebook PFT ChatGLM the influence，大家好好看一下，就这个notebook，就这个同学问的，你再看一下这个，就问这个问题的同学，你要用它的时候，你本来就需要先加载原始模型，再加载量化后的lora apter，然后把它们变成一个PFT的模型。这变成一个PFT的模型，然后就可以输出了。这个model点chat不就跟这一样的吗？
	下节课会讲基于特定的知识来如何制作数据集，也会讲是吧？对的。企业级的最佳实践如何搭配？是自己私有化部署还是购买？这个问题我解决不了。同学就是这个完全看你的企业需求，这没有一招鲜的，就是你有钱的你自己请保姆给你做饭，对吧？你这个厨艺好的自己做饭，厨艺差的点外卖对吧？甚至外卖都不想点了，你就出去吃。
	这个没有标准答案。对你只能说你在这个课程上，你尽量把你的厨艺学好，这样你可以有多种选择，但是没有说只有哪种选择最好。非要说的话就是还是按照。
	经济效益。
	来讲比较好，对，经济效益来比较讲比较好。对。升腾910显卡的推理结果和100推理结果不一致，是因为精度的问题吗？升腾这个我还真不确定，有可能就是你说的这个是有可能的。如果你是同样的模型，同样的精度，同样的加载方式，出来的结果不一样是有可能的。但是需要注意的事情是你的一些参数的处理。就比如说你的temperature如果设置的比较大，那哪怕是同样的A100，就是你就在一百上同样的模型运行了多次，它推理结果也会不一样的。因为这个不一样，它就是不一样，它它这个是by design的。这个我上次也讲讲过，就像你同样一口锅，同样一个厨房，同样的食材，同样的人，他炒出来的口感也会有差异的，对吧？毕竟他他这个就跟烹饪很像，尤其是中餐的烹饪，他很难炒出一盘一模一样的菜。对，这个edle上部署open I edge上部署的，你可能是指azure代理的这个OpenAI对吧？去看azure的文档，所以这个应该是不复杂的对。
	这个你哪怕问牛逼，他应该都会回答你的，不是一个特别复杂的问题。行，那我们今天就到这儿。我看大家关于这个课这节课的问题应该不多了。但这节课重点是让大家多动手，就是你现在有卡了，把今天讲的这些东西都去跑一跑，尤其是这两个模型都拉下来跑一跑。然后我们静观其变，看看16号质谱他怎么说，第四代的ChatGLM会不会出来，出来的话我们就以不变应万变，给它弄进来。好，我们今天就先到这儿，感谢大家，我把代码就提交上去了。
	好，大家再见。