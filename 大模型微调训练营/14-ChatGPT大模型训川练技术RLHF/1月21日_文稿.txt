	好，大家能看见屏幕吗？声音是不是正常？
	看到有同学回复一了，行，那我们今天就今天跟大家分享的这个技术叫做ILHF，是一个很多人应该都听过，但是很少有机会会实际上手去做的这么一个技术。今天我也实际的去试了一下hugging face，开源了没多久的这个TRL，但是确实没跑通，所以花了一些时间。但后续如果这个代码有机会在小的GPU上面跑通的话，会把对应的示例代码放出来，因为RHF本身就非常的费资源，今天我们可能主要就把这个IHF技术跟大家做一个大的一个分享吧。
	然后有什么问题的话，我们以QA为主，接着RHF其实是一个很复杂的一个词，它叫基于人类反馈的强化学习这门技术。这个技术其实是非常算是非常新的一个技术。因为本身大模型的训练，通过我们前面理论课的学习，大家应该有一个基础的了解了，尤其是像bert、GPT一直到GPT3这段期间，所有的大模型几乎都在使用未标注的海量的数据来进行训练。但是训练之后，我们也讲过从预训练加上反应tone这种模式，从18年开始成为了一种标准。但是这个标准也遇到了一些问题，就是我们提到的成本很高。比如说bert如果你要去进行这个下游的训练，你需要把全量的模型参数全部加载进来。然后在下游的任务上面去做微调，这个成本是很高的。
	后续我们又讲了PFT，比如说这个adapter，各种各样的adapter，包括prompt，p20等等这些技术。但他们也存在一些问题，比如说它针对的是下游的一些特定任务，然后这些特定任务它的通用性就不够好。就比如说我一个特定的任务做完之后，我就只能完成那一类任务了。比如说我做这个file mask做填空，或者我做这个生成，然后生成可能是在特定领域，就像咱们用的LORA，它可能是一个很小的一个旁路的网络，但它不够通用，而不是一个我们想象当中的基座模型。
	那么怎么样从GPT31步一步发展到了ChatGPT，其中很重要的一个技术就是ILHF。当然GPT3.5到GPT4以及GPT4.5又出现了MOE混合专家模型。那个可能是更多的在线上线之后，更进一步的去降低成本和更进一步的去提升它的通用性。因为我们始终需要有一点需要注意的，就是说不管咱们现在是GPT3.5、GPT4，还是所谓的GPT4.5都没有办法做到一个全知全能的模型，所有的模型都取决于我们的训练语料以及我们的训练技术。那我们就来看一下IHF是怎么样在尝试解决。第一通过训练尽可能保证模型的通用性，同时还能让模型的生成结果和人的预期，也是我们所谓的价值观是对齐的这套技术是怎么运作的，然后在这个过程当中，其实就会涉及到一个在tense flow里面其实也没讲。
	在前面讲语言模型的时候，我们提过机器学习有三种类别，无监督，有监督和强化学习。强化学习其实很长一段时间从阿尔法狗出现，又掀起了一股热潮。但是在深度学习非常火的那几年，就计算机视觉，包括我们的自然语言处理特别火的那几年，其实强化学习算是一个就相当于不在这个50的1个小派别，他没有掀起巨浪，包括像当年瑞这个框架就是为强化学习做的。但是后来因为ChatGPT使用强化学习来训练语言模型，当然也就用了瑞这个框架，使得瑞又火了起来，同时，也让大家发现，原来强化学习是可以用来做自然语言处理的任务的，是可以用来训练语言模型的。这个是一个非常大的进步，也是最近这段时间有人在讲，甚至爆出一些一些新闻，说open a内部已经学会类似于q learning的技术来做大语言模型了。
	当然最后这个事情也没有公开的去讲，但我们先了解一年前的这个RHF是做到一个什么样的程度。好，那么整个要做这个RHF，其实它内部有三个经典步骤，我相信大家都看过这个图了。但是从ChatGPT它不是从这个IHF开始，它是大家都知道它从GPT1开始一直到了GT3，然后才到了GT3.5。然后在这个过程当中，其实一个典型的大模型，比如说不管是ChatGPT还是number two，通常都有三个大的阶段。在这三个大的阶段之后，其中或者说这三个大的阶段里面，第三个阶段才是我们强调的RLHF，那么在这第三个阶段里面，又可以分成大的三个步骤，其实这个有点层层嵌套的意思，也能想象得到。
	因为本身我们都知道训练一个这样大的模型是几千千万的投入训练一次。那这一次不可能就是你你就点一下这个按钮开始训练，钱就花出去了。肯定不是，这里面有大量的人工的不管是造数据，训练的参数，包括这个过程当中一些训练技巧的调整，都是非常消耗人力资源和物力的。也是后续现在大家对于这个IHF有一些改进的方向，但还没有一个替代性的方案出现。
	好，那我们就来简单讲讲这个流程和历史。其实这张图大家应该也看过几次了，这张图片我们之前讲的所有的重点都在上面那条线，就是怎么从GPT1到GPT2到GPT3，一步一步的增加了transformer的层数，然后增加了我们的语料的数量。然后同时在GPT3跟code x之间把代码引入了进来，让我们可以在这个code x后续的版本上面实现这个chal sauce所谓的这种思维链，一些提示词它能够被响应起来了。
	但是到了code x这个阶段，事情并没有得到一个非常好的发展，就是我们并没有看到一个真正这个杀手级的应用出现。真正杀手级的应用出现其实是在右下角ChatGPT。但ChatGPT的出现其实除了上面这条基础能力的搭建，下面这条核心的这条线，就是我们看到的指令微调IHF，人类价值观的对齐，包括这个chat base的这些语料的一些灌入，才是真正的关键。也是现在我们说你要重新搞一个大语言模型，有很多的这个坑。所谓的AI工程的这些核心的竞争力，其实是在下面这条线。因为上面那条线就是怼语料，然后对算力去训练就好了。但下面这条线你才是埋了无数多的技巧。然后下面这条线也是相对来说没有怎么公开的。
	这个是我们今天这个课程里面也会跟大家讲，下面这条线的技术数据都不是一个很开放的状态，那我们来一步一步看到底怎么做的。一个总体性的用RLHF训练大模型，大的三个阶段的一个示意图，让大家先在脑子里面埋一下这个概念。就是做一个IHF来训练大模型的话，有三个大的阶段。第一个阶段就是左上角的这个蓝色的框，training of tokens，就是万亿级的token。这个是咱们可以先理解一下，就是我们在GPT3这个训练过程当中就已经提到了左上角这一步。大家有印象的话，后面还有一张图，就是给大家看GPT3的训练语料。其实GPT3当时就已经用了大几千亿的tokens在训练了。当然GPT4肯定是超过至少一个数量级，至少是大几万亿的token可能是有的。
	那么在第一个阶段，其实不管咱们做不做RHF，现在几乎都是一个统一的范式了。就是用足够多的语料去训练一个基础的模型。然后这个基础的模型训练出来之后，你可以理解成就是我们算力足够强，然后我就有多少算力，有多少数据我都先给它灌进来，把它变成一个五六十分的基座模型，变成一个五六十分的击中模型之后，才开始有人工的部分进来，就是人来打标注。就是我们看到的第二阶段，supervise的这个fine tuning。但中间这里出现了一个叫做domain adaptive的pre training的一个绿色框，这个大家可以理解成是一个可选框，就是我要不要去做一些领域适配的预训练，这个是完全可选的，它不算是一个大的关键步骤，但是super rise的fine tuning是一个非常重要的步骤，这个步骤其实就已经开始。
	你如果按照一些传统行业的做法或者说说法的话，就是开始精炼模型。他使用到的这个数据集是纯手工的，就是人造出来的一些数据集。然后这个数据集是为了要么就是为了设定一些特定的指令。就比如说我们在上节课去教大家用q lora去微调chat GM36B的时候，就出现了一个我们让chat GBT4去帮我们想象人会去怎要问问题，问关于这个卦象的问题，比如说它的含义是什么？这些都是属于照这个特定的instruction，这一类其实都可以被叫做supervise的fine tuning，或者说叫做instruction tuning，也就是我们看到的阶段2。
	大家先把这个图埋在脑子里，后面我们可以细讲。然后有了这么一个中间的步骤之后，我们就可以理解了。就相当于有一个很就是像张无忌一样，对吧？我们还是举这个金庸小说的例子，有一个像张无忌这样的选手，他在前面的阶段他什么武功都没学，但他有内力的增长。
	就像我们看到万亿级的token上训练出来一个模型。那现在你想指导他学会一些特定的指令，让他来帮你干事情。那其实这个就是属于instruction tony或者我们的有监督微调super rest fine tuning这个步骤。但是这两个步骤做完之后，它也只是一些基础的招式，它并没有说跟我最终要用的这个场景非常的贴切。这也是ILHF进来之后带来的一个质的提升。
	这个质的提升一个最简单的逻辑就是我们已经做过上节课的这个作业了。我相信有的同学可能已经跑过了，在上节课的作业里面，大家虽然已经用GPT4去做了数据增强和合成数据。让我们的ChatGLM36B也能响应我们的一些输入。但是你始终会发现有一个弱点或者说缺陷，就是我们输入的这个训练集它有一些特定的问法。比如说我们想了20种问法，但是这20种问法始终不可能穷举这个用户真实去提问的这些方法，或者说他的这个方式。那么怎么办？我们不可能穷举所有的这个问法，因为这样数据集就会变得无穷无尽，我们造不出来。强化学习其实就是在一定程度上去解决这样的一种问题。
	强化学习提出来的核心跟所有的我们之前了解的这些有监督的机器学习方法都不太一样，有监督的方法就是我有标签，然后我在训练，让我训练的这个成果就是让我的模型尽可能最终输出的这个结果是符合我训练集的这个分布的。但强化学习的核心是学的是一种策略，或者说学的是他的这个最终理想，就真的像有一个可以思考的一个智能体一样的存在，这个是强化学习的核心解决的问题。那么把强化学习引入进来，其实某种程度上，我们能迎刃而解的第一个问题就是如果用户的提问方式是无法被数据集重启的那有没有可能训练出一个模型，然后这个模型是能尽可能的去响应用户的提问。然后我们能把能抛去用户提问的方式这一层的概念，而是直接通过他不同的方式能指引到我问的到底是什么问题的本质。我能把这一层抓出来，让我们的大语言模型抓到这一层的本质，然后来回答问题。
	这个奖励模型还有一个很大的价值，就是我知道他要问什么，然后对应我生成的这个结果也不可能穷举。那有没有可能这个奖励模型通过我的这个强化学习的训练，让它生成的这个结果也是符合我人对于这个生成结果的一种预期。听起来就很玄学，他不是说我能明确的说这句话是99分，那句话就是98分。因为你没法枚举所有的话，但他就是把这个所谓的很玄学的价值观，人类的价值观给学习出来了。至于怎么学习的，我们待会儿可以再逐步的跟大家去探讨。但大家把这个三段式三个阶段先在脑子里面先记下来，因为这个是非常重要的一个整体性的一个知识框架。然后在这个过程当中，到了第三阶段的RHF，其实它核心就是要训练两个模型。
	一个就是从前面一步一步通过万亿token的一个基座模型，然后通过指令微调变成了一个精炼的语言模预训练的一个语言模型。然后把这个语言模型用强化学习的方法再进一步的训练。那怎么样能够用强化学习的方法去训练他呢？所以在第三阶段他还需要再额外训练一个reward model，一个奖励模型。这个奖励模型能够给当前的我正在训练的这个模型去打分，然后这是一个大的一个框架，然后通过这个方式训练出来的最终的那个语言模型才是我们用的那个语言模型。甚至如果我们想要把他的对话能力再去做提升的话，还可以去提升它的关于对话的响应。
	所以如果大家把这个事儿捋顺了，你就会发现整个这个流程这三个阶段，其中第二阶段跟第三阶段是可以无限去套娃的。因为它就是两种不同的能力，一个是增强对指令的响应，一个是给通过人的打分，去尽可能的让大模型生成的这个结果，这是对的。因为它最终这个生成结果里面，它它有两次的惩罚项的一个计算。待会儿我们会去看到，既是一个对的结果，同时不是一个乱的结果，就不是一个不符合人类的期待的一个结果，这是一个大的一个架子。
	那我们一个阶段的一个阶段来看，第一个阶段其实是大家应该算是最熟悉的一个阶段。就是我们没有训练这么大的量级，就是万亿级token的预训练语言模型的阶段。在第一个阶段从18年开始，其实open I就一直沿着这个路在走，一直到20年。
	阶段一的核心就是要去训练一个自回归的模型，在transformers里面叫做因果模型。然后这个阶段其实我们看到像GPT3它用了这个common core，然后像athrob c像拉玛他们都用了这些公开的数据集。而这些数据集它就是一个纯粹的语料，就跟我们看这个新闻读报纸一样。在这样的一个大的语料上面，我们之前的理论课应该都讲过了。通过transformer我是可以不用任何标注。然后因为它本身的这个decoder的结构就决定了它能输入原文，再加上mask的原文，然后去做一个自回归的模型去预测。就比如说这里的这个图例，当我读到这里有一个digital的时候，下一个词是什么？这个是我整个阶段一的模型需要做的最大的一个任务，就是预测下一个单词它是什么。
	我们在transformers里面也用到了这样的一些模型，就比如说当然它在transformer里面叫因果模型，它不叫自回归。像我们看到右边这些图，其实全部都属于这一类模型。只有的模型它的容量吃不下这个万亿的token，他可能只能在几十亿或者几百亿的这个token范围内去做训练。太多了之后，其实他不一定能够把这个知识点容量给压缩表示出来。然后在这个阶段，其实我们能看到从这个千亿到5万亿的这个数据集的规模，几乎已经成了阶段一。大家如果要从0到1去做一个大元模型的话的一个标配。而这个阶段的任务其实就是一个预测下一个token是什么？一个自回归的模型，在这个阶段的产出就是咱们刚刚看到大的框架里面，左上角的那个蓝色的方块。
	但这个阶段最难的事情其实是什么呢？是很难收敛。在实际的大模型训练里面，就不像我们做的这个几十亿的规模，在真实的甚至到几百亿这个规模的时候，就会出现一个很难收敛的问题。你可以想象一下，当你的训练语料变成非常大的时候，你一次给它的一个batch，就我们设计的这个VH可能连它整体语调就是这个air pod的1‱都没有。如果你给的这个best size的值越小，其实它越难收敛。我不知道这个大家能不能理解，就相当于你每一次喂给他的这个数据占他数据集的总量的比例。如果只有1‱甚至10万分之1，其实对于他的训练他是很盲目的。因为他不断的给他的这个best size可能是极其离散的，甚至连这个领域都不一样。
	所以为什么facebook又要买几十万张卡，就他这个卡买进去，你说这个模型本身我们也交过，1750亿的模型，需要多少的显存是能算出来的。然后他明显是不需要10万张卡，他可能几万张卡就能把它装下来了。但为什么还要买那么多？就是因为在一个bet size，它可能设计的就是E级别的这个token，只有这样才有可能达到一个效果，就是达到训练的效果。但是你每一次都拿一亿的token去训练，几乎是不太现实的，因为这个成本太高了。
	真实的训练可能就是先拿极小规模的样本，因为你样本本身也是要去做这个处理的，然后在小规模的模型上去做训练。然后其在这个过程当中，如果你发现它收敛loss的降低的水平是比较合适的，是一个稳步在下降的状态的话，才有可能把这个模型的规模再升一个数量级量级，然后再把这个token的规模再升一个数量级。那这个过程，其实是有很多的tRicky的训练技巧，然后也没有特别多公开的技术，因为它试验的成本很高，但它难点就是在这个训练比较难收敛，在这个阶段的代表其实就是GPT3。我们看到在19年20年初的时候，其实GT3它的训练语料就已经达到了5000亿这个规模还是它的早期版本。后面的这些GPT3的版本肯定是远超这个数量的，像他用到的这个代码都不在这个列表里面。
	我们其实是用过这个因果模型的。大家可能到了IHF这个技术的时候，我们去回顾的话，其实它并没有那么复杂。如果我们是在几十亿或100亿以内的这个量级去研究它的话，其实也没有那么复杂。我们看到在最早我们用这个AWQ去量化模型的时候，去量化的就是这个OPT2.7B，一个27亿的模型。然后我们再对它进行这个它本身也是一个支持因果任务的一个模型，就支持自回归的。本来对标的也是GPT，所以要去用transformers去量化，包括去使用推理一个因果模型。这个技术并不复杂，咱们已经学会了，包括像GPT q也是一样的。
	那如果要去训练一个因果模型，其实也是有很多方法的。其中用的最多的就是q lora或者说Laura。像我们在这里OPT6.7B67亿的模型，我们使用这个q ora包括咱们的ChatGLM，我们的whisper其实都使用q ora是可以去完成这个因果模型的训练的。所以简单来说，如果大家想要自己去完整的体验这个预训练的流程，你是可以找公开的语料的。比如说common cross后面也会有一些有监督训练的一些数据集，也给大家做了一些筛选。那么就这个预就是第一阶段的预训练的模型，大家可以去使用像这个common cross就C4或者说wiki text这样的一些数据集，直接去从0到1的去做预训练也是可以的。就你加载的这个权重，你不去使用facebook的这个OPT6.7B你用它的时候，你把它的模型网络加载进来了，你把它的权重也加载进来了。但如果你想要去体验第一阶段的训练的难度或者技巧，你可以把它的这个模型权重加载进来之后，去给它刷成一个初始值，再来做这个训练，你会发现这个loss值的变化其实没有那么好做到收敛。
	第二个阶段叫做有监督的指令微调。可以简单理解，就是其实我们做的很多的这个微调，都是在阶段一的基础上去做的。就包括咱们刚刚做的这些量化，其实都是在facebook或者说OpenAI之类的这样的大厂，他们已经预训练好了一个模型基础上，我们再去做的微调。包括咱们的这个用的QLA，或者说像这个LHF，他要做别的事情，但他也需要做这个有监督的微调，都是在一个相对来说有五六十分的一个预训练的语言模型上去做的。但是它不像我们写这个传统软件会有一个严格的界限。就比如说很多同学会问我，我要把阶段一训练成什么样，我才能开始做阶段2。这个其实是很难有一个楚河汉界的，但是有一些经典的benchmark可以做一些评估，只能这样去跟大家去对比。就像我们在理论课的这个一二做的时候讲过P3，包括这个lama他们自己有一个千亿级别的一个横向对比，是可以做一个大概的评估的，但是不可能是说在那个阶段一个中间态就去有一个明确的界限。
	但是在在这个benchmark上达到一定效果之后，其实我们阶段二是非常考验做数据的水平了。也是大家在讲所谓的更多人工，更多智能的一个内涵的部分。在这个阶段其实我们会看到它的数据集并不要求多，它要求的是质量非常的高。刚刚我们看到的是万亿级别的token，但是到这儿其实就不是了。
	第一，它的训练集的格式发生了变化。前面我们是读书看报，就是我天天把人类的这个自然语言的一些成果就停在那看看完之后我大概知道人是这么说话的这个词后面比如说刚才说的digital，后面不太可能会接个什么writer或者reader，这个更多的可能是接的是一些其他的词。这个大的概率上他学会了，但他还不会认。
	第二个阶段，其实就是开始给他做任务了。所以其实我们前面讲的fine tuning，super rise的fine ti instruction tuning，这些带有我们叫标注，带有有监督信息的一些微调，其实都在这个阶段去介入的。只不过如果我们要去做RLHF，它介入的方式可能和我们直接去反映出到一个特定任务上会有不同。
	那对于我们要去做RHF来说，其实第二阶段的这个super west有典型的两种做法。一种做法就是我在去开始这个IHF之前，我会给这个初始化的第一阶段的模型去做一定程度的指令微调。但是也有的这个做法是我就不管，我直接在我的IHF里面再去做这个微调。因为RHF里面的第一个阶段也会去训练这个预训练的语言模型，这也是一个叫什么楚河汉界不清楚的地方。完全处完全取决于这个大模型的算法leader怎么样去操盘这个大模型的整个训练的端到端的过程。我们可以去完整的讲有这么多候选项。至于实际情况，比如说OPI的这个GPT5，它会不会把这个阶段二直接给干掉，从这个第三阶段的这个步骤一开始做SFT，这些都是有可能的。但是SFT的核心其实就是为了让大语言模型能够更好的去响应prompt，就像响应这个指令，根据这个指令去生成结果，本质上和我们做的其他的翻译错误是一回事儿。只不过它的构造这个训练集的方式有很多种，就比如说在这个OpenAI，它自己的这个InstructGPT就在GPT3.5 turbo出来之前，有一个很应该叫做在当时在ChatGPT发布之前，它的内部应该是instruct的GPT的生成质量是最高的。这个大有兴趣可以去读一读他的论文，那怎么做到的呢？
	其实就是他做了各种各样的，你可以理解成给大模型报了各种各样的补习班，只不过这个补习班的权重比例不一样。可能他有有一半的时间是在补这个文本内容生成，然后还有一些时间是在补这个问答。然后这个比例其实也是由这些公司自己去研究的。据我了解，他现在把这个instruction InstructGPT的训练过程当中，这个比一是来自于当时GPT3上线之后，就是有很多的用户去调用它，这个时间是在ChatGPT发布之前，就是JS play AI这些公司，他们在用GPT3的API做很多的AI的应用，包括那会儿的这个open I也上线过像web GPT这样的应用，就相当于ChatGPT的前身，也是有一个网页，能聊天，能访问一些，在线的信息。他通过这样的一些用户数据，其实从那个时候开始就已经了解到要去拟合真实的上线之后的用户的一个需求。通过这些用户的日志，他打了一个类别的标签，就相当于他这边打出来了八个类别的标签。这八个类别的标签就是OpenAI的用户，他们使用这样的大模型想要干什么事情。当然文本内容生成是最多的。
	还有一些开放式问答，就像我们经常会问他一个什么历史问题。题，地理问题等等。当然也有一些头脑风暴，包括跟他自由的聊天，然后去做一些文本的优化，做一些摘要，做一些的内容的分配等等。
	通过这些日志的分析，他做了这样的一个数据集。这个数据集它的格式其实就跟我们现在这里看到的一样。它有一个instruction，就是要让我们的这个interact GPT干什么事情。然后有一个output，就是这个大模型生成的一个结果。然后有的指令其实是不需要input的，就比如说这个文本内容生成可能就不需要input。还有一些像下面的这个例子，它可能是需要像让大模型去做选择题一样，来给你一个任务。然后选项有1234，你去帮我把这个正确的答案挑出来，以这样的一个格式去训练instruct的GPT。这个阶段其实他用到的token就是人造的。
	1000条到万条这个规模到现在为止也都是这样的一个规模。因为也就是一两年前的一个成果，它的格式上跟之前最大的一个区别就是有标注了。这个标注就是明确给了指令和大模型的回复。通过这样的方式，其实大模型在训练过程当中，他已经可以知道，当下一次一个用户用了类似的指令的时候，我要用类似的回复来回应用户。其实现在绝大部分的几十亿的模型都是按照这个套路在做。包括我们做的这个QLA前的GOM的这个36B也是如此。
	因为这个方式这是最简单直接有效的，但它的难点就是我们开始看到的这个数据是最难的。就怎么样去构造高质量的提示词和对应的生成结果这一对儿，这个是最难的。然后包括它的格式上也是比较难。那么在RH是RLHF内部，其实也是一样的。它可以去做一个人工human argument，就是人工增强的文本。这个就跟我们上节课用GPT4来生成的文本是有不一样的，但是它达到的效果和质量也是不一样的，大家可以记住这个是IHF里面的一阶段要做的事情，然后把它先放到这里，它本质上和咱们SFT是一回事。
	当然要去实现SFT，我们不可能课堂上造很多的数据。那有没有一些好心的机构，或者说一些非盈利性的一些机构，把数据给开放出来呢？其实有的，就右边我们刚包括这个InstructGPT，他们也开放了自己的这个数据集，就SFT的数据集，像nama two他们也开放了自己的数据集。这里我挑了应该是三个，其中三个给大家做一个简单的介绍。
	大家如果有兴趣也可以去去研究这个SFT在hugin face的TRL上也是能够去做训练的。但它这个训练的成本相对来说就要高一些了，因为投入比较多。我们重点来看一下这几个数据，起亚大家分享一下。
	第一个是这个epoch，它其实是通过self instruct这种框架。这个框架简单来说就是让大模型来生成这个数据，但是人会对它进行校验。这个数据集建设的比较早了，然后他们都是在hugin face上开源的。这个数据集它是用这个text达芬奇003已经下线的GPT3去生成的，包括过52K的指令和对应的文本，可以用它来进行指令微调，然后基于它有一些变种，它的核心就是我们能看到这个。
	现在大家看这些data set应该比之前有一些感触了重点怎么看呢？就首先你会看到它的instruction的长度是不长的。所以这个数据集如果咱们要拿来做自己的事情，你会发现如果你的场景是一堆的长文本指令，这个肯定是的帮助没有那么大的。因为它的这个分布来看，就是10到20个就不超过30个token，作为它的这个instruction。那它的output也是一个不算特别长的一个输出结果，然后参考的输入也不多，更多的是像咱们讲的这个zero shot这样的一种prompt的方式。
	第二个是data bricks，这个公司做了一个15K的数据集，它会比阿帕卡相对好一点，因为它是人写的，它比较符合咱们SFT的要求。Data bricks做的这个大力15K是应该是去年3 4月份。我记得Q1到Q2之间发布的一个版本，好像还有在做迭代。这个版本其实是什么呢？就是他们按照OpenAI的，InstructGPT它不是做了八个类别吗？他按照这个八个类别人手工写了15K的训练集，这个训练集是很友好的。
	协议开放出来的，使用的这个CC，然后attribution share c这样的一个协议。这个协议是一个数据协议，不是一个代码协议。这个协议的好处是说你用这个数据集去做的发表的论文这些学术成果，或者说你创业做了一个公司，然后这个公司的模型是用的这个数据集训练的话都是没有问题的。他的协议是许可你去做这两种事情的那它的优点是用人来写的这个数据集。
	其实大家如果经历过计算机视觉的话就会发现，首先这些都是论文，它不是单独发了一个数据集。然后我们知道之前的image net也是一个数据集，但是发了论文。然后后续也有很多的公司去做数据集，发论文，包括我也做过这个事情，就做数据集。这个发论文是一个非常讨巧的一种学术成果的发表方式，因为会有很多人引用你，这样他上个月就有26000多人下载了这个数据集。这个数据集正如刚刚讲到的，它的类别标签是按照InstructGPT的这个分类方式，有8种类别八个category。包括这个封闭式的QA分类，开放式的问答，信息抽取等等，包括什么头脑风暴然后它的这个质量会比刚刚看到这个阿帕卡会高一些。但是这两个数据集都不支持中文的，都是英文的。
	然后第三个是我们还挺有意思的，我就是这个sit，它是来源于一个非营利性的组织，叫做LALN。这个组织他们做的这个数据集其实不是做自然语言出名的，应该是stable diffusion这个纹身图的模型。当时是有很多的数据是用了LLN他们的发布的一些数据，因为这个事情得到了很多关注之后，包括GPT3用的一些数据也都他们去整理发布的。然后他们开始聚焦，怎么样让ChatGPT这样的能力不只是被OpenAI所拥有。所以他们做了一个叫open assistant的一个项目，这个项目也在发布对应的数据集，就是这个OASST two。
	这个数据集很有意思，它跟前面都不同。前面的一个是由模型生成的这个数据，一个是由人类的专家或者说人类的从业者生成的数据。他其实是把一些真实的人的聊天记录，通过这个open assistant采集下来，并且是获得了大家的许可，把人跟人的聊天这个聊天记录变成了一个数据集，然后这个OASST two是有差不多应该是对129K13000名人类的真实的聊天数据。但是脱敏的。然后里面有这个信息数，就所谓的你来我往的聊天记录，这个messages，包括一些标签，像我们看到的这个就是OASST two的一个，典型的数据集的一个结构你在里面能看到有message的信息，包括这条message的用户。因为还有可能有多条的信息，它里面还有一些类别标签，比如说有些是垃圾信息等等，有些是一些不太好的言论，谣言等等。
	这个其实对于如果咱们要去构造对话型的这种模型，其实非常有帮助的。就包括我们的RHF做完他如果要提升他的对话能力，像我们刚刚看那个OpenAI的那个发展，从上面的基建模基础的这个模型到下面开始做这个instruction turing，做IHF，最后还要加上其实就类似于OASST two这个数据集要做的事情。然后open I内部肯定也是在强化这个对话属性的，这个是不用去质疑了，一定在做这个对话方面的一些增强。然后我们能看得到现在国内的像GOM的几十亿的模型，对话能力也还是比较弱的。但他还没有开我印象中他还没有开放这个对话的训练，就类似于这个OSST two的这种数据集的训练，好像是啊。
	接着再看第三阶段，第二阶段核心其实是给大家讲，为了把这个要做的下游任务，以一种特定的数据集的方式交给大模型。其实就相当于让大模型看图说话。上补习班，就是让他知道什么样的输入，应该有什么样的输出去做响应。其实这就是SFT这个阶段的核心。
	然后我们用的这些不同的技术手段，其实本质上都算是SFT，或者你要叫指令微调也可以，一个新的名字。但是第三阶段算是一个全新的门类，或者说强化学习到自然语言模型的一个成功的嫁接。然后这个嫁接就实现了很玄学的一种说法，叫做人类价值观的一个对齐。
	那这个具体怎么对齐呢？我们得给大家细聊一聊了。这个算是今天的主菜，就怎么样去实现对齐呢？首先它的目标跟前面两个都不同，前面两个不管怎么样去改这个数据集，核心它还是一个自回归的模型。就你给我一个前文，我给你生成一个下文。无非是说第一阶段我就是学了这么多的看这个报纸，然后书籍，那我大概就猜猜你这个下面是什么。
	第二个阶段的SFT之后，我知道我有一些特定的意图了。就你说的这个上文里面不是没有信息的，是有这个指令的。然后这些指令我因为学过，我就知道这个就像这个新人入职一样。
	我知道这样的公司里面的这种要求是要什么样的一个格式的输出，它有这方面的一些提升，所以这个两个阶段他的训练的这个模型最终生成的这个方式是一样的，只是说训练的这个数据有不同。但第三个阶段有明显的区别，第三个阶段的目标就是说我的这个模型已经能够很好的去响应我的这个任务指令了。但是他说话的方式不够友好，就像之前我给大家做的一个例子。
	第二个阶段是新人培训。新人培训来了一批可能100个新人入职，三个月的试用期，在SFT这个阶段就是三个月的试用期，教你在公司有什么样的规章制度，然后你有哪些任务要去做。三个月下来可能筛选出来了一批人。比如说筛选出来了十个人，然后这十个人里面肯定就会完全一样的十个人。有的人可能是说我生成的结果其实是挺好的，但是领导就是不爱听，因为他有些说话方式可能比较直接，领导不爱听，但他的结果是很好的那还有一类是说话特别漂亮，都是场面话。但是其实它的内容也言之无物。
	大家想象一下，其实很多时候，现在的ChatGPT在刚刚出现的时候就是后者。你感觉他说了一堆话都很漂亮，都是好像结构上还挺工整的，但是好像他又什么都没说。这个其实是是IHF带来的一些结果，就他他把这个模型变得更像人说话的。你叫八股文也好，或者人说这个车轱辘话也好，他他做了这样的一个强化学习的一个训练，导致了这样的一个结果。在他没有做训练之前，其实instruct的GPT更像前者。就是我是一个生成结果很好的一个模型，但是我说话的方式不是很友好，让你觉得我好像不太行。所以其实价值观对其我用一个我自己的理解，就是让大模型的生成结果更像人话，更像人说出来的话。
	在这个论文里面，其实他做了一个对比，也是咱们比较符合直觉的一个对比。首先我们看到最原始的GPT3，跟GPT3的prompt去做对比，这个在GPT3的论文里面就已经对比过了。那么单纯的prompt和做了有监督微调，做了supervise的fine tuning的比较拉出来的区别也是很明显的。在这幅图里面大家要去理解supervise的fine tuning和我们之前做过的很多PEFT是等价的，只是他们在不同的场景取了不同的名字而已。
	大家得把这个技术本质给抓住，就GPT3去做了Laura也好，去做了其他的这个adapter的技术也好。其实它本质上和我们下面的这个GPT3的prompting肯定都是会拉开差距的。因为它更面向下游任务了，面向人类的特定任务了。就包括OpenAI拿了用户数据，分出来了八类任务，然后还有不同权重一样。那么在上面那一类就是GPT3加上了SFT，再加上了IHF的一个得分情况。显然这个就会更好。
	因为它生成的很多结果是人想听的，尤其是有很多有很多还是举人的例子比较好理解。有很多时候其实我们并就是这十个人他的输出，我们有很多时候并不是说非最牛逼，但是脾气不好的人不可。大部分工作其实是平均值，就是可能八个人都能达到这个平均值。
	所以从这个概率分布上大家就能理解了，所以大部分时候你说的更像人的话，会得到更多的分数，而不是你每次都特立独行。但是你给出正确答案会得到更多的分数。我不知道这个比喻大家能不能理解，其实是一个挺符合这个真实场景的一个数据的。
	但这里要训练它也有很大的难点，也是现在RHF被诟病的两个地方。一个就是人类的反馈，这个训练的核心是人类的反馈。因为中间要额外训练的这个奖励模型reward model是人来打标注的人来打分的。这个人类的标注它就是自带偏见的。简单来说这个大语言模型它要去训练一个奖励模型，那就是要不断生成数据，然后人去给他打分。那我不可能只找一个人，这个人只找一个人就完了，对吧？就单机故障了。这个人要是不行，那肯定就整个的数据就废了。
	那你这个时候可能会找1000个人去众包来打这个标注，实际可能找不到这么多。比如说找了100个人来打这个标注，这100个人的价值观他也不是统一的，100个人的价值观不统一，那你说这个事情怎么搞呢？对吧？就是他们的价值观都是不统一的那训练就无法收敛了。那这个时候比较聪明的计算机科学家就尽可能把这个排序就打分的这个事情给他去做量化。然后在训练过程当中，去通过设置这个惩罚项和他的这个优化方法，尽可能去降低它的风险，但无法完全的去解决掉。
	第二个就是因为是人在参与反馈，所以它的训练效率还是比较低的。就因为人得介入其中，要对AI的结果去做反馈。然后一个经验值就是在这个过程当中，需要的训练数据，大概就是50K的一个样本来做这个训练，这个功能其实SFT作为RHF的第一阶段，我们待会儿也能看到它有一些很有趣的地方。
	开始我今天会不断的把这个IHF的三个阶段给大家去做灌输和植入，希望大家能把这个概念的魔障给它破除掉。其实它并不是一个很复杂的技术，抓住一些本质就能get到它能够起作用的方法。这个是RHF很经典的论文当中的图，就是我们阶段三是用基于人类反馈的强化学习来进一步训练这个语言模型。然后这个阶段三里面又会分成三个步骤，然后这三个步骤就分别是我们现在看到的，一个是有一个SFT对吧？就是我们前面说的阶段二会用有监督微调去训练一个语言模型。但是在RHF里面，当然也需要去做一个有监督的微调去训练我们的语言模型。这个为什么我们待会儿再给大家讲，里面有很多的细节。SFT是我们要训练的语言模型，也是我们最终要带走的这个语言模型。
	中间这一溜是一个叫做奖励的模型，就我们的reward model，这个是强化学习必须要的一个很重要的一个部件。这个奖励模型是由人来密切参与的，相当于做了一堆评委，对每一个新来的十个人他都会去打个分，然后最后几个评委之间再加权平均一下，最后得出一个结论。第三步其实是把咱们的这里涉及到了一个不得不提的一个新的概念，后面我们也会展开讲。
	就是这个PPO，GPU是一种用来优化强化学习，优化这个词，就optimize个词，我们之前也用过，就是在训练这个transformers s和PFT的时候用过这个optimistic像adam这些基于梯度下降的优化器，都是用来优化这个模型训练过程，反向传播过程当中，去优化这个模型的。相当于就是去迭代这个梯度的，去更新这个参数的。那么PPO就是对应的这个概念，在强化学习里面有一个类似的优化器，叫做这个PPU。然后它是用来优化我们的这个模型的那具体怎么优化的呢？其实它会拿到这个奖励模型的一个值，你可以叫奖励值也行，然后去算出一个一个RK，然后这个RK会最终你可以理解成这个RK就类似于我们算出来这个损失值，但没有这么简单，还会再做一步计算，最后就会叠加回到我们的这个模型参数上。我们后面会去展开看啊。
	那我们就接着展开介绍这三个步骤，就大家有一个大的三阶段的理解预训练的这个语言模型，然后是啥也不会的，就只是看懂了人类的知识。SFT是让这个语言模型有了一定响应人类任务的能力，然后第三阶段才是IHF，那么第二阶段对于有的这个技术团队，他可能会直接就。省略掉，直接会做到RHF的一阶段里。
	但这个具体实际上怎么选择呢？我们看这幅图就比较好理解了，RHF是咱们用强化学习来训练大语言模型的一种技术，很复杂，有很多个阶段，里面有很多个步骤。讲叫做步骤，有三个步骤。然后这三个步骤，第一个步骤就是我们要去预训练一个语言模型，用SFT。因为这个在这里，如果大家有疑问为什么要用SFT的话，你就可以理解成因为前面这个阶段一样，我们都已经讲过了，他需要先有点这个基本的底子，他他是会认字的，他才能开始去做下面的IHF。反正再怎么训练也没用。然后那前面的那些语料肯定都用完了，剩下的自然就是用SFT了。
	然后第二个步骤其实是要训练一个奖励模型，就我们的这个reward model，就这里看到的这个红色的这个部分。第三个其实就是用强化学习的方式来微调我们的语言模型。就是咱们看到这条线里面有一个鼠标，有一个蓝色的框。看这幅图，其实我们也在不断的讲这个三个步骤，就能理解了。什么个意思呢？就是我们看到在这幅图里面，其实强化学习的这个作用主要体现在这个reward model和这个蓝色的reinforcement learning update。用CPU来迭代它的这个部分。
	我们先忘掉它的话就看前面这一部分其实是好理解的。他其实一开始在这个RHF里面就有两个语言模型，这个绿色的部分就我们看到这个绿色的部分，它就是一个初始化的语言模型。你可以理解成前面不是有两个阶段，前面那两个阶段的最终产物就在这个绿色的模型里面，相当于这就是为什么阶段二的必要性。如果你不做阶段2，那你在整个RHF里面的这个绿色的基础的初始化的语言模型就很弱，那他就很难做到去在微调在这个强化学习微调过程当中做到一个你可以叫teacher也好。
	这个取决于这个不同的流派里面对这个模型的一个称呼它的核心逻辑是说，无论人类怎么样去对它打分，因为有左边这个绿色的语言模型的存在，所以它不会太离谱，它不会生成的结果太离谱，不会因为人的干预，人老是去给它乱排序而让它太离谱，为什么呢？是因为这个初始化的语言模型是不会去动的，就相当于经过前面两个阶段，我有一个七八十分的模型了。就相当于这个instruct的GPT，它的能力已经很强了。他是一个输出能力很强，但是说话很有棱角的一个同学，大家这么理解。右边这个同样的language model使用强化学习策略来改的，就是想要把这个有棱角的同学变得能力，不要去有钱省的情况下，然后跟领导能够好好聊天。
	我不知道这么描述大家能不能理解，因为这个绿色的和这个twin的language model就是用RHF去微调的这个语言模型。他们一开始是一样的，就是三个月试用期过了，选出了一个能力最强的同学对它进行RLHF。然后这个能力最强的这个绿色的，它的价值在这个F的这个训练过程的价值，就是为了让他磨去棱角的同时不要丢失能力。那怎么样能够不丢失能力呢？
	在经典的强化学习和深度学习的去比较这个输出结果的过程当中，有一个叫KL散度的东西这个KL散度我不展开讲，你可以简单理解成就是它是用来评估这两个语言模型生成结果之间的差距。就跟我们去算loss有点类似的，你就可以理解成是一个高阶版本的交叉熵，它是用来去评价这两个语言模型的生成的这个结果差异度大不大。所以这个惩罚项存在就会使得即使他再怎么去响应人的价值观，响应这些标注人员的打分和排序，但它生成结果的质量也不会差的太远。我不知道这个有没有要说清楚。
	好，那我们去细看一下这几个。第一个步骤就是我们的使用SFT来微调我们的这个语言模型。这里就提到了我们开始提到的阶段二，如果你是一个非常叫什么私有化的场景或者领域垂直的场景，那你可能就只能在这个阶段的步骤一才能去实现SFT了。就相当于你在公有数据上不论怎么去做，有监督微调，都达不到你要的这个initial language model的状态。它的训练方法是完全一样的，就是所有的SFT它的训练方式都是给一个process，给一个text的这个pair，就跟我们做的所有的微调是一样的，就是你给一个提示词，给一个用户输给一个用户期待的输出结果。这个都叫做super rise，都叫做有监督的，有标签的。这个数据集具体它的格式是什么样，取决于你最终想要让它用的场景是什么样。
	我们再讲一下开始说过的InstructGPT，它就是分了八个类别，相当于8种任务。然后这八种任务有比例。然后如果你的场景是没有8种任务，只有一种任务，那你就使劲的往你那个任务上去怼。这个SFT是比较好的一种做法。
	那么显然实际情况下，像OpenAI，包括anthropic和deep mind这些前三个大厂前五个，还有没啥没讲。这几个前五个大厂他们要做的目标是整个世界的通用大模型。所以他们不太会说在这个阶段就啥也不干，或者说在这个阶段就让他只能响应某一种指令了。所以我们看到这三家公司他们的使用的这个初始化的语言模型各不相同。像OpenAI它最早去做RHF的时候，用的就是在GPT3的小版本上微调出来的InstructGPT，然后anthropic试过各种版本的，deep mind用了自己的这个gold film。其实这一块你可以理解成没有任何必须要做成什么样的级别的那就必须要做成一个千亿的，或者必须要做成一个几十的，这里是没有的，因为它本身就是一个在最终如果你要到ChatGPT的那个步骤的话，这里也都只是一个中间结果。就是为什么值1000亿？因为过程当中有太多的训练细。
	那么第一个阶段的核心就是让我们的人类构造的这些数据给到大语言模型之后，让这个初始的绿色的大语言模型能作为我们RHF的这个teacher，作为他的一个参考，不要让他迷失自己，这个我相信应该大家能理解了。第二个阶段就比较有趣了，就我们已经在这个一个初已已经在一个初始化的语言模型上又去做了有监督的微调了。然后得到了一个输出成果很好的语言模型。只不过它的输出的格式，输出的语气也好，输出的这个排版也好，不符合人的一些要求。那么我们就把人给引入进来，这个是在以前都没有的。
	为什么RHF是一套独立的线，跟咱们讲的其他的微调都不一样？就是因为你就算SFT，你讲束花，它跟PFT跟na本质上还是一回事儿。因为它的核心是用人类标注的prints加上text的这种这个就相当于指令加生成结果的这个对儿这种有监督的数据去做训练，本质上是这么一回事儿。但是RHF是说我需要你做这一步骤，但是我还需要让你的能力不丢失的情况下去响应人。怎么响应？就额外做了一个模型，叫做奖励的模型。所以这个也是它区分于原来的这些训练方法的一个很重要的一个部分。
	这个奖励的模型它接收的是什么呢？就跟咱们的第一阶段的SFT不一样了。大家记得SFT接收的是prompt text，那么reward model接收的是同样有咱们刚刚前面的这一部分，额外加上了一个reward，就是一个评分，就相当于咱们现在有一个这个我还讲一个什么例子，就是对这篇这个文章做出一个评价。
	一开始我们在训练集里面可能就是有各种各样的数据集。然后在语言模型生成的结果上，他可能有输出了一个最高进度的，然后这个最高的logo解码出来的有一个sample，这个是它根据你实际的这个输入，我生成了一个simple。但是我现在不只是让你输出一个simple，我可能把前五或者前十前十的这个simple都输出出来，我印象当中他好像最多就到9个，4个到9个，大概这么一个数字输出这么多simple，相当于top ten的这个simple让语言模型都输出出来。
	这个是语言模型输出的那还有一个叫什么？叫一个分数。那这个分数怎么来的呢？这个分数是人先去打的，就是让我们的标注人员去给生成的simple去打分，就相当于现在我们在ChatGPT的页面上做的事情一样。只不过它只输出了两个，你是选择这个赞还是这个dislike，就是有两个标签。但是在实际的IHF去训练这个奖励模型的过程当中，其实是我们的标准演员会拿到应该是将近十个不同的simple，他会去打分。然后这个打分的系统有一套叫做ELO的rating system，这个也是一个很。这个需要再进一步深入讲展开的概念，大家可以先把它当成黑盒子。简单一点就是通过这个排名，就相当于他只是去排名。
	然后这个排名可以换算成一个分数，换算成一个rewards，然后这个rewards最终给到了RM这么一个模型，也是一个神经网络，给到这个RM这么一个模型。然后这个IM这个模型就是通过这个方式在训练，相当于它让我们的一堆的标注人员针对大语言模型的结果，让它不断的去生成结果。然后排序，排完序给到这个IM，IM就知道这样的输入，针对这样的一个顺序，是这样的一个分数，不断的经历这个过程就训练出来了一个RM，那么RM是这样被训练出来的，我们不知道这个大家理解没有，就它是一个额外新训练出来的模型，然后这个模型的输入就是simple和reward这样的一个pair。然后这样训练完之后，当我们第三个步骤要去用它的时候，RM就已经比较稳定了。因为那个时候要用它的时候，它就是给我一个simple，它吐出一个rewards。这个大家应该能看得到，这就是他第三阶段要干的事儿了。
	就是我们又回到这个图了，再回到这个图的时候，我们就能get到。第一，我们的这个绿色的初始化的语言模型，一开始在这个RHF里面是稳定的过了试用期的最佳学生。这个tone的language model，灰色的这个是我们最终要带走的这个语言模型，它是在被不断的微调，怎么微调的？
	通过第三个步骤，用PPO来进行微调的。然后这个PPO又是怎么去得到它，应该怎么调整它的参数呢？因为我们已经微调过大模型了，我们知道大模型就是一堆的权重的参数。而这些参数无非就是你告诉我一个这个值，我去把它改掉，它那个值怎么算出来的？
	就涉及到了这里的上一个步骤的reward model，就是我们的奖励模型。奖励模型又怎么样能够得到一个这个值呢？就是我们在前一个阶段知道的。它会得到很多的simple，得到很多的输入的就是我们大模型给它输入的生成的结果，它会针对这个simple去打一个rewards，这个rewards就是它下面的这个R西塔，就是这个rewards西塔。然后这个real words ta它还会带上一个K2闪度，相当于他不是直接就对我们的结果去打了一个分数，他还会再额外加一个惩罚项，这个惩罚项就是我开始讲到的我们的原始的没有经过任何修改的语言模型和不断经过强化学习调整的语言模型之间的差异。这个差异非常的重要，如果这个差异被干掉了，会出现一个什么事情呢？
	你可以想象一下，首先它会变得无限的去接近这对标注人员的喜好。因为这个是RM决定的，IM只有这一个惩罚项，但还会出现一个问题，就是它有可能会变成一个人很喜欢，但是言之无物，甚至说说出来的话都是不连贯的这么一个模型。这个也是有可能的。所以整个大的架构，我不知道有没有给大家表达清楚。所以其实你看整个RHF里面的核心，你把它拆解起来之后就好理解了。
	两个模型都要被训练，一个是灰色的大语言模型，一个是红色的奖励模型。然后这两个模型同时在被调整，它不是说我把步骤一训练好了，再把步骤二训练好了，然后我就开始训练步骤三了。其实不是的，它是像一个DNA螺旋一样的，在不断的旋转，在不断的去做迭代。然后在这个过程当中，通过初始的语言模型跟不断去调整的语言模型之间的计算，可以让它保证生成的质量不会太差。然后通过reward模型可以让它保证生成的结果尽可能去靠近这个人的这个价值观，就是人想要的这个排序。
	在这个过程当中，他也不是去全量调整所有的这个模型权重的。正如我们这儿看到的，它冻结了很多的参数，只调整了一部分的参数，所以整个微调的过程就是按照这样的一个方式在做调整。然后我们知道PPO是一种强化学习的微调方法，然后他也有很长的时间了，之前都是在一些游戏领域去做的微调，但PPU也不是说IHF里面不可替代的一个东西。就像我们现在用其他的PFT的这个方法去微调语言模型的时候，其中有一个很重要的参数叫做optimized。那个时候还有同学问我为什么要用adam？能不能用其他的opt matter？
	这里也是一样，只是说PPO它有一些特点，PPU它叫近端策略优化，就是proxy policy optimization，它也是一种优化方法。而这个优化方法它有它的一些特性，然后也有一些别的优化方法也可以用来微调。比如说google deep mind，他的这个2800亿的这个gover的大模型上面去做强化学习的微调的时候，用的是一个叫a to c的方法在优化梯度。但无论如何，其实核心就是这两个。当你用不同的优化方法的时候，你在这个RC塔就相当于拿了K这个两个语言模型的差异度加上人类的评分之后，再去做的一个损失函数的一个计算，区别就在这儿，但他们的本质的大的框架逻辑是不变的。
	然后还有一些新的一些技术在讲，就是怎么样能够把reward model建设成本降低。我用不要用人来做反馈了，用AI来做反馈。因为用人来做反馈效率又低，人还有偏见，所以这一块有很多的咱们去搜的话，有各种feedback相关的论文，有的是说用AI能够消除一些人的偏见和一些叫做helpfully less，就是毫无帮助的，毫无建设性意见的排序。
	还有一些就直接拿AI这个模型来做反馈，就相当于让AI来排序，大家把这个技术拆解的一步一步拆解成组件之后，你就能够想象哪些小的部分，小的模块会有一些创新点。其实做科研发论文都是这样的。你拆解之后你就知道哪个部分有目前的缺陷，有可能可以怎样去改造它，拿到一些更好的结果。刚刚我们说的那些在RM这个第二个步骤里面做的改进都是这样。好，那么到这儿为止看看大家有没有什么问题，我来做一些这个QA。接点水。
	我来看一下大家问题。
	第一个同学问GPT3加上SFT加上IHF是红色橙色两条线，这个应该是开始有一页。是这一页是吧？这个同学是指的，我不知道是不是确定是指的这一页。就是有个同学问这个不同的两条线，这个同学。这个是个好问题，我还没有去细细看啊。我回头把那个论文的这个图解图示再细读一下，我回复一下这个同学的问题。对我估计他应该是这个地方应该是要么是不同的数据集，要么是不同的训练方法。这个我查完之后，我回复一下这个同学的问题，那更多的应该是关于这个RHF里面的一些问题。我再细看一下，就是lora的模型蒸馏其实只是对Laurena模型参数的初始化，lara不会使得模型变小。
	一个同学是这样问的，首先这个Laura其实严格意义上不特不是特别算是这个模型蒸馏。因为模型蒸馏这个词是模型量化这个技术门类下面的一个词，就相当于模型蒸馏像什么呢？就是像我们之前讲的那几个量化技术，比如说g GPT q或者AWQ这一类技术。它的核心是说我的模型几乎不怎么变了，然后我要把模型变小，然后算是蒸馏，这个一般会叫做模型蒸馏。而Laura他其实是想说我要去训练这个大元模型，这个训练的成本可能比较高。然后举个简单例子，比如说QLA，它就是一个典型的可以降低训练的时候资源开销的一种训练方法。
	Q ora它可以让我们的原始的模型以一个非常低的精度加载进来。然后同时让我们的微调过程使用的就lora的这个技术，它本身用的精度还是比较高的，用的是这个DF16，还可以用这个32。所以它是一个让你能够以低成本来微调大模型的方法。
	然后这个Laura本身它是不太会把模型变小的，但是看你怎么理解，就比如说这个模型是在基础模型，在int 4的精度上面加载出来之后去做的微调，那么你实际要去使用它的时候，你也可以用四的精度来加载这个原始的模型，这个是没问题的。但如果从这个视角来看，其实模型是变小了。因为你如果在原始的模型上用int 4的精度直接下载进来，不用loa肯定用不了。是因为你用了这个lara造了一个1‰甚至万分之几的权重的一个小模型。然后他让你既用到了原来基础模型的低精度的这些产权重，又在下游去做了这个适配。所以你要从这个视角来看，它其实也让模型变小了。
	我们的这个解释有没有说清楚？推理的时候需要加载IM吗？推理的时候应该不需要加载IM了。但是如果你是像OpenAI一样，它在不断的采集用户反馈，我觉得是有可能的。你可以理解成这个其实取决于你的工程团队怎么去部署这一整套模型了。我可以这么表述，就是在真正推理的时候，肯定这个大语言模型是直接就生成了结果。但是也不排除它同时实时的有一些不是线上的模型在走这套流程，然后又把生成的这些结果给到他的标注人员，他的标注团队可能一直都在打标注，一直都在不断的迭代。这个RM我不知道这个表述有没有说清楚，但如果你是说你作为这个终端用户，那你感受不到的，但是它内部要去部署它的时候，它是可以走这套流程的，是两种作用。
	灰色的模型是哪来的？这个同学我再讲一下，灰色的模型就是这个绿色的模型。但是当他第一次被更新了模型权重之后，它就变灰了。我不知道这个有没有说清楚，就是一开始这两个语言模型是一样的。就是我我做的这个比喻就是十个通过试用期的员工，绿色的员工是能力最强脾气不好的员工。然后我们做RHF，就是说我要从一个100个人里面选出一个能力最强的，同时还能够符合公司文化的那这个IHF就是在给他调整，让他适应公司文化，又不去丢掉他的能力。所以它俩一开始是一个，但通过RHF之后，它变成了最后的这个微调之后的版本的模型。
	训练强化学习所需的得分值，必须是需要标注人员进行打分的，这个我刚刚也提过了。第二个阶段本质上是为了训练出一个RM这个模型，二阶段的核心是为了训练出一个奖励模型，这个是二阶段二步骤二要干的事儿。然后要训练这个奖励模型就需要有review的先验知识，需要有这些分数，或者说需要有这些排序。这个排序可以是人，也可以是AI，这个是都可以的。只不过目前来看，我们都知道，你如果能找到高质量的标注人员，他们做出来的这些数据肯定是事半功倍的。因为有时候AI的这个排序和打分是很不靠谱的，尤其是你现在用的这个打分排序的AI本身如果还不是很强的话，就相当于好好的老师教出来的这个学生，怎么的也都还不错，甚至还可以超越老师。但是你两个本身就很一般的学生在给你做辅导，那他可能讲的都是错的，他给你排的序都有问题，这个是有可能的那他就会影响你的IM，就会就造成一些很不必要的一些浪费。
	三阶段训练用的是同一套数据集吗？肯定不是。兄弟这完全没听。这怎么回事？三个阶段怎么可能是同一套数据集呢？第一个阶段就是能给我多少数据就给我多少数据，对吧？海量的数据去喂。
	第二个阶段就是让这个还是这个逻辑，就是你如果非要举例子的话，第一个阶段就是K12教育加上大学的教育，第二个阶段就是进入工作岗位的试用期，第三个这个阶段RHF这个阶段就是要去适应公司文化。我不知道这么讲表述的清不清楚，三个阶段要做的事情截然不同，但是你说有没有可能没有阶段二就直接来适应公司文化呢？也有没有怎么读过书的同学，因为比如说上一辈人，他高考都还没怎么恢复，是吧？大部分人都是没上过大学的那能不能进入工作岗位？也可以，只是适应的成本不一样。
	然后SFT和其他微调方式的目的可以是一致的吗？同学这个问题你可以再展开问一下。我没有特别理解，就是和其他微调方式的目的可以是一致的吗？是什么意思？
	一些领域的题库可以用来做SFT。我刚刚有列，SFT有一些做的还不错的，大家可以去关注一下，就这一页。对，就这一页题库本身不叫做SFT，题库本身不太适合做SFT。因为SFT的核心应该是给他的指令，对吧？就是我相信题库大模型在预训练也看过不少了。但是核心是你这个题库最终你是要做成一个，比如说做成一个阅卷的一个助手，做成一个自动答题的一个机器人还是什么，这个就跟你最终要用什么指令有关系了。
	有传言说智谱用了很多人工进行手动训练，雇佣了很多人工去手动优化模型的回复。对，如果你们在讲手工训练，那肯定是在训练那个RM，训练那个奖励模型。就这一步肯定是纯考验人力物力投入的，你看最大的这个human story，就是人类打分这一部分是非常重要的，它也是跟其他的训练方法差异化最大的一个地方，你可以理解成这一部分做的好是能明显拉出差距来的。就相当于大家都要考试，有的同学他就是有名师辅导班，那它就是会涨分。
	为什么奖励模型需要更新参数？这个同学问的是在哪个步骤？在步骤二它肯定需要更新参数。因为奖励模型是被训练出来的，但是在第三个阶段奖励模型是不会变的。第三个步骤在这个步骤里奖励模型是不会变的，只有咱们的这个twin的language model，就是个语言模型会去调。
	现在模型反而越来越小型化，小模型训练会不会简单点？是这样的，还是有多大能力干多大事儿。为什么这个RHF它不会变成一个非常所有人都会去用的技术呢？就是因为它的启动，它的成本太高了。
	举个简单的例子，就大家现在都工作了，IHF这套方法就是需要你立项，立项至少需要投入10到20个人。那你现在有没有管10到20个人？如果你没有管10到20个人，你想去立项做这个事情，不就有一个天然的障碍，对吧？除非你已经管了这么多人，你拥有这么多的公司资源和项目资源，你才能用这个方法去干事儿。
	但是当你消耗使用调度这些资源的时候，对你的预期也不一样。就相当于你在用RHF的时候，本质上其实是希望你能够获得更多的产出。就是你变更牛逼。但是实际上大家都不不是每个人都能管这么多人，对吧？所以我们才会说先把你手上的这个几十亿的模型去微调到，就相当于你把你自己的这个阐述的效率变高了，然后才会去说一步一步去往后走。
	但现在很多同学，我理解应该是SFT都还没有做的特别熟练，CFT和PFT本质是一回事儿，我不知道大家理不理解，就不要被这个词给绕进去了。它是维度不一样，就是SFT强调的是这个监督，强调的是这个人的监督信息。PFT强调的是说我不用改全量的参数，但它俩本质上是有重叠的。甚至在很多的语境下面，在语言模型的很多语境下面，它俩是一回事儿，我不知道这个大家能不能get到。
	不是很明白奖励模型是在什么阶段使用的，这个阶段是指什么？这个同学在再再展开问一问。对我理解in IT model输出给奖励模型的simple，in IT model输出给奖励模型的simple是相同的问题，不同的是什么意思？这个in IT model输出给奖励模型的samples是相同的问题，不同的随机的prompt会不会输入给奖励模型的simple没啥差别。
	没太get到这个同学的意思，你能举个例子吗？子弹的代码我这两天在跑，坦白来说这个在T46GB上还没跑通，就是整个流程要的资源挺多的。你可以想象一下，你要跑IHF，你是不是至少要部署三个模型。大家去捋一下，这一套跑起来，你是不是至少要部署三个模型？然后这两个语言模型至少都得部署起来，而且消耗资源还挺多的对吧？因为它俩是完全对等的两个模型。然后reward model可能还稍微小一点，但是你要去把这个beat size拉大的话，也是很很消耗资源的对。
	然后我现在在在几亿的这个模型上在做实验，但目前还跑不通那个transformer的这个TRL。后面我会再尝试一下。如果能够在16GB上跑通的话，能够把这个代码普及上去。但比较悬，我看到的大量的参考都是要至少4 50GB才能跑起来这个流程，我们尽快我会应该下周把这个实验做完，争取能够把代码传上去。
	SFT的数据量和后面的im。工作中好落地，这个工作中不需要你们很快落地的。就是我我不建议大家把这个当成最重要的要学的东西，最重要要学的东西还是前面的高效微调，那个是最实用的技术。然后IHF这个玩意儿它太消耗数据的准备和标注了。对，如果你是现在已经管了这个团队，比如说咱们现在同学当中有人已经带了算法团队了，带了七八个人，十来个人，可以私信我，我们可以再来根据你的具体场景，我们再来深入交流。但如果你现在只是想学一下这个技术，我建议把这个高效微调，把这个SFT好好研究。这个是很实在的，包括前面也给了好几页的这个数据集，大家可以去实际跑一跑。
	还有同学问了PPO和红色框的reward model之间的关系是如何联系起来的这不是就带进去的，就是这里有一个目标函数，这个reinforce就PPO它会有一个函数，能够把你的所有的模型参数带进去算，就跟标准的做这个learning的这个套路是一模一样的。上面的那个PPO就是一个典型的去做模型权重更新的一个梯度更新的这么一个公式。阶段二。阶段二。阶段二和阶段三是不是还需要交互奖励？模型和SFT都是越来越强，互相进步。是啊，你没咱们看到OpenAI每周都在更新模型，就是在做这事儿，他他不就是在这么迭代，所以我说线上的我们的这些输入也会有标注人员去做标注的，甚至咱们对他的一些用户行为都会影响他的打分的对。
	Loss中来自奖励模型的部分，如何平衡两个任务？什么叫如何平衡两个任务？这个我没看懂，这个同学奖励模型是否可以通用？目前有开放的开源可用的吗？同学是这样的，就是奖励模型它不存在通用。
	对，奖励模型它不通用。不重叠的部分是什么？不重叠的部分就挺多了。因为有监督的微调，他可以是很这个叫什么，他可以是不高效的，对吧？他可以去全量的去调，他也没有说他必须得只调一部分。
	Ppo更新的就是语言模型当中的一部分的参数。对，PPO更新的就是我们现在灰色，看到没有？这个灰色部分的一部分的这个参数，就top player上面的一部分的参数。
	SFT没有参数，SFT是一种微调方法，它更新的是语言模型，那同学。
	还有个问题吗？开始有的同学问什么奖励模型的部分和来自KL散度的惩罚要如何平衡两个任务？你是不是想说他这两个值在这个PPU里面怎么样能保证互相不影响吧？我理解一下这个同学的问题，如果是这个意思的话，这个就是PPU本身的特性带来的。PPO它是一种按照这个说法，应该是他的他在训练过程当中，他的梯度会约束，他你可以简单理解成这个PPU这个算法本身带进去的这个惩罚项和rewards这两这个东西。它其实本身能够保证训练过程当中梯度更新是比较稳定的，就不会出现那种一下给它刷掉，通过这样的方式去保证的。但是也没有说他就最高效，或者说它就是最稳定的对，我不知道这样能不能表达清楚。
	Reward模型是模型太多了。这个同学你说清楚什么？Reward模型是哪个模型的什么优化目标之一吗？
	最后再回答一个问题，让我们看一下要实现RHF现阶段我们有哪些工具可以用，但是我们没有直接的数据可以用，这个数据就是这个reword模型的打分，可能是需要去研究有哪些数据可用的对，看大家还有什么问题吗？最后再找一个再回答一个。
	奖励模型是不是可以跟传统深度学习当中的，这个应该是传统强化学习。深度学习里面没有这些q learning ADMM这些方法。对，然后你说的PPO其实就是强化学习当中的一种优化方法。这些概念是这样的，还是理解它的作用是什么？就是所有的方法都有它的作用。
	PPO它本身在做这个迭代过程当中，它其实是跟我们的reward的是组成一组的。它本身是一种策略学习方法，就像刚刚这个同学其实是有一些强化学习经验的。小孩学习它本身是一个什么逻辑？就是核心是我要造一个environment，然后我本身是在学习一种策略，就是策略学习的方法。然后这个策略学习方法就是因为有环境，然后我会不断的做出这个action。这个action在咱们的这个自然语言模型的HF里面，就是我的prompt和我生成的这个结果，包括这个分数，他其实在这个过程当中，他学习出来的这个策略就是能够去比较稳定的去更新这个语言模型本身。这个是它引入进来的一个大的逻辑。
	然后这个奖励模型在里面，其实他承担的这个价值前面也讲了很多遍了，这个其实是就不再过多的去提了。然后这个同学说的互相替换，这个你就要去简单理解一下。就刚刚我说的PPO像google也会用a to c这些都是强化学习的一些优化方法。然后奖励模型本身它是构造出来了一个很好的environment，对吧？
	然后有个同学说environment出来的R西塔怎么可能是同样的值呢？这个同学你把你的这个问题你稍微去搜索一下，或者去了解一下我们之前讲的，不管是机器学习的这个优化过程，还是深度学习的优化过程。就包括前面讲的这个第二节课语言模型的发展，你稍微去看一看对吧？这个是基础的数学公式，你可以理解成这个表达方式是指这个J它是在对整个语言模型需要去更新权重的这些参数，再求这个高阶导。然后求完这个导数之后，要把这些德耳塔加回去，这个是PPO那个框里表达的意思。然后这个RC塔是指在我特定的输入X的情况下，我输出的Y在reward这个函数输出的一个结果，并且携带了我的这个模型参数。就是西塔。
	好，那我们往后看啊一个简单的介绍，就是这些IHF如果想要实现的话，我们能用哪些工具？这个也是建议学有余力的同学可以去研究。但是我还是从实际角度出发，建议大家把微调的这些部分先好好搞搞明白，搞搞熟练。我记得有个同学问过，就是会不会讲到hugin face的这个TRL这个库这个库我也是最近在用，但是没有那么那个就我也比较稳定。它的API的设计和之前的PFT会有一些略有不同，但是他们的本来也是同等级的抽象，当前面看过那一堆介绍之后，再看这部分的三个核心的step和它对应的trainer应该就好理解了。
	在这儿我们可以把TRL这个库和PFT这个库做成同一层的这个概念，就相当于最底层是transformers。Transformers有一个叫做trainer的抽象，用来做模型的训练。然后在这个transformer这个库上面长了一个库叫PFT。相当于它的更高层应用层也好，或者在它上一层的抽象也好。然后PFT里面有各种各样的trainer，比如说能够实现q lora的这些adapter，把我原来的模型包一下，包一下之后甚至直接用那个trainer就能训练。这个是PFT的好处。因为它相当于是嫁接的，就往原来的模型里塞了一点新的网络结构，然后用原来的trainer就能继续训练，但是当然也会有一些区别，比如它的sequence to sequence也得是一个独立的trainer。因为训练的网那个大的结构变了，然后TRL就是属于大的结构变了，所以它几乎就不太会使用这个裸的trainer。
	就像我们用PFT，大家回头再去看看代码，几乎都是直接从transformers里面import一个trainer。只不过trainer接受的模型是一个q ora的模型，或者是一个其他adapter加进来的模型。但是我们的TRL它大概率是反过来的。你想象一下它的模型是初始的那个模型，它不会去改这个模型的网络结构，不会去塞各种各样的adapter或者塞像Laura去加一个旁路。他不去动模型，但是它的训练方法略有不同，所以他会把这个trainer都会去用的，是改造版本的，或者说它的子类派生的类。然后这个派森的类，它的名字上倒是跟RHF的命名很像。
	比如说这里他用了这个SFT的trainer，但是我刚刚也提过，SFT本质上和PFT有很多类似之处，对吧？那么他用的这个SFT的trainer来训练3.5亿的OPT，然后用了这个reward trainer来训练其他的奖励模型，也是一个神经网络。而用这个PPO的trainer去训练其PPO的trainer就需要叠加进就把这俩模型都加进来了。就我们刚刚讲的，大家想象一下刚刚那个图，这个图里我们如果要去用PPO的trainer，那他肯定得把这两个模型都放进来才能够去做训练。所以大家能看到这个PPO的trainer和它的这个configure，要的这个模型也都不一样。但整体来看是跟我们刚刚讲的这个理论的部分是能完全对得上号的。但是实际的部署和运行，目前还有一些小的麻烦，大家也可以自己去试一试，看能不能很好的去运行。
	然后这个trainer的训练也很简单，它就是不断的从我们的这个训练的语料里面拿出这个query。然后这个query我们能看到通过第一个model，有一个model生成了一个response。这个response就是我们输出的这就是模型，相当于最终的语言模型它生成的一个结果。
	还有一个叫做reward model，他是拿着这个response给它打一个分，然后把提前把这个用户提的这个query和response还有reward一起丢给这个PPO的trainer。它有一个step方法去迭代，去更新这个参数，相当于把XY和R给到了这个PPOPPO去迭代。然后如果我们用刚刚说的这个hidden face的TIL库，再来理解这个PPO的流程。我说理解，这个PPU是指我们刚刚看代码，那个代码其实就包含了前面三个步骤的事情了。所以我们这样去理解它的话，其实是好理解的。但是他这个图有点老，写的是GPT two，但不影响，就GPT two也能实现RHF，这个逻辑是OK的。
	那么我们看他，如果我们先看这个evaluation评估是好理解的。就比如说我们有一个query，有一个response给到奖励模型，奖励模型能打分吗？这个是最好理解的。比如说我们步骤二人干的这个活儿，只不过现在人干的这个活儿已经把模型训练好了，就是RM已经训练出来了，所以RM能完成这个打分。但之前就是人来打，那上面这个roll out就是我们的语言模型干的事儿。就是输入一个X通过语言模型输出Y，就生成的结果response就是XY和这个R，那么如果我们要去进行优化的话，其实就要两个语言模型。
	大家还记得两个语言模型，然后一个语言模型就上面这个active model，这个应该是偏深红色的这个语言模型。这个语言模型是我们要用RLHF，用PPO去优化的语言模型，它是需要被不断迭代的，然后下面这个它叫reference model，或者有的地方叫teacher model，是不会变的。它这颜色确实跟前面的图没对上，这幅图是来自于TIL这个库里面的，大家把那个知识记住，别去芰颜色。下面这个是不动的，就是我们前面那个图里面的绿色的那个模型，它是initialized的一个language model，它是所有的参数都不会动的，他只是给一个参考答案的这个active model是我们真正要去微调的。
	他俩会算出一个KL散度来，KL dev然后算出来之后，这个和我们的reward一起给到PPO，就跟我们前面那个理论的流程是一样的，PPU再去去更新这个active的这个语言模型的参数。这个就是用hugin face的TRL去实现，用PPO来优化它的一个流程的一个图。然后大家如果把这个流程整明白了，其实RHF也就这些东西了。然后我们要去部署一个IHF来进行优化的语言模型，就是我刚刚讲到的，至少咱们需要部署两个语言模型和一个reward的一个模型，这个应该是好理解的。好，然后我们看一眼这个库。
	这个T2L也是hugging face开源的一个项目。然后我们在在这儿能看到，这个是他所有的dogs，怎么样去找到TRL呢？很多种方式。
	第一个先从这儿开始，让大家熟练一下。就在hugin face hub上有一个dox的目录。我说访不访问，这个目录里面就全部是他开源的一些项目。然后我们目前熟悉的hub，我们经常访问transformers，经常访问data sets，也是经常访问radio，是它的这个GUI的库，也经常访问PFT，已经经常用过了。然后像这个accel rate其实我们也用过了，只是我们没有直接去用它它是一个底层的库。然后token nier其实也能够也用过，invented我们也介绍过了，然后TRL在这儿，大家可以直接搜，像什么safe tensor都是一些底层的库，那么TRL点进去之后。当然大家要装它的话，也就papin store 72L。
	有点慢，直接在这看吧。这个点进来其实就这个页面，就这个TL这个里面其实有做一些介绍，包括刚刚这个图，它也是跟transformers去做的集成。然后在整整个事例里面，比较重要的，比如说在这儿我有试过他的这个视力，没有那么顺利的能跑起来这刚刚我们看这个介绍，在这个事例里面其实就是展现了一个GPT two比较小了，是一个10亿10亿左右的一个模型。然后使用这个base size为一这样的一个最小值的一个数据来进行训练的这么一个例子。然后大家如果跑得起来的话，也可以告诉我，我现在在T4上面确实没有跑起来，回头我也可以再试一试这个流程其实是比较明确的，大家如果有兴趣非常想要试一试的话，可以去感受一下。对，但核心是要准备好足够的数据才有意义，不然就只是跑了一下代码而已。对。
	然后他的整个trainer里面，除了PPU的做trainer的这个优化以外，也有这个DPU。这个也是有一些有一些同学可能了解过的。DPU其实跟PPU在整个RHF里面是等价可替换的一个优化器，一个trainer，那我们可以直接去替换它，使用DPU trainer来来优化它，也是OK的。别的暂时就这么多，看大家还有什么问题吗？
	这个同学开始没有加入吗？就是为什么要用2个LL模型？因为需要保证在优化过程当中生成的结果，它的质量没有太大的下降。
	有个同学问，跑不起来是不是因为显卡是T4的问题？有可能因为目前还没有换过别的卡来运行，咱们可以试一试。
	是的，这个同学问的是对的，就是整个这个架子其实是不挑模型的这也是为什么这个IHF这个模型，它这种调模型的方式也有它的通用性。对，但它的通用就跟就跟你要用的成本有关系了。对，就他本身需要的资源，这个资源不一定是GPU一定要干到多大，是你需要投入的。比如说你对于这个数据的一个准备和你对这个奖励模型的一个训练，这个是额外的一个工作。不用在同我看这个同学说。PPO没有包含两个模型，PPO是一个优化器，就跟adam 1样，它不是包含两个模型，他是把这两个模型拿进去之后，他才能知道这个模型的哪些权重要更新。它不是两个模型。对然后RM模型训练的时候也需要把数据集做成train test和split吗？是的，理论上是的。
	还有同学开始问的这个PPO它不是加载了，它不是两个模型，它就所有的trainer它都只是优化器。然后模型是模型优化器是用来迭代它的一个方法，我不知道这个我表达清楚没有。然后这个优化器的train方法相当于就是开始去训练它了。然后我们整个三个阶段其实是一步一步，就是相当于阶段一的这个SFT是你我们开始就讲了，这个初始化的语言模型在RHF里面的这个SFT，你是可以准备一些数据集先训练的。训练完之后然后再去训练第二个单。第二个训练完之后才能去开始训练第三个。然后在第三个训练的过程当中，这个PPO是会去迭代语言模型的，但是不会去动这个IM。
	不行的，这个我开始花了好多精力去讲的。就是SFT的过程是不是不是必须的，也可以通过IM来修正原始模型的输出。然后这是不一样的，就是一个是企业文化，一个是你的能力水平。就是我专门举了这么一个例子，包括我说了就是100个人选出十个人过了试用期，这十个人都是至少能力还是比较到位的。你要想象我们的这个ISF的训练核心还是要找出一个最适应企业文化的能力最强的人。他两个条件缺一不可，然后能力强是前提。你不能说我一个，你就想象你没有经过任何微调的语言，模型生成的数据都是乱七八糟的那你的奖励模型拿到的九个答案我都不想要你再怎么排序，这个也上不去的，对吧？就是他他第一个条件没具备。
	最主要的原因是因为金条的效果还不够好吗？我说了是企业文化问题。这个同学我不知道这个企业文化你有没有切身体会？对，就是不能讲政治。对，就是对企业文化问题。对，有的人他能力很强，但他不认可，他就是没法适应的。然后很多时候是这个是大问题。对，大部分人是有能力的，但他没适应这个环境，所以他需要适应这个环境才能说合适的话做能做的事儿。
	如果对这个问题很有兴趣的话，可以建议去搜索一下InstructGPT和GPT3.5这俩模型。就是当时的ChatGPT最开始上线的时候用的是GPT3.5，就是经过RHF深度调整价值观对齐的模型。但是其实上线之后没多久，后来又换回了InstructGPT的一些衍生版本。就是因为它太会适应人的想法了，以至于他说的都没有那么好了。相当于为了为了文化，为了适应企业文化而放弃了一些好的生成结果。所以这个地方是有一些trade off的，然后具体就看怎么去调了。
	哪里体现了结合reward和care散度？我放的这一页还没看到吗？还刚才专门讲了，就for query in data loader这个循环体，你好好看一眼。对，就最后这两页就是在在讲这个事情。对XY和R都丢给了这个PPO的trainer。
	Rm模型训练数据的规模和模型参数有关吗？IM模型它不会训练特别大规模的。我们刚刚看到了就这个小几万条，甚至可能几千条。如果你的单元模型比较小的话。
	很吃奖励模型的效果，要是reward模型效果不好，岂不是越调越差？也不会越调。他再差有个底线就是你我不知道大家有没有体会，就是优秀的基础模型就是你的阶段一。如果做了一个优秀的基础的语言模型，他再怎么差，他有一个底线。就像成绩好的同学，他就算天天不学习，他也不会不及格。但是一个不及格的同学，你天天给他做心理疏导，让他好好学习，但他就是没有去学，那他的成绩也不会特别高，他可能只是会跟老师打好关系，不会乱吵乱闹，但是他还是做不来题。
	我们把这个解释有没有回应到这个问题？对，因为整个大语言模型现在就是在无限的向人的学习过程靠拢。你想象一下人是怎么学的，人就是我开始说这个大的三个阶段，还不是IHF，大的三个阶段就是K12。然后试用期和公司内部的晋升。你要在一个公司内部长期晋升，你肯定有能力，同时你也很符合公司的价值观和企业文化。如果你不符合，你是待不下去的，你会很难受的。
	对，这个同学说的很对，所谓的情商的学习就是rm奖励模型也很看环境。所以开始有个同学问奖励模型能不能通用？不能通用，就你在一个公司学到的企业文化，换了个公司可能你会很很不适应。对，或者说这个公司也很不适应。对，就是你在google学的企业文化，去了这个阿里或者拼多多，那肯定是不一样。你得重新再再适应，调整，你才能风生水起。
	最后再回答三个问题，然后吃晚饭去了，大家还有什么问题？就是这个RHF其实确实是一个很新的东西，包括这个过程当中具体应该怎么去设置去调整，也都在探索，这玩意儿也就才出来一年多对。
	你就看它对应的这些库，这些技术站其实都还没成熟，transformers这个库都已经出来好几年了。对，但是TRL包括我知道的还有几个类似的库，好像是什么RL for l ms就是reinforcement learning for language models，有好几个类似的库都是在做这个强化学习去微调语言模型。这个事儿属于一个目前比较热的创业方向。
	对，但是终局是如何还不知道，但无论如何把强化学习引入进来是好的，因为它能弥补像这个机器学习里面的这种监督学习。其实严格意义上属于机器学习里面的一个小的分支。强化学习其实都都不完全算了。对这种监督学习的方法，它有它的好处，但他也有他的缺陷。对，就他能学技能，但他不能学学文化。对，最佳实践讲一个案例，你不天天都在用ChatGPT吗？对，这个ChatGPT就是最好的案例，对。
	再再回答一个问题，看那有什么问题。
	看大家还有问题吗？没有的话，那我们就今天先到这儿，回头我们看一看这个TIL库能不能运行起来，在比较少的资源上面，还是要考虑到大部分同学的一个实际情况。对，有的同学问reward trainer当中的training argument是什么？刚刚讲了怎么上这个文档，可以自己去搂一眼。
	可以搂一眼这些参数就可以了，然后也没有那么复杂。因为reward这个模型其实相对来说是比较简单的，就是一个典型的神经网络。OpenAI是怎么降成本的？得问OpenAI了，这个问题我回答不了。好，今天就先这样，回头大家有问题群里我们再再一起讨论，感谢大家。