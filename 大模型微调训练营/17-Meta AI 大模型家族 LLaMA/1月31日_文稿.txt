	一直到今年年二三年年底，我们看到像mixture o这样的MOE出现之后，又进一步的让大家理解了，其实单个模型它并不需要做到特别大，不需要像20年的时候做到1000亿，单个模型做到百亿这个规模，几十亿这个规模就已经很好了。接下来更多的是要么就做多个专家模型在前面去做路由，要么就是你并不需要一个通用的大模型，那就像lama一样，去做一个几十亿甚至几百亿的模型就可以满足咱们的需求了。
	这个是lama带来的两个非常好的，算是认知和经过实验验证的技术路线。我们今天就来看一看拉马这个大模型，它的第一代和第二代到底是什么样的一个模型，又给我们在2023年带来了一些什么样的新思想。当然这两篇paper是值得大家去读的，尤其是第一篇，就lama一的这篇paper。大家能看到它最后更新是二三年的二月份，然后lava 2其实是二三年中更新的一个版本。然后这两篇paper发的时间前后不到半年的时间，所以里面有很多的相当于lama two有很多的能力，其实跟拉玛一是一致的。只不过他去做了一些额外的扩展，加了一些新东西。这个我们待会儿讲到number two的时候再说。
	纳玛伊拉玛伊这个羊驼应该是在去年风靡了twitter和朋友圈，很多同学应该都看到过这个头像，或者说以这个图为主体的各种各样的模型，阿玛尼的核心总结起来就是相比于千亿的GPT4，或者GPT3.5这样的模型。它的核心是说跟这种千亿的比，它是一个小模型，用几十亿和几百亿的模型参数加上足够多的数据，因为GPT3的训练的token数量其实跟这个拉玛一比起来还没有他多。或者说如果你把数据总量去除，以一个模型参数的规模，那单位的模型参数需要用到的训练的token数量，那么一是显然比GT3要多出非常多的。
	然后同样的基于lama一开始因为它就开源了，就各种各样的lama的大模生态。比如说左下角的这个stanford appa是我很还自己试过的。我觉得它单个当那个当时刚刚开源的时候，就是stanford的实验室把这个公布出来的时候，他做了一个线上的demo，那个demo我也去用过，是用的这个radio，类似于stable diffusion的一个在线的demo，第一天出来就给打爆了，就崩掉了。现在它这个demo也没有再持续运行了。然后右边的这个叫这个词我一直读不好，叫什么carver还是什么，这几个我不太能读，好像是个西班牙语。这个大模型其实是更多的研究机构，不只是stanford，很多的研究机构一起基于lama去做了更多的改进，我们待会儿也会去讲到他。而他的因为研究机构很多，也有足够的资源在支持，所以他做了在线的一些demo。包括他针对多轮对话去做了一些优化，训练数据也有些不同。
	好，我们接着就来看看nama一样，大部分lama不管nama一还是nama 2，它的技术上的改进主要体现在纳玛伊上面。那么一它不是一个一个模型，所有的大模型学到我们十六课了，应该都知道通常都是做了一个系列的模型。这个系列的模型根据它的场景训练数据的总量，它的应用的一个场景会有一些不同，所以它会做多个不同的版本。对于number one来说，它有从70亿到650亿这样的四个不同的版本。而这四个不同的版本在训练过程当中，他们使用到的参数是有些不同的。我们后面会专门去讲他们的base size，都用了非常大的一个basic。大家可以看一下这个400万，我们平时使用的倍数是四，差了100万倍。所以你想真的要训练一个预训练一个大模型，你需要使用的这个GPU，真正的算力投入是非常夸张的。
	然后咱们看到它的token使用上也是一样，就是67亿和130亿的这两个小尺寸的大码1。他们训练的这个token数量是one training就是1万亿个token。然后325亿和650亿的这个规模的number one，使用的是1万4000亿的tokens。这个是number one的一个四个不同系列的模型的一个基础情况，那么在训练过程当中，我们看到它的loss是比较稳定的在下降的。当然要训练百亿的模型，不太可能说那么完美的一个下降趋势。能做到像咱们看到的这幅图当中的这种波动率就已经很好了。
	所以大家在训练过程当中，有的时候首先预训练肯定会比咱们做微调还要难。然后如果我们在微调过程当中发现有一些异常值，抖动的非常厉害，有一个loss。先不要着急，再等个几个step，你看一下它有没有恢复正常。因为它有可能就是触碰到一些奇怪的数据，然后我们使用的beset就特别小。当使用特别小的beset的时候，你就会比较容易遭遇到这种loss的抖动。因为你的这个base太小了，所以它不一定这个batch的数据符合整个训练集的分布，这是比较典型的训练知识。
	那么接着往后看啊，就是lama这个1.4万亿的token都是些什么数据？它整个number one这个meta想要告诉世人，告诉大模型的研究人员和使用人员，就是想说清楚，并不需要把模型的参数一味的往大了做，这个是挺实用主义的。因为在2023年了，所有的人都在聊万亿模型了。但是number one出现之后，告诉你650亿的模型就能做到你这个两三千亿的模型的效果。或者四五千亿像google的这个pm one，5400亿就是一味的去加模型参数，而忽略了数据的准备。
	其实是不算是一个特别高性价比和双碳经济的一种训练方法。因为现在整个大模型训练消耗的能源实在太多了。所以大家如果去细读论文，尤其是二三年开始的这些paper都已经开始。甚至之前我们在上上节课讲MOE这个GCR的，就是讲google做万亿模型百万亿模型的尝试的时候，都开始去考虑训练大模型的能源消耗、碳排放。所以本身把模型参数做小，也是碳比较友好的，环境友好的，符合咱们国家的这个双碳战略。Number one它的万亿token一共取了七个不同来源的数据集，最多还是这个common co网络爬虫的一个数据集。这个数据集从17年到2020年，一共发布了五个版本，然后进行了各种去重，包括对一些非英语页面的一些剔除，然后过滤低质量的一些内容等等，最终占了67%的数据比例。
	第二个就是这个c four，也是咱们非常早的时候使用这个量化技术。大家如果有印象的话，这个C4数据集是经过预处理的这个common co，然后还进行了一些多样化的生成，这个相当于是一个优质版本的，精挑细选的common co的一个数据集。然后github就是我们的典型的代码的训练。Github它用的是google big query上面公开的数据集。然后协议也都是使用了一些可以拿来做这个训练的，就没有使用咱们这个不友好的。比如说这个带有污染的这个协议，主要使用的是这个阿帕奇BSD和MIT这个license，这些数据集其实就在代码上训练已经成为大家的一个共识了。
	然后后面还有四个不同的是来源加起来可能有13到14，就13%到14%的这个量级。包括这个VC pedia，书籍的这个语料库。我们从GPT one就能看到有这个books one、books two、books three, 这个语料库本身也在迭代，然后也去训练了论文action。大家现在去问这个ChatGPT也好，问chat BDF也好，或者一些新的创业项目，像TXYZ这样的一些网站都有一个很常见的场景，就是去问一些特定的论文是讲了什么东西。比如说你去问他什么是GPT，什么是GLM，什么是lama，都需要去训练，这也成了一种共识，就像githa一样。
	最后还有一个就是这个stack exchange，它就是一些高质量问答，就跟早期的GPT3去训练的redit训练了这个quora是一个逻辑，就这种社区问答。所以整体来看，其实在训练的成分上跟GPT3是类似的。但是它训练的质量，就这个数据的质量和总量上是超过GPT3的。大家有印象的话可以去看一下。而GPT3是一个1750亿的一个模型规模。如果我们按最小的这个number点1的话，就是它的几十倍的规模。但是即使是这个number one的最小尺寸，也是用了1万亿的token在进行训练。
	而在这个训练过程当中，大家可以关注一下左边这个训练方式，我们到现在应该很多同学都实际训练过几个模型，可以慢慢加深一些。就比如说我们看左边这个训练的数据集的分布比例，我们有一个大概的了解了。3分之2是common co几乎就是等于预训练，跟最早bert GPT去用无监督预训练是一个逻辑，包括这个c four也是类似的一个逻辑。然后训练过程当中，像这个common control c four，archive和这个stake exchange，他们的训练的ipod都只训练了1%，不，都只训练了一次。说错了，都只训练了一次。现在大家应该对apple这个参数有概念了，就是我们只把这个训练集给模型去用一遍，用完这一遍之后，它就不需要再去第二个apple去做训练了。
	大部分的我们在做预训练，无监督的学习的时候，这种语料就是一个air pod就够了。然后我们的wikipedia和books是更偏一些概念的学习，就相当于咱们学的这个K12教育里面，如果是正常的普通的难度的知识，你可能就看一遍就可以了。有一些比较难的晦涩的概念，然后还会有比较常引用的，可能会训练的更多一些。所以他用到了将近2.5个a book。然后github上面的代码就用的更少了，我们可以看到在github上面他这他首先把github上面的代码只选了协议友好的。第二个就是把一些根据，比如说行的长度太长的不要，然后字母和数字比例不要，其实也是在筛选这个高质量的代码，这里其实是很tRicky的一个训练。就是所有的预训练和微调里面怎么样对代码进行训练。现在是一个比较难，也是大家正在探索的一个领域。
	然后代码的训练，包括我前两天还看见基于lama two去训练这个github的代码。我们知道它的训练集用的英文的，然后它的代码肯定也都是用的英，这个也不叫英文的，叫非中国区产的代码。但是有一些同学使用了这个nama two的预训练权重，然后又使用了大部分国人开发的这个代码之后，发现它的代码生成能力反而还下降了。
	这个也是一个很有趣的现象。我们可以理解成本身可能number one在预训练语料里面就没有针对非英语的语言做过什么优化。所以如果咱们的这个大陆地区的代码有很多中文注释，包括一些变量命名，那么可能是会对它的这个质量造成下降。所以整体的你的语料是要搭配着来的。比如说咱们如果现在要去对number one这样的一个规模去做训练，然后你在github b的代码这一侧，既使用了这个英语的代码，然后又使用了很多比如说带有中文注释的各种代码去训练。有可能反而会对它的这个效果有一定的影响。这些就是数据准备当中非常费时间的一个必须得做的一个步骤了。
	我们稍微展开一点，就是怎么样对于数据的这个准备，包括对于不同类型的数据使用的去做训练的时候的一个使用，包括它的一个比例，其实是有一些最佳实践的。然后这个比例从20年的GPT3到23年的nova one，都是一个无标无标注的预训练的语料，相当于提升他语义理解能力的占了80%。剩下的各种功能性的，跟下游任务也好，跟高级概念也好，跟领域知识有关系的，占了不到20%这样的一个比例。
	Number one它作为一个二三年的新模型。它有什么样的改进吧？它相比于其他的一些大模型，为什么number one有就相当于长成了一棵新的大树了，就是基于number one的改进，或者说基于number one的这个衍生品派生出来的模型这么多，它有什么样的优点？相比于这个GPT3，或者说相比于其他的一些大模型来说，它的优点是什么？
	在number one的这个论文当中，其实他自己也有总结，主要就是三点。然后这三点，其实有个同学问apple k不应该是整数吗？其实不一定，你可以apple k你可以用steps来做换算的。我不知道这个在在PFTCGM的那一节课的时候就教大家算过，就是总的训练步数和你的number train epoch的数量和你的这个批次梯度更新的几个参数之间要怎么去换算。对它不一定是个整数的，就是我我还是把模型训练跟做菜比起来，大家去想象一下，就是中国的菜谱，就是我们不看西方的牛排，中国的菜谱大家去读一读，就跟现在你去研究怎么样去做模型训练是很像的。
	你只有不断的去调模型去试，然后你才能知道是这个盐适量，这个酱油两勺多大的勺，对吧？这个两勺就一定是整的两勺吗？你会不会摇的时候没有摇螨？或者你摇满了过程当中撒了一点呢？啊，这个就是各种超参数很tRicky的地方。
	我们说回来，number one本身它同样是基于transformer的架构，它融合了很多改进。因为transformer已经这么多年了，17年到23年六年的时间了，然后各种基于transformer的架构也非常多了。在这个基础上，有很多的paper都在讲原始的transformer的架构是有它的局限性线性的。然后GPT采用了这个transformer decoder部分。再去做深层次的训练，也是有可以改进项的，包括google自己的大模型胖，也有一些新的一些改进。
	具体来看，其实有三个大的优化项被加到了number one这个网络里面来。第一个就是这个规划，就是我们看到经典的这个transformer，它的结构是muti had attention，然后这个fate forward network，然后接着是我们的ad and layer longing zing，就是我们的去做规划，然后接着才是下一个模块。下一个模块可能就是marti had attention，或者直接借一个fit for network都可以。但是这个规划通常都是放在后面的，在我们以前看过的各种网络里但是GPT3里面就已经提出来可以去做前规划，什么意思呢？就你想象一下刚刚那网络结构是先做注意力，再做全连接，再做归一化，再去下一层，那有没有可能我不要在每一层的末尾去做规划，我把这规划做到下一层的这个开始了，大家不要小看这个变化，就看起来好像这个不是没有变化，对吧？
	我在最后一步做和我在我在一个模块的最后一步做和在下一个模块的开头做有什么区别呢？这个区别就是咱们想象的是单一流水线，但实际情况下，你看多层注意力机制里面就是muti head contract在一起的，就多个head attention加在一起的那你的上一个的这个模块最后不做规划，加到下一个模块去做的时候，那下一个模块可能本身就是多个输入，所以相当于这多个输入以前是归一化之后给到我，我直接去做处理，变成了这多个模块不做规划给到我，我先做我，然后我汇总之后，我做完规划再进行处理。这个是一个，变化，然后他用到的规划的，函数叫做IMS law，这个规划的方法具体的形式是这个公司是长这样的，这儿我们就不再展开了。有兴趣的同学可以把这些优化技术深入看一看。
	我记得好像非常早的时候，有一个同学提过，能不能讲一下这个sweet blue激活函数。这个激活函数是google提出来的，在palm里面去用的这个激活函数替换了一个非常经典的review的这个激活函数。我记得之前也有一幅图专门讲过，review就是小于一个值的时候就是不激活，然后大于一个值的时候线性激活，这个是reu然后这个sweet t glue它有一些超参数，它不是单纯的像瑞，当然reo也有，就是这个值可以选。那么sw glue不是说我小于一个值的时候就不激活，它是先下去再上来，然后上来的时候，这也是可调的。比如说如果是一个线性值，就是X和Y是相等增加的，那么这个three glue可以让它大于一个值之后是超过Y等于X的时候，是这个Y比X大，或者是YXL这两种方式，这取决于它的超参数。它的第二个优化就使用了更好的激活。并且当时在他们当中提出这个激活，一方面是改善这个激活效果，第二个就是这个three blue应该是比在这个transformer这种transformer这种计算的这个就QKV的计算里面，它是更方便去做做这个计算的那具体它有一个公式证明，然后大家可以去这个相关的paper里面再去详细读一读。
	第三个优化，其实就是这个位置的编码，就position的这个embedding。我们用过这么多次token ized了，我相信大家应该对这个位置编码有一定的了解了。如果没有印象的话，可以回到我们第一个这个transformer的反冲的课题的那一节课里面。当时我们是在bert这个模型上面做了两个任务的反应称一个任务是做的这个类别的这个呃就呃应该叫什么sequence classification，就是序列的一个分类。在EP这个数据集上面，根据大家的这个评论去给他打分，一星到五星这个是一类的，这个fine 2在bert上，还有一类是做的QA，就是在bert上面去做一个问答的一个反应to当时使用的是这个school，就是stanford的这个question answer data set来做的一个find two。然后在那个代码里面，大家可以去细看一下。当时详细的给大家讲了这种相对复杂一点的文本生成类的这种下游任务，它本身就会出现一个样例，就一个ample超过了我们输入的这个序列长度，你可能需要去做拆分截断。截断之后我们为了能够找回原始的这个单词，应该说那个单词在原始的那个example里面的index那个位置，就会去做这个offset的这个indexing，相当于就是去对它的这个位置进行编码。不过那会儿是最简单的，只是去记录一下它的位置，然后没有去做额外的事情。
	那原始的transformer里面大家如果有印象，它没有使用RNN这样的结构。它的每一个token，就每一个输进去的这个单词，它的位置其实是单独用了一个编码记下来的。就是这一个token是在原来的第几个，另一个token是在第几个，这是一个绝对的位置编码，就跟我们在find QA里面一样，它甚至是连续的这个位置编码。然后它跟这个内容的这个input token ID也是等长的，因为它就是记录它的位置。
	在这个GPT NEO这个paper里面，其实引入了一种新的效率更高的，叫做相对旋转的揉rotate，rotary这样的一个position embedding n。就是跟原来的这个绝对的位置编码比起来，第一它的效果不错，第二它还能够去类似于RNN的一些效果。因为我们知道transformer本身没有RN的这种序列的这个信息了然是连续的这个击鼓传花一样，前一个到后一个，前一个到后一个，这个信息是能够被保留下来的。但是通过这个RPE这个旋转的线路之后，这里有一些比较复杂的一个矩阵变化。大家如果不感兴趣的就不用去深入看这个。
	但是RPE的这种旋转嵌入，确实现在也有很多大模型都在用，简单来说就是没有增加这个计算量。同时还把咱们的这个token的一些序列关系能够利用起来。甚至它还不只是支持二维的输入，可能还支持三维的。那么在一些其他的模态里面，说不定也能去做进一步的延展。这个是ROPE的一些介入。说起来比较抽象，就这三个改进。但是其实如果我们落到这个网络结构里面来看，是比较好理解的这就是我们刚刚提到的三个不同的改进项。
	在标准的transformer结构里面，我们的number one是做了一些调整。比如说我们看到在最经典的这个transformer的结构里，我们应该是第一个接受处理的模块叫做marty head attention。经过my head的tension之后，是我们的softmax，然后是我们的这个fit for world network。但现在他把这个Normalization放到了前面，就是我们看到这有一个IMS的这个law，同样的fit forward前面也有这个Normalization，因为本来应该放在后面。
	然后第二个改进就是这个激活函数，之前的激活可能这个real大家如果这几个激活函数不熟的话，可以大概去搜一搜。这是用的这个sweet lue的这个激活函数，然后这个位置编码也是一样，使用了这个LPE的一个旋转的一个位置编码，使得整个decoder的这个transformer decoder模块的结构做了一个。大家可以认为提升了他的学习能力，这个是改进之后的这个lama one的一个网络结构。
	那这三个结构是不是最优解呢？其实也不是，因为我们都提了跟做菜一样，对吧？其实本身对于数据是就是什么样的数据搭配什么样的网络结构会输出是一个最好的模型，这个是无解的。就是你相当于就是在问一个厨师，什么是全世界最好吃的菜，或者说你能不能烧一道全世界最好吃的菜，这个东西是很难回答的。但是我们只能通过不断的去增加这个考试也好，基准测试也好，或者是去找出一些特定任务，下游任务来做评分。像刚刚提到的这几类网络架构的调整，其实这个差异性的配置也不是危险。
	对，也有一些很多别的方法。比如说就拿这个Normalization来说，最经典的transformer它用的其实是叫做后归一化。就是我计算完了之后，我去做一个归一化。这也是最早的这个lomi's ation提出来的时候的常见做法。因为它要解决的就是我经过激活之后，我我需要把我的各种各样的不同的数据样例去做一个规划。这个是最初的样子，就是破这个post Normalization。
	但除了这个post Normalization，后续其实大家也都会发现各种各样的Normalization其实都有可能对最终的结果带来一些好的影响。像number one它就使用了另一种，就在前前面去进行处理。在我们这儿看到的这个pronunciation，除了这个以外，还有这个所谓的三明治，我我在前面处理，我在后面也处理，这样也许会效果更好一点。所以你说这个跟做菜像不像呢？我自己也做菜，我觉得挺像的。因为你说他有什么纯粹的数学上的形式化的一个理论依据呢？没有，只能说这些数据就是食材。
	然后我在这儿各种调试，调试之后，同样的数据我去控制变量法。同样的数据，同样的模型结构，我只改了这个Normalization的位置。结果我发现在一些数据上它的pre Normalization效果更好，我觉得它就是可用的，然后除了这个怎么样去调它的位置以外，刚刚我们提到这个方法也是一样的。
	最常见的这个layer Normalization就是经典的transformer的结构。而这个number one使用的是这个IMS的Normalization，就我们这里第二行，这是他number one pay，这个不是number one paper里面的截图，这是一篇综述性的大模型论文当中的一个截图这个论文应该在非常早期的时候，也给大家提过一两嘴，非常好的一篇论文。然后这个deep lomi's ation其实就是在这个laar org ization基础上，再去增加一个超参数，对于我们的输入的下游去进行调整，所以规划的方法也有很多种不同的，包括这个激活函数，从最早的很多激活到上一个深度学习时代大家卷到最后只剩下这个人录了。但现在又提出了各种新的基于transformer的激活，像sigma y的，现在就成了其中的一个组件了。
	最早其实SIGOY的是上一个时代的最早期的激活函数，包括位置编码，从这个绝对的位置编码，这个XI等于XI加上PI是transformer attention is oil need里面提出来在用。后来出现了这些相对位置编码，旋转位置编码等等。这些其实就是大模型这个时代对于一个神经网络的架构里面有哪些东西是可以改。
	大家还在调、还在试、还在卷。然后也不需要去焦虑，就是你也不用去穷尽去所学习所有的这些方法。因为随着时间的流逝，这些方法最后剩下来的一定是好用的。就像在七八年前，所有人都在纠结我要用什么backbone，有VGG，有inception，有exception，有点net，有各种各样的网络。最后剩下来的发现还是renee效果最好好用。然后这个skype connection是有价值的，包括像我们PFT介绍的第一篇paper这个adapter tuning，这个adapter内部的结构也是用了这个短连接，这个是实际有用的。
	我们同样能看到未来这一堆激活也一定会卷到最后，有一些可替代性的方案。要么就是出现了一个特别好的，把大家的优势都集成起来了，就像lama one现在正在集成很多大模型的优势一样。MOE也是一个逻辑，就是我们用混合专家模型把以前各个领域里面的好的模型的训练技巧，都挪到一个特定的模型里去。然后最终去做三个臭皮匠顶一个诸葛亮的这种故事。
	所以这个是我们如果很多年的AI的经验，算法的经验之后，一定会得出来这种体会。所以大家不用无尽的焦虑，最好去解决问题的方式就是聚焦你现在的这个资源和你学到的一些技术，不断的去做调整。要遇到问题解决问题，也不要想着说这个预训练这么大的资源消耗，我们怎么玩，暂时也不是咱们的烦恼。
	然后网络架构的对比，刚刚我们说了有这么多不同的方法，那经典的这些大模型做了哪些选择呢？这里是有一些国内外的比较典型的像GPT3，这个盘古应该是华为出的这个模型。我没记错的话，OPT我们也用过了，OPT的这个开源版本的GPT也是facebook去主导的。然后google的palm这个balloon，包括像这个nmda lama GLM130BT5都是来自于不同公司，然后甚至有不同的结构的。比如说我们的上面的都是decoder only on，或者叫做这个Carol decoder，就是我们在transformers这个库里面使用的最常见的，以生成为主的，主要使用了这个mask的self attention去训练的的生成类的模型。还有这个encode decode的这个T5的模型。我们在介绍应该是上节课的时候，这个deep speed用过这个T5模型，去做过这个deep speed的一个加速训练，也是google发布的一个模型，非常好用。
	你看这些不同的模型，不同的参数规模，他们使用的Normalization就有位置上的区别，也有这个方法上的区别。然后他们的这个最终性能，你说跟这个nominalists ation的measure和它的position差距有多大，然后起了什么样的决定性因素？太难讲了。因为这个事儿就是影响因素太多，包括你在训练过程当中使用到的这个数据，因为你是在万亿token里面去抽出400万作为一个batch，那么你抽的这个shaffer的这个方式随机化的种子，其实都会对它有影响。
	这就跟我们做一道菜，你说我这个盐是盐可能还好还好还好搞一点。因为它它的参数总量，我们做的菜的参数总量，调味料可能也就十几个，然后你的做饭过程可能也就十来分钟。但你越把这个做菜的时间做长从炒菜变成那个炖菜。比如说你炒一个青椒肉丝，你可以去控制变量法，还比较好去做预估。但是你炖一个佛跳墙对吧？那可能就是几天几夜，就像周星驰的这个食神一样。这几天几夜的过程当中有很多变量都在作用于这道菜。
	最终的结果就像这个模型训练一样，我们只能说不能乱搞，就比如说你不能把这个盐加到无限多，你不能把这个best size设到1，这肯定是不好的。一定是在有限的资源下面，然后有一些最佳实践的选择。然后我们去做这个最佳实践选择，去做一些参考，这个是Normalization，这里是位置编码，包括它的激活函数的选择，有没有使用这个偏置，然后我们在罗拉的微调里面，大家应该也知道这个参数了，就是调不了，调不调这个BIOS。然后它的这个layer的数量，这个head的数量，还有这个呃d model，就这个维度的数量等等，这些其实就是我们平时看得上眼的，就是能入各位法眼的还不错的大模型的一个选择。所以大家如果要去说推荐什么样的超参数是最好的，什么样的模型网络架构是最好的，这个没有答案，但是比较省钱。
	ROI比较高的一种做法就是像拉玛的生态一样。就基于一个已经效果还不错的预训练权重，再去做下游的训练微调是比较好的。这也是我们做这个课程的一个初衷。
	Number one刚刚讲了它的网络结构，它的预训练超参数又是怎么样去配置的呢？我记得在上节课应该是在上节课给大家讲过这个adam优化器是有两阶动量的。然后这两阶动量就会使得optimized state里面会多存很多的参数，在整个优化过程当中，然后这两阶动量其对应的有它自己的超参数，就是这个beta一跟beta 2。然后我们在做这个调整的时候，大家回头去看一下那些训练配置，都有这些超参数。
	然后还会有一个warm up的过程。就是咱们知道为了使得咱们的这个模型训练比较顺利，参数这个loss下降比较顺利，通常会做的一个做法是刚开头的时候步子迈大一点，越到快收敛的时候步子就越迈越小。而这个步子其实就是我们的learning rate。就我每一步梯度算出来之后，我要把这个梯度这个德尔塔以什么样的一个比例加回到我们的。的模型权重上，通过这个learning rate来缩放这个delta。通常来说到最终的这个学习率就我们这里写的这个final narrate最终的学习率，比起我们最开始设置的那个最大的学习力，这肯定是有一个很很大的一个差距的。在number one里面，就是只剩原来的10分之1，然后这个权重衰减，就是指它是怎么从百分之百衰减到10%的。这个有一个衰减率，它是一个衰减的函数来控制，然后梯度裁剪就是把一些这个有可能超过。
	因为在计算过程当中有可能精度爆炸，所以梯度裁剪是一个比较重要的超参数。这个warm up就是热身，我们在这个训练过程当中也经常设置这样的一个超参数。这个warm up的steps，就是前面多少步是这个warm up的一个步骤。就是我的这个learning rate是比较比较维持比较高的一个频率，而不是一来就开始衰减。因为有可能一来你的还没找到状态，对吧？你就开始每一步都往小了这个步伐走，等你开始两千步，你都还没走到第一个下坡点，都还在那个山顶上晃悠。这个是我们up of staves，除了设置warm up steps，也有一些超参数是直接去设置这个warm up ratio，就相当于我热身的这个部署占总的训练部署的一个比例。比如说设置一个5%，10%也有。
	然后除了上面看到的这些优化器的关键参数以外，number one还有一些，开始我们这幅图也看到过一些其他的超参数，就比如说best size，然后the new rate我们能看得到，越是这个大的模型，它最终的能源rate越小。越要精挑，然后token数量反而要多，这个是只能说熟练了，你大概就理理解里面的这些调配关系了best size肯定是越大越好，这个是毋庸置疑的。所以我也在告诉大家是说，你如果能够把best size弄大一点，肯定就弄大一点。前提是你的精度是有效的，你别搞成一个只有4 bit，然后再搞预训练，这个是很很麻烦的。至少你也是这个16个bit的浮点数去做预训练是比较靠谱的。然后在优化器里面可能使用的是这个32位会更好一点。如果你用不到32位，就要去用一个loss的一个缩放。我们上节课也讲过，就为了能够让16位单精度的浮点数也能够提升它的精度，可以通过loss scale，但那个loss scale也会消耗一定的显存，只是可能它不用直接放大一倍的显存，好吧。
	然后我们能看到整个训练过程当中，除了刚刚提到的这些参数以外，我们的其他的大模型也都有一些不同的超参数的选择，就跟刚刚那个模型架构是类似的对比。比如说对于咱们的这个GPT3，或者说对于我们的这个GLM130B来说，它的besides也是一个不断调整放大的一个过程。然后learning rate的选择，从这幅图来看，大家应该能看得出来。比如说T51B是用的这个十的负2次方，GM130B是用的这个8乘10的负次方，但是他俩用的优化器不一样，这个时候我们就要看，当我们用的是同样的adam w的优化器的时候，lama a65B用的是1.5乘上十的这个负4次方，就我们也刚刚也看过了，然后nama的7B是用的3乘10的负4次方。
	所以这些learning rate有没有？应该怎么设置？这个是经常我被问到最多的一些问题第一不能直接看这一列对吧？像我们刚刚一看，T5怎么是这个GLM的1000呢？这个怎么这不太靠谱对吧？仔细一看，是因为两个用的优化器不一样。现在我建议是大家可以无脑的去选择adam或者RMW优化器，这个算是比较主流的选择了，它比较稳定。上节课也讲了它为什么稳定，也是吸收了原来的这个ID grade和带动量的IMS的优化器的特点，整合到了一起。所以这个就是相当于绝大部分的研究人员帮你做了这个选择，已经卷完了这个事儿了，咱们就不用再去纠结了。
	像现在大家也不会去用C其他的一些SGD这样的一些优化器了，当我们锁定了不同的，应该说锁定了optimize之后，再去看不同规模的模型参数使用的learning rate，其实是能很明显的找出规律的。说一个大白话就是越是大规模的这个参数，越需要去保证你的这个learning rate最终收敛的时候，就它最后的那个值要足够小，但也不能太小，太小可能就没用了你看到这个65B10的负4次方，130B的时候负5次方，175B-5次方。这些规律是很好总结的，只要你去花一点时间去研究的话就能看得到。但如果你只是想，只是很难，对，那warm up也几乎现在是一个常用策略了。像我们的微调训练代码里面也都会去使用warm up。然后这个decay的manner就是指我们开始提到有一个work weight decay，就是权重衰减的方法。这个方法也有很多种，像number one，像GPT都使用的叫做cosine，就是这个余弦的decay方法，也有一些linear的decay的方法就比如说这里就有对这个galaxy galaxy我也不太懂这个模型我没用过，就是用的这个liner decay，这也是一种比较常见的一个做法。
	然后类似的这个precision type wait decay的超参数，是否使用这个梯度裁剪，dropout这个joke out第一次出现在我们的这个课件里，但是是深度学习训练非常常用的一种技术。就是每次训练只激活10%的这个参数，去参与这个前向去前向计算和这个反馈场反馈的更新。这也是伊利亚，就是OpenAI的首席科学家发的一篇paper。所以这些经典的训练方法，这些参数最终沉淀下来了也被hugging face这个transformers这个库收集到了training arguments里面。所以大家还有一个方式就是去看看training arguments里面的这些默认值。这个training arguments本身不同的模型，它都有自己的这个默认值，所以大家可以去研究，也是一个学习路径。那么number one它的这个实现就是用meta自己的这个团队去实现这个number one模型的时候，其实还做了很多的工作，他并不是说把刚刚说的那几个优势集合到一起搞搞数据就完了，确实也做了一些额外的这些实践上的工作，主要其实就体现在三个层面。第一个就是我把我的猫关起来，稍等一下。
	对，然后在实践上面，为了提升这个训练速度，因为他是做预训练，他不是做微调，所以做了一些改进。主要是有三个方面，在paper里面也着重提出来了。
	第一个就是这个注意力机制使用了一个叫做x formers的库，去实现了这个因果。我们现在提英国就是这个Carry这个因果的Martin had attention，能够减少显存的使用和实际去算它的时间，实际实现它的时候，其实就是有一些attention with它没有存下来，可以减少一些不必要的开销。因为整个那些被mask掉的这些key和query是不重要的，都看不见，得生成到它的时候才。或者说大家想象一下，再回想一下这个decoder的结构里面，它是逐步的去解锁后面被mask掉的内容。那些现在还没有解锁被mask掉的内容，他的很多注意力权重、参数都是不必要存的。因为还没被解锁，相当于就跟是今天还是昨天有个同学问，chat GM3在微调的时候，这个label怎么被mask了？做成了一个ignore的一个ID类似的就有很多训练过程当中的，不是全量的这个embedding n的vector都是要用到的那这些不用的，你一种做法就是你给他mask了，然后你让你的模型不要对它有任何的关注。
	这个不关注它的方式就是比如说在softmax里面就给它刷掉了，那还有一种方式就是我直接就把它丢了，我就不让它占我的这个显存，这个是x former的它实现的一个优化。第二个类似就是到另一个极端，我不是不要了，而是说这个激活我要，但是它的计算量太大了。你想象一下就是在计算过程当中有很多激活值是反复计算的。因为它其实参数没改，我的这个训练过程当中，我的参数值没有改，位置没有改，然后我现在又得重新计算一次得到这个激活值。那这个步骤其实是存在了一些重复计算的那有没有可能拿空间去换这个时间，就是把它算好的这个激活值存下来。然后我现在记录一下这个激活值是谁谁谁得来的那当我下一次要去算它的时候，我就不算了，我直接把它加载回来。
	这个是他做的第二个优化技术。但这个优化技术不能直接用patch ch的排到你自己实现的这个反向传播，这个求自动微分的这个方法是不行的。所以这个meta团队自己实现了一个back方法，去完成这样的一个操作。
	还有一款就是上节课刚好也提过的模型并行和这个流水线并行，就他在过程当中，因为它本身是一个非常大规模的2400个，2048个在这个A100GPU上面去进行的分布式训练。所以本身这里就可以做大量的优化，就是我们上节课讲的deep speed里面各种优化。他就使得这个激活计算，就GPU在做激活值计算和这个网络通信的这部分，让他们的在pipeline里面是尽可能重叠的。因为我们上节课也讲过，这个all reduce了，它本身有一部分时间是在通信，还有一部分时间GPU是在计算，这两部分如果能够在时间就是这个time这个层面上能重叠的话，它效率是可以提升的。就流水线并行带来的一些好处。
	然后整体最后number one这个模型，它训练的消耗了最大的尺寸650亿的number one在2048个A100，每个100都是满的。就是这个80GB的这个版本上面训练，然后1.4T就是1万4000亿的这个token的数据集。然后训练在刚刚那个那个规模上，大概需要21天的时间，然后这个是非常夸张的，它换算成GPU hours就是100万的GPU hours，并且这个GPU是A180GB的版本。大家去这个就是明面可以算的硬件的成本了。大家现在如果有空的话，就可以去搜一下国内或者海外的一些公有云。
	A100，然后80GB的版本1个小时是多少钱？你1个小时乘上100万就是训练一次，拿one 65B的成本，你还不能打包票说你训练好它就是可用的。因为它的loss在过程当中可能还会出问题，所以这个就是现在实际的，全世界最强的几个公司搞大模型投入的成本。这里还没有算他额外的一些做实验的成本。因为他真正花100万个小时训练出来了，但可能十倍甚至几倍十倍的时间是在做测试，最终得到了比较好的效果。
	但现在做测试的这个技术也在越来越好，就可能会用小样本，比如说0.1T的tokens，然后先来训练，发现找到了一组还不错的超参数，然后在这个超参数下去做进一步的放大这个tokens，然后再去做全量的训练。就跟试菜一样，我炒菜我一开始不能把过去是把一周的量都拿来炒了，我就先炒一个样这个菜的这个demo，然后你来试一试，这个不错再去放量，这是现在真正去做预训练的时候会采用的一些手段。然后通过这些数据我们其实也能算出来，它的训练速度还是挺快的，相当于每个GPU每秒钟可以处理380个token，这个数量其实是挺高的了。大家如果有有这个概念的话，就能够去知道这个值很高。
	最近比较火的这个，我记得这个蒋青老师，那个创业公司刚刚做了一个叫做search with snap ton。他们的这个项目用RAG搭的一个平台。然后这个平台底层是用这个mixture的这个8乘7这样的一个模型在做的这个RRAG的一个应用，搜索形式的，它的这个是每秒钟200个token，所以其实整个训练过程的效率相当高了，然后他的实验结果我们也能看到，其实number one相对来说比其他的几位老大哥是小了不少的，小了几个尺寸的。比如说palm，比如说这个goal和GPT3都是比他大一个数量级的。但是在各种各样的任务类型上，纳瓦并没有比他们差很多。甚至我们看到这个nama 65B在一些特定的测试上还比他们高，在在尝试推理这个任务上是如此，那么在QA和语义理解上也是一样。这个travel QA就是一个问答数据集，就跟我们开始提的这个，那马里面有一部分叫做stake exchange，像这样的一个名字，它的数据集里面就各种知识问答，然后去回答问题。这个是travel QA的一个典型的数据集的一个类型。
	然后在这个数据集上面他还做了不同方式的测试。就像GPT4当时被劈柴哥发布吉米来二手这个版本的，就是吉米来最大版本的这个模型，说比GBT4效果好。但是当时有一点不公平的对比，就是GPT4是用的zero shot，这个gym I artro l是用的好多轮，可能三十多轮的short，或者说三十多个shot，或者好多轮的对话引导，才把这个最优解得出来。Number one是用同样的方式，zero shot、one shot、few shot和非常多的参考事例之后得出来的一个答案。从这个结果里面我们也能看到，现在的大模型就是你要把它用好。如果我们单讲用它不去调它的话，你给的参考示例越多。并且你确保你给的是参考示例不是干扰它的示例的话，那大模型一定会效果更好的。从很多个不同的数据集，包括现在去刷榜都是这个逻辑，我们能看到在这个维度，包括reading comprehension。
	这个比分上lama也是非常强的，跟5400亿的pum，就第一代的这个谷歌模型和GPT3比起来，那GPT3肯定跟这个二三年的模型没得比了，至少它的模型规模是很大的。Palm也是一样，one是取得了很好的一个成果，这也是整个开源社区非常满意，就是原来这个650亿就可以跑一个非常好的模型。而且我们知道如果用了A180GB，然后去做lama的这个65B的q lora是能跑得起来的，这个大家能算过来账的，是能跑起来的。所以相当于对于开源社区来说，一张A100就能够去做nama two或者nama one的下游微调，这个QO rua那就能搞各种各样的领域模型了，这也是它能开枝散叶的非常重要的原因。底子够好啊，然后大家还能用Q罗A来在单张上面去做下一步的调整。
	在一些数学和代码生成类的任务上，lama的表现也还是不错的。但是相比pm还是是可以讲的，但是相比一些专门对数学和代码有过优化的，他就一般般了。就比如说在我们的这个max和这个GSM8K，这个应该非常熟练了。
	这就是个小学应用数学题的数据集，纯人工造的。在这个数据集上我们能看到65B的lama和咱们的这个palm比起来，如果是同尺寸的palm。显然nama要好得多。但是如果比起5400亿的他M的话，要略微有一些差距。毕竟这个模型的规模还是只有650亿，尤其是这个数学任务本身在lama的训练集里就没有专门做过强化。大家去回想一下刚数据集的类别，它的构成成分里面就没有多少数学题。但是这个能力又是现在所有人都认为非常关键的一个能力，因为数学能力在某种程度上或者说做数学题的能力在某种程度上体现的是这个模型的逻辑推理能力强不强但不过这又是一个新的特别大的坑了。
	简单提一句就是整个语言模型的发展，最新我看到的一篇paper，最近在讲的就是这个，也是伊利亚在提的。我们现在的语言模型发展走到了一条线的极端。就是你可以叫在往之前我们的应用开发课也给大家提过，就几十年前就有一个科学家和哲学家提过两个大脑的这个概念。一个是且这个快的思维，就是脱口而出的这种东西，是现在语言模型天天在研究的，就是看书，然后这个无监督预训练去折腾的东西。
	但还有一类是深思熟虑，像数学的这个逻辑思考这条线呢叫做辩证思维能力。对于我们的语言学也好，语义学也好，而这条线呢其实现在的建模还非常早期。这也是我们现在大语言模型为什么好像逻辑不太行，就是只是文科生不是理科生，没有经过理工科的训练。那理工科的训练到底是怎么训练的？其实人现在也说不清楚，然后人说不清楚，也就见不出一个很好的语具有这个思辨逻辑能力的语言模型。但可能也就两三年的时间，这条线应该也就会有一定的突破了。
	如果这条线有比较好的突破之后，那一个千亿的具有两种思维模式的语言模型应该就会真的比现在又有一个大的台阶的提升。那会儿可能就是会有很多同学焦虑，真的会下岗了。因为他他不只是一个文科书记员了，他是一个有思考能力的书记员，那这个上限就很高了。对，这里稍微延展了一下，代码生成能力，这个相对来说比数学表现要好一点。
	他数据集里本身有get hub，然后它的多任务能力，这个MMLU，这也是非常常见的一个评估数据集了。我们在不少的模型里都看过，它就是各种知识领域，语言理解能力都拿来测，然后在测的时候，我们看到拉马相比于这个GPT3是有显著的进步的。跟这个GPT6X，这个GPT6X就是我们刚刚提到，lama在参考它的旋转位置编码的一篇paper，用到的一个文章。用这个文章里面用到这个技术，跟他比起来，也还是有这个优势的，只不过跟这个pm的5400亿比起来确实是有差距，这个没有办法，差了十倍，很夸张的一个差距。但是在一些其他的能力上，没有差太远，跟其他的模型比起来好，我们大概对lama one本身这个模型有了一点初步的了解。
	我们再来看看lama的衍生模型家族有哪些首先回顾一下在nama出现之前，大于100亿的这个模型，其实是百花齐放的，我不知道大家有没有关注过，就各种各样的模型，这种图也有很多。然后google家的有这个encoder decoder结构的T5，还有T5的衍生品。Auto encoder这种结构的bert和bert的这个衍生品，然后这个GPT参考这种GPT auto regressive的模型也有很多衍生品，还有像GOM这种另辟蹊径的这个广义线性模型的，做了一条线。但是娜娜出现之后，我们发现他自己独立的开枝散叶的更厉害了，上一次看到这幅图应该是以bert为主的模型的一个衍生。我相信很多同学应该都看过，是清华的一个做这个GOM那个大实验室出的一个图。当时是在19年20年的时候，海军face上面transformers最早实现了bert的开源，然后开源版本实现之后，大家都在上面做研究。二三年的热点就变成了基于纳马去做各种各样的衍生模型。
	而基于它的衍生模型里面，又有两个尤为突出，一个叫做阿尔法ca一个叫做vicuna，这个阿尔法克是stanford发布的一个在非常低成本，我记得当时发布的时候是几百美金就能去训练出来的一个模型。然后是用合成数据做的一个模型。这合成数据就跟咱们有一节课用GPT4来帮我们去改造这个问题，或者说去帮我们构造这个问题，然后来训练chat GM3是一个逻辑。这种方式现在几乎是主流的一个方式了，就是人来造数据的成本太高，尤其是你不是像OpenAI、meta这样的大公司的话，那用GPT3、GPT4、GPT3.5来造数据是很合适的。哪怕是大公司也这么干嘛，对吧？我们就不说是哪家了，肯定很多都这么做的。然后像我库纳这条分支，更多的是增加聊天对话的数据。就我把人跟人的聊天的数据加到原来的这个lama模型里。显然我put的成功直接就使得他发2，自己就去加了很多人工的数据，聊天增强的一些数据，包括人工标注的数据，所以这里很有意思。然后这条线呢它也不是说一个的单一路线，这里的每一个叶子节点不是都完全不同的一项合成数据里面有去加LHF的，也有在阿pack上面去加这个对话数据的。
	其中大家能大量的发现有一些中国的公司和团队都是在lama的基础上去做的。包括像百川这个灵异智能，都是基于纳马在做研究，为什么？其实前面有一张图能回应大家的问题。一个国内的初创公司要投入100万个A100的GPU小时数来做公司的创业启动，这个是一个天文数字。就A100应该1个小时的费用是在小几十人民币，那相当于就几千万下去了，很夸张的一个规模。然后在这条线里面，我们除了看到这两个就阿帕卡跟这个ma之外，我们还看到一条线是chinese data就是中文的这个数据，这里就衍生了很多open chinese lama，还有其他的一些像这个panda chinese lama等等各种各样的数据。然后在这条线下面又去加了阿帕卡data，做出了这些律师等等。
	这里就再讲一句，因为我也是这两天才刚刚拿到这个纳马二的模型权重，所以咱们的实战可能会放到年后，以一个直播的形式，就相当于放到年后再来给大家讲这个实战的内容。华为的这个硬件也是一样，我们在做这个课的时候，华为是有这个升腾910给我们去租用的。然后最近好像他们年底就已经租完了，此时此刻是租不到升腾910的。但是我们也跟华为联系了，在年后他们会上一批升腾910。到时候我们可以租了这个升腾910，再给大家去做实战的直播。本身其实我们规划的内容16周是比较完整的。但过程当中因为涉及到了一些应用开发课的知识，相当于补充了两节课是给大家做科普的，所以就有两节课的内容去做了延后，所以大家也不用担心，那两个实战没有了，我们会放到年后找一个合适的时间，华为云OK的时候，我们来做对应的这个实战。
	好，我们接着来讲这个阿帕卡跟维克纳这俩在nama下面最火的衍生模型是有什么样的一些特点，又是怎么训练的这幅图里面也区分了PEFT和这个FFT，大家如果有细细看的话，不是就是不是这个shape for不是这个颜色块全部填充的是PFT，全部填充是FFT，也做了比较细的一个区分，整理的非常好啊。整个nama的衍生开源模型的生态是非常繁荣的一个状态。我们在hugin face今天去搜的话，像阿帕卡基于他的这个模型是两千多个，我哭也有1000个。所以就这俩带着这俩名字的模型就是三千多个，这个是非常夸张的一个数量。然后阿pack里面大家也能看到有把这个GPT two，然后阿帕卡GPT4这样的一些模型，我能猜测可能是用GPT4去评估，然后在GPT2跟这个上面改的，我猜，然后还有一些chinese的air parking的模型，然后我也是类似的各种各样的版本。大家在这个hugin face上应该能找到这些模型的可用的衍生版本。跟咱们的实际落地的场景去做一些比对的话，要么就是在上面再去做一些微调。
	阿帕克本身是stanford的一个，当时我估计他们也没有想过有这么火的一个项目。这项目就以一个stanford的blog的形式发布出来了，然后当时当天还发了一个线上的demo，那这个模型其实解决什么问题？就是在最开始的时候就是GPT GPT3，以GPT3为主，这一批GT3的API发布了。又过了一段时间，ChatGPT发布了。然后学术界其实像斯坦福大学也好，其他的一些美国大学也好，他们都很困惑，然后也很焦虑。就是他们没有办法做研究，GPT3太贵了，就跟我们此时此刻绝大部分的团队一样，没有办法有足够多的资源去复现和进一步的去研究这些千亿的大模型。那怎么办？其实南马某种程度上也是在解决这些问题，就是让AI真正的民主化，让绝大部分人能用起来这些大模型。
	在这样的一个背景下，其实阿尔法卡这个项目是首先回应这个问题，让学术界能够参与咱们这些新的大模型，然后参与的方式就是的低成本，然后性能好啊，这是他们的一个最大的目标。那他具体怎么做的，第一就是数据怎么来的，我们刚刚看过那条线合成数据谁来合成的？Text达芬奇003，这模型现在都下线了，都合成不了了。是在ChatGPT刚上线应该是三个月之后，发了这篇vlog这里当然能看见这个URL对，应该能看见，没有被人脸挡住。我先大概知道人脸的位置。
	这个paper大家能看到这个stanford 3月13号发了这篇阿帕克的文章，是一个7B的模型，就是跟着nama 7B来的。Nama 7B应该刚发没多久就发了这个阿帕克7B。然后这个方式其实也很简单，首先用GPT3来用这个叫做self instruct，就是自我指导的这个指令，然后去生成数据，生成多少条呢？生成了52000条，这个example相当于52000条样例。然后这52000条样例用来干什么呢？去训练我们的拉马奇B然后得到了一个APEC7B，然后给到达芬奇003，就GPT3这个模型的self instruct这种指令是长什么样的？其实下面有啊，就比如说brainstorm a list of possible new year's resolutions output，什么lose weight、exercise more IT healthy，这个是他给到GPT3的一个样例。
	然后像这样的example有175个，这175个种子的任务就相当于给这个003这个达芬奇003去说你要生成类似于这样的一些指令，然后它生成的指令其实就跟右边我们看到一样，比如说brainstorm creative ideas for designing，conference room这个模式其实是非常好用的，包括你看咱们去做合成数据也是一样的。我们给到GPT4，让他去基于一个问题，或者说基于一段内容去做提问，或者去巴拉巴拉。就这种方式大家一定得学会。因为第一在GPT本身是免费的，你能拿下来GPT去做生成，你哪怕慢一点，或者你用这个GPT3.5的这个turbo便宜的版本去做内容合成的数据的生成，是一个性价比极高的生成数据的方式。在这里他们调了达芬奇003，这还是最贵的模型。当然如果在下线之前用过的话，你会发现GPT3这个API是OpenAI非常不想让大家去用的，以至于它的价格在下线前是比GPT4还要高的。然后他们用了这样这么贵的一个API，然后生成的52000条的这个example去meta的这个nama 7B上去做了监督训练之后，得到了这个ipad 7B，整个当时的成本是不到600美金的。
	还有提到然后训练的这个时间，我看他们的这个blog里面也有提，应该是八张A100训练了3个小时，其实是相当于就24小时的GPU的这个时间，跟那个100万比起来确实差的多，这个差了很很很几万倍的这么一个成本，但他就可以做出来了。做出来之后，当时的这个生成的效果还可以，是跟您您怎么应该叫二三年3月份的ChatGPT，感觉是差大差不差的。在一些特定的例子上，但肯定离真正的这个HI GPT还是很有差距的这也是阿帕卡后来就没有，就好像这次起了一个大早很火，但是这个模型本身并没有再火起来，但是它这种造数据的方式火起来了。就是阿帕克这种造数据的方式是让大家认知到是很香的一种模式，很多人在镜像的追随。
	而这个瓦库A就是stanford后来又跟很多高校一起做的下一个模型，反而是质量上要高了不少，这个是ipc 7B当时做的一些事例，就他通过合成数据，然后去训练拉玛之后给了一些示例。比如说他去问什么叫阿帕卡，这个肯定是合成数据里带的它跟lama有什么不同？因为lama是它的基座模型，那么这个模型自己有回答，这个阿帕克是一个uh dea demonstrated spaces of说一大堆，然后什么写一封email，这些肯定都是他的175个instruct instruction里面有的指令，所以它的结果还不错。但如果你的这个指令不在它的训练集里，那它效果就比较差了。
	这就反过头来告诉我们，如果我们要在哪马上去做指令微调，使用阿帕卡这个pipeline去造一些你的下游应用场景里要的一些特定指令，是非常好用的。尤其是说你的这个指令是比较聚焦的。比如说就干那么十种活儿，只不过是说这每种活它的描述方式有很多种，比如说red and email，比如email me，email him之类的。这个可以变化，就跟我们去改变提问方式是一个逻辑。那么库是一个什么样的模型它其实是有好几家研究机构一起推出来的一个模型的研究机构，就包括这个uc berkely UCND ago CMU，stanford等等这样的一些机构一起做了一个模型。
	这个模型是在number one 13B的基础上，去用这个监督数据来做微调的。然后这个监督数据不是合成数据，是用户真实的聊天记录。聊天记录来自于一个叫做share GPT的网站。大家知道GPT的聊天记录是可以share的，我们的课程里就share过对吧？把我去合成数据的这个GPT聊天链接share给大家了。所以share GPT就这么一个网站，可以去让大家把自己的聊天记录分享过去。就跟我们AI的vest模型使用的这个common voice一样，大家可以把自己的语音上传上去。
	那么在share GPT上面用了70K7万条用户对话的数据拿来训练lama 113B，这个是voci na这个模型怎么来的基座模型用的什么数据。然后同时这个训练过程当中，去修改了阿尔法卡的训练脚本，然后它能够支持这个多轮对话。因为阿帕卡本身是一个指令跟随的这么一个生成模式，就相当于给一个指令让我干什么活。而vokins a是想做这个是想做这个对话的，所以他把这个序列长度也增长到了2K，然后这个过程当中就各种各样的条比较有意思值得大家分享的。第一个就是它的评估是用的GPT4来做评估。就是说沃克的这个模型他训练的好不好，他让GT4来做裁判来评分。
	然后还有一个项目是我们在一起参与运行的一个项目。这个项目就是一个分布式的一个所谓的聊天的demo的一个项目，叫fast chat。我今天试了一下没跑通，我不知道是不是我的网络问题。大家有这个时间，有这个机会去试一试的话，左下角是这个主页，它也是放在一个blog上面的。然后这个blog上面有这个fast chat的这个链接，大家可以去试一试，也是用radio搭的一个对话。这个里面是体现了有多个模型可以去PK，这是我们下一页要看到的。在blog里面是可以去做测试的，它连接的模型比较少，展示了它的训练方法和训练流程。
	就比如说这个category是类型，就是我们现在在做什么样类型的事情，这里有不同的question就是你去提问。然后这个就像打擂台一样，你选了两个不同的模型，就是这里的assistant one，这个assistant b assistant to，然后这个assistant to选择是这个13B的这个word，比如说这是它的模型hours，左边你可以切好多，比如说这个阿巴克的13B，或者说这个ChatGPT的3.5都可以。然后问题就是这里的这个问题，比如说你可以挑各种各样的问题，这个问题会同时发给这两个不同的模型。这俩不同的模型会由GPT4来做evaluation，做评估。这个评估就相当于他给这两个模型的回复去做打分，还会说明原因。比如说他说左边这个阿尔法卡13B只有七分，右边这个有十分，原因有一个说明，给了一些这个得十分的给了一些detail的和engaging的travel blow。我说的更细，然后跟ChatGPT3.5也可以去做对比，因为他本来就去调这个API就好了。
	这个是一个很有意思，也是未来的能看得到的一种方向。我们的模型生成的结果是是自然语言的文本，就像QA的翻译就是这个问题，生成的是一堆自然语言的文字。然后这些文字，你单纯的去跟一个基准测试的数据集去比。比如说你说这两个人字是不是一模一样的，就跟背标准答案一样的。那这种评估方式本身就有问题，就像死记硬背。任何一个question只有一种answer肯定是不对的。
	包括今天我发到群里的这个文章也是一样，大家都在探讨基准测试被刷烂了，这个chinese evaluation这个很好的中文的评估集也被刷烂了。如果我们没有办法去改进这个评估方法，那么这个测试集一定会被刷烂的。因为这个测试集就是死记硬背，我有可能这个模型甚至就只为了这个测试集评估集去死记硬背。
	他能在这个榜上刷到无限高。你像GPT4现在连前十都进不了，他们在榜上刷的无限高。但是可能他只能刷榜，他别的事儿都干不了。因为他已经完全的过拟合了，他只会回答这些问题，这个是很有可能的。而且在刷榜的过程当中，其实本身测试集就会泄露的，这个是有办法去给他搂出来的，具体怎么弄我们就不讲了，这个不算什么正道那文章里面也有提过一些对啊然后呃所以未来这个大模型的分数也好，能力也好，怎么样去做评估，一定是有更加的柔一点的，不是那么硬的评估方式。我姑娘至少用GPT4当前看下来最强的模型来当这个裁判，是一个相对来说令人信服的一种做法。只不过GPT4的这个evaluation本身应该给他一个什么样的prompt，让他来做评估，让他来稳定的评估是另一个研究命题。
	但是这个方向肯定是我觉得是很正确的。就是说你要让你要去训练一个模型接近GT4，相当于你有两个学生，他们要比试谁的学习成绩更好，你肯定得找一个比学生成绩好的老师来。而这个老师，因为你现在又不是做数学题，只有一个答案。你做的是语文、历史、文史哲相关的题目，那就跟写申论似的，对吧？申论可能都还要把股的更更format一点，但是你现在做的是这种generation本身就是要不一样的，要多样性，要高temperature。那用GPT4来做evaluation是靠谱的。所以这个是我们展开讲的一个探讨。
	它的数据集有一个对比，就是nama阿巴卡和这个va包括bard和ChatGPT，主要对比前三个，我们能看到数据集上面首先是少了非常多lama 11000小版本的，因为他们都用的小版本，一个是7B1个13B都是11000，没有用1万4000亿，然后阿法卡是合成数据52000，图是7万条用户分享的这个GPT聊天记录。对，规模小了不少，万亿和万少了1亿。这个很夸张。然后有没有去训练这个code，这里其实是打个问号，这个是我库纳的官网给出来的比较有没有training code？从拿马自己的这个公开来说，是有github的，所以我不太清楚这个code是什么概念。这个training code，然后这里的evaluation matrix GPT4作为评估依据，这是瓦库纳的评估方式。阿尔法卡是由人来做这个评估，用户或者是这个专业的标注人员。
	然后lama本身这个基座模型是做各种各样学术认可的benchmark，就是至少有论文的benchmark，然后训练的成本也不一样，82000的这个GPURS，然后13B的话是135000的GPURS。如果是65B的话，就是100万的GPURS，这个成本不太一样，还有一个大概的估算，所以阿帕卡和沃克纳至少让绝大部分普通人能够用1万人民币以内的成本去训练出一个模型了。这也是为什么哈根上面现在有3000个经过审核的发布出来的他们俩的衍生品，所以这个其实对于整个大模型的生态繁荣和各领域就可用的数据和可用的数据造出来的模型是非常有好处的。就像今天大家如果要去写个软件代码，会先去github上面搜一搜有没有开源的。未来我相信大家要去做一个有AI能力的应用，也是类似的方式。去找一下网上有没有对应的已经开源了的这个模型，这个模型的部署成本又不高，那就可以用起来直接用，或者说加prompt用，或者说微调之后再用。
	这里我们再简单看一下，就是这些主流的大模型，他们的预训练数据的一个构成对比。比较有意思的事情，我们看到像妈妈，她已经开始去不像这个T5和这个fano，他已经逐步开始不只是使用互联网的数据了，然后开始去使用这个code和这个对话数据。然后越往后面出的这些像我看这个阿尔法code，这个是纯代码的数据。然后palm其实是code只用了5%，用了很多conversation的数据。然后今天是我们之前讲过的google出过的一个通用语言模型，这个通用语言模型里面这个外page也只用了48%，然后这个books and news和这个conversation data用了3分之1。所以这个比例其实现在大家也都在探索。但有一个现象是说，只要是百亿，或者说你不是要做一个足够通用的大模型，互联网的这种数据80%的这个比例是差不多的。这个可以去看lama GP3这种比较典型的。
	像这个pum本身是5400亿的一个规模了，它的第一代的模型现在也不再是它的主流模型了。因为它相当于投入产出比出现了不及预期的一个情况。那么他们two的一个训练数据已经不再像之前这样这样的一个调这样的一个比例了。这幅图其实值得大家去看一看，就是我们不同的大语言模型，他们的训练目标和他们的一个比例构成。但现在越来越多的大型模型会把数据来源做的更加的多样化。就像我们看到下面扣的近，包括这个galaxy看啊，他们用到的这个比例都不一样。然后像这个galaxy卡，它可能更加偏向于科研的领域。
	如果咱们要自己去做一个几十亿的模型，或者一两百亿的模型，然后这样的一个规模然后去做下游的预训练，做这个知识的这样的一个灌入。大大概率就是会相对来说一开始可能会花很多的资源。比较好的方式还是找一个已经预训练了比较长时间，然后在各种学术的分析mark上面效果还不错的预训练权重，然后再去做微调是一种比较好的方式。而这个微调因为你本身已经准备做预训练了，你肯定是有资源能够把这个权重全部加载进去的，而不是去做这个锣A那这个时候就是去做全量的参数微调。就像我们开始看到的这幅图一样，我们刚好说到这儿，就这幅图一样，更多的可能你会是这个full翻译tuning，four five toy就是带了这个呃填充的这一类。实际上国内的很多大模型公司和一些高校也都是这样的一个思路。比如说我要学法律条文，我要学金融知识，我要学医学知识，我要增加他的语言能力，几乎都是把lama的基座模型拿来做全参数的微调，或者说你拿了一个lama的65B那你去做PEFT是可以的这是一个比较常见的实际的训练方法。
	接着我们来看看这个拉马图做了什么事情，很酷的一个logo。Lama two很多的开源爱好者们给lama的这个羊驼做了各种各样的表情包和图片。那么nama two相比，一做了什么样的改造？简单来说就是第一上下文长度提升了，就nama一是只能支持长度为2K的，然后阿帕卡是只能到0.5K512，我空是给它做到了跟number one一样的2k number two是长度支持4K的，然后训练集做了扩张，从1.4T做到了2T并且是全量的，就是所有的nama two的模型，哪怕是7B的，也是训练了2万亿的tokens。然后learning rate没有什么调整，在lama 2的基础上，这是跟lama one比起来几乎没有什么变化的一种方式。只是改变了训练，相当于多加了一些训练数据，然后把最大尺寸的模型增加了，这个5B就是50亿的这个参数规模就没有变化了。这是na two跟nama one的机种模型。但是nama two在机种模型上，他竟然看到开源社区都干得这么火热。自己是不是也能像刚刚那个羊驼的那个生态图一样，官方的去做一些扩展呢？显然这个事儿是靠谱的。
	Nama two的chat模型就是把这个RHF这一套做到了nama two上面，就相当于我们先用2万亿的token预训练得到了一个na 2的模型。然后像ChatGPT1样去找很多的标注人员，标注了100万的人类的标注数据，来进行RHF的训练。然后得到的这个模型叫lama to chat。这个流程其实跟咱们以前讲的这个RHF，就是在ChatGPT里面提的那一套是很相似的。
	就预训练一个模型，然后预训练出来的模型就是左下角的这个南马土。我知道鼠标看得见吗？好像看不清楚，南马土。然后这个nama two跟人类的反馈，human的feedback，这里有一个reward模型，这是一个强化学习的一个奖励模型。这个RM也是要单独去做训练的，然后这个RM最后就会把妈妈two生成的结果去做一个打分排序，用PPO来做这个强化学习的一个优化，然后去迭代这个number two chat这个方式和ChatGPT，包括其他的这些RHF的方法都是一致的，这我们就应该之前讲过的，我们就不再展开讲了。结果来看，所以大家去理解一下，就是na 2不再是只给大家做基座模型了。他自己现在也有各种想法，包括最近刚刚出了二的这个code，还是叫code number one two一个版本的模型，也是专门去做一些代码相关的工作，然后出了一系列的模型，各种各样的数据训练。
	Na 2的不同四个尺寸的chat模型，在安全性评估方面是非常高的。这个也是meta一直在强调的一个重点。所谓的安全性的评估就是不要产生一些叫什么有违公序良俗，LGBT或者各种各样的偏见的内容，然后这个值肯定是越低越好啊，相当于你是安全的那跟一些其他的开源模型比起来，比如说我们的版本，包括甚至palm的版本，ChatGPT的三月份的版本，它都还有一些优势。这个是第一个很重要的一个评估，当然这个是人来打出来的一个评估，就是由人类的评估员标准员根据2000个提示打出来的一个分数，表示na 2是是一个听话的AI是好用的。
	第二个就是对话能力，对话的一个性能对比。在这个图里面，二的chat模型去对比了跟各种各样的一些闭源模型去做了，也有开源的去做了单轮和多轮的一个对话，比如说single turn market turn。然后这个单轮和多轮的对话里面又分了这个win，lose就赢了输了或者这个中间你可以认为是不赢，一个僵持状态。那么这个win的话肯定就好，这个深蓝色的高的话就足够好。明显看得到其实在跟这个MPT7B，它它是同尺寸在比，没有欺负人。然后和这个防控，包括和palm比起来都有明显优势，甚至说跟ChatGPT的三月版本比起来，他还有略微的优势。这个是还蛮惊艳的。这也是拉玛图出来之后，好多人看了这几张实验结果非常兴奋的一点。
	相当于你用一个700亿的模型干赢了GPT3月1号的这个版本，3月1号的版本应该CPT3.5，也是一个几千亿的模型了，所以而且又是对话能力，那前面是安全性，这两个性能是非常重要的。就相当于ChatGPT的chat能力和moderation的能力，都已经跟这个number two 70B差不多了，那么训练成本，这个也是我们需要去关心的一个点，就是对整个大语言模型的预训练。大语言模型做这事儿需要多少资源，大家有一个直观的感受。
	最后一节课我们可以讲这种事情，免得把它全退了，在number one的训练上，我们知道65B的number one的训练时间是要应该是100万出头的一个GPU hours。我没有记错的话，那么到了这个大妈兔70B的时候，70B本身是需要172万个这个GPOR。如果然后这四个肯定是分开训练的，这个大家得理解，参数规模都不一样。分开加载训练的总计是330万的这个GPU hours，这个是巨夸张无比的。
	然后刚刚我们提到的这个碳排放，meta自己还在论文里面解释了。第一是说在在海外的公司都有碳积分，已经做的很很成熟了。我清楚大家了解这个概念吗？就现在国际的各种环保组织，是要求法人，就是公司是要要有碳积分的，你得干些什么活才能积累碳积分，才能抵消你的碳排放。但这个不是强制性的。
	然后在这个meta的paper里面有讲他们消耗的这个碳排放的量，有一个单位叫什么，二多少吨的二氧化碳的当量，叫做T，就是蹲二氧化碳一口当量，然后是539吨，然后他说他的这个碳积分是远远超过的。所以大家也不用担心，这个模型是环境友好的。其次话锋一转说因为他把这模型开源了，所以大家不用重复来训练了，你们就直接用这个na 2就好了，还环保。所以这个画风运转其实有两层含义。
	我觉得个人的解读，第一就是说确实这个很环保，不是人人都花得起这个钱，然后也不是需要排放这么多二氧化碳。但同时另一个方面想就是在逐步的想要去收敛这个底层大模型，让大家都用它的这个大模型作为基座，就跟百度想要做的事情是类似的，大家都去做应用，底层的模型交给我们，我们来把控这个呃这个相当于石油一样的一个逻辑，但这个就仁者见仁智者见智了。我们对这个nova two本身有一个整体性的概念，总结一下就是序列长度变成了4K，增长了一倍。训练的数据量从尤其是最小的这个版本，number one 7B和numa 2的这个7B训练数据量也增长了一倍。那边是1万亿，这边是2万亿的投肯数量。
	第三个就是说整个nama two的7B是有一个经过RHF的一个nama two 7b chat模型的。然后这个chat模型是我们真正拿去用并且非常好用的一个模型，这个是nama two的一个迭代，然后nama two和nama一在架构层面上没有本质的一个区别。他的这些关于transformer的改造，其实就是我们刚才那么一看到的那些成果。不过他在训练方式上加了RHF，这个是值得去好评的，也是一个新的技术强化学习，来提升拉马二基础模型的能力，让它的安全性得到进一步的保证，能生成一些符合人的价值观的一些结果，对其的更好，同时还能把这个对话能力也做了提升。
	最后我们来花一点时间跟大家讲一讲怎么样去获取a two的这个模型训练权重。这个我是自己吃了亏的，我自己是直接在hugin face上面老早就提交了这个模型权重的访问申请，应该是在九月份还是八月份就提交了，但是一直都没有给我审批。直到今天跟朋友们交流了一下，说你这个方式不对，不应该去hugin face上面申请，那个没用，应该去哪儿呢？应该去meta的官网，就很搞，meta自己其实官网有这个表单可以去申请，但是他在hugin face上面也有表单，也可以去提申请。实际操作下来hugin face那边的申请他应该是不看的。然后meta自己官网的申请你填完之后，就是你点这个，就我这儿给的这个官方的链接，就是你去了meta的官网，meta I的官网，你点这个download model，点完之后，你就可以填这样的一个表单。
	这个表单填的内容里面只有一点需要注意，就是目前科学是没有国界的，科学家是有国界的。在大模型这件事情上，科学也是有国界的，所以country不能选china。如果你填这个表单的时候，你啥都选好了，最后country选了一个中国完蛋。直接告诉你你所在的区域不开放，没法用，所以你得随便选一个国家，但他不会真的去审核，就哪怕你所有的内容都跟你平时填的一样，country随便选一个。我不确定这个世界上还有哪些国家也被他屏蔽了，但至少china是不行的。
	你选个什么united states，它它就通过了。然后这里其实就可以选，填完之后就可以选你要去拿什么样的模型权重。Lama 2是它的几种模型，lama check是在几种模型上面加了RHF的模型。这俩我们刚刚讲了，code lama和这个lama gard是它最近出的一些新版本的模型。Code主要是在代码生成上去做了一些能力的增强。
	下面这个我还没有实际去玩过，这个不太好去做实际性的评估。但这三个你都可以打勾，打完勾之后，你会分别收到3封邮件。这三封邮件里面就会告诉你，你获取到了nama two这个模型权重的这个license，然后包含哪些模型权重。比如说这一篇就是第一个那个勾会返回你不同尺寸的。然后我们就能看到他的论文里面是四档不同的模型。但有可能第三档的这个35B的模型，它的效果介于这两个之间比较尴尬。原因不知道，反正最终是没有开放出来的，最终只开放了三个尺寸的模型。
	然后你可以通过这个邮件拿到这样的一个customer UIL独一无二的每个人啊不要把它共享到网上。然后拿到之后，他会我不知道是不是facebook跟这个hugin face有beef，反正他不会引导你去in face。整个流程里面，in face是一个很尴尬的状态。这也是现在没法给大家上这个课的原因。大家也得这节课之后先去把这个审核给到位了，能够下载这个模型。
	下载这个模型也不是用hug in face，是用这个github上面的一个脚本文件，一个需要脚本，然后大家需要去github的这个网站上。就是facebook research，就整个facebook内部现在大模型资源是炒的比较厉害的。因为有些朋友就在大模型的团队，刚好还几天前还去参加了他的一些吃饭什么的，在聊聊起这事儿。就整个meta facebook research，meta AI然后OPT就这一堆。Facebook这个公司下的大模型团队，包括它的这个XR做这个眼镜的团队，就很多大公司的典型问题。然后anyway我们回过来就是大家去github上，就是这里这个链接去去看一下这个项目。然后会有一个download这么一个脚本，去运行这个脚本。
	然后运行这个脚本的时候，它会告诉你让你去输入一个UIL，从email里面的UIL里面，从这个email里面把这个UIL给复制出来，然后输到这个命令行里，他就会给你对应的一个反馈。就是我们开始提到3封邮件，第一封邮件给大家展示的是nama的这个lama two和lama chat的UIL。如果你同时打了另外两个勾，还会有额外的两封邮件是不一样的UIL，然后你就可以输进去，他就会告诉你这个叫做你要下载什么，就跟你在邮件里收到的那个表单是一样的。然后如果你啥也不输，它会默认要把所有的都下下来，那就比较费硬盘了。比如说我这儿下载的是7B，这个是7B是它的几种模型，你再下载一个7b chat，就可以在这两个上面去做进一步的微调，下载之后它就开始正式的下载，直到最后下载完，下载完了之后，它也是，我印象当中，这幅图没有接到，应该也是用patch ch实现的，用PyTorch的这个格式去保存的这个模型权重。所以这个流程大家搞完之后，应该本地就有lama two或者lama to chat的模型权重了。然后我们年后可以找个时间给大家直播去实战一下，怎么在这个模型上面去做进一步的微调。
	好，那么今天的内容就是这些，看大家有什么问题我们来分享一下。然后整个这个课程，其实除了国产化的升腾，这个其实是强调硬件，怎么样去用国产化硬件来做ChatGLM的微调，然后我们这个硬件本身，它确实有它的国内的一些应用场景是大家必须要去用的那我们讲一讲，但除开这个硬件的技术，其实我们都已经在ChatGLM相关的内容里面都讲完了。然后lama 27B又是一个比较特殊的模型，它跟chat GM的技术路线都不一样，然后它的这个预训练的内容也都是英语，所以我们要在那个27B上面去做的这个内容，可能也是一个英语的数据集，然后再去做扩展。也不排除我们找一个已经是南马兔7B训练过中文数据，再在上面去做训练的这个数据集。也有可能到时候我们看找一个什么样合适的数据集来给大家讲这个实战。看大家有什么问题，我们来一起讨论讨论。
	大家不会都没有问题了吧？没有问题就新年快乐了啊。
	7B的，我们也是用这个lora的方式。针对多轮对话的微调，一般要怎么做？这个主要是涉及到咱们的这个数据集的设置，这个问题我展示一下，跟咱们使用的模型不一样。对，就比如说这个多伦对话在nama里面。
	比如说在这个chat GM，我们看到它的这个微调其实是有多种格式的。我们没有讲这个对话的训练，因为这个训练流程数据太复杂了，那应该是在。
	高效微调模型，微调套件。
	比如说这个多轮对话的数据应该怎么调？有的同学在问多轮对话的微调它的数据会不一样。你可以这么理解，就是你要进行多轮微调有两个前提。
	第一个前提是这个模型本身它是支持对话的。就像我们看GPT3，就是达芬奇003这个模型，它只支持OpenAI的completions接口，它只支持一个文本生成的接口。它的GPT3模型本身是没有对话的概念的，也是没有有角色的概念的。他必须要在预训练阶段就针对这个对话去做个训练。相当于这种这个大模型，它是能对这个对话的这种prompt去响应的才有用。你像ChatGLM1，它也不支持这个对话，我没有记错的话，至少不支持多角色的这种，比如说这个function calling，system这样的一些角色。那也都是他的预训练里面加了足够多的预训练数据，它才能够支持的。所以这是第一个前提模型得支持。
	第二个就是你要去训练多轮对话的示例的话，你的prompt，你的训练集也得做成一个多轮对话的格式。并且不同的支持多轮对话的模型它的格式不一样。对于chat GM3来说，它要求你的多轮对话的格式是长这样的，就是一个外层是一个像队列一样的一个list，里面是一个一个的对话记录，相当于你可以传很多条对话记录，就像我们的瓦库纳，是叫瓦的。我那上面能够去用share PPT的7万条对话记录，那这个conversations里面就是一条对话记录。然后这一条对话记录里面的这个内容就是一个roll一个content。然后显然这个chat GM3它是支持system这种road的。但是function tony就是我们看到function call这个tools这个功能又是得单独去训练调整的，它没有放到这个conversation里面，是一个单独的一个内容开头的，tools就是对应着OpenAI的这个function call。但这个比较尴尬，因为本身这儿其实有说明，这个拓不是他的原生角色，所以你得单独调。
	然后这个对话我自己尝试过，因为要造很多对话，这个场景挺难想的。就是你想象一下就挺挺就大家如果有多轮对话的数据，可以私信我。如果这个数据确实是脱敏的，就是值得拿来做多轮对话的微调训练的，我们可以讲一讲。不然的话造一个多轮对话其实挺挺分裂的对，很难去体现出他的这个效果确实提升了。这个是多轮对话的一个回复，就是两个前提要具备，然后你得自己去造好这个数据集去试一试，它的核心本质上还是对于这种格式的一个响应。
	微调预训练分别适合什么场景？如果要落地业务场景有什么差别？预训练成本好高。还是那句话，就是你想要但凡你想要去做预训练，你可以先来私信我，告诉我你有多少预算，你准备怎么干这事儿，甚至可以给你提供这个咨询服务是吧？为你想你只要预训练，那就是几百万往上走的一个事儿了。你首先想一想你能不能决定几百万的资源调用，如果不能的话，你就忘掉预训练，这事儿就跟咱没关系，你就想想怎么把微调做好。
	那怎么做微调呢？其实刚刚这幅图就是给大家一个非常好的一个示例，就lama的这这幅图就这一张，这个其实就是基于lama去做微调的生态，刚看到已经有三千多个模型了。然后你想象一下那些现在在中国估值好几亿、十几亿的公司，也就是拿着nama去做的微调。
	所以你问我什么场景下用微调，什么场景下用预训练？如果这个劝退型的说法就是不要用于训练，只用微调。只是说什么场景下用全量微调，什么场景下用这个PFT。如果你的数据比较多，你有足够高质量的领域数据，你就用全量微调。如果你没有你就用PEFT。因为如果你数据质量不高，你拿去做全量微调，你可能把模型给搞坏了。我不知道这个回复有没有回应到大家的疑问。
	然后GPT3.5的微调，这个我们没讲过，因为GPT3.5是调API来微调的。这个同学如果对GPT3.5的微调有兴趣，OPI的官方有非常详细的文档了。Hugin face上有lama 2的中文模型吗？就这种问题你自己搜一下你就知道了，有还不少。
	Nama系列的模型和之前的ChatGLM系列的模型该如何选择？我刚讲的时候难道这个不明显了？就是lama是纯英文语料训练的。这个同学你注意一下，lama是纯英文语料训练的，所以你才会看到这个图里面出现了一个chinese data，就是在nama上面去做中文的数据的增强。然后ChatGLM是主要用中文训练的，所以这俩的区别应该挺明显的。就是你要用中文的你就用ChatGLM，你要用英文的就用拉玛，然后那么多模型你都可以不用管。其实本质上你先把基座模型给整明白，然后去做微调。然后你只有你先自己微调了，你才知道这些模型这么多好几千个都是些什么水平，对吧？然后硬要说有什么样的参考，应该怎么选，我觉得还是其实之前提过的，就是LM的这个leader board。
	就是hugin face自己做的一个大模型的榜。这个榜有一定的参考性，不能说他一点用没有，但也不能说拿着这个榜的第一名就能干活了，这个榜单的这个情况，其实今天那篇文章也说的挺清楚了。然后这个榜是如果传了新的模型，它会实时去算的有点卡，他自己也会有新模型，就会实时去算。然后这个模型的这个榜单上，他怎么算的，这个谁第一谁第二呢？就是有不同的基准测试集。不同的基准测试集本身就是体现的模型的不同的能力。你看课件里面每讲一个模型，我都会放上它在不同基准测试集上的分数，就是这个意思。咱们如果现在连那些基准测试集是什么，体现的是什么能力都不知道，那你得赶紧回去看一下。
	相当于我们现在在评价哪道菜好吃。然后懂的人至少会讲这个菜咸了，那个菜有点酸，这个菜甜了，什么有酸甜苦辣咸，对吧？这个就是不同的能力，就跟他的推理能力、阅读理解能力、代码生成能力一个意思。但如果你连这个都说不出来，我们就没法沟通了，就是没法去聊哪道菜好吃了。因为你找不到一个维度去评价它。我不知道这个概念大家有没有理解到。如果大家至少对这些非常严肃的基准测试有了一个概念，而不是只是去看他的中文强还是英文强，而是说他的能力有了概念，那就是去刚刚说的那些像GSM8KMMLU glue这样的一些测试集上去看一看，这个是有价值的。
	没有那么多的没有你想象的那么多。开源是一个逻辑，开源是领域数据和中英文的逻辑。而基座模型没有那么多可以选的。你现在真正能够拿去做微调的不就lava和chat GM是吧？这个是一回事儿。就是你现在也很难会去问出一个问题，说github上面有100万个开源项目，我现在要干一个事儿，我应该选哪个开源项目？这个话你说不出来的，因为你已经会写代码了，但是现在你没有怎么去调控模型，所以你会问这个问题。所以我希望大家得得让以后你去training点train去执行这行训练代码的时候，跟你现在敲出一个hello world一样熟练，就没有那么多这个感觉上很奇怪的点就就行了。
	然后这个榜单是随时动态更新的。只要有模型上传到了hugin face的这个model hub里面，他就会去去执行这些。只要他支持的话，大家也可以观察一下这个，但不把它作为唯一的参考。
	前沿技术前沿技术肯定得关注，但是得把基础打好。咱们课程里面的这些内容是不是真的消化了？如果真的消化了，去读过那些论文，你就至少知道这个大的技术门类了，然后接着才会去看这个下一步有什么。其实你把这些论文真的好好消化了，你就没有这个疑问了。因为大的技术门类就已经列好了。你说还有什么新的前沿技术都是沿着这个走的。混合专家模型MOE都有一些新的东西，你就去看这几个头部公司他们每天在聊什么。
	然后这个问题也也提过不少，就是你要了解前沿技术，是不是你首先得学会去一些前沿的渠道这些前沿的渠道都有一些现在这些大厂的，不管是这个mixture还是OpenAI，他们的老板首席研究员都在发twitter，都在发这个信息，去看一看。对，不会错过的。然后还是要沉下心来，把讲的这些理论和实操的东西多多做一做才有感觉的对，不然这个就白白浪费了时间了，对吧？也浪费了大家的这个资源，就先把一给学好才能反3。然后那些好的渠道，你得习惯去刷twitter，就像你刷朋友圈一样，对你就有不会信息毕设了。这个max平台我其实不想去，因为这个太利益相关了，我不去做评价，这个会影响到很多人的。
	然后因为咱们也是会录下来的，我自己认为模型作为一种平台或者作为一种服务，开源挺好的，就像我们现在这一页，有各种各样的模型开源出来了，你可以拿去用，挺好的。然后如果你说你要把它变成一种服务，需要有那么多的用户来用模型吗？或者你想象一下，在公有云市场里面最早分的是SaaS pass和S你去看一看真正的各种数据统计，这都是查得到的，我就不会是主观了。你去查一查pass这个赛道，pass这种销售模式是还在不在，他为什么不在了，就跟模型as a service是一个道理。
	然后图生图有这个领域吗？我没见过图生图。这个同学你有图生图的相关研究可以发到群里，我也学习。然后SD stable dimension纹身图本身就是一种技术，是不是一个技术领域？我不知道这个怎么理解。
	然后stable diffusion也可以拿lora来微调，他准备的数据又不太一样，然后diffusion和transformer也不一样。为什么能拿lora微调？我们Laura那个课讲的挺清楚的，就是把高维的权重矩阵做成了低维的小矩阵来拟合。所以它肯定能微调，因为diffusion也是矩阵，它的权重。
	咱们刚刚看到了阿帕卡几万条，这个我也是几万条。这个其实之前说过的，至少你要几万条，就几十K条数据去调是有价值的，并且还不一定是全参数的微调。
	还有同学用ChatGPT的来构建多轮对话的数据，肯定可以构建，这个没问题的。你写好prompt就好了。你要是不会写prompt，你就让ChatGPT帮你写这个prompt，你就说清楚你想干嘛，让他来构造这个prompt都是可以的对。
	还有个同学问inference的这个访问量和资源需求是什么？怎么评估？试一下。你部署之后，你搞一个client的客户端发API请求去测一下经典的压测，这个要测一下就好了。对，这个每个模型都不一样的，没法说。然后并发量1000，我觉得这个挺挺夸张的，很少有这么高的并发量，除非你真的上线了一个引起大家关注的产品了，对，并发量1000是很高的数字了。然后你要测并发量，这个你随便问你们公司的测试人员，你问他怎么测并发量，他都能告诉你。
	对，就是说模型部署成一个服务之后，它就变成了一个经典的API服务提供者了，一个web server一样的API。不过这个API它背后不是去查数据库，而是跑了一波GPU的运算。所以他该怎么去测并发和资源需求就怎么测，这个完全是不用变的测试技术。
	垂直领域数据量是否可以适当的少一些，你可以适当的试一下。同学就这个事儿，就是我也不是上帝视角，我也不知道这个数据，你说适当少一些，少多少就能用，对吧？这个你要想求个心理安慰，我告诉你肯定可以对吧？但是你实际上了，你说不行，你说老彭说的有问题，那这也不合适对吧？
	试一下是最好的。对，如果你愿你连试一下都不愿意干，就说明这事儿可能你就是真心不想干的那就别干了。对，但如果你觉得试一下是有乐趣的那说明你是对大模型是真的有兴趣的那你每天都多干一点点这事儿不就变成你的核心竞争力了吗？你就有优势了，对吧？但肯定是得自己去试才能实践出真知对。
	这个leader board是值得大家关注的，然后你看其实lead board里面也比较明确的，现在这个safety是很重要的这也是在nama 2的实验结果里，我放的第一个safety。因为尤其是在国内，这个事儿是非常严肃的一个话题。如果你要去做一个生成式人工智能的，就是产品里面带有生成式人工智能的产品上线，尤其是面向C端或者是一定人用户数量的产品，是一定要去备案的。备案过程当中的这个safety就很关键，最近刚好我有一个朋友的公司就在网信办备案这事儿，那玩意儿考的挺挺复杂的。而且网信办的同志们手上的刁钻的corner case，尤其是一些敏感的一些话题，人家那个数据很全的。所以这个safety很重要。一旦你真的要做一个产品上线，这是首要任务，它比性能还重要，对，不然你就没了。
	好，那我们要不今天就到这儿，然后我们应该正式的这个内容，到今天十6节课就是上完，还补充的我们两个实战放到年后，具体的时间会由浩哥，群里的这个账号和班主任沟通之后，他们来给出一个时间。然后希望大家通过这几10个小时的学习，把咱们的这个大模型微调的这些技术理论的我们也讲了不少，然后实战的也有各种各样的模型，能够都实际的去实操一下训练。然后有实际的问题我们多交流，好吧？最后祝大家新年快乐。好，再见。