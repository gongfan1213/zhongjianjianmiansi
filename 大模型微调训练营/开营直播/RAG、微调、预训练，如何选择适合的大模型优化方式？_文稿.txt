	好的，大家好，非常开心今天能够在这个时间，有机会跟大家做一个分享。这个分享的主题叫做AIGC的下半场，打造AI2.0时代的核心竞争力。我不知道现在声音正常吗？大家听得清楚吗？可以在观众讨论区里面做一下分享脸是红的是因为这个PPT是一个色的，我们切到这一个就好。现在大家声音都没有问题，对吧？那我们就直接开始。
	好，那我再简单做一下自我介绍。因为可能今天来参加这个直播厅分享的一个是我的老同学，参加过我们AI大模型应用开发实战训练营的同学。但我相信可能也会有一些新朋友，所以我简单介绍一下自己。我我叫彭婷婷，是平台的联合创始人和CTO。我其实一直从高中开始就在研究计算机这个行业或者说这个专业，从中学的信息学奥林匹克竞赛到保送，一直到从大三开始创业，一直到116年的时候，从加州大学回来，进入了华为的2012实验室，参与了华为2012实验室深度学习。这个团队从0到1的一个过程，不仅看见了团队如何从三个人发展到了一百多个人，现在有小几百人的规模。同时也在华为2012实验室，由我们三个人的小团队，一起打造了华为的深度学习平台的原型，也就是现在的华为的model us，我们三个人也一起写了一本书，叫深入理解test flow。
	当时也是在深度学习这一轮热潮的时候，唯一的一本，包括到今天应该也是唯一的一本去深入的去给大家把TensorFlow这个最有名的，然后也最广泛应用的深度学习的框架，做了一个架构层面上和原理层面上的一个拆解。然后在这个同时，也面向了一些小白用户。因为这本书可能写的深度相对比较深，在这个时间第一次合作做了test flow快速入门和实战这么一个视频课程，然后也得到了很多同觉得这个好评。这个课程的代码也都是开源的，就算没有参与这个test flow这门课的同学，也能在网上去folk和改这些代码，学习这些内容。
	今天其实是想要跟大家分享的这个主题，叫做AIGC的下半场，也是我们做的这门新课AI大模型微调训练营的一个上新的直播，一个开一个开始。为什么在这个节点其实有两个原因，大家可以看到今天的内容分成三个部分，我会主要讲讲前两个部分。第三个部分关于课程的内容，以答疑的方式为主。
	前面两个部分其实最近有很多的事件发生，不管是国际社会上面的各种战争，还是说贸易战也好，其他的战争也好，一直到科技领域，我们看到昨天晚上丁伟达又被禁了，我们很多的同学可能都拿不到这个GPU了。然后不管是公司还是个人去去买一些超过300T的算力的GPU都成了一个很麻烦的事情。包括之前的H800这种特供版也无法去采购了。这个问题其实在当下这个节点看看也特别的有代表性，待会儿我们在讲的过程当中，包括有的同学对这个话题，还有想要深入了解的，我们可以再深入的去聊，这个是一个方面。我们看到中美的博弈会带来各种各样的全面竞争，不管是科技技术还是其他方面的那作为我们生在这个局中的人，我们看到去年11月30号的时候，ChatGPT发布了。这门课程的开营的时间刚好就在THPT发布的一周年的左右的时间，应该是在11月底，就刚刚一周年的时间。那么这个时间点也选择是有意而为之的，我们看到整个ChatGPT的出现带来了一个全球的狂热，也是我待会要讲的第一部分。
	就AIGC的上半场，有非常多的人躬身入局，各个行业的人，创投圈以前可能都不看这个赛道的人，或者说在这个创业领域有一些founder有一些创始人，之前不是做这个赛道的人，也都all in到这个赛道里面来。但是我们经历了大半年的时间，从7 8月份开始又有一些媒体开始炒作，每一轮的AI都会经历这样一个过程。四期现在没有在失落的状态，还是在第四轮的这个上升期，大家在炒作ChatGPT的访问量下滑了，这一轮的这个生成式人工智能与通用的人工智能还有很遥远的距离。然后又在吹这个泡泡的过程当中，一帮人说这个要破产了，同时也有一些GPT套壳的项目，确实现在很难以为继了。但是大家如果关注新闻的话会发现深创投刚刚又投了百川智能，投了3亿美金，所以其实每一轮这样的新技术，尤其是AI的技术，都会引来这样的一个周期。你说他叫gartner的技术传输曲线，也可以叫任何的商业曲线都可以。因为它就是有一个一开始没有人关注，而且潜心在研究。
	像OpenAI从18年开始到现在，2022年底终于有了一个大的突破。四年多的时间迎来了高光时刻。高光时刻之后，就是大家看到的这个吹泡泡的阶段，吹完泡泡始终就会有人在泡泡破灭的时候什么都没有。这个呃没有其实没有参与这个的，他只是一个观众。
	但还是会有一些人all in进去或者说参与进去，拿到了一些自己的结果。那么我们看一看，在当下被称之为AI2.0也好，AIGC的下半场也好，这个下半场更多的就是泡泡逐步破灭的过程。然后怎么样在这个干净的技术曲线往下走的时候，你能够沉稳的稳下来，然后去迎接下一轮真正的上涨，真正的生产。这个是第一部分我们想要讲的各种各样的现象和我看到的一些真相或者说解读。第二个部分其实是我们预热片，就这个训练营预热篇里面的一些内容。我这边做了一个简化的一个分享，就是叫做AI大模型的一个四阶技术的一个总览，这个是我取了一个小的名字，这个司机技术也有它的原因，是一个由易到难，这个易既体现在你的要学习它的成本，同时也体现在你要投入的资源上。
	最后我们会去讲一讲这个课程设计的动机。也就是我们认为在AIC现在肯定离通用人工智能有距离。但是在当下这个AI第四轮的热潮当中，我们明眼人都看得到AI2.0也好，生成式人工智能为主的这种可以明显提升生产力的技术也好。在这个时代上面，我们作为一个个体，作为一个研发，作为一个产品，应该怎么样去保持自己的核心竞争力能力。怎么样让自己在职场，在个人的这个职业追求上面，保证自己是啊一个向上的一个发展。这个是我们做这个课程的一个动机，所以会有一个简单的AI2.0的一个关键技术上的一个分享。但它不一定全面，大家还可以再来探讨，最后会花一些时间给大家去做一个课程大纲的解读。
	好，我们就正式开始今天的分享。首先我们来看看最近我们在大环境下看到的一些AIGC的变化。我们从最开始开始，就从这个ChatGPT拉开这个序幕开始，带大家来回忆一下这一轮的AIGC的上半场、下半场到现在为止发生了什么。
	其实时间过得很快，从去年的TI GPT发布，2022年的11月30号到今天，已经过去了十个半月了。这十个半月全球的经济创投圈，所有的围绕着AIGC的这些资源的人，都在发生变化。现在GPT当时这个时刻出现的时候，其实是给全球经济下滑的这个摔迷的状态打了一个强心针。他他让大家发现，其实这个世界的经济还是有希望的。因为我们的科技发展了，所以所有的人都开始关注现在GPT。
	在GPT也成功的打破了历史上的很多记录，就包括最快时间获取到1万注册用户和1亿注册100万用户和1亿用户的一个记录。我们看到他只用了五天的时间，在12月5号的时候就拿到了5 100万的注册用户。然后花了一个一个多月2个月的时间就拿到了一个亿的注册用户。但是他现在有十几个亿的访问的频次，所以整个现在GPT的出现，让大家发现其实互联网还没有到下半场，互联网还没有这个游戏还没有结束，我们还是有很多的事情可以去做的那怎么做呢？
	包括像前两天李彦宏开这个百度的大会的时候，有一个很明显的讲法，就是要用AIGC或者叫用生成式人工智能，把前面就我们看到前面这一批APP全部重新做一遍，为什么需要用AIGC全部重新做一遍？这个逻辑其实也挺简单的，就是前面这些APP其实是用移动互联网的技术，把我们传统的生活方式做了一遍。但是因为移动互联网已经几乎把这个相当发达的国家和发展中国家的用户都覆盖完了。
	这个时候大家太阳下面已经没有什么新鲜事了。大家都在内卷，所有的服务提供方都在内内卷内容的提供者也在内卷。作为消费者，作为用户来说，大家对个性化的推荐广告，或者说这种海量的投放的广告已经不厌其烦。烦了。这个时候我们继续的去卷广告投放，继续去剪推荐系统。其实对于所有人来说是一个内耗的过程。
	要分层式人工智能最大的一个好处是说，第一他把这个以平台为主导的这种模式，有一定的希望变成一个由个体为主导的。这个个体就是指你自己的一个AI助手。这I助手能够跟所有的平台的API去做对接，相当于你有一个自己的钢铁侠的js但这个是一个往后看的一个模式，而且我相信应该在3到5年会跑出来很多这样的公司。就是把原来由平台塞给你说你在抖音刷刷视频的时候，刷三个视频就有一个视频的广告，这种讨厌的广告强塞给你的模式，变成一个由你的这个agents主动的去找各种各样的公域流量，然后他再做一遍筛选，筛选之后给到你。这个是一定会发生的。大家可以想象，并且在技术上现在已经有这样的一些路线可以去实现了。所以现在GPT的这些想象力，导致全球的AIGC的这个技术热潮和创意的热潮开启。
	它的开启在上半场最直接的一个体现，今天有很多这一节的内容是公开可以访问的资料，我也留下来了这个访问的链接，大家有兴趣可以去看一看详细的内容。在上半场我可以把它定义成是2023年的上半年，就H1。上半场的一开始，我们看到一季度是一个非常夸张的状态。所有的生成式人工智能的初创公司，它的这个融资是一个突飞猛进的状态。我们可以看到在在这个图里面对比，这里是生成式人工智能的公司，他能拿到的一个融资。大家看到这根柱子，2023年Q1 110亿美金，但OpenAI做了很大的贡献，他拿了微软的钱。但是整个发展从2021年开始，就一直在稳步的增长。虽然相比前面肯定没有，因为前面没有做深圳CAI的，都是在做这个深度学习的，但是到2023年Q1的时候，这个数字是非常夸张的。
	按照这个文章标题来讲，就生成式AI的这个全球投融资本达到了220亿美金的一个规模。但是美国占比将近90%，然后OpenAI贡献了超过60%，这个是一个Q一的状态，因为那会儿市场上没有太多好的标的。你看像那会儿像王慧文，像这个百川智能都还没有公开，在外面去讲他们的这个融资，甚至有的公司都还没有成立，那这个是Q一的一个状态。我们看到所有的资源，发现AIGC有搞头都来了。
	然后到了上半年末的时候，我们把两个季度拉到一起来看的时候，其实就会发现一些有意思的数据了。第一在2023年的上半年，非常有直观的一个数据可以体现，21年和22年的全年融资额也就30亿到40亿美金这样的一个规模。但是2023年的上半年，就已经达到了15亿美金，相当于过去两年的两倍还要多的一个投资额。所以所有的机构股东也好，或者说这个投资人也好，是非常热切的想法参与进来的，因为市场经过了这两三年的疫情，已经处于一个非常低迷的状态，所有人都想做一个新的赛道，一个不卷的赛道去获取一些机会。这个是一个从数字上能直观体现到的，就是大家对AIGC这一轮的一个热潮。但是如果大家有细致的去对比，可以看到其实二季度只有40亿美金，一季度有11亿美金。从这个数据来看已经有一些下滑了，到我们Q3 9月8日的时候，有一个外媒叫similar web这样的一个公司，他发布了一个数据，它发布数据其实主要是看从八月份、七月份、六月份这三个月的数据来看，ChatGPT的访问量下滑了。这个也是很多国内互联网转发的一个很大，当时引起了蛮大人关注的这一个信息和新闻。
	当时有讲就是大家能看到这个数据连续三个月下滑，不管是桌面端还是我们移动端，只有TIGPT air都下滑了，下滑了3.2%，然后即使下滑这么多，大家能看到其实它月度仍然有14亿次的一个访问访问量。大家可以有兴趣的可以去往全网上面搜一搜，有多少的APP或者说应用能够达到这样的一个访问频次，这个是独立UV的一个访量。这其实是非常夸张的，只不过因为他前期大家知道这个泡泡吹的太大了。所以很多的用户对这个事情的关注可能就会停留在这个标题上，就是ChatGPT不行了，然后AIGC的泡沫破裂了。但其实不是，我们深度去看这个问题的话会发，第一它即使下降了，它的这个房品质仍然很高。第二要分析背后的原因，一个本质原因是新技术来了之后，有很多本来不是不属于这个行业的人进来了，然后会把他们洗掉一部分，因为有的人是来看热闹的，但有的人是真的来做建设或者真正来体验新技术的，早期吃螃蟹的人。
	我们仔细来看这个数据，其实会发现一个很有意思的现象，就是还是刚刚这个时间节点，我们用不一样的视角去分析的话会看到第一ChatGPT它的这是一个外媒叫s camp research他们做的一个访问数据。在这个数据里面我们能看到从四月份这个节点是星期四发布的日子，到后面的这个六月份，其实它的这个用户访问总量没有一个太大的下降。他只不过是说没有保持这个高增长了。但他没有保持高增长的原因其实是很复杂的，其中有一部分原因是因为这个政策原因，当时封了大量的GPT的号，包括到现在为止也是一样的，就是有很多的用户按照GPT的这个协议，其实是没法访问的，我们需要用一些特殊的手段。那这个其实是会直接对他的流量造成影响，毕竟这个是一个咱们国家是一个很大的人口的基数。第二个原因是因为其实GPT开始希望赚钱了。
	你要知道一开始的整个OpenAI的，你可以认为ChatGPT的这一年多的将近一年的这个事件，其实是一个非常成功的一个商业的一个事件。包括他一步一步的去发布它的新功能，从ChatGPT到3.5的turbo到GT4，到它的plug in，到他的code in prior，到dari three，然后一直到现在还在有一些铺排，然后给大家有预期去做T5。其实整个OpenAI公司的商业化做的非常好。
	然后在上半场我们认为上半年的时候，其实整个OpenAI是没有那么赚钱的，并且它有大量的流量上面的一些压力，因为他有很多的GPU去供给大家使用免费的GPT3.5。当时有一段时间还有一个新闻，在六月份的时候去讲GT4的效果不如GPT3.5了，也是因为成本各种原因。那么到下半年的时候，其实我们明显看得到星期四开始做商业化了，他需要去挣钱。一部分在刚刚看到的similar web这个机构去统计的时候，他只统计了ChatGPT这个页面或者说APP这种访问方式，而忽略了API的访问方式。学过我们上上一个训练营，就是应用开发的训练营就会知道，其实你免费的流量对OpenAI并没有任何的价值。到下半场这个阶段，赚吆喝的这个阶段已经过了，接下来应该赚一些实惠了。但是GPT3.5是不收钱的。如果你使用了GT4的这个订阅，每个月是20美金。
	但如果你真的去接入了GPT的API，比如说我们在之前的实战里面做过OpenAI的translator，做过auto GPT，做过这个sales consultation这种聊天机器人。你就会发现API其实挺贵的，尤其是GPT4的API。你一个调用这个没有注意的话，可能是这几美金左右了，这个费用其实是非常高的那这个也是整个OpenAI的一个商业策略，它开始从一个玩具走向工具这样的一个变化过程，这是一个非常成功的一个转型。具体怎么看到呢？就是在上上周还是上周我看到这个外媒the information，然后后来内部中国互联网也在报道，这个OpenAI的CEO sam奥特曼。他对自己的员工去透露，因为他们最近在融这个融新的一轮，据说已经达到将近1000亿美金的估值，大几百亿美金的估值非常夸张。然后拿了沙特的一些主权基金的钱，并且开始向员工去透露一些信息。其中一个最重要的信息就是收入。
	大家能看到下面这个曲线，其实是他们从移动端，从市场store和google play这个移动端的APP的一个增长率。光九月份就有1560万人下载了它的ChatGPT这样的一个APP，但现在还在增长，还在复合增长。它不过是从一个网页，就本身大家现在都已经习惯使用APP了。
	OpenAI让大家硬生生的又回到了一个web应用的一个界面里，然后还达到了这么快的增长，这本来就是一个很让人匪夷所思的一个状态了。你要知道tiktok创造的奇迹是通过短视频加APP的形式。但是OpenAI不仅在速度上面和这个增长上面比tiktok还要高7到8倍。同时它还是一个web端的应用，还不是一个APP。当他把APP做好之后，你就能看得到我们可以预见到现在的这个多模态一定会在F端再去做整合。然后他会真的去做AI agents和应用层面的各种操作。所以完全不用担心下半场好像这一轮的AI就结束了。其实才刚刚开始，就是像gartner的曲线热潮过了之后，现在才真正开始做实事儿。
	然后你从收入端也能看得到，其实OpenAI的这个收入增长非常夸张。我记得在GP4刚刚发布的时候，因为我们看得到去年的数据，去年OPI的年收入才2800万美金。很多人当时对于微软的这个投入，认为微软100亿美金投进去肯定是会亏本的。但现在我们回过头来看，光今年，而且上半年是没有那么多收入的。就下半年来说，就已经达到了年收入13亿美金，甚至这个数字还有可能增长。因为大部分的创业公司都开始使用NGPT的API，还有这么多创业公司融了这么多钱，就像英伟达也会水涨船高的去增加收入一样。OKAI现在收入相比去年同比涨了将近50倍，那这样来看的话，其实它几百亿的美金的估值也就没有那么夸张了。甚至我们可以预想到明年它可能就是一个大几十亿美金收入的一个公司，这是非常夸张的。
	所以外行人看热闹就是内行的人。我们其实深入静下来去看AIGC的下半场，是真正值得大家去投入的一个时间节点。不管是有很多的公司融到钱，还是说这个行业其实在前行，在默默的前行。然后这个前行的过程当中，很多的技术如果你没有继续保持关注，你可能就又会被甩下车。
	我们再看另一个现象，就是很多GPT的套壳项目。我们看到在1 2月份的时候，有很多公司或者说很多项目，他甚至就是卖这个ChatGPT的注册，卖这个GPT的API做一个页面，聊天就赚了一波快钱。但是这些公司都很难以为继了。
	坦白来说，第一个是我们从宏观数据上来看，在2023年的这个三季度的时候，其实这个融资的这个比例也好，或者说就融资的这个频数，就投资的投融资的这个笔数，以及它的总额都在下降。大家刚刚有印象的话，一季度11是11B就110亿美金，然后第二季度是40亿美金。但那个是这个深层次I的局部的一个领域，这个是全球交易总额也降到了61亿美金。然后相比这个环比就是指相比二季度，就三季度相比二季度降了29%，而相比一季度降了80%，所以其实是一个非常夸张的一个下降了。
	有很多的你可以说非头部的，甚至中间力量，包括腰部，甚至一些长尾的小公司已经拿不到钱了，他们是很难再去进一步的去做增长了。因为他们没有构建自己的核心竞争力，就只是套了一个壳。这种公司本身就你从这个商业的角度来说，它的价值也不大。但是我们反过来看一些头部的大模型公司，是你还不能讲赢家通吃通吃。因为这个AIGC的下半场才刚刚开始，大家都还在使劲的加油的去获取更多的资源，然后去做更多的实事儿。就比如说anthropic这家公司，这个公司其实是在在海外，至少是在硅谷。像我上个月刚刚回去跟一些朋友去做交流，跟一些google的AIGC团队的，包括在硅谷创业做AIGC这个赛道的一些朋友。
	大家都在讲，其实SOO pic目前来看应该是OpenAI最大的一个竞争对手，他的这个CEO其实是之前OpenAI的研究的一个VP副总裁。但是因为在21年的时候，那会儿GPT3已经训练出来了，然后正在做各种instruction tuning，包括这IHF。在那个时候因为有一些商业的发展理念不合，所以带了一批OpenAI的前员工创办了这家公司叫做tropic。然后他们的这个大模型其实也挺有的，或者说他的聊天机器人就对比ChatGPT，它有一个对应的叫聊天机器人叫cloud，这个其实也是挺有名的，它就是一个ChatGPT的一个竞品。
	然后他的融资的节奏也是非常的猛，大家可以仔细去看这个频次，就2023年就已经C轮有三个序列，C轮C加轮和C加加轮光。C轮的序列到目前为止就已经融资了超过十个亿的美金。他们现在这个融资总额是将近15亿美金，非常夸张的一个估值。然后现在也是大的将近50亿美金的一个估值了。
	所以大家能看到他的投资方有google包括self force zone。这些其实都是既有资源又有影响力的一些投资方。所以整个大语言模型的状态可能在国内和在湾区、在硅谷的期体感是完全不同的。我有一很多在湾区的这在大厂里面的朋友啊，比如说在google在meta的朋友都出来了。包括google这个论文transformer 8个作者也都出来创业了，并且这八个作者都拿到了不少的融资。
	所以其实整个AIGC现在真的是一个非常，如果你要做实事儿，你不只是来看热闹的话，是一个非常好的一个状态。或者换句话说，如果你不是一个founder，你是一个研发或者一个公司的研发团队的一员。这些公司拿到钱了，就跟五六年前商汤刚刚拿到钱的时候，开的平均工资都是100万年薪一个人，就整个公司商汤最夸张的时候是100万年薪的一个人。那你想象一下s ropy或者说类似的一些公司，为了抢人才也是类似的，因为会这些技能的人太少。我们再看一家公司很有意思，叫hugin face。
	这公司其实我们在课程里面也会讲他们的两个库，一个transformers一个PFT。他也是在今年完成了第一轮的融资，融了2亿美金。然后领头机构也是刚刚看到的sales for这家机构的这个呃CVC，它的企业的这个VC部门投资了2亿美金。然后hugin face的估值现在也有40亿美金了，非常厉害的一个成长。然后他的整个hugin face的布局占位也都很强。所以头部的大模型公司现在就是一个强者恒强，小圈子在不断的聚拢资源的一个状态。
	像国内也是一样的，我们在国内看到这个ChatGLM，就智谱华章这家公司。智谱AI他们最新一轮也是融了数亿人民币的一个估值，应该是美团龙珠投的amin max，也是最近融了超2.5亿美金的估值，这个融资然后估值也是100亿美金左右了，100亿人民币左右了。像百川智能刚刚拿了深创投3亿美金左右的一个融资，然后也是一个超10亿美金的一个估值了。包括这个月之暗面今年刚刚成立的，前两天刚发了一个新闻稿，然后有产品支持非常超长文本的中文的上下文。他们没有透露具体融了多少钱，但有一些估值上面的一些信息，透露3亿美金的一个估值了，非常夸张。
	当然还有一些公司我们没有列完，但是能够看得到，这一轮和深度学习这一轮的和深度学习上一轮的AI创业也好，AI的这个大市场也好，有非常大的变化。第一它的强者恒强头效应非常明显，他一开始可能就只剩下十家公司能够拿到比较多的钱，剩下的公司可能都不行了，因为他需要的资源非常多。那对于一个非founder团队的人来说其实挺好的。你就不用去想，你想象一下在1617年的时候出现了这个四小龙，但四小龙那会儿还没有被称之为四小龙，因为还有太多的我都知道名字的公司，当然后面可能没有非常成功的上市死掉了。那么这些公司你可能就没有更多的资源。
	但是在AI大模型的这一轮，其实你会更明确的找到你应该去哪儿，对吧？如果你是一个学会了这个技能，然后你又想做点事情的人的话，那是非常好的一个状态。好，这个其实是我们想要给大家分享的一些宏观层面的信息，就AIGC有很多的谣言，有很多的现象。这些现象有不同的看法，也有一些背后的信息，我们能够去在公开互联网，在网上就能找到的一些信息，会有一些不一样的体感。
	这一部分我们结束，让我们看看下一部分想跟大家讲的这个偏偏技术的部分，也是很多同学可能想要来听的。就是AI大模型所谓的这个世界技术也好，RAG微调预训练也好，到底是一个什么东西？快速的讲一下，就是有很多同学可能如果完全没有AI背景的话，可以稍微了解一下。
	AI是一个1956年被正式提出来的一个词。然后到现在为什么说是第四波？是因为前面有三波的一个发展，以这个最基础的这个AI到机器学习、深度学习。上一轮一直到这一轮2020年GT3发布之后，有很多的硅谷公司开始基于GT3。API去做各种各样的创业，所以大家才能看到在融资这件事情上，2020年开始有生成式人工智能的start up开始拿到钱了。包括像jasper copy AI这些硅谷的公司都是在那个时候成可以的。
	那么四阶技术是什么意思？其实我这边做了一个简单的一个示意，这些值其实是不是一个准确值，是为了给大家一个示意图。简单来说我们看这幅图可以分成横轴跟纵轴，横轴是一个资源的投入，这个资源的投入可以简单理解。第一就是如果你要学会掌握这个技术，你需要投入的资源从个人的层面上，那你需要投入什么资源呢？你需要投入你的时间，你需要投入你你不要你需要不断的练习。然后你如果是一个公司的话，你还需要投入算力，当然前两者的算力资源消耗几乎为零。
	如果你要使用这个提示工程或者说去搞一个AI agents，你都可以基于GPT API去做对应的这个事情。他就按量付费的，那这个消耗是比较少的那但它能够达到的预期效果也有限。就像很多人在讲这个prom的engineering是一个玄学，就跟炼金术一样。不知道你改了一个某一个单词，好像它的效果就变差了或者变好了。但其实也是有一些理论依据的，并且是有相关的论文在做研究的，甚至我们现在讲的AI agents这个范式当中也有这个提示词的策略，所以其实整个过程，这个四个阶段它是一个你可以也是一个由浅入深的过程。不仅是你学习，它是一个需要投入的时间越来越多。
	同时如果你真的技术掌握的到位我们前一个训练营其实讲的是前两个部分，就是我们会教大家怎么用open a的API用能欠去做提示工程，去开发一个AI agents。但是这两个如果你真的理解到位了之后，首先他们就提示工程和AI认识是相辅相成的。因为agents的核心就是要把提示词模板用好，把这个大语言模型的潜力挖掘出来。但是它也有一个上限，尤其是我们会发现通过这个prompt，它不能改变大元模型你的参数，这个单元模型是不变的你，只是改变了你的沟通技巧，那么要想达到更高的效果，尤其是一个最典型的现象就是比如说这个大元基础模型里面没有训练中文，然后你非要去让他回答中文的问题，那它的天花板就很明显，就是达不到一个你想要的非常好的一个效果。比如说纳马图，比如说现在的ChatGLM是用中文跟英文语料去训练的。你非要问人家这个法语西班牙语的问题，他可能就答的不是特别好，那就需要去做这个翻译聪明。
	那这个翻译tuning具体怎么去做，我们在这门课里面就是一个最大的一个重点了。只不过翻译tuning相对前两个来说，它会有一个小的门槛。他需要你有一定的GPU的投入，因为你需要去把模型加载到显存里，然后再去做去微调。但翻译推理的核心是想解决我不用花大资源，就相当于花小钱办大事儿。
	办的这个大事儿一定是想清楚了，我要干一个具体的什么事儿，然后我针对这个问题去做翻译to，而pretrail ing更多的问题是说，我现在从0到1的想要做一个新的大模型，我什么都没有，我只有数据，然后我有算力，我想要训练一个在大领域级别的，就所谓我们讲的这个垂类的大模型，就这个意思，就我不要他万能他，我不要他会各种各样的能力。但是在一个特定技能上，我希望他是一个特长生，他是一个在专业领域上非常强的能力的一个大模型。这个可以通过这群里去做训练，当然这里我们还得讲一个问题，就是为什么讲大家学习这个技术最好也是沿着这个曲线，因为它背后的很多理论工具和应用实践是一个循序渐进的过程。如果你一来对这个提示词都没有用过，你就非要去搞这个find，你会有一些吃力。所以我们在预热片会给大家带一带这些内容，今天也会给大家简单提一提。
	然后最后这个是从横轴的角度，包括我们学习的角度来看，从这个纵轴的角度我们叫预期的效果，为什么这么讲？为什么叫预期的效果？而不是讲这个效果？预期的效果叫期望对吧？这个期望是指如果我做的好的，我能达到一个非常好的效果。但如果我的技术水平不行，有可能我使用了fine tuning或者free training的效果还不如一个AI agents。
	因为你训练的方式不对，你的这个参数训练的不对，你的这个loss没有设置正确。你的使用了一个q lora这样的一个量化的一个可以用来以低成本来做微调，或者说全场。但是微调的方法，但是数据没有处理好啊，数据的质量不高。这些都有可能导致你使用了算力，投入了时间，投入了资源，最后还不如不训练，这个是非常有可能的。
	所以要去掌握这个技术是第一你要搞清楚要办的大事儿，那个任务，具体场景是什么。第二要解决怎么样去准备好数据。第三步是怎么学会工具，并且知道这个工具使用的通信技术是什么。要结合的起来才能把这个事儿做好。所以我的建议学习路线也是按照这个四阶的一个发展。如果大家连现在GPT都没有用过，或者类似的这样的大模型都没有用过先去用，然后用完之后去看看AI agent值是什么。
	好，我们接着就来简单的讲解一下这四个技术。第一个就是叫做提示词工程或者提示工程。什么是提示工程？我今天就用这个GPT4和dari three问了他什么是提示工程，但我给了他一些输入，一些描述，然后做了一些调整，它生成了四幅图，我挑了两幅图，这两幅图其实描述的还挺到位的，就是提示工程是什么？
	左边那个这个大家能看到，有像一个大脑一样，然后这个大脑其实就是我们的大语言模型。然后我们这个下面像个这个机器一样叫提示工程，这个提示工程连接了各种各样的线路，其实就大语言模型它很强。然后我们也有各种各样的应用需求，想要去找这个大语言模型去做这个，相当于让他作为助手帮我们解决问题。我们想从他那儿挖掘知识，但是怎么样挖掘呢？有各种各样的手段，其实是工程，其实就是各种各样的方式跟他去做这个交互，然后拿到你想要的结果。
	第二个，其实它更像一个实验室里面的一个设备，我们把这个提示工程给他，你可以认为是产品化了，这个有点像我们在南茜的学习过程当中，会知道南茜有一个很重要的功能模块叫model IO。Model IO其实就是把模型大约模型和提示模板做了一个整合，包括它输出的这个out to poser。如果我们把model IO当成一个整体模块，它的具象化就有点像右边这幅图，我们把各种各样已经用好的prom tempt做成了一个可以相当于build in预制的这么一个拿到固定结果的一套其实模板，这样就能高效的去获取我们想要的结果。这两幅图还挺有意思的，也分享给大家。有这个大力斯瑞，然后GPT4给的这个prompt生成的两幅图，那么总结来说我自己的认知from engineering这个歧视词或者歧视工程，它其实是我们可以把它定义为是大模型。这个时代的沟通艺术，这个沟通的对象就是大模型。
	最简单这幅下面这个对比其实来自于chain force这篇论文当中的截图。他要解释的一个现象就是说在2021年的时候，这个时候大家对于其实工程这个概念都还没有。GT3这篇论文当时提出来的概念叫在上下文中学习，也就是in context learning。有的时候你读论文会有个ICL，它的简写是in context learning。
	然后开创性的提出了这个zero shot one shot的这个few shot提示方法，然后不改变我们的大语言模型，因为要改变这个大语言模型需要的资源太多了。我们已经花了几百万美金，几千万人民币训练出来一个模型。能不能先把它的潜力挖出来，大家都在研究这个沟通艺术，那chal source就是告诉大家，大模型它其实已经学会很多东西了。只不过我们去问他的时候，我们不知道怎么问。就相当于有一个很强本领的一个高深一个武林高手，他可以帮你的忙，但是你说不清楚他就没法帮你的忙。就比如说这里的例子其实是问这个网球网球和网球桶，然后这个里面有几个网球，最后又换过来问了一个别的问题。但是在这个过程当中标准的这个提示值没有说清楚它是怎算的，但是后面使用了思维链的这个提示词，其实给了一些解释，就是为什么这个答案是十一，就是这个网球为什么是十一？
	没有一个计算过程，这就涉及到一些我说的理论的基础了，因为大语言模型核心是一个概率模型，他没有去学习这个符号逻辑推理。所以他自己是不会加法、乘法之类的东西的。但是你如果告诉他一开始五个球，然后每个盒子里面有三个球，然后加起来是5加2乘3，是11个球。求这个他是能理解的。就像大家也不是天生就会学数学的，也是一点一点通过这个参考事例的方法去学会的。只不过人确实比大模型要强，就强在人有抽象符号逻辑推理的能力，但是我们现在不要求大模型有高等数学的这种符号抽象能力，至少小学数学你能够搞定，那就可以了。就应用起这个级别，你能搞定就能搞定很多生活中的问题了。
	也是通过这个开始，chaf force开始，大家发现大模型的能力很强，是人的想象力太弱了，人的格局没有打开，需要把人跟大模型的沟通方法做提升。所以我们出现了前后forth这样的一些包括后续的研究，开启了这个提示工程。再一个简单的例子，就是之前给大家给上上一门课，应用开发这个课的同学做了一个很有意思的事例。这是我自己编了一个prompt去问GPT3.5一个，这个问题其实挺复杂的。我问的是问题其实是说我在一个繁忙的一天工作结束之后，去最喜欢的咖啡店放松一下。走进咖啡店点了5杯拿铁7杯美式。三杯拿铁是热的，有四杯冰美式。然后找了一个靠窗的位置坐下，喝了一杯拿铁咖啡，送给了朋友5杯咖啡和两杯热拿铁。最后我从咖啡店出来了，回到了家里，到家的时候我还有几杯咖啡。
	其实是个小学生数学应用题，大家如果仔细考虑的话。但是看今天3.5的回复，他其实回答错了。这个问题看似简单，其实人要回答的话也还是需要记录一下的。他需要记录一开始有几杯，然后分别是什么类型的，然后喝了几杯，送给别人几杯，最后还剩几杯，其实是这么一个问题。但他第一大家要理解为什么他会答错，是因为GPT它的底层transformer是用的注意力机制，但注意力机制如果你给的重点太太繁复了，你给的点太多，他不一定找得到，那他回答错误了，首先他回答错误了，那我们看看正确答案是什么。
	他告诉我们回到家的时候还有六杯咖啡，这个是不对的那正确答案是GPT4的答案，这个prompt没有变化，但是GPT4做了非常好的一个抽象，这个抽象就已经接近于人的一个抽象了。GPT3.5和GPT4，首先我们提示词没有变化，那它为何能够做到同样的输入，他得到了不一样的答案，一个错误，一个正确的。第一是因为OpenAI本身一直在迭代这个模型，基础模型在迭代，那他在在迭代过程当中使用了一些finding的技术，就导致他的逻辑推理能力更强。你可以简单理解成把我们刚刚看到的chair salt这种其实工程的能力内化到了它的ChatGPT的应用里面。因为ChatGPT本身你可以理解成它就是一个AI agents这样的一种APP大语言模型的应用。所以你看到GPT4的这个理解就非常成功，他理解了。
	一共购买了12杯咖啡，这一点是今天三点我还能理解的。然后喝了一杯咖啡送给朋友五杯，然后这个地方还喝了两杯热拿铁，一共消耗了8杯咖啡，所以最后只有四杯咖啡了。并且我们会发现他的回答是挺干脆利落的，并且没有太多的发散。我问的是到家还有几杯咖啡，他就告诉我几杯咖啡就好了，然后在今天3.5他还会去发散，他会去因为我中间描述了各种冰的热的拿铁美式，他还回答我一个这个答案是美式拿铁、热拿铁这个归类都出错了。
	有一个常识问题，这个是关于基础模型的能力评估方面的一些概念，而且不展开了。所以其实他的注意力机制做的还不够好。但其次回答正确了，这个就是通过基础模型的迭代和AI agents的from处理带来的一些争议，所以他通过了这个测验，关于刚刚提的这两个例子，大家有兴趣的话可以去看看这三篇论文。
	这个课件回头大家应该也能在这个录播里面看得到。一个是刚刚的cham forts，一个是self consistency还是trial thought，就是我们这个应用开发课当时的提示工程推荐的一些论文。好，那什么是AI agents呢？这边我们稍微轻松一下，看一个我们理想当中的AI agents。
	这是钢铁侠的这个电影片段。IT is efforts on expert to standing who makes them. 
	What is that? Look at me, but not unlike another. I like future. 
	出来i'm caving。这里其实钢铁侠想要重新发现一种元素来拯救他自己的问题。然后贾维斯作为一个AI的智能助理是非常强的。
	大家可以看到这里有各种AR技术，然后多模态的这算什么技术？包括跟海量数据去做检索的这个技术，甚至这个创造力。其实大家想象一下，我们离他还有多远。其实有很多的限制不在大语言模型，在一些其他的技术里，包括像阿法four，包括阿尔法four two发现了新的蛋白质结构，跟这个视频当中其实要用的一些核心技术已经水平没有差太远了。但是我们还有很多其他的技术没有达到，比如说AR元素。
	好，这就是我们。看到的理想中的状态。这个贾维斯可以说是当下所有人对于未来生活，如果你想要拥有一个智能助理，要达到的一个最完美的状态了。
	我相信那么AI agents AI的智能助理是一个什么样的东西？它什么时候被提出来的？然后为什么它能成立？为什么大家对AI agents，包括像auto GPT这样的一些agents如此的感兴趣？其实是跟大模型息息相关的。就这一轮的大模型的发展直接导致了我们觉得像嘉伟这样的一些技术形态，或者说这样的应用是有机会的。只不过要达成javis的视频电影当中的这个效果，还有很多技术拼图需要去补充。
	但是好在这个世界上有很多牛人，比如说这个and mask对吧？他就在做推进基础科学和AI包括很多很硬核科学的一个进展。当然这个世界上还有很多人都在做这样的事情，包括像OpenAI类似的一些公司。所以我们能看得到技术是在飞快的一个发展的。
	大家如果去回望十年前、20年前的整个世界，那个时候是技术的增长是非常缓慢的。但就像刘慈欣讲的，这个技术会爆炸。十年前的我们连微信都还没有，20年前的我们打一个电话可能都很麻烦，还要去找一个座机。但现在我们再来看这些技术就一天一个样，因为它处于一个高速密增长的一个状态。
	我们回到AI agents这个角度来看，怎么理解agents？它的核心就在大学模型。在react这个范式提出之前，其实大学模型一直被两种方式在使用。一种就是我们看前面看到的这种chair shots。就是我可以把一些复杂问题拆解之后，交给大模型，大模型来帮我一步一步拆解问题，然后去思考答案是什么，然后去做推理，它有一个推理路径。另一种就是用大语言模型去调用外部的各种各样的工具API平台，就像强化跟你一样，他可以跟环境去交互，然后拿到一个结果。只不过他不会在这个过程中再去训练大模型了，而是说根据这个结果由大模型再去判断做什么事情。
	就比如说一个最简单的例子，就是一个经典小品，那个把大象塞进冰箱需要几步，对吧？那你用act only的模式，那大模型就会告诉你需要三步。第一步把冰箱门打开，对吧？第二步调用一个机械手臂把大象装进去。第三步把冰箱门关上，这是act only。那么这两种模式他们都有局限，所以提出了一种新的范式叫react，就是reasoning加action。
	就像刚刚视频当中的一样，就是一个个大语言模型，它其实就真的像一个核心大脑了。包括我们的这个开始有一幅图，这个大脑它会去像人一样的去思考，去把一个复杂问题或者一个大的目标拆解成很小的目标。然后拆解成小的目标之后，他不仅有脑子，他还通过环境当中的对接，接上之后他能做各种各样的事情。机械手臂包括像刚刚查询数据库，然后帮他去查询历史数据，甚至用大学模型基于这个环境当中的一些反馈再去做进一步的推理。这个其实是一个非常常见的状态。而目前南茜的agent生态其实也在往这个方向去做发展。
	我们时间关系我们就简化一下，就agents现在大的方向来看，其实是分成三种不同的类型的。我们刚刚看到的javis其实是属于自主智能体，它是非常深度的还原了react这种模式。因为react是一个非常通用的底层的一种范式它的核心是啊你想象一下就有一个jv斯，他能自主独立的执行长期目标、大目标，这是自主智能体。当然除了这些这种agents以外，这种是相当于比较高级别的智能那么除了这种以外，还有一些简单的智能体。比如说这个action agents，它其实就是你可以把它想象成一个带有一定智能的传送带。然后这个传送带上是按照既定的这个序列去生产内容的。
	比如说像OpenAI的这个function calling，包括像简化版的这个react，这个react特指的是这个南线里面实现的这个react，还有一些模拟代理，比如说我们看到的stanford这个AI的这个小镇，还有一个类似于twitter的一个创业项目。把一堆AI agents放进去，大家一起发twitter，人不能参与人最多看一看，这种类似的属于模拟代理，他可能会大量的未来应用于这个游戏领域，他他在模拟环境当中去模拟一个人，他的终极可能就像我们这个西部世界一样，有一些模拟角色，大概是这样的一个智能代理的一个分类，我们可以有一个简单的了解。如果这些同学没有学过南线的话，可以去看看，已经做的非常好了，并且这个数据应该还不是最新的，现在应该还有一些新的contribution进去。
	那么RAG是什么呢？就我们讲到AI agents里面有一个很重要的词，最近非常火，叫RAG，这个retrained l augmented generation就是检索增强的生成式的应用，其实RAG不是一个很新的概念，甚至在rng这个词提出之前就已经有这个东西了。只是有的人很会抽象，很会提概念，把这个词又整的很新潮。
	那么igg核心是什么？它其实就是把我们从外部拿到的各种数据，通过处理之后变成向量数据库当中的知识。就我们这儿看到的storage会有一个vector store，有向量数据库。然后这些知识可以通过提问的方式，因为最终是一个深深层次的应用，我们可以提问通过提问的方式把问题交给我们的大语言模型，或者说交给我们的这个RAG的应用，然后这个单元模型它首先也许会去做一些处理，就看你的这个RAG的应用做的好不好了。那如果你做的非常简单，那可能这个问题就会直接变成一个embedding，变成一个向量。这个向量就会去你的知识库里去做检索，这就是检索增强生成的第一个R的部分。Retrial一些跟你的问题相关的答案。就是我们这儿看到这个retrained l的这个模块里找到的一些相关的一些文档碎片，release please，我们能看到在这儿，因为你的文档太大，不能直接丢进去，把这些碎片做一些整合，可能是直接结合在一起，或者是做摘要，或者是做各种各样的情况都可以。
	取决于你最终这个增强部分要怎么处理，增强完之后，最终要变成一个prompt给到大语言模型，那这个prompt就看你自己的提示词模板怎么去处理了，如果这个提示词模板是交给大元模型说，我现在就是一个FAQ的场景。那这个代替我们原来的让用户去搜索FAQ，我有一个机器人，这个机器人是可以把我们就比如说国家政策，法律法规，比如说这个建筑行业的这个相关规定，比如说你公司的这个员工手册都可以变成知识护理的内容。然后通过一个RAG的chatbot，它就能够把查出来的相关内容整合润色。然后再告诉你，比如说你问公司几点上班，那这个知识库里面存的有啊每周一到每周五早上十点下午七点，中间可以休息一个半小时，吃饭之类的，那这个可以通过大元模型来。但如果你问了一个问题，这个很刁钻，知识库里没有然后RAG的这个应用通常也都能够知道检索不出来答案。那检索不出来答案你就可以再做进一步的处理了，你可以把它作为一个离线记录去补充知识库，也可以在这个过程当中，直接跳转到一个人工逻辑，去联系HR或者行政人员，或者说一个既定的模板套路告诉他这个问题我们还在研究，请你等待我们的回复，我联系谁谁谁都可以。这个是一个典型的RG的一个实现。为什么叫基于能欠？
	是因为这里的很多模块都是用能嵌已有的这个基础模块。如果上过咱们这个大模型应用开发的同学，应该是很了解这个背后这个技术了，这就是一个IG的实例，通过radio这样的一个hugg face开源的一个库，把它变成一个图形化的界面，然后就可以模拟一个房产销售的机器人，通过这个数据的处理变成知识库，然后通过大语言模型再去做结合，就能回答类似的这个IG的一个效果。接着再讲这个四阶技术，讲到了前面的两个部分了。那后面的这个fine tuning和我们的free training，其实为什么会有这两个技术被提到这么高的高度，是因为基础模型时代到来了，就foundation model这个时代到来了。
	我相信很多同学提过一个听过一个词叫做OOP，就是面向对象编程。就很早以前学我高中学这个pyy cle，包括后面学这个C加加的时候，都有学这个面向对象。那会儿可能你当你理解面向对象之后，你知道它的好处，它是一种很好的设计模式，能解决很多的问题。但是到今天这个时代，其实关于编程这件事儿本身，我们怎么样去学习编程，怎么样能够高效率的去产生有价值的有生命力的代码，是一个新的命题。那这个新的命题是因为什么产生的？是因为foundation model的一个快速发展，我们现在讲叫做基础模型时代。其实最核心的就是我自己在看到这个ChatGPT出现之后，尤其是看到它的效果之后，当时发了一个朋友圈，我就在讲。我们未来应该叫做面向基础模型编程的一个时代了，就是FMOP的一个时代了。
	大家其实看这个图就非常显著。左边这个基础模型的左边，大家去做free训练，训练的是什么呢？训练的是各种各样人类在这几十年互联网发展过程当中积累的资料。然后有文本、图像、语音、结构化的数据，三季的信号，当然还会有各种各样的多模态。这些数据喂给了大语言模型，变成了一个基础模型。这个基础模型为了适应下游的任务，去做了一些反应处理。
	或者说这个应用可以变成各种各样的应用场景。比如说QA我们刚刚看到的这个RAG的这个聊天机器人，比如说这个语义的理解，做舆情的监控，信息的抽取，图像识别。当然最明显的感觉就是好像foundation model要its all，它要把整个世界都吞噬了一样。
	我们看到ChatGPT最近增加了图像识别的能力，它的OCR技术和图像识别技术非常强，有很多人在做测试，你把这个自动驾驶应该去识别的这种高速公路场景或者这个公路场景丢给ChatGPT，让他用这个图像识别技术，它是能回答解析的非常准确的，它的结构化的抽象能力非常强。那么condition model一定会成为一个巨大的大脑，然后这个大脑会去解决很多的下游任务。在之前的这个分享里面我讲过foundation model未来就会变成一个接受过通识教育的一个大学生。我们要学会利用，身边多出来了这么多助手，他还很便宜，甚至是免费的。你要怎么样把自己变成一个团队，然后你跟一堆的AI去做协作，就能达到一个生产力的提升。这个是未来基础模型时代所有人都会去改变的一种生产方式。我相信很快就会有一定的公司跑出这样的结果。
	说回来技术的部分，fine tuning跟retraining这一段怎么出现的对吧？其实回到18年的时候，就大语言模型这个词刚刚出还没提出来，甚至大家都不叫大语言模型。那会儿大家都叫free training，大语言模型都是后面提出来的这个词了。那会儿其实我们就已经有这样的一个概念了。因为在深度学习的这个领域就有five的概念，我们训练了一个大的网络，然后在一些小的场景里面，用一些小的数据去做一些微调或者叫金条，让他对这个场景里面的数据更敏感。
	在大语言模型早期的时候，像bert，像GPT阶段，其实就已经提出这种范式了。对这种范式它能解决很多的问题，这个就包括或像first是首先提出来用这个未标注的数据就能够去，所谓的未标注的数据就是我不需要有很多的人再去像image net这个图像数据集一样去做分包，然后大家去标。这是一只鸟，这是一只猫，不需要了。
	Bert的训练最核心的点就在于使用on label的sentence。大家能看到这里，去做preaching。这样就可以把我们需要做预训练的数据扩展到所有的互联网的数据，只要你的资源够。然后翻译成当然需要去做一些针对特定问题的一些标注，所以大家能看到这里这个MNLINER明天实际的T恤school，这个stanford的一个benchmark都是不同的特定任务，针对特定任务再去做反处理，那这种范式其实18年就提出来了。并且因为bert有这个开源版本，所以在ChatGPT火起来之前，整个以bert为主的家族，其实是更在整个聚光灯的这个中心，在舞台的中央。
	GPT其实是一个小的分支，当然GPT的成功就不是在简单的原来这个范式下的一个成功，就是pretend和by train的这个成果。然后我们看得到整个train的bird或者叫sixers to c ers。因为它本质是一个序列到序列的一个网络，有大量的一些到今天来看都还在做的研究，包括多语言的，多任务的，包括像我们看到加知识图谱的，多模态的，这些其实都是现阶段我们看到都还在持续去做。包括像这switch transformer使用的这个混合专家模型，我们在这文课里面也会去给大家讲对应的free train和这个five twenty阶段的各种理论的基础。这个混合专家模型像我们的GPT4也用了。所以整个我们这幅图里涵盖的技术，几乎就把大语言模型的prefer和factory里面的free train阶段的各种各样的妖魔鬼怪的这个搞法，就大部分都已经覆盖到了。
	但是GPT为什么杀出了一条血路？是因为他有很多的巧思，他在AI工程这条路上做了很多的建设。我们看到首先他跟bert选择了不一样的路线，它是一个decoder only的一个架构。然后在过程当中，他选择把我们的这个代码就get up也好，或者说其他的这个代码构建代码去做了代码的retraining，这个代码的free training，生成生成的模型code x，直接就使这个大语言模型开始具有一定的思维链逻辑推理的能力包括大家看到下面这条虚线框做了大量的魔改，包括这个指令微调，这个R1HF就基于人类的这个反馈的一个强化学习，包括去把这个chat，就是这种对话性。这个其实就是想跟大家强调的，为什么ChatGPT能成功具有核心其实是下面这条虚线，上面这条现在大家都是名牌，就像上面那幅图，看到了各种各样的技术，但下面这条线才是核心科技，因为它每个场景需要去做的都不一样。
	到今天我们使用的GPT3.5，你偶尔会去看到他仍然会使用这个text。达芬奇002这样的一个模型，而不是003这样的一个模型，他把一些人类对齐给回退回去，然后直接去去去一个更早期的版本去做调整，也是这个原因。我们不要简单的理解成大模型，下一个版本就一定比上一个版本强。每个版本它就像加feature一样。这样你打游戏，你给这个角色点了不同的技能，他有可能点完之后就不平衡了。一个敏捷型的英雄，英雄到后期你点了很多的力量，对吧？他可能两个都不强了，就变得贼弱。所以最终还是因为他要做这个ChatGPT这个chat的应用，所以他把chat这个功能加上之后，会让大家觉得跟人的想要的答案很像。因为做了人类的对齐，然后也做了指令的微调，让你不用去设计特别复杂的prot就能拿到一个好的结果。
	讲了这么多，什么时候应该用free training，什么时候应该用fine training呢？我们再来总结一下。首先从目的的角度来说，free training他更多的是当你有很多的没有标注的数据。比如说你现在就是要做一个中文的一个大模型，然后你恰巧你能访问中文的很多你拥有数据权限的数据。比如说你有所有的中文的书籍图库，然后比如说百度，为什么说百度文库是新的工作方式？因为他有啊他有这么多中文数据，假设你也有，或者说你要做一个特定的学科，然后你把这个学科当中的所有数据都能拿来去做这个微标记的训练，这个是比较好的一个场景。
	而微调通常是比如说我刚刚在这个场景里面，我把中文训练好了，然后我现在想要微调出一个模型。它就是专门用来发小红书的，他专门用来发这个微博的，专门用来发twitter的那这个时候你要干的事情其实就是让你的这个模型更适应在那些平台上面的说话风格，甚至说随着热点的变化，你还需要去调整。就像人看到一些热点新闻的时候，你跟你的朋友聊天，你会提一下比如说网上的一些新的热点，对吧？有好的有坏的，那么什么时候去用呢？其实刚刚我也讲到了，就是未标注的数据，大量的勇于训练。然后你有一个特定任务，你要用小钱办大事儿的时候，用微调。
	现在我们使用q ora比如说像几十亿规模的大元模型，一个T4或者说4090这样16GB的，或者24G的显存的GPU卡就能够去做微调了。优点当然非常明显。第一，预训练最大的价值就是把人类的知识，我没有标注的人类的知识用起来了。而且它可以使用这个大力出奇迹的方式，非常适合有资源的大公司去做。如果大力出奇迹，为什么说很多国内的大模型公司一开始搞的这个预训练模型也不错？就是因为只要你有资源，然后你的数据清洗做的相对到位，甚至你可以用GPT4去帮你洗数据，那你pretend的这个模型就不会太差。然后你再用这个模型为后续的一些微调任务去做这个特定的反射，那你基本上能达到一个正态总体平均分的一个水平。
	Finally的优点就是，第一他是站在巨人的肩膀上。它就像我们把pretrail当成一个通识教育下的大学本科生来说的话，微调就是要去把你训练成一个研究生，或者说让你在工作岗位上学到一个特定技能。那这个时候可能。就是师傅去带你，或者说你看一些这个工作手册，那他就是去适应，要么就是适应在研究生阶段适应一个特定领域二级学科，要么就是在工作岗位上适应一些特定任务。
	比如说现在有很多的研发就是这个circle boy对吧？天天面向数据库写这个CRUD。这个其实就是一类特定的训练。但是它的价值正在被消减掉，为什么？因为微调就能干这个事儿了，这也是为什么大家要逐步的去学习新技术的一个改观，再给大家看两幅图去去缓解一下情绪。
	也是用GP4和darling three生成的。我们刚刚看到那个大模型的四阶技术是我拿excel画的，然后才会有这个值，我也想让大力three帮我生成一个类似的这个图，但我们明显看到这两幅图就乱七八糟的对吧？他肯定是从他已经学过的，大家理解用bardi three和GPT4去做结合是很难的。
	我们在刚刚的这个birth那个图片里，我们再回到那一页给大家做一个分享。在这一页里其实是有一个多模态，就这儿有一个cross model，这么一个多模态的一个训练方式。但多模态的对齐非常难，他要把语言模型就in bedding之后的这个语言模型和你可以认为是图像视觉类的模型去做概念上的对齐是很难的。所以在这儿它就只能我让它生成图表，我告诉他横轴是什么，纵轴是什么。然后在这个增长的曲线里面是由低到高的，然后里面有四个关键节点，大家能看到首先横轴、纵轴，然后增长曲线他整明白了，但是这四个点他没有写清楚。
	比如说左边这幅图有prompt，它写成了什么叫prompt different in，我都不知道这是不是一个他造出来的词。然后find training他写出来了，free training它有啊，但是你像AI agents好像就没有了。那右边这幅图一样，就是明显是把一些原有的图去做了一些擦除，然后擦除的还不够干净，带来了这样一个效果。这个就说明还有很多的机会。
	为什么说多模态有机会？就在于现在的大语言模型还是一个语言模型，一个语言概率统计模型。怎么样能够把人类历史上的这么多图像信息结合起来，尤其是短视频起来之后，有非常多的视频信息。这些信息如果能够跟语言模型去做结合，那我相信我们离贾维斯又进了一步。
	好，这个其实就是我们给大家分享的AI大模型的四阶技术。接下来最后再跟大家简单讲一讲，如何去打造AI2.0时代的核心竞争力。就是我们看到了现象宏观的内容，看到了这个技术发展的四个不同的层次和它的一些关键的要领。那我们自己怎么办？对吧？
	我简单列了一个技术站，当然这个可能不一定全面，我们看这个图分也是分层和纵向去看啊。首先分层来看，最底层我们认为需要有理论基础，然后你需要去多用工具，最后你要去应用实践。从纵向来看，颜色也能区分，我认为左边这一部分其实是偏向这个大模型应用的，就是我们前面两阶技术可能会覆盖到的一些内容。那你需要用的技术水平，你要达到这些技术水平和你要用的工具，就跟我们那个大模型应用开发训练营高度相关。右边这一部分，蓝色的，包括右下角的这一部分，其实是跟大模型微调训练高度相关的内容。所以从这个视角来看，大家可以把它一分为二来理解需要使用到的理论，包括工具和这个应用实践的不同。
	好，我们再来看看这个关键技术战里面有哪些是我们应该关注的。比如说我们现在想要去学习这个大模型微调训练，应该具备一些什么样的基础理论，我们认为最关键的部分其实就是跟之前我做过一些分享类似，就没time learning，就是你要有知识结构的概念，你不需要把这幅图里面的所有东西都搞得非常清楚。但是你知道每一个框框它的输入是什么，输出是什么，解决什么问题，什么价值，这些是很核心的问题。像我们在经典的这个拷问，那时候5WYH对吧？是什么，为什么在哪儿，然后之类的，这个其实是一个理解一个概念最快的一个方式。然后当你理解这个概念之后，你形成了一个知识结构，你就能够更快速的去做学习。
	在这个课里面我们希望要理解怎么样去做大模型。微调和训练有两个层面上的理论基础，一个是需要了解什么是基础的神经网络。当然这个课里面不会去写神经网络，也不会去碰这个py touch或者TensorFlow。但是你要了解什么是神经网络，有个大概的概念，它其实就是一个用来拟合函数的一个玩意儿，你他是表示学习的一部分。你给一堆数据进去，它能拟合出来一个函数，这个函数的分布符合这个数据的分布，这就是神经网络。那他怎么样去训练让这个函数符合这个分布呢？需要通过反向传播，做梯度下降，去修改这个模型的参数。
	这一套东西在大模型里面仍然适用第二个就是要理解什么叫嵌入表示，什么是in bed，为什么要用英贝利可以去统一知识表达，我们有这么多的不同类型的模态的数据，需要把它弄到一个统一的向量空间当中，我们就能够说普通话了。这个是embedding的价值。那什么是提示工程，你要去简单的用一用这个类似ChatGPT之类的这个呃应用，使用prompt来实际有体感的去使用它。然后接着我们就会讲到大语言模型和大语言模型的微调和训练了。
	在大语言模型里面上一门课其实是理论篇，给大家花了几个小时的时间讲过attention和transformer，以及大元模模型GPT e和bert。那这节这门课其实更多的会聚焦到四阶技术的后面两个部分，会跟大家讲一讲这个find tuning当中的一些核心技术，当然没有列完，比如说P20，p tuning，比如说这个lora PEFT是一个更宽泛的概念，大家就刚好看到上面有一个PEFTPFT是大模型之后提出来的一个新的概念，叫做parameter efficient tuning，简单来说就是大模型太大了。原来大家一张卡就能find充电了，现在一张卡反应通电不起了。但是我又想翻译充电怎么办，那就想想办法做量化，然后做不用全参数的fine tuning等等的那这一些所有的fine in的技术都可以叫做PEFT。那么这个PFT hugg face有一个库，它其实是实现了很多经典论文a state of art的这个PFT的方法，我们会去讲这个PFT这个库，当然还有transformer这个库。因为我们就想象一下，为什么把hugin face称作大模型时代的github。就是因为在大模型出现之前，我们大家都用github去存代码就够了。
	因为存源代码其实不需要特别多的空间，但是大模型不一样，一个大模型可能就是几个G甚至几十个G，那几十个G的模型参数我存到github上面，大家都知道github可以走HDP或者SSH的协议那这两个协议本身倒没什么毛病，但是你去拉这个代码的时候，经常会拉了半天拉不下来，因为几十个G，所以哈根face这家公司很聪命，他们的发家史也很有意思。大家可以看一看，这公司一开始想做个ChatGPT，技术不成熟。后来就做了一个服务于所有想做ChatGPT的人的公司，并且是从开源开始的，做了这个库，然后把很多机器学习相关的算法和工程师想要解决的问题，他帮你解决了。所以它是一个很成功的一个开源和商业化的公司。它有transformers，有批EFT，有radio，有一大批非常火的开源项目，并且非常有生命力的被大家在广泛使用。这个是工具层面的，这个PFT的库就对应着翻译处理里面的各种技术了。当然transformers是一个更基础的库，它是用来你可以理解管理这个大模型的加载，包括这个呃embedded ing，包括去计算你的这个token之类的一些库的，都放在这里。
	然后还有一个库是叫做deep speed，这个是微软开源的一个库。这个库你可以简单理解成，你看这个图标就能知道这个logo，它可以提速，它是用来加快训练的。这个加快就体现在分布式，它能支持分布式多GPU卡的训练。同时它还有一些RLHF，包括chat相关的一些你可以认为是模块或者它的延展，可以让你更方便的去做find doing的这个提速，这里还引入了两个模型，一个是ChatGLM，一个是lama。
	当然我们课里还会教大家怎么样去训练这个OPT，也是meta这家公司开源的对标GPT的大约模型，GPT OPT就是open的这个general transformer，这个是我认为工具层面上大家要去学会的，就是先有理论，理论决定了你做事情的天花板。但如果理论你一开始学的学不来，你就那你就只把这个理论这里的每一个小框框当成一个黑盒子。你知道这个黑盒子输入什么，能拿到什么，也可以能支撑着你先去把工具用起来，先去把应用实践起来，然后变成一个环，就像我们刚才看到react一样，变成一个环，你的理解不深，但是你先用起来了，你开始做action。然后action之后你拿到了一些结果。这个结果你有的能理解，有的不能理解。不能理解的部分你把你自己当成一个大模型，你再重新来看看理论的部分，你发现你现在能理解的更多了，就变成一个好的react的一个模式，这个是我们认为在当下来看，大模型的从四阶技术的角度来讲。从prompt engineering到我们的AI agents到fine到free training，需要掌握的理论和工具层面上的一些必要的概念，到应用实践部分，其实我认为就更更清晰了。比如说从应用实践的角度，如果你不会写代码，你只会是自然语言，对吧？
	那你甚至连英语都不会，那你就去把ChatGPT的prompt用的好一点，然后去看一下有很多经典的problem都怎么写的。当然看这个也可以去学习理论。因为你学习了理论，你就知道什么是思维链了，你就知道要怎么写出好的problem。你能自己具备一堆的prom template。这个过程不就像大家早期的时候，自己会有一堆PPT模板吗？或者有一堆各种生产力工具的模板。其实一模一样的这个视角来看你把ChatGPT当成生产力工具，你做了一堆模板不就是能起效，对吧？
	如果你现在不会做微调，你不懂什么是GPU，然后你也不想深入去研究，那么去搞搞RAG，搞搞agents。当我把AI agents放在右边，是因为AI agent是一个更大的概念，RAG只是一种AI agents，那么RAG就只需要你调OpenAI的API就够了，你不需要部署大模型，你甚至都不需要了解find phone和free drink，你就可以通过open a的APP PI和冷却构造出各种各样的工具。这些工具是非常好的，能提效的。就比如说刚刚的那个销售机器人，或者你做一个公司的这个产品文档，或者这个手册，或者员工手册之类的机器人。
	这个一定会是未来的趋势的。因为所有人现在都信息过载了。我们一定会从一个一开始我们上网搜信息，因为网上没多少信息，到现在充满了信息，我们要去一堆信息的海洋里面找我们要的信息，并且找的过程当中充斥着广告，非常讨厌。
	然后进化到下一个阶段，一定是说好，我们还是会去一堆广告里面找我们要的信息。但是不是我们去找了，是你的AI agents去找了，然后你会告诉他怎么找。但这个时候会有一个魔高一尺，告诉道高一丈的过程。你会想象广告也会越来越巧妙的。但是这个过程很有趣，对吧？因为人就是在不断的技术进步当中，把商业和技术做一个双螺旋一样的上涨。然后如果你觉得你想要更进一步的去学习，你想要真正去做生产级的部署，你想要把公司的这个大模型转型整到位。
	这个时候，有一些应用实践是值得去尝试的。第一个就是我们在这个大环境下，大家都在讲美国把中国的GPU卡死了，其实这不是一个问题。大家看手机芯片性价比出来了，我可以想象得到如果再过几年，可能国内的GPU也一定会有一个更大的一个进步。这个是从宏观层面上我们看得到的一个路径。从个人层面上，我觉得有一个思考方式是这样的，就是如果中国大陆的所有人都买不到GPU，那这就不是一个问题了。因为当你的竞争范围没在海外，你只是在国内卷，那你无所谓，大家都一样，大家的起点是相同的，那么就不要担心这个问题。因为第一这个问题你解决不了。第二就是说他一定有可替代方案，你不用去想，因为卖GPU的人，挣这个钱的人会去想你更多的要解决的是怎么把GPU用好。
	那有两条路，第一个是说你还是能找到路子，你可以用英伟达GPU。那么课程里面肯定会还是以英伟达GPU为主，但是我们会有一张去以华为的生成910为主来讲怎么样去运行微调一个ChatGLM26B，在国产化的这个GPU上面去做。当然大模型的微调这个技术是非常复杂的技术，我们刚刚也讲过有很多的这个套路，我们也会尽可能的去覆盖多。我们通过三个模型，不同的硬件组合，然后不同的框架，让大家尽可能能学完这个课程之后，在公司或者在你想要去的某家公司里面，能找到一个好的工作机会，然后能够像滚雪球一样去做更多的事情。
	整个课程，其实仍然是一个八周的时间，会有六个实战的案例。然后我们会从理论工具到应用实践这三个层面上去跟大家讲我们的大模型的微调和训练，这个技术应该是要怎么去做。包括这里我们看到的这个大模型微调的技术有很多种adapter的prompt prefix，包括lora这种Laura ADA的这个Laura a和我们的q Laura，然后我们也会去介绍不同的这个库，就我们刚刚讲到的这个工具，hugging ACE的transformers，PFT，以及microsoft的deep speed这些工具库。最后我们也会花时间跟大家讲一下这些国产硬件，包括在这个meta OPT上面去做这个pressure。然后这个是刚刚提到的几个主流大模型的微调技术的一些截图，就是我们最开始第一周第二周就会给大家讲的这个理论部分，我们的这个课程授课也会沿着理论工具和应用的这个层次，让大家不断的去学习。我们大模型应用开发的这个训练营的同学应该已经有体会。我们的课程设置是一个由浅入深的过程，让大家能够第一遍就能上手搞很多事情。然后当你搞完之后，你回过头再来看一遍课程的时候，会有更新的一些体会和成长。好，这个其实就是我们今天想要分享给大家的这个内容。看看主持人这边有没有什么线上的问题，我们来做一些QA。