	我们就正式开始今天的分享，非常高兴能有机会跟大家在工作日周二的晚上来做这样的一个分享。今天跟大家分享的这个主题叫做大模型微调的前沿技术与应用。我相信很多之前听过我课程以及听过我分享的同学对我比较熟悉了。我叫彭俊田，是谷歌的开发者专家，也是能券的developer，一直在从事AI这方面的工作研究创业，也有十多年的编程经验了。
	今天我们直接切入主题四个部分来跟大家讲一讲我对关于这个大模型微调技术的一些观察和总结。第一个部分我们会来讲一讲大模型微调技术的一个发展与演进，就大模型微调这个技术本身是怎么来的，一步一步，尤其是几个比较主流的技术路线和方案。第二个是大模型既然要去微调它，要去应用它，那需要我们利用工具和框架。那现在有哪些开源的大模型微调的框架和工具，我们做一个简单的介绍。第三部分是我们国产化大模型的一些技术站，也包括我们国产的大模型ChatGLM以及国产的算力，华为的升腾，最后会有一个关于未来的一个趋势的一些分享和挑战的一些观察，一共这四部分。这四部分其实跟我们明天要正式开始授课的大模型微调的这个训练营是息息相关的，这些内容也是我们整个训练营课程要讲到的内容。今天只是抛砖引玉，也给没有报名的同学有一个初步的了解。
	好，那我们先来看一看第一部分，就大模型微调技术，大模型微调技术是怎么来的，它不是从这个石头里蹦出来的，我们又得看到这张经典的图，就AI历史上的第四次大的发展。从最早的1950年代我们看到AI开始做一些早期的研究，到80年代的机器学习。一直到2010年开始，通过互联网我们产生了大量的数据。然后通过image的这个数据集以及英伟达显卡的一个发展，我们发现大规模的分布式的深度学习模型可以开始训练了。然后我们在深度学习这一轮的浪潮，也诞生了非常多的厉害的公司和算法模型，包括大家都知道的race NET这个网络，它可以用来做识别，就像我们这张小图里面看到的，能够做猫的识别。
	一直到2020年，我们看到GPT3的论文发布，并且基于GPT3的API也在年底正式对外开放了。所以在2020年底2021年的时候，其实在硅谷，就是在美国的加州，已经有很多的AI基于AI大模型的初创公司开始成立，并且拿到了大量的融资，比如说jasper AAI或者copy I这样的公司。只不过这些公司经过了这两年的发展，没有赶上OpenAI自己的发展。OpenAI从GPT3到GPT4，一直到我们最近看到的这个dev day发布的各种各样的新鲜的技术。
	比如说我们的这个code interpreter，比如说我们GPT4V这一大类GPT s等等新技术其实是超过了大部分初创公司的进展的那我们到大模型的这个时代，AI的AGI的开启的这个时代，这四个时代有什么共性呢？是有一个最大的贡献，就是每个人都会谈到的一个词叫做模型，模型是什么呢？其实很有意思，我相信很多不是CS背景或者做过AI的训练同学可能会对于模型是充满神秘感的，就感觉模型很神奇。然后我们不管是什么样的模型，不管是下象棋的、下围棋的，还是我们刚刚看到的猫狗识别、人脸识别，甚至是我们的GPT或者其他的大语言模型，都感觉模型很神奇。就像很多人说的，它是一个黑盒子，我丢进去一个特定的输入，经过模型的一堆操作，得到了一个我想要的输出。我们也通常把模型当成一个黑盒子。因为随着深度学习第三轮的到来，以神经网络为主的这一类模型，它的可解释性确实不够好啊，因为它需要大量的数据喂养去让我们的模型学习一个分布，当我们提到分布的时候，讲一点点的这个基础的数据，我们都知道一个模型其实它的核心是为了拟合一个特定的数据分布。就像我们刚刚讲到的猫狗的识别也好，我们的ChatGPT使用的这种大语言模型也好。
	它的核心都是说给你一个特定的输入，然后我有一个期望的输出，比如说针对猫狗的识别，我希望它的输出是一个类别标签，给你一张图片，你告诉我这里是一只猫还是一只狗，给你一个检测模型，然后给你一张图，你告诉我这里有几个人，每个人是谁啊，这些人在不在我的人脸库里面，有没有这些人脸的ID？如果有ID就在我的人脸库里面。如果不在这个人脸库里面，可能就是一些未知的没有入库的人。就像我们在每一个小区的门口都会有这个门闸。那你进得去，因为你在小区的入库里面了。如果你进不去，因为你还没有录入，你不是他的这个业主，你无法进入这个小区。
	这个都是生活当中随处可见的一些深度学习模型，而大语言模型其实也不例外，它本身也是以神经网络为主的。我们都知道像GPT或者bert或者T5这些主流的大屏模型都是基于transformer的架构。而transformer本身就是一个神经网络，所以本质上它们是一回事儿。
	而说了这么多的模型，什么是输入，什么是输出？回到数学的形式化的表达上来说，不管是图像还是音频还是文字，我们最终都可以把这个输入变成一个X等于X1到XM这个M为长度的一个向量。这个向量我们学过应用开发的同学都知道，我们讲过evading。Embedding就是用一个统一的向量空间来表达我们的输入。简单来说就是统一了知识的表达，那所有的输入都可以变成这样，图像也可以通过RGB，灰度值等等变成这样的一个向量。语音这些声音的信息也可以通过识别变成文字，文字再变成这样的向量。所以输入我们整明白就是一个很长或者说很高维度的一个向量，输出也是一样的，我们的模型如果我们的模型本身像transformer，它是一个encoder decoder的结构，输入一个X输出一个Y可能他们的维度不同，但输出的这个Y，这个向量最终是能够被解码成一个我们识别得到的一个人能够读取的知识。
	比如说我们刚刚说的类别标签，比如说我们能够读懂的某一门语言，中文、英文、法语等等。中间的这一部分我们称之为模型。而模型其实简单来说，我们在学习这个应该是中学的时候就学过什么叫函数或者什么叫映射。其实整个深度学习的模型，或者说这个连接主义神经网络这些模型都是一个这样的范式。
	Y等于WX这个范式其实就是一个函数，而这个模型就是这个函数F，或者我们在这个表达式里把它称之为W一个带有权重的矩阵。不管是我们在应用开发课理学的注意力机制里面的注意力模型，还是我们学的transformer里面的编码解码的模型，或者说其他的大模型，最终他都可以用一个矩阵来表达。而这个矩阵就是一堆的参数，而模型在学什么呢？你可以简单想象一下，一开始这个模型什么都没有，这个W里面的值它也不知道该取什么。那为什么就要用数据来喂养它呢？就是因为我们用大量的X去输入，输入给到我们的模型里面。然后让我们的这个模型在学的过程当中，有一个所谓的标准答案或者说参考答案。根据我们的模型不同，根据我们的训练方法不同，就像我们大家在这个K12教育里面会有很多的考试，有月考，有中考，有高考一样。
	这些考试都是为了让我们这些人在学习知识的过程当中，去去印证你这个人的哪些知识点学到了，哪些没有学到。让你的大脑学会这些知识。这个逻辑其实是一样的。通过这个数据和标注的一对一对的输入，其实让我们的模型开始把这些一个一个的小的W把这个权重的矩阵能够学出一些能够符合一对一对XY输入的这么一个值。这个值其实就是这个模型，所以也就是Y等于WX里面的这个W简单来说我们在hugin face，在github，在各种平台上下载下来的模型文件，其实里面存的就是这一堆W。我不知道这个有没有让大家初步的理解，至于真实的情况，可能这个W很复杂，不是一个简单的W乘X就能表达出来的，可能还会有很多的加减，包括政治化等等。但万变不离其宗，是这样的一个范式，而这个Y等于WX是一个抽象的数学概念。
	最终这个网络要怎么样被计算机跑起来？这里就要提到上一轮我们第三轮的时候，有很多的神经网络或者说这个深度学习的框架。比如说PITOCH TensorFlow等等。这样的框架现在变成了构建我们的神经网络和我们的大语言模型的一个基础框架。只不过大部分我们学习微调的过程当中，其实不用太关心这个tensor FLOW和py touch是怎么样去写的这个模型。因为微调的技术核心是要说模型已经在这儿了，这些参数，这些W1W2，我要怎么样去微调其中几个小的W的权重值。让它能够更符合我这个场景里面的X和Y，这个其实是微调要解决的核心问题。
	那么为什么叫大模型？我们再举一个很实际的例子，大模型大在哪儿？一个数据直观的对比。相信很多有过AI的经验的同学知道一个模型叫做rest net 50，rest net 50的作者也很厉害。Rest net 50现在也已经成为了学术界和工业界相对标准化的一个用来做识别的一个主干网络。这个网络其实大家今天我们都知道有什么千亿级的大模型。但internet 50其实它只有2500万的一个模型参数。
	如果我们单看这个2500万的模型参数本身，它所使用的显存甚至连可能就100兆1到200兆，0.10.2GB这么样的一个显存就能够存下来了。为什么写一个GB呢？是因为大部分的renee使用场景里面，它会批量去处理图片。而这些图片我们刚刚提过图像会有把它转换成不同的分辨率，高分辨率的像它占用的这个显存就大，然后它它会占用一部分的显存。
	而大语言模型又会有所不同。大语言模型其实跟我们的这个regnet 50这种工业级的常用的视觉模型比起来，大家看一个是啊65B，65B就是650亿，这还不是最大的。我们知道GPT3是1750亿，GPT4可能是八个迁移模型，那么resent 50是2500万，这个就已经差了三个数量级，几千倍的一个将近3000倍的一个数据规模模型上，所以为什么我们叫大语言模型？
	其实这个大字是真的很直观的。如果我们之前没有学过这个应用开发的课程的同学，可能没有看到一步步GPT1是怎么怎么过来的。其实GPT1那会儿也是一个1亿左右的一亿上下的一个模型。但为什么后面变得越来越大，跟我们大家都知道的一个词，什么涌现都有关系。但是涌现是不是必须要这么大，这个是我们后面看到未来趋势和挑战的时候，我待会儿可以分享的。
	只不过我们今天强调一下，这个大可以通过数据的方式让大家直观的去体会，是一个三个数量级，几千倍的一个差距。如果我们是一个千亿规模，甚至说万亿规模的话，那就是四个数量级的一个差距。所以大模型是真的很大，要把它跑起来也真的很不容易。所以我们既然这么大的模型，普通的一个个人开发者或者说一个小团队要怎么样去应用和使用这些技术，这里我们就要去考虑到更落地一点的微调这个技术本身了。
	为什么需要去微调大模型？我们都知道renee t50很少会简单的去用翻译to，可能大家都只会直接会去训练整个模型。因为它小，但为什么到大模型这儿就需要微调？第一个直观感觉就是我们刚刚提到的成本实在太高了。如果我们要去完成完整的去做一个大模型的预训练，就从0到1的去做运行点，把它的所有的模型参数都加载到我们的GPU里面，加载到显存里，然后再去跑一些数据来去刷这个W其实就是去训练它，这个成本非常高。780GB的显存只是加载这个模型本身。
	那我们知道我们训练过internet或者训练过其他深度学习模型的同学就知道，你的这个batch就是你每次丢给他的这个X如果越多其实越好。因为他更多的能学到一一段儿一段的输入。然后这一段的输入如果量更大，就相当于你每一次刷的题，你不是只刷一道题歇一会儿，唰一道题学歇一会儿，而是你能一次刷一本书，那你肯定学的就快。所以你如果真的要去进行这个预训练的话，可能就是上万GB的这样的一个显存需求。我们都知道这个几乎不太可能，对于我们的小团队和个人开发者来说，这个是一个直观上的一个限制。
	第二个就是说很多同学都在想，应用开发的时候，我们也都在讲提示工程，其实工程是非常好的一种做应用落地的方案。但其实工程从模型侧，它会有它的一些天花板，最大的天花板就是任何提示工程，它都要依托于一个基础模型。而这个基础模型它是有一个token上限的，我们知道这个GPT4之前的上限是32K最近有了一些增长，有四倍的增长。我想中def day开发了一个是128K还是64K的一个更高的更长的上下文的一个版本。但是这个上限其实就卡在这儿了。
	而我们用过提示工程的同学就知道，不管是这个react这种范式reasoning action，还是像auto GPT，会反复的去读取memory，然后来给到大元模型去做这个推理和操作的这样的一种范式，其实工程会大量的消耗token，如果你是直接通过TPTAPPI的话，可能你一个月会调用大量的API的成本。我们知道上个月的时候，应该是赛奥特曼在某一个线下的论坛上面有提到，现在OpenAI一个月的收入有1亿美金。那么一亿美金的收入其中大部分其实来源于GPT的API调用，所以大量的调用其实是也是一个很高的成本。同时假设你不是去调用API，你是自己部署了一个比如说开源的模型。比如说lama，你把这个lama部署到你自己的云或者GPU上。但我们都知道，学过transformer的这个理论的同学都知道，这个token你要去生成一些内容去做generation。你生成的这个completion越多，其他的推理成本越高，因为它跟你的长度是成平方关系的。
	假设我们举一个简单例子，你生成十个词和生成100个token的成本，它不是一个十倍关系，是一个100倍的关系。那你要生成1000个的话，那它就是一个这个就不再是一个百倍关系，是一个1万万倍的关系。相比于生成十个token，所以这个时候你想要很稳定的提供一个服务，在你的私有化环境里就需要部署大量的GPU去做这个多个节点，让你的提示词能够快速的得到它的响应结果。所以提示工程的天花板其实非常明显，这个我们就不再做更多的解释了。第三个就是说假设你没有去做微调，你只是部署了一个开源版本。但是我们要解决一些特定领域问题，而这些特定领域的问题，可能在我们的基础模型里面就是没有这个数据的。
	为什么现在国内有这么多的大模型公司在创业？有一个客观存在的价值就是在GPT也好，cloud也好，palm也好，这三家硅谷目前最大的，包括lama，这四家非常大的大模型公司，成果非常好的大模型公司，他们的中文语料都非常少。而中国人有这么多人，十几亿的中国人，这么大的一个市场，他需要有非常擅长中文的大模型，以及对应的中文的benching mark。所以像中把中文搞好，为什么大家都在讲这个汉语最好的大模型，中文最好的大模型？就是因为中文的语料确实少，在我们的刚提到那四个大模型里，所以哪怕是一个特定语言或者说更小一点，在一个特定行业里面，它的领域特定数据都是有价值的。而这个数据不在基础模型里，你通过提示工程也很难给到他。那么这个时候通过微调是一种比较好的手段。
	第四个就是我们刚刚提到的，既然提到领域数据，数据安全和隐私也是一个方面。那怎么样能够在自己的信可信任的环境里面去做微调，就能照顾到我们的数据安全和隐私。还有一些未来我们畅想，比如说基于大语言模型会出现各种各样的应用。那这些个性化的服务一定也需要这种私有化的微调。
	大模型就像上一轮深度学习发展，我们一开始都是在云端用TP serving来部署我们的深度学习模型。后来出现了TFTF light或者这个hony x以及这个entire的open wino。Open wino, 都是为了在边缘侧去部署一些轻量的模型。未来我们看得到大模型也会有轻量的版本，比如说几十亿的这个规模，那几十亿规模的模型未来会逐步的部署到，我相信会到智能家居，或者说自动驾驶等等的场景里可能都会有模型。那这些模型就需要私有化的部署，个性化的服务，那自然需要微调，不可能大家都用一个，包括联邦计算，隐私计算等等的一些技术也都在逐步的迭代，去使我们在这些个性化服务里能够支持私有化的大模型。
	这个是从，我们分析了一下这五个角度来说，微调大模型是一个非常客观。不管从学术上、技术上，还是我们的市场应用以及未来的发展上都是一个一定会需要的技术。只不过根据你的个人场景，你需不需要去掌握这门技术，包括最值得我们学习的一家公司。在当下来看就是OpenAI。它的GPT1到GPT3到GPT3.51，直到GPT4，以及ChatGPT这个大模型应用。其实它的整个模型迭代过程就是一个预训练加微调的一个发展的过程。
	这幅图其实也是非常经典的一个这个图，03年的一个综述论文，关于大语言模型的，其实我们能看到从GPT1到GPT3阶段一直在做各种各样的预训练。因为整个GPT的这个词，我不知道大家应该都理解它这个词背后的意思，就是general pre trained transformer，就是一个通用的预训练的transformer。这个模型本身就是一个预训练模型，只不过为什么到后面预训练人搞不动了，一直到dev day的时候才有一个大的跟进。就是我们这个预训练的数据从2021年变成了2023年的这个四月份，因为预训练的成本确实太高了，这是第一个问题，就我们刚刚提到的。第二就是说他把数据也用的差不多了，简单来说就是把我们公开可用的数据都导入到模型训练之后，还想要再进一步，这个时候微调也是一种手段，就是怎么样把这个公开的大量的数据训练好之后，把这个金矿的价值发掘出来。其实微调也是一个很重要的手段，就像我们下面这条线能够看到的这个code。
	达芬奇002通过这个instruction turning变成了text。达芬奇002也是我们现在的ChatGPT的GPT3.5版本经常会用到的一个模型。它他跟我们的003就是最有名的这个GPT3API的这个版本，text w003最大的区别是没有做RRLHF，就是我们的这个基于人类反馈的一个强化学习，他的目标主要是用来做这个价值观的一个对齐，让他的回答更符合人的预期。但是tex w7002的准确度召回度其实某种层面上是超过003的。然后到后面为了支持对话，让他的回复更像是在跟你聊天啊，最终做出了GPT3.5 turbo，一直到后面的GPT4，现在的GPT4S等等一系列的发展。所以整个OpenAI的迭代也是一个预训练加微调的过程。并且现在我们所有人都会发现，所有的初创公司，他们的发力点已经不在预训练上了，而是在微调上面。
	这个是最近闹出了很多的新闻，这大家也都知道，因为大家都在搞微调这个事儿。而怎么样把数据用好，其实就是微调技术衡量微调技术高低的一个很重要的点，所以我们不去评价这个别的东西，就微调本身来说，它是一个数据驱动的了。所以单纯看模型没变，数据训训用的好不好，训练的好不好，是能看出人的水平高低的。我们也不能说这个没有改代码就没有价值，这个也是不对的，但具体的这个情况我们要具体分析。
	好，我们再来看，就是大模型微调，我们刚刚说过了需要微调，OpenAI也在微调，那么微调具体有哪些技术路线？其实从大的方向来看，有两条技术路线。一条技术路线就叫做全量微调，four five teen, FFT。然后还有一条叫做高效微调，也是现在用的最多的，叫parameter efficient，efficient find tune EFT高效微调。而高效微调现在被广泛的运用，最大的一个原因就在于全量微调。这个全量的意思就是说我要把所有的模型参数都加载到我的显存里。只不过我在微调，我不一定全部都去调整它，我可以锁住一些就跟我们做regnet 50或者说我们做深度学习阶段的时候，我们也都会知道它的fine two也是全量加载，但是会冻结一些layer，那么全量微调类似高效微调，就相对来说简单一些。
	那高效微调里面有一些三个大的方向也是经常会出现的。尤其是在像国内的这个文馨包括其他的一些公司，都会有这个SFT这样的一个词，叫做super rise find，有监督的微调，简单来说有监督的微调就跟我们高考一样，有标准答案。就以前监督学习的这套基于标注，有监督的这套训练方法，叫做有监督的微调。那么还有两个其中RLHF是大家听得最多的一个词，基于人类反馈的强化学习，其实它它的优点是在于它的训练质量会比较高。但它的缺点在于因为需要人类介入，所以它的效率不会特别高。所以因此在这个发展路线上，现在又有一条新的路线叫做RLAIF。就是把人类换成AI跟我们的这个auto GPT挺像的，或者类似的这种自主智能体，这叫做基于AI的反馈的一个强化学习，很有意思。
	整个大的技术路线我们可以分为全量和高效。但我们为什么通常不会去讲这个全量微调呢？第一是因为它的两个显著问题，训练成本高，灾难性的遗忘。训练成本高我们就不说了，一直在讲这个问题。
	灾难性的遗忘是什么呢？就是说其实真正能够掌握全量微调技术的公司非常少，就是刚刚提到那四家公司，甚至说就是OpenAI、anthropic和meta这几家公司，他们的全量微调简单来说就是你可以认为做菜可以分成几步，我们在做菜的过程当中，先把互联网上的大部分数据作为预训练做出来。一个预训练的模型，我们通常叫PLM，就retrained language models，这个预训练的模型训练好之后，接着去做全量微调。就像我们刚刚看到的OpenAI的发展预训练加微调。整个从2021年到现在，OpenAI一直在做全量微调，而且这个全量微调的工程技巧是非常多的那为什么要做这么多工程技巧？
	我们先不看成本的话，第二个问题就是在于当你不太懂，然后你有资源，你非要去做这个FFT的的时候非常容易遇到的一个问题叫做灾难性的遗忘。简单来说就是你把这个特定的领域数据或者说一部分的数据你去做微调。也许他训练的挺好的，那他原来擅长的一部分知识没了，已经忘了。
	这就是很简单，我们回到开始的Y等于WX里面，可能就是这个W被你强化了某一部分的权重之后，原来的均衡被打破了。导致某一块的可能这个矩阵里面，这个高危矩阵里面，某一块矩阵的值被调整的比较厉害。这个部分的原来的知识能力没了，这个是非常常见的一个问题。
	而能够解决灾难性遗忘就是现在这几家核心公司为什么这么高估值的一个原因。因为它其实就是把知识压缩，把人类这么多几千年的知识压缩之后存储到一个矩阵里。这个其实就是大模型的本质。而谁能够压缩的好，谁能够在这个过程当中，还能把同样规模的模型压榨更多的知识塞进去。这个其实就是这些最头部的基础单元模型的核心的差异点。对于我们来说大部分普通人来说，其实更多的还是要解决这个PFT这样的场景。并且其实在2020年的时候，GPT3这篇论文里面也提出来了这样的一个发展方向，这个是GPT3的论文里面的一些截图。
	我们简单来看，其实当年OKI发布了GT3这篇论文的时候，就有提到一个词，这个词叫做in context learning，在上下文中学习。这个in context learning其实当时被提出的时候，还没有这种所谓的其实子工程，或者说这个cham forts这种思维链之类的概念。那会儿为什么要提这样的一个词？包括meta learning言学习，以及最近我们看到OKI这个宫斗，然后又这两天提出的这个q star，q用这个q learning加上A心算法，其实都是自主学习。那么整个这个过程其实OPI一直在研究怎么样用最好的ROI或最佳的ROI去让我们大语言模型加上这堆数据的价值被发挥出来。
	其中in context learning提出的概念就是到当年的那个节点，就三年前GPT3，每一次训练的成本都是几百万美金，非常高。训练出来之后，我们发现一个大语言模型的价值每没有被挖掘完就它其实是一个很强的大模型。但是我们人类不知道怎么跟大模型沟通。那会儿的prompt其实极差，大家都不知道怎么跟大模型沟通，只知道这个大元模型它是一个生成器，它是一个decoder only或者说GPT3是一个以decoder为主的一个生成器，你输入一些内容，它能输出下文。那会儿的认知是这样的那我们就会发现这个in context learning一个核心概念就是大模型预训练好了，然后我不再去动这些参数了，我把这些参数固定好。
	但是我通过我的沟通艺术的提升，我通过这个怎么样能够跟大语言模型聊天聊得更顺畅，让他GPT3知道我想要什么，来得到更好的结果。这个其实是prom的原型，也是三年前OpenAI就已经在提出和主导的一个发展方向。所以in context我们会看到通过不改变模型而给他一些不同的示例，我们能拿到我们想要的结果。不管是做这个算术，还是说做这个单词纠正，或者说做这个翻译等等，这三个场景都能做。他把这个进一步提炼提炼出来了，这个叫做在上下文中学习，也就是后来的这个prompt。
	然后这个prompt又被它分成了三类，一类叫做zero shot，一类叫做one，还有一类叫做fuel shot。那会儿提出了这个词叫做natural language prompt，自然语言的一个提示。Zero shot就是我只给你我要做的任务，我不给你参考答案。而few shot就是给你一些参考答案，one shot就只给你一个参考答案。那么通过这样的prompt，他会发现大圆模型的水平居然提高了，就是它输出的质量变高了。这个其实是一个在当时算是一个很震惊的一个发现，尤其是在足够大规模的大语言模型上。这也是GT3这篇论文当时带来的两个重要的一个贡献。
	一个就是在足够大规模，我们能看到这里三根曲线，其实分别是GPT1、GPT2、GPT3的这个模型规模，一个是十亿，一个是130亿，一个是1750亿。在GPT1和GPT2上面这个小规模的模型上，其实prompt没有带来太大的价值。我们现在知道，我们站在这个上帝视角，回看三年前的时候，我们会发现参数已经不是绝对的智能涌现，或者说提示值有效的标准答案了。其实跟他的训练过程，跟他transformer的结构，甚至transformer的这个训练技巧都有关系。现在我们会发现几百亿规模的这个，但是几百亿参数规模的大模型上也会有in context或者说prompt tuning的提效的实际的一个结果了。所以我们看很多大元模型的一些结论，一定要看到本质动态的去看啊。
	我们回过头来说，在GPT3这个时代，我们就会发现预训练到了一个阶段性的阶段了，阶段性的一个结果了。然后我们会开始发展这个微调。而最早的微调是OKI说的，我们提升我们的沟通技巧。我们甚至不去调整大语言模型的参数本身，而是调整我们输入的X你会发现调整prompt我们回到Y等于WX这个模型里，我们其实是在调整这个X我们人为的去把这个X做的更符合那个W那就会达到我们的那个Y，在这种场景里W不变，Y是我们预期的答案，Y可能也是不变的，或者Y有一个小的范围。我们在干嘛？我们在调整这个X，调整X不改变模型，但这也是一种很重要的繁重的方法。我们待会儿会去讲他的理论进展，这就是具体的zero shot，one shot for your shot的这个表现形式和示例，这个现在大家都很熟悉prompt了，我们就不再讲了，就给不给参考示例。
	那么我们回到PEFT这种主流的技术方案这个话题上来，刚刚我们看到open OpenAI，这个GPT3，它已经人为的开始去设置一些prompt，或者叫ICL in context learning。那么这些东西都在干嘛呢？其实我们刚刚讲到它都是语言模型本身不变，W不变，然后我们再围绕X围绕token在做文章，所以我们今天可能会简单的分享两个围绕token做文章的这种翻译通灵的方法。一个叫prom tuning，一个叫prefix tuning。那除了这两个方法以外，就比如说我们的W不变的这个情况以外，还有没有什么方法可以去做PEFT呢？有并且我相信很多人都在各种各样的去这个平台上面看到这个词，叫LORA这么一个词。Laura Laura其实它也是一种用来做高效微调我们模型的方案，它其实我取了一个名字叫更本质的去训练它的低维模型，这个也是这个方法的一个核心思想，包括Laura的变种q lora和这个艾达Laura，当然还有一些新的思路，今天的时间有限，我们放到课程里面再去讲。
	既然fine tune我们可以在不断的去迭代这个fine tone的技术。那有没有一些用更少的数据，用统一的框架去做训这个微调训练的方法呢？像这个I3和这个uni poot，其实都是属于这一类发展方向，我们就具体来看一看。
	第一个这个prot OON prom tuning它的核心其实是想用一个小模型来撬动这个大模型，去适配下游的任务。简单来说我们可以这么理解，在GPT3这篇论文里面，我们的OpenAI的科学家们想到了一个方案是说人自己提升他的沟通技巧，把我们的沟通的水平提高之后，让这个X变得更符合这个W，就能得到我们想要的Y。大家再来捋一下自己，X是输入，我们不会说话，我们就让自己变得更会说话。这个说话是有套路的，这个套路就是zero shot、one shot、few shot. 
	那么把这个X变得更符合这个W那就能拿到我们想要输出的Y了。好在这个场景里面W是不变的。而prom tony这篇论文它的核心思想其实就跟我们刚刚讲的GPT3这篇论文，其实非常一脉相承。它有这个思维上非常好的一个衔接。怎么想的？就是我们刚刚的套路是说我们人来学这个套路不同的问题，我们都沿用刚刚的这个in context learning的套路。那能不能把这个套路学出来，怎么样让这个套路变成一个模型，我们能够去输出one shot、few shot甚至zero shot这个token，就是我们的X把这个X变成一个模型，然后我们就能够批量的自动化的去生成大模型他想要的这些token了。
	他想要的X所以我们看到这个方式它的核心在干什么呢？他用了一个小模型，这个小模型接收了用户的输入，然后把用户的输入去适配下游特定的一些任务，就我们要干的这些活。然后这个小模型的核心就是把原来的X去加一些前缀或者后缀，让它包装后的这样的一个X一撇更符合我们的W那就能拿到我们想要的Y了。这个其实就是prom 20的核心，用一个小模型去适配下游任务，去撬动我们的这个大模型。那这个核心其实它的W是不变的，大模型是不变的，它的大模型参数都不会调整的。这个PLM就是指这个预训练的大语言模型。
	Retrain pre train的这个language model OK那么proper憧憬其实就这么简单，你看了它的左边的网络结构和右边的这个论文的一个结果，model tuning和这个from tuning这个from design就是人工设计的。我们看最下面这条线其实已经很厉害了。因为在GPT3的论文里面，我们看过刚刚的曲线。如果你没有用prom design，就是很裸很原始的去跟他沟通，你没有沟通技巧，你都没有去人为的去训练你的沟通技巧，没学套路那个分数就更差。但from design在这篇论文里面，就是我用了你GPT3里面提到的in context learning我去设计了。
	但是我不是自动化的设计，我现在绿色这条线叫做prom tuning，我自动化的去设计了from design的套路，然后我的分数其实已经很高了。接近于什么呢？接近于我们看到这里有两条曲线，一个叫model tuning，一个叫model to multi task多任务的训练。上面那个其实就是更高成本的一个要去动这个W的一个微调方法，我们要知道这个绿色的曲线和蓝色的from design都是不会动大元模型的，只会去在外围做一些工作。这是prom ti。
	那什么叫prefect tony呢？这个也很有意思。我们严格来讲，其实这些词就不陌生，也不难了。我们再来看一下刚刚套路是什么？就是把人的沟通技巧套路化，对吧？就跟我们人会去书店里面看如何提升沟通技巧一样的。其实就是人也在训练这个prom tony。
	那什么叫perfect tuning呢？这个更有意思。这个其实有点像我们把这个套路再提炼一下，提炼到哪儿呢？提炼到transformer结构里面，大家看到这幅图的左边的这个论文里面的截图它是指我们可以用原来的大元模型去做各种各样的下游任务。
	其实所有的retraining加上fine tuning这个范式是bert提出来的应用开发课的理论篇我们讲过的。如果有咱们学过应用开发的这个同学应该知道，free training加fine training这个范式非常重要。2018年的年底十月份，具体应该是啊google的birth这篇论文提出来之后，其实有三年的时间。Bert都是整个大元模型的研究中心，各种各样的大语言模型都是基于bert去开发扩展延展的。在上面叠加各种各样的下游任务和训练技巧，bert也是很早就被hugin face这家公司做了开源实现的这也是为什么hugin face这一轮能够起来的一个原因。这些我们应该在用开发课的时候都跟大家讲过，如果有这个学过的同学应该很熟练了。
	那么回到这儿，transformer, transformer本身它是一个encoder decoder的架构。就transformer这个网络结构本身来说是一个六层encoder，6层decoder，然后decoder之间可以直接连接。然后第六层的这个encoder是直接连接了所有的decoder这么一个结构。在这个结构里面，transformer其实学习的是大量的知识，他使用的这个meti head attention的这样的一个注意力机制，在学知识表达，在学我们的知识。他没有去学特定的任务，那我们要让他能够去做特定的任务。就是我们看到这个论文当中写到的，这里有一个prefix前缀，这个前缀有三个不同的任务。比如说翻译任务、摘要任务和table to text的任务，这个也是很常见的。给他一张表让他翻译成文本，或者反过来给一个文本让它生成一个表格，这都是非常特定的下游任务。
	那通过加了这个前缀之后，我们看到灰色的这部分transformer print是原本的预训练模型，它也是不变的。他在这个预训练模型的前面加了一节内容，加了一节prefix的我们要去训练的小模型。然后加完之后就相当于变成了三个不同的transformer。因为它的。预训练的这个模型不变，但是前缀是不同的，因为前缀适应于不同的下游任务。通过这样的一种组装，我们可以锁定这部分transformer的参数，去只训练prefix这部分的参数，就变成了一个下面的形状，这个就很有意思。
	我们X区不搞了，X有很多人搞，这个是一条技术路线。我们现在X不弄了，我们去弄W但W它本身不变。我们看到这样的一个W1撇，W它本身是不变的，它在W的前面加了一些新的W，加了一些新的小模型的权重，然后去训练这些W1撇，W21撇一直到WP1撇。所以我们看到其实在perfect 2里面W还是不变，我们的预训练模型也是没变的。然后我们的W一撇变了，所以预训练模型加上这个prefix出来的这个新的完整的transformer它是变了的。但是我们把这个新的transformer给拆开之后，它的后缀或者说抛开前缀这部分的transformer其实也是不变的那通过这个方式，我们会看到它的效果也挺不错的。
	这里其实我们看到这个measure，这个FT其实就是去调整模型的这个方法。然后PT其实就是我们的prefix处理，在对应的这个表现上面其实是蛮不错。是甚至有一些场景在小数据规模上还超过了我们的这个fine tune。这个其实就是prefix to me也不复杂对吧？
	那我们再来看一看Laura是什么？Laura这个词也出现了很多回了。现在我们看Laura先看右边这个一个特定的benchmark，上面有一些方法对比像什么prefix embedded prefix，layer adapter等等这些名词，其实也都没有想象中那么难。
	我们刚刚讲的这个prefect ti大家记住是在transformer里面做文章，然后transformer本身不变，最开始讲的那个proto I其实是transformer都不去碰它，在这个输入里面做文章。那这个输入层具体在哪呢？其实就在这个embedding这一层去做文章，embedding的这一层，我们再看左边这幅真正的Laura这个图，这幅图很有意思，它其实是一个我们学过线性代数都知道的一个很有意思的公式就是我们的这个矩阵相乘的时候，大家都知道一个矩阵要跟另一个矩阵能够相乘，它它的这个维度是有一些要求的那我们看到左边这幅图的Laura，他要做的一个最大的工作，其实就是把大的W给它简化一下。你可以简单理解成我们认为要去训练整个W太复杂了，就是这个地位它是D乘D的一个时速空间，这个W的维数太大了。我们假设这个W是一个100亿的维度，那么这个100亿的维度要把它全部加载到，当然没有那么大。我们现在我们现在实际微调的没有百亿的，可能就几十亿的规模。但你要真正像这个OPI，他们可能千亿的规模也会有。
	把它全部加载进来之后。全部加载进来之后，其实你需要你需要去调整的参数非常多，同时你需要占用的显存也非常多。那有没有什么办法？我们就可以看到下面这个公式很有意思。
	我们假设有一个delta w这个德耳塔W就是A和B相乘得到的。那这个A和B中间有一个维度是一样的，就是这个R维，也就是我们要降的这个rank这个质。我们学线性代数的时候都知道，线性代数里面的学矩阵，矩阵有一个质。这个质是我们通过操作之后知道这个是它的一个真正的维度，可以通过降维达到一个真正的维度。这个字可以是原来这个呃完整的预训练模型的可能是它的1‰，甚至是几1%，甚至几1‰都有可能。通过这样的一个降维，可以直接把我们的大语言模型简化成两个小语言，我就不叫小语言模型。
	两个小矩阵的相乘，就是把一个完整的大的矩阵变成两个小的矩阵的相乘，这种方式其实是可行的，这种方式的灵感或者说底层逻辑在哪？你可以想象一下，我们一个比如说上亿维的一个矩阵，这里面有这么多的参数，但不一定是每个参数它都重要的，一定是有些参数重要，有的参数不重要，有的参数权重值比较大有的权重值比较低。那可不可以把那些权重值低的我们给它拎出来给它干掉，就是在R这一维之外的给它拎出去。其实他的逻辑就是在这儿把那些最重要的最高优先级的占比最大的那些维度拎出来，拎出来之后把它放到A和B里面，然后我们去训练A和B用来代替完整的一个预训练模型，通过这样的一个逻辑来完成我们的反应。这个其实是我们可以看到在这个lora里面实现的一个方案，它的核心其实就是去训练一个低维的模型A和B，就这么一个套路。那么比起lora这个q lora它的目的就很单纯了，我们知道要去训练这个lora，它也还是需要把这个模型先全部都加载进来。我要先全部加载进来我才能拎出去，拎出去之后我才能够开始训练这个A和B所以它要加载的还是挺大的，它需要有一个完整的单精度16个bit，我们叫float，完整的16 bit用来表示一个W，就我们要去怎么算这个模型占了多少这个显存也就可以这么算了。
	就一个W1个权重占16 bit，然后现在有650亿个这个参数，那你就算650亿乘上6 bit，你就可以去算它要占多少显存，其实就这么回事儿，因为你还有token，然后这token也要加载到显存里。所以这就是为什么一开始你把模型加载进去，好像还没有爆你的显卡，但是你丢了一个长的上下文进去，它就爆了。因为这个上下文也进到显存里了，那自然就overflow了。就这逻辑。
	那么现在通过这个speed，这个transformer，q ora的技术可以降到非常低的一个维度。那为什么降的不只是这个四倍呢？这个我们放到课程里面再去跟大家去做一些分享，这里它不是一个简单的说每一个参数我都只是缩小到原来4分之1，那这样也有点太笨了，对吧？一定还有一些别的优化技术。但通过这样的一个优化技术，我们会发现780GB变成48GB了。
	这个显存的占用其实是非常好，将近20倍的一个降幅。这个其实是就让我们的个人开发者也有机会去微调一个百亿级别的650亿级别的一个lama的模型。这个其实是Q罗A带来的一个直接价值。包括这个方法其实它本身也是通用的，它也不是说只能用在我们的某一个大模型上面。像很多纹身图的模型现在也在使用这个q lora来做微调。
	除了q lora以外，还有这个AA lura。AA lura其实简单来说就是我们刚刚都提到了，要把一个大的矩阵变成两个小矩阵相乘。那这不就回到我们大学里面的线性代数、高等代数的这个包围圈里面了，对吧？那有各种矩阵分解的方法，SVD、importance of wear I包括很多基于机器学习的方法来做这样子的都有。所以这里面其实就还有很多更巧妙的一些像低质分解，或者说我们叫去做矩阵分解的一些套路了。这个其实就是艾达罗尔带来的一些核心。A达洛尔也不是说一个特定方法，而是指这个套路，这个套路也是有效的。
	好，我们再往时间原因，我们已经讲了50分钟了。我们再往后简单看一看开源的框架和工具，以及我们对未来的一些观察。因为开源框架和工具我们就不太好像理论片这样，给大家一下讲到那个点上，一下一下好像就通了。我们更多的是做一个介绍。而主要介绍的这些开源框架和工具，包括一些国产的技术站，会在我们的微调训练营里面逐步的给大家刨丁解牛的，让大家能够上手，去把它用起来，至少把第一步走起来。那之后你要往哪个方向去深度发展，就看你自己的一个实际情况了。
	这个是我今天刚好画的一个大模型微调的技术站当然技术站它不是全面的，它不是说这个大模型微调所有的事情我都一张图能画完，而是说跟我们课程紧密相关的一些内容，我把它放在了这里。就比如说我认为这个大元模型要做微调第一要解决的问题是算力的问题。就你没有算力，这模型文件都没法加载，对吧？这个W不知道就漂浮在空中很虚无，那你需要把它加载到一块算力里面。
	比如说我们最有名的，现在全球最炙手可热的一家公司就是英伟达那NVIDIA。它通过A100包括H100以及现在出现的更新一代的这些显卡，其实成功的吃到了整个AI的红利，所以英伟达的显卡，包括像google从出了tensor f flow，应该是在1718年的时候开始做自己的神经处理的芯片TPU。从TPU1现在应该到TPU3，甚至好像是有TPUV4了，具体的我没有太跟进了。以及我们国内的像华为的生存系列，这个也是我之前在华为的时候，就跟这个对应的还是团队简单有过一些接触包括像还有很多的后起之秀，也都想做这样的一个市场。比如说微软AWS都在研究自己的芯片，包括英特尔也在做自己的NPU。
	因为整个AI算力未来就像电一样。大家都知道能源很赚钱，卖石油的，卖供电的都很赚钱。那未来算力也跟他们一样，因为算力变成了一个数据的加工者，只有通过算力设备才能把这么多数据加工成我们要的那个Y所以谁能够去生产算力设备，可用的有竞争力的算力设备一定就能挣钱。所以你能看到云厂商现在都在干这个事儿。为什么？之前有平头哥阿里要做这个芯片，然后华为已经做出来了，然后像微软AWS包括MD这个厂牌也在做，这就是这个原因。
	再往上是模型，现在大模型公司百花齐放，各种声音。有的人说浪费，有的人说现在大家都是同一套架构，没有什么本质的创新等等。我觉得这个大语言模型本身，现在也不到一个终局。的确transformer的架构不一定是最好的架构，未来一定会有新的架构出现，这个是必然会发生的。
	但是在当下去投入大模型，我觉得是非常有必要的。从国家层面和企业层面都是需要有战略眼光去做投入的，因为这个玩意儿就跟说难听一点，或者是说借用and mask的这个话就是它就跟核武器一样，它甚至比核武器还要危险。因为它的控制是让你都感知不到的，是悄然的就发生的。而你要去按下按钮去引爆一颗核弹，其实是需要极大的心理成本和战略成本。但你如果去运行一个大模型的服务，去控制舆论控制数据，其实大家不一定能感觉得到的。所以从这个视角来看，它有非常大的战略价值。
	而在大模型这一层，其实现在国外，尤其是美国应该是遥遥领先全球的进展的。不管是OpenAI还是meta以及OpenAI的这个高管出来创业做的这个etho pic。现在google也投了，他们很多的这个弯曲的基金也都投了他们。包括google自己做的这个pum two，其实都是现在在大模型的这个底层模型的竞争上是处于一个领先位置的。然后国内的像这个质谱做的这个GLM130B，应该算是国内的第一梯队里面的这个大模型了。他们其实现在也有开源的版本，也是我们课程里面会去讲的这个chat GM6B，包括他们在十月份发布的第三代的chat GM这个6B也会在我们的最新的这个训练营里面去做一些应用，包括微调。
	在框架层面上，其实我们会发现我一直开放了痛的这个API，不管是咱们看到的GPT3到GPT3.5，以及未来会开放的GPT4的Q。对于海外的个人开发者来说，其实GPT的fine two是一个比较高效，起步比较高的一个状态。因为它基础模型领先。但如果对于国内的大部分大陆的用户来说，为了合规等等，那我们可能还是会趋向于使用一些开源的大模型再去做微调。那这个时候就会用到一些开源的框架。
	比如说他跟face开源的这个PEFT，他这个站瓷都站的非常好啊，他跟face是一家商业化非常成功的公司，他开源了两个项目，一个叫PEFT，就是我们刚刚讲到的这个premier efficient IT factor，这个词就抢到了这个词。然后transformer也跟这个transformer有我觉得有称的嫌疑，然后transformers其实也是一个很好用的库，机器学习的库。以及我们如果要去做更大规模，我们可能会再介绍有一周的时间跟大家介绍微软的一个框架，叫做deep speed，用来去做大规模分布式，用来加速我们训练过程的一个框架。那么有了这个算力，有了基础模型，有了框架，具体要怎么样去调微调它就跟我们这个做菜，我们现在有了厨房，有了这个原则材料，有了这个餐具，锅碗瓢盆都有了，现在我们得做菜了。这个上面的微调也是我们今天给大家抛砖引玉讲的这些东西其实就是体现厨师水平的高低了。微调里面三种不同的颜色各自有它的一些特点。像前面的这个prom tony、perfect tony，以及我们还没有讲到的p tony，adapt tony, 这些微调技术可能会成本更低，更清亮一些。像na包括IA3、uni PLT，可能算是会更深度的去了解一下模型，并且需要一点点的数学的理解，就能知道它是怎么样去运行的了。
	以及像GPT4，宣称的他们用的这个MOE，以及基于人类反馈的这个强化学习应该怎么做。他会放在一个更更高深的技巧，就像你的你会炒番茄炒蛋和你会炒鱼香肉丝，那你会炒会做什么佛跳墙。它肯定是一个逐步变得深刻的一个过程。但这个过程其实都离不开算力模型和框架。我们也会逐步的让大家去学习这些不同的微调的技术，用对应的这些算力设备和这些框架和模型。
	好，我们具体来看，这三个库其实也都是非常有名的，大家也都可以自己去搜一搜。第一个这个PFT还跟face开源的，它的词也用得非常好叫sota，就是当下最好的state of the art的这个PEFT的方法，它都放到这个库里面了。比如说Laura，我们刚刚看到这些一些IA3，多任务的prom，tony locker和这个罗哈，这两个我都还没去细看这些新的东西大家都会放到这个库里实现了，你可以直接用。然后像这个hugg ging face的transformers里面就把它的这个trace，把它的这个hugging ing face hub，其实就像github一样，穿这个hugin face自己也做了一个网站。这个网站上面存了大量的模型数据以及benchmark。那transformers就可以方便的把这些拉下来，并且它和我们的刚提到的底层的py touch tensor floor和google的jack这些机器学习框架去做了很好的整合，然后它的这个商业化宣传也做得很好。
	Sota当下最好的继续学习的配套的工具库，这个我们也会去做介绍，以及这个我们会放在后半段，后半课程后半部分的应该是在第六周第七周的时候会去讲这个deep speed，微软的这个库，它会覆盖训练，推理、压缩以及跟这个SARS相关的一些内容。那我们可能会窄一些，跟我们训练有关的主要是为了解决假设我们的企业应用里面需要真的要去做分布式大规模了，怎么去做？有了这些微调的大模型，其实未来我们会看到AI agents的应用开发生态，就会像现在的web mobile一样，我们的大元模型的APP一定会变得非常的丰富，它的形态，它的开发框架一定也不只是一个南迁。我们会相信现在的AI agent创业也是非常热的。我们看到最新的这个奇迹论坛，就是陆奇他们的奇迹论坛的AI agents路演非常多。那我相信我们的这个学员一定也有一些AI相关的从业者。希望你能通过你自己微调出来的模型，去做一些更深度的agents。
	那么国产化的大模型技术栈国产化大模型技术栈其实主要就是两个部分。我们刚刚这儿看到的一个是ChatGLM，一个是华为升腾，一个占了模型这个位置，一个占了算力这个位置。而GLM其实在我们一个月前的分享里面有讲过，其实整个质谱团队从今年的年初3月14号发布了chat GM到6月份的chat GM26P一直到10月份十月底的时候chat GM3的6比。其实今年发布了三代的这个ChatGLM6B的模型，我们也会用中文的或者说双文的对话模型去做它的微调，以及基于它的一个RAG的开发，在我们的课程里面。
	同时chat GM3我们也会做一个比较详细的介绍，以及基于chat GM3，其实这幅图是质朴的AICU的张鹏在十月底的时候，在应该是CCF的一个年会上做的一个分享，当时他有一个agent benchmark，是他做的这个，这里有写的最新的agent benchmark上面去做了一个分享。就基于chat games turbo的这个agent的水平，那个bench mark应该是做了八个不同方向上面的agent的一个对比。那么chat GM3 turbo就是它的130B1300亿的这个GLM做出来的那个chat的模型，已经接近于GPT3.55的这个水平。当然这个是没有开源的，我们只能使用GIGM36B，不能使用130B，确实也部署不了。但是这套套路学会之后，其实是可以运用到硅更大规模的这个大模型上面的。
	然后华为的这个升腾的910，其实也是目前国内相对来说，我们能够接触得到，并且能够去部署适配的那如果我们的这个同学里面有一些是需要去做政企合作或者私有化部署，那么我们有应该是一节课，相当于一周的一半，3个小时的时间。未来讲也是一个选修，教大家怎么样使用华为的model 2S升腾云的服务来部署国产的亲爱的GM3这个模型，然后也用它能够去做一个RAG的一个应用刚好这个model us也是我之前在华为的这个团队，所以也跟大家也很开心，他们真的在这件事情上在国内是处于一个比较领先的一个位置。那最后再来利用一点时间谈一下这个大模型微调技术的未来趋势和挑战我认为其实未来的趋势可以分成五个方向来看。
	第一个就是我们今天也有提到的这个新的架构，transformer。这篇论文attention is oil need，其实是17年的时候发布的一篇论文。由google的学者，google的工程师们一起发布的一篇论文八个人。这八个人现在也都离开了google去创业了。Transformer 17年六年前的一个架构，现在是否还能支撑全球这么多资源，这么多人才的投入，其实本身就要打一个问号，也是为什么现在这么多，包括像硅谷的国内的都在探索这个新架构，也是因为transformer的确它也不一定是一个最终解。那么transformer的新架构的探索一定会是未来的一个发展方向。
	第二，就是这个模型压缩和优化，不管是在深度学习阶段，还是在我们现在大模型阶段，模型一定需要压缩。一个训练好的模型，参数稳定的模型，当它要被部署起来的时候，我们一定会去把它压缩，压缩既能降低成本，还能提升推理的速度。那么它的压缩水平如果越高，其实它对于它的精度损失其实是越小的。而这个压缩技术又是相对通用的。所以这一块的发展方向如果有了进展，其实我们的AI agents也会有更多的进展。然后这个auto ML其实是也是一脉相承的，在深度学习阶段也有就是我们怎么样利用AI来设计AI包括像我们的q star、a星等等，这些其实都是要用AI来自己完成这个模型的设计。NAS neural protective search也是曾经神经网络的一个研究方向。这个也是我认为未来有可能会发展的。
	然后第四个也是现在非常热的，叫做跨模态的学习。就现在怎么样用一个模型能够学习多个模态，然后统一输入输出的这个模型。但是这个输入输出支持不同的模态，声音、音频、文字、视频等等，包括图像输出也是这个也是GPT4V现在遥遥领先全球其他的竞争者的一个状态，以及一些更广泛的一些应用范围，包括随着新的架构的出现我们的这个模型压缩技术的出现，跨模态的进步，一定会有很多的应用出现的。
	就像我们在iphone 1出现的时候，在HTC在这个。第一代的谷歌的这个手机出现的时候，我们不知道APP的市场会有这么活跃。那个时候也就是0708年的时候，APP真正的万众创新出现在四五年后。一二年的时候APP才出现了大范围的增长。但是很多的公司其实也就是在0809年的时候成立了。然后他们的持续耕耘使得他们在移动互联网阶段有很多的收获。
	我相信大元模型现在就像是0708年那会儿的移动互联网，它还有很多的工作要去做，也像这个英伟达金范，也是我们群里的一个群友在硅谷的一个群。他也有在讲，他昨天的推特有提现在的大语言模型，还有很多的工作要去做，这些工作其实跟我这提到的这个五个趋势有一些相关性，最后我们再来讲一讲挑战，就是我们会面面对什么样的挑战。这就相当于今天我们坐在这儿，假设我们坐在08年的时候，我们刚刚看到iphone发布，刚iphone 3其实是比较有名的一代。Iphone刚刚发布，然后这个android也才刚刚写出来，我们就开始去想他的挑战。其实是差不多的节点那么你就会想象到在那个时候肯定会有很多东西很新，每天都有新东西。那现在也是一样的，包括我们的架构本身，我们看得到transformer未来有可能会被超越，并且这个概率还很大。
	那么当出现新的架构的时候，我们能不能去找到一些或者说我们谁是最先找到有效性价格的人，这个是一个挑战。因为当他出现的时候，其实很多采用老架构的人就会被甩开一个比较大的身位，也只有这样他才能称之为新架构，而不是一个小改进。这个其实是对于很多基础大元模型非这些公司非常大的一个挑战。
	然后对于我们的微调的技术的使用的学生来说，其实我觉得反而还好。但是我们需要适应，就是大家在学这个微调的技术过程当中，我也会尽可能的让大家举一反三，触类旁通的去学习这些知识。因为你会发现这些微调技术，它不是一个石头缝里蹦出来的。就像我们今天抛砖引玉在讲，它有很多一脉相承的技术的。这个你说方法论也好，或者你的认知结构也好。当你把这些认知结构理解清楚之后，就算它的架构从transformer迁移到了别的，比如说RWKV等等这样新的架构，它的微调手段是不变的。因为大家记住Y等于WX是一个数学上的东西。而这个数学上的东西，你的微调技术无非就是变了一个算力设备，变了一个微调它的框架，甚至变了一个写出这个模型的框架，无非就是从tensor floor变成pyo ch变成jx从英伟达的GPU变成华为的升腾，这个模型从lama变成了ChatGLM。
	为什么我们这个八周的课程要有两三个模型让大家去玩儿？有两种硬件让大家去玩，就是让你理解其实适应这些东西并不复杂，而他们的套路也都有套路可循的。然后模型的可解释性，这个是应用推广一定会解决的问题。不然你想象有一个非常聪明的agents，一个智能的一个机器人，甚至是人形的，他天天为你服务，但你整不明白他怎么运行的这不就是跟很多科幻电影最后这个AI造反了吗？把你给干死了。所以可解释性一定是一个研究方向。然后迁移学习和这个伦理和社会责任自然也是一个很明显的挑战了，这个其实就是我认为可能未来我们会遇到了一些比较明显的挑战。好，今天的分享其实就到这里，我们总共讲了四个部分的内容，然后我们可以再回顾一下这幅图。
	这幅图其实整个大模型微调，无外乎就是这四个层面。算力是大厂他们去搞的，我们要会用，模型也是大厂要去研发的，我们目前主要还是用把它用好，能不说话，我们大模型微调真正要掌握的是最上面的两层，把框架用好啊，然后把微调整明白。整明白之后，首先最主流的微调技术我们会用，出来了一个新的微调技术，我们能看明白它怎么回事。比如说鱼香肉丝对吧？你不用胡萝卜丝，你不要用这个，用木耳那也可以。但是炒这个鱼香肉丝的技术我是知道的对吧？比如说这个麻婆豆腐，我里面不加肉酱，我要加这个牛肉也可以。但是炒麻婆豆腐、烧麻婆豆腐的这个技术我掌握了，这个是我们去学习一个东西的时候最重要的，也是现在AI想要更进一步需要解决的问题。就是怎么样能够自主学习，能自主迭代掌握一些本质。
	好，那么今天的分享就到这里，看大家有什么问题，我们在直播间里回答一下微调和应用两个课的重叠度有多少？几乎没有重叠。硬要说有重叠的话，他们最终的应用形态AI agents都会有一个RAG的一个例子。
	对这个课可以帮助比如国内企业训练自己的模型。私有化部署可以帮助国内企业微调自己的模型。你如果要从0到1去训练一个自己的模型，那么你需要有足够的硬件设备，如果你的算力足够，应该也是可以的。比如说你就用Laura对吧？
	我们学的模型都是用hugging face的模型吗？首先这个同学他跟face只是一个平台，一个仓库，模型是由这些模型的开发公司和团队发布的。你可以去哈根face上面去，你也可以去别的地方去。只不过hugin face可能这个仓库大家用的多，所以取起来方便。
	框架是这还有什么？我们还有必要学习transformer和bert吗？这个同学看你的必要是指什么？就是我们的应用开发课其实讲了transformer和birth，讲了这两个网络架构。其实一个是网络架构，一个是大语言模型是怎么来的，以及它的一个底层的一个逻辑脉络，发展的一个脉络。对，至于你说的必要性，完全取决于你自己。我觉得你如果能整明白这两个之间的关系，他们的发展脉络我觉得就可以了。你如果非要去做公式推导和复现，你有足够的算力也是可以的。
	我看一下对GPU的最低要求是什么？显存多少？这个每次都有人问，16GB，16GB的显存训练营自己训练营不提供GPU。有自己的知识库，训练自己的私有化模型。有个秋天的同学，是这样的同学，我们一直在讲下游任务你有自己的知识库，你要训练自己的模型，然后干嘛呢？用这个模型干嘛？训练他干嘛，吧？这个是问题的关键，而不是说你有自己的知识库，然后我们教你的是说针对几种特定的下游任务，我们用这个开源的数据，开源的框架，我们能训练对。
	我挑一些问题回答，有些不具有代表性的我就不太回答了。然后要使用多大规模的模型来学习微调？用量化后的模型微调可以吗？要使用多大规模的模型来学习微调？我们课程里面主要使用几十亿这个规模，就不会超过100亿这个规模。
	然后量化后的模型微调可以吗？如果你一直使用的量化的方法来微调，那也是可以的。比如说你一直用q lora，那么就可以。然后对于本来是应用开发的，不是搞算法的，学了微调转微调的工作难度是不是比较高？
	也不是，就是看怎么理解，以及你转过去之后，你的团队是什么样的。就比如说一个真正搞微调的团队，比如说现在这个OKI也有七八百人了，他们就一个大模型对吧？那么真正搞微调的团队里面，大家分工就正儿八经搞大模型的公司，它的微调其实是好多人都在参与，有做数据的，有做框架的，有做工程的，有做清洗的，也有做这个微调本身的。所以其实你越熟练，你能够接触的活儿越多。然后如果这些活儿你还交付的不错，你其实就会变成这个活的负责人，这个winner。
	我们再来看看。搞微调比应用的要求是不是高一些？硬件上的要求会高一些，我觉得对人本身的要求没有那么高。就是你需要有1块16GB的显卡就可以了。
	亚马逊云上可以私有化部署GLM吗？如果你用的是公有云，那就是部署在你自己的亚马逊的这个服务器上。对，但这个叫不叫私有化呢？每个人对私有化的定义也不一样。反正是部署在你自己的云的账号里，学习这门课，买GPU服务器这些以及使用大概要花多少钱，能讲一下吗？这个我没有太大概念，因为我一直用的云服务器，或者说我在大厂的时候，都是公司去采购的GPU的这个服务器。对，然后我了解到的，我大概了解这些消费级的显卡，应该就是在两三千、3 4000左右就能买一张这个卡来做对应的训练，我大概的一个数我不准确，课程里有全量微调的内容吗？
	有个同学问，课程里会介绍全量微调，然后我们会用lora或者q lora来实现全量微调。对，但它是有量化的。如果不使用GPU，使用普通的CPU微调开源大模型，这个同学我劝退你了。如果你要用CPU来微调开源大模型，你可以换一个活去做，因为用CPU来微调大模型这事儿太不靠谱了，对。自己有好的途径获取训练营。
	彭老师你的应用开发课非常实用。这个微调课面向的是以后从事微调模型的吗？是不是还是需要有相应机？这个课相比应用开发课就是需要一块显卡。对，或者说你去各种云上面去租一个服务器就好了。一般什么场景下需要进行微调？现在有哪些实际应用场景？这个刚刚有提到特定领域中文，然后你的这个服务的客户他有要求等等。微调的工程方面的技术知识在课程当中有讲吗？这个同学你可以展开讲一下，你说的微调的工程方面的技术知识是指什么？
	Collab pro我印象当中应该是有配google的TPU的。但我们的课程是按英伟达GPU和华为的910，会有一节课讲910为主TPU我们没有去做适配，所以TPU上的环境可能需要咱们自己去安装一下。建议加一个场景化的实战，这个如果有同学能够提供数据或者具体的场景，我们可以去加。对，因为这个实战其实它会比较容易落入一个不一定大多数人都bin买单的一个场景里。我们也在接受建议，我们的课程在这个研发过程当中，其实大家如果有一些特别想听的场景，我们也可以加进。
	企业如何选择开源模型，大家一定要理解，我们开源模型它不是一个最终的成品。对企业要先定下来你要用模型做什么任务，我们再来谈用哪个模型，是直接用还是用提示工程，还是用微调。我们会做全参数的微调，不过是量化后的参数。因为不是每个同学都有足够的硬件去做这个全这个非量化的全参数微调。
	可以推荐一下哪个云厂商的GPU的性价比好一些呢？4090现在也买不到了，这个确实比较难推荐。因为其实真正大家去买这个GPU的时候都会有折扣的。都不是那个listing Price，都不是那个页面上的价格。然后你跟哪个云厂商的销售关系好，你拿到的折扣就低这个我建议就是咱们自己多发挥一下这个主观能动性，去看一下你的公司有没有折扣，或者你的朋友有没有折扣等等。3080可以。
	针对一个垂直场景介绍不同的微调方式，不同的效果吗？这种这个同学问的很有意思。这种情况一般是企业培训或者企业的定制化服务，不太好做成一个课程的。咱们如果企业有这个需求可以私聊。
	知识库类型的应用需要微调技术吗？咱们展开讲一下知识库类型的应用是指什么？微调的工程方面是指如何分布式的部署微调的模型，减少微调中出现的错误，如何进行监控。这些课程中有讲吗？会涉及到，但不一定覆盖到你心目当中的所有的工程方面的内容。因为这个微调其实就跟创业一样，就他会不同的人，不同的数据，不同的框架和微调技术，会出现各种各样的问题。我们尽可能去讲一些通用的问题。
	Deep speed的内容有对应的代码练习实战，就是咱们微调其实不会写太多的代码的。我觉得这个可能咱们现在就得给大家用有一个认知，就像这个灵异智能的这个大模型，它没有改多少代码，更多的是在怎么样把这些框架用好用什么样的微调技术。然后数据要怎么样去处理，为什么要这么处理，这些框架之间要怎么样去衔接，可能这个是它的关键，尤其是数据的处理，所谓的AGI native，这个AI通用AI的native的研发团队和方式，其实就是以数据为中心，数据驱动的去研发我们的模型和应用。
	这些问哪些大模型好的，我一概不回，除非你具体举了一个具体的任务或者场景，我们再来回答。对，因为金融大模型太大了，不知道什么叫好的金融大模型，还是要在这个场景下。非量化的全量微调可以指导一下操作吗？这可能要私下来沟通，问一下咱们的上课时间或者什么的。在课程里面不一定会有这个非量化的全量微调。因为我们知道几十亿的这个参数加载到这个里面也很夸张的。教课用TF不是py touch吗？我们课程不会涉及到TensorFlow和py touch。你看这幅图里面也没有他们，就是不太会直接涉及到的。
	有个同学说IT自己的场景，比如但是咱们不是每个同学都是IT的背景，所以我们可以让咱们的班主任去提交一个表单，然后每个同学去里面去填写你希望讲什么样的一些场景。然后我们可以收过来汇总之后，看看共性最大的，又又能够比较跟我们的要学的这些内容相关性高的，我们可以找一个场景。行业报告生成需要微调技术吗？可以用微调技术来增加它的生成质量。微调的语料是问答是吗？还是选择？是的，这个跟咱们下游的任务有关，会讲模型部署和推理加速吗？会讲会最后会用docker来部署，在后面的部分。看大家还有什么问题吗？我们没有的话，就9点25的时候停止回答，看看大家还没有什么别的问题。Apple的M1 pro的max芯片可以玩微调吗？这个我真不知道，而且我不建议。
	看大家还有没有关于这个课程的一些问题，微调不一定会让模型变得。如果变差了该怎么处理？对，我们今天的分享你就讲了，你做菜不一定会做的更好，然后这个。变得更差了。两种方式，一个就是回到原来的那个版本，就你训练前的版本去微调，去调整你的微调的数据，一份数据有问题，或者是你微调的方法可能有问题对。举个简单例子，你现在微调就是在炒菜，对吧？你菜都炒糊了，你说这个怎么救？你要么就重新用原材料炒呗，对吧，就这么个意思。
	我看很多现在都在问这个智能运维的机器人学习构建内部知识库。可以，会用到微调的这个技术，你可以把内部的知识库加到这个大模型里，也可以通过RAG的方式在相应数据库里检索。但如果你内部的知识库这些单词名词是很不通用的，很垂直的那你就可以训练到你的模型里。对这个人问的也是一样，就是所有的RAG和微调的选择，就是在你可以最简单的把你的本地的这些知识库让大模型去理解一遍，让他理解是不对的，理解是不到位。如果他理解都有问题，那你就find to。如果他理解没问题，那你就直接让他去检索。
	16GB的显存，如果咱们都不知道怎么看自己GPU的显存，需要去问一下ChatGPT了。16GB的显存是能满足我们绝大部分课程当中的find home的这个场景的。好，我们今天的QA答疑就到这儿，明天就是我们正式的开课了。然后大家有什么问题，我们可以在明天的课程里面，我们也都会有这个QA的部分再来回答。好，那就感谢大家。