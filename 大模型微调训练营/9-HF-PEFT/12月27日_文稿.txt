	好了，应该是有网络的延迟，声音和画面正常吗？
	画面是卡住了吗？因为我刚刚那个摄像头出了一点问题，然后调试了一下，现在应该画面正常了，对吧？我这边是卡住的，我再看一下。
	画面有点问题。我看看。
	我重新启动了一下这个直播的客户端，大家看看现在是不是好啊。现在看的好像是动态的了。
	好，那现在终于好了，刚才就感觉好像这个客户端出了点问题，那现在正常。好，那我们就正式开始今天的这个分享。好，今天我们是整个课程16次课程，今天是第八次课程，算是上学期的最后一节课，今天我们终于走到了这个hugger face的PFT大模型高效微调的工具。今天主要分成这么几个部分我们来做分享。
	第一个是hugging face的这个PFT这个库，我们希望能让大家快速的入门，快速的入门最好的方式就是上手来实际的去使用实战训练，这个是最快速的方法。然后整个PFT其实在我们真正前面三节课程，学了这个transformers这个库之后，已经用了很多transformers的核心模块了。然后如果大家是真的去学习了那三节课，并且也跑了这些对应的代码的话，今天学PFT应该是没有任何的障碍的。因为他们衔接的很好，整个hugin face的这个接口，包括上节课跟大家提这个auto class，都是有一定的铺垫的。那么PFT这个库它的名字也很巧妙，跟这个hugin face的取名艺术是非常一贯之的，一个叫transformers一个叫PFT。
	今天我们来用PFT来讲一讲它的核心的几个抽象，也就是下面这几个核心类的定义。包括它自己抽象的auto PFT models，他的PFT model，还有它自己的这个PFT相关的配置，以及我们都知道在讲transformers的时候，outclass很重要的一个功能。除了统一接口以外，还有一个就是它的自动检索。在PFT里继承了这样的一种设计理念，通过PFT type和task type，我们的PFT库也同样能够去做很多自动检索的功能。
	最后我们会去用PFT库里面的这个lora来完成两个实战项目。一个是文本生成类的项目，一个是语音识别类的项目，分别对应着两个不同的模态。然后AOPT的6.7B，上节课大家应该知道，我们如果要在16GB的显卡上面去量化一个OPT6.7B几乎是很难完成的。因为量化它的过程，比如说像TPTQ这样的量化的方法，那么它会需要额外的一些显存。那么像67亿这样的一个规模就失败了。但是如果我们使用lora在八位的精度下面去对它进行微调，在PFT的这个库的加持下会非常的简单。
	同样的有一个非常值得大家去关注的语音识别的这个模型。Master也是这个GPT1的作者OpenAI的一个研究员，在去年10月份发表的这个vest这个语音识别的模型现在也非常好用，是一个以它的这个large v2是它最大规格的版本。也就1.5亿，一应该是15亿还是1.5亿。我待会我们待会儿可以确认一下，好像是15亿15亿的这么一个规模，这样的一个规模我们用ra来进行微调，也可以成功的去理解这个lauda它的一个优势就是把一个在全世界这么多种不同语言上训练出来的预训练出来的一个vest per large v2，用一个特定的中文语料去把它训练成一个小模型，Laura的小模型。它可能聚焦在中文的语音识别任务上。这个其实是我们要做的第二个实战项目。
	好，今天我们就正式开始今天的内容，PFT这个概念我们在前面的理论课都或多或少的讲了很多回了，叫parameter efficiency fine tuning，就是高效微调。简单来说，还有in face一家公司就直接取了PFT这个名字作为它的库的名字开源库的名字。那么PFT到底是什么呢？其实hugin face的PFT要落脚点的话，它还是一个python库。只不过这个python库是为各种大型的预训练的模型提供各种各样的高效微调方法的这么一个python库。
	为什么会出现PFT？在理论课我们已经跟大家讲过了，传统的这种微调的范式。比如说在1819年的时候，在GPT1和bert这个年代，大家都还是百万千万级的规模。那么我们针对每一个下游任务去微调，基于任务的微调还是行得通的。但是随着我们这几年的模型规模逐渐变大，就像我们现在看到的几十亿就已经算是一个大家直觉上好像不大的模型。但其实几十亿的规模放在四年前是不敢想象的，就没有几个人能跑得起几十亿的模型。
	几十亿到几百亿甚至上千亿这样的一些大的模型的时候，它的参数总量非常庞大。如果你还要每个任务都去微调一次，你光凑够加载这个模型和对应的数据，包括在这个特定的算力上面去微调这些模型参数，都是一个非常昂贵的事情。所以这就使得这种传统的基于下游任务，一个任务微调一个模型的这种方式变得不是特别的实际了，尤其是你要微调它的全量参数。
	所以PFT这一类的高效微调的方法提出来，就落脚点有两个。第一个就是我们不要全量微调，我们只微调一部分。第二就是说我们能不能用这个叫重参数化重重参数化的方法，就比如说Laura，它就属于典型的，我不在原来的这个模型上面去做文章。我们也讲过Laura这个课，我们不在原来的这个模型上面去做文章了。我用一个B乘A这样的小矩阵，一个低质的一个适配去替代原来的theta w，就是我们的大模型通过这样的一个方式去重参数化，来微调我们的小模型，就是我们的lara的这个模型来做一个平替，就能使得我们以更低的成本去完成一个特定领域任务的一个模型，还有一类就是这种prom的处理，就是以少量的提示参数来进行微调的方法。那它可能就或多或少的有各种奇技淫巧，有的是在这个transformer结构外去做专门的一个模型的训练。有的是在transformer内部，比如说加前缀，比如说在in bedding里面，加上可以学习的神经网络层等等，各种各样的方式。
	无论如何，PFT的核心就是我不用去训练这个全量的参数。同时还ging face的PFT库，及时的跟进了大量的。比如说就算2023年，今年发布的很多的新的PFT的方法，它都已经做了很好的支持。那这个是为什么现在有这么多人愿意用它的一个非常重要的原因。第一理论上我们必须得用PFT了。第二，技术上它跟进的很快。所以我直接用它可以获得很多的这个，不需要我去额外开发它的代码。
	那么PFT这个库它支持了哪些模型和微调的方法呢？我们可以看到这一页这个课件的右下角，是给了这个对应的链接的这个是hugin face的PFT库自己提供的一个小插件，其实就是一个H5的radio的一个界面。这个界面里面我们是可以选择不同的task，不同的任务。这个task就跟我们开始在课件的这个大纲里面提到的task type其实是一类，就是我们的这个不同的下游任务，它会有不同的任务，不同的模型都支持这样的下游任务。然后不同的模型上面又有各种各样的PFT高效微调的方法。通过这个下拉框我们就能去实时的查看现在的PFT这个库它能支持哪些模型和哪些方法了。这个就跟我们去看大模型的这个leader board是类似的套路，也是现在我们要去实时的去检索数据，一个比较常见的一种图形化界面。
	PFT它是一个单独的python库，它又怎么跟我们现在学过的transformer结合起来集成起来。这个其实是一个很多人都关心，然后也如果没有去深度使用过，也不知道怎么去讲的问题。当我们学过transformers，我们就能很清楚的理解他们之间的关联关系和他们的这个继承包含关系，包括他们统一的设计理念。
	我们先看这张图，这张图是一个非常应该所有人都看过并且非常熟悉的一张图。我们都知道现在的大模型几乎都是基于预训练的transformer，包括这个GPT就是一个通用的预训练的transformer。虽然它的版本在不断的迭代，然后像这个T5，它也是沿着这个encoder，decoder里面使用了大量的transformer的经典结构。
	这个是我们跳出框架，跳出这个hugin face的两个库来看。我们就是在这个学术界也好，在这个前沿的研究也好，设计出了这么多神经网络。那这些神经网络最终怎么样落到我们实际可用的这个代码里来的呢？我们知道底层有py touch，有TensorFlow，有jx等等。但是高层次的抽象transformer这个库帮我们做了一些特定的抽象。就比如说我们这里看到的，在transformers里面，我们上节课学过了，它有很多的auto model。其中最常见的是我这里列出来的这三类的auto model。而这三类又跟我们刚刚看到的那一页理论的这个模型其实是比较能对起来的。
	就比如说我们讲到的这个bert。我们讲到的像这个Robert a，他们都是属于这种，就像我们一直在讲的完形填空这个逻辑，它是用这个mask的方法，完形填空的方法，以encoder为主的方法来训练的预训练的transformer这个有点绕口，就是用这一类的网络架构来训练的这个预训练的transformer，因为大家都是预训练的transformer。还有一类叫自回归的，或者叫因果类的，为什么叫因果？就是给上文，然后他帮你生成下文，所以这也是一类它的命名方式。但这个不重要，他们跟我们前面看到的这种decoder only以GPT为主，包括我们今天要讲的这个主角，vester为主的这些其实都是属于英国类的模型，或者我们又叫自回归的模型。
	自回归是体现在这个decoder only这个属性上，因果是体现在最终这个网络结构，它会有一个top player，就它最终输出的这个你可以理解成这个模型很深，那它最终会有一个从这个输入一阵的矩阵运算，算完之后最终会有一个输出的结构。那输出的那个部分，我们通常叫做这个模型的head。它的头，比如说上面的这种bert，它有一个mask language modeling head，比如说这个GPT two vesper，它可能有这个Carrier的这个language model in head。还有一类，比如说sequence to sequence，它既有encoder又有decoder，它会有这个sequence to sequence这个language model in head。
	这些都是hugin face在实际去应用我们的这些大模型技术的时候，所抽象出来的一些概念。跟前面我们知道的这个论文里面这些理论是有一个对应关系的。并且我们现在其实用的最多的应该是中间这个部分，就是因果类的这一类auto model。这一类auto model其实就像我们上节课讲到的，transformers的这个auto class里面还有很多的这个model。我们刚刚看到这个因果类的，包括这个musk的这这这一类的和这个sequence to sequence都在自然语言处理里面。Auto class或者说在transformers里面，我们要去做好一个模型的抽象。最重要的三个要素，我们再强化一下这个概念。有config，有tokenizer，有auto model，有这三类。
	假设我们现在要把这三类基础的大模型加载到这个系统里面来。就像我们现在学过的所有的技术一样，我们已经可以使用transformers把这些东西加载到显存里了。但是我们现在要解决的问题是只加载只推理还不够，我们希望能够的对它进行高效的微调，进行PEFT。我们还需要在现在已经学会的技术上再额外增加什么，这个其实是我们去思考问题的一个思路，最好是说原来的很多概念我能继承，我不用再额外造太多新概念。这样是对于我们学习来说最简单，也是对于所有的开发者来说最友好的那怎么做呢？
	我们可以看一下，在PFT的抽象概念里面，他提出了这个auto PFT model，就跟上节课大家看到的auto AWQ model很像。他们都会基于auto model去做一些文章，那他们又保留有了auto的属性，所以它不是PFT auto model，是auto PFT model，很有意思这些命名。那么auto PFT的这个model的设计，简单来说就是它既保留了原来transformers里面对于大型的预训练的模型的这一套，比如说统一接口自动检索的这样的一个功能。同时它又不简简单单是一个单元模型，它还能接受这个PFT，各种各样PFT的方法，它是一个这样的模型，这样的模型跟普通的大模型比起来，就是它能对接很多PEFT的方法。
	哪些PEFT的方法呢？我们以之前讲过的一篇paper来做一个介绍。因为这幅图很有意思，它叫uni PLT。这篇paper他其实是总结了一下目前市面上比较主流的一些PFT的方法。主要是这三类，一类是adapter，适配器，一类是perfect tuning为主的这种soft prompt的方法，还有一类就是非常通用的柔软。这三类方法其实大家看在我们中间这个黑色框的部分，这个是PFT这个库里面的抽象，在这个框里面他们统一都被放到了attempts这样的一个类的抽象里。但这个不重要，我们不用跟论文里面去抠字眼。
	我们简单来理解一下，就是我们要去做PFT，可以把PFT这个抽象的概念高效微调，分成几类技术。就像我们之前讲高效微调理论篇的时候，我们就知道有一篇综述的文章。对于现在的大模型有好几类PFT的方法有A有adapter的方法，有这个prompt的方法，也有重参数化的方法，repaired ization这样的一个方法。Lora就是属于我们说的从参数化的方法。好，如果有了这个概念，这里的adapters某种意义上就是现在的所有的PFT的方法。
	那这些PFT的方法我们既然要用他们，那一定需要一些配置。因为我们看过了这么多的论文，知道每个论文都有自己的一些特定的超参数。比如说Laura它有很多超参数。这个rank要设计成多少？大家如果有印象可以回头再看一看课件，rank应该是多少？86算是一些比较经典的超参数。然后Laura它最终抽象出来要去从参数化的这个参数量占整体的原来的大模型的参数量的百分比是多少。实际上我们看得到一些精简值，比如说1%甚至25‰，这个小于1%。比如说0.5%，0.25%都是一些可以还不错的一些值。甚至我的lora要去重参数化我的原来的大模型里面的哪些部分，比如说QKV，是不是都需要参数化我们的这个前馈网络，就是我们这里看到有个fed forward，其实Laura未来，或者说Laura现在的这个主流使用方式已经不局限于这里的QQV了。
	我们之前也讲过，除了QQV以外，这个feed forward就是我们的前馈神经网络里面也可以被它从参数化。包括这个输出层和注意力的一些参数，都可以对它进行从参数化进行一个低质的适配。这些都是属于Laura的一些超参数，但是这些超参数最终我们都需要有一个东西来承载它，这个时候我们想到auto class的三件套，有model，有config，有tokenizer。不管我们是经典的这个大模型还是PFT的模型，输入侧我们几乎都没去动过，所以说就直接继承下来了。Model我们已经有了一个新的继承的设计，从auto model变成了auto PFT的model。剩下的部分我们可以继续沿用这个config的设计。所以很自然的，这里所有的adapters都可以去沿用这个conflict的这种继承关系，所以他们其实都可以去使用这个PFT的这个config作为它的一个你可以认为作为它的一个负类。然后所有的这些方法都可以继承这个PFT的conf。
	再去做一些延展，就跟我们之前看过的量化的逻辑一样，量化也是在这个out configure上面去做了一些延展，做出来了这个AWQ的config NB的conf。那么这些PFT的具体的方法也可以有一些具体的config的子类，去继承它，然后有一些不同的超参数，这个其实是整个PFT的核心，就是这么一个逻辑。然后我们用右边这幅图是非常好去理解它的。就是原来有一个大模型，现在我有各种各样的adapter，就是往这个大模型里塞，要么是嵌入一个adapter，要么是我的lora去做一个你说叫旁路网络，也可以就在旁边外挂一个新的小模型。如果我外挂的是这个QKV的这个权重矩阵，就是我现在图中的这样。如果我是去外挂在这个feed forward，那可能就是旁边这个粉红色框框这里就会有一个Laura的小模型，这完全取决于Laura的超参数。类似的像其他的这些，比如说prefix tuning，比如说我们的其他的像IA3，他们也有自己的一些超参数。他们的这些超参数的作用就是让我们的这个模型去增加一些别的模块。而这些模块在PFT的统一有一个名字叫做adapter，然后这个adapter的配置依托于这个PFT的config。
	不知道表达清楚没有，那具体来看，怎么玩，就是呃通过上节课的学习，我们了解了out class的设计之后，我们知道一个大模型它有两种方式被加载起来或者说被实例化。一种叫做from free train，就是从一个预训练的权重文件里面把它直接完整的加载成一个大模型，既有网络又有权重，还可以通过conflict来加载，通过config加载就可以只加载这个网络的壳子。这个权重可以根据需求lazy load，根据需求去加载，不一定一来就要全部加载，这个是auto class。对于我们的PFT model其实是类似的思路。
	然后PFT的这个model，它也沿用了类似于像from retrain这样的方法。我可以通过一个你可以说根据这个类型，然后自动的去把这个模型给实例化出来。甚至它可以更简单一步。因为在前面已经有transformers的这些设计了，它最终可以基于我们刚刚讲好的，你实例化好了一个transformer的一个transformers一个model。然后在这个上面再额外添加一些配置，就变成了一个PFT的model。
	就像就像我不知道大家有没有经历过QQ秀这个年代，大家可以把你一开始注册的这个人物形象当成一个最最裸的最难业务的一个auto model。然后你的PFT model就是在这个人物上给他加装备。然后加装备的过程，那你总要说好你要加什么装备，对吧？你是要给他加衣服，还是要给他加帽子，还是要给他加围巾？这个衣服帽子围巾具体上面有什么样的花纹，是哪个品牌的，这些就是具体的这个PFT的config决定的。但是是加围巾还是加帽子，是PFT的type决定的。我不知道这样讲清不清楚。好，如果是这样来比喻的话，我们就可以知PFT的config就决定了我最终要在原始的这个大模型上怎么样去装扮它打扮它，怎么样去给它加适配器。
	这个PFT的config它的设计上也有三类不同的抽象。先看最最简单的最下面的这一部分叫prompt learning config它是一个最好理解的就是这一类的config它是一个鸡肋，它是一个base class，而不是一个鸡肋。它是一个base class，是一个基础的类型。然后我们都知道PFT有两大类方法，一类叫做Laura a的方法，一类叫做soft prompt的方法。那么所有的soft prompt的方法都需要有一个基础类型，这个基础类型在PFT这个库里面叫做prompt learning conf。然后我不同的soft prompt的方法，他们的这个config的参数都不一样，所以我这边写的叫做case by case的一个parameter。这些常见的soft prom的方法，就比如说我们看到的P2 prefect tony，prompt tony这些都是属于典型的soft prompt的方法，他们都属于prompt learning的一些派生类，还有一类叫做PFT的咖啡，这个其实是更常用的。
	而这个PFT的config它作为一个鸡肋来说，它它有很多的参数，最重要的两个参数，我觉得是大家这里标标红能看到的。一个叫PFT的这个type，一个叫task的type。PFT的type其实就跟下面我们看到的这个prop learning config一样，他也要去在右边选一个我到底要用什么样的PFT方法。通过PFT的这个type可以给它选出来。第二个就是这个task type是我们这个PFT的model，它下游支持什么样的任务，这个我们待会儿再去讲，我们可以先忘掉这个参数，那么通过PFT的type我就能描述清楚，我现在这个需要高效微调的大模型。
	要给他装什么样的一些外挂？我说要给他装什么样的一些adapter，那么这个PFT config mixing，这个configure，它还内置了一些操作，他跟这个hug in face的hub去做了一些，你可以认为做了一些连接。通过PFT的config mixing可以很方便的去跟这个hunting face hub去做一些交互，比如说把你的这个训练好的PFT的model存下来，然后他做了一些额外的一些小的优化，让你可以很方便的去跟他交互。同样的PFT的这个config mixing也可以去做一些自定义，去做一些自定义的apter。然后当然PFT的这个config作为最基础的类，也可以去做这个自定义。
	大家都可以去做自定义，但是有一些不同的特点，上下的这两个是从PFT的技术理论上去做了拆分，然后上面最上面这个是最通用的，下面这个只适用于soft prom的这样的鸡肋中间这个会跟hub有一些交互。大家如果对这个鸡类的一些设计，包括PFT的库本身它的一些设计有兴趣，可以去深入看一看。但我们去理解它就抓住几个本质。第一，它是在原有的transformers的model上面增加了一些模块，我们可以统称为adapter这样的一个模块。这个adapter的模块它是一个抽象概念，我们落到具体的模型里它会有一些类型，这个类型有PFT type作为它的这个key标志出来。那么有了PFT这个type之后，我们其实就构成了一个完整的auto PFT的model了。
	因为作为一个out PEFT的model，就像一个QQ秀的人物，对吧？你已经跟我选好了，我现在要额外增加什么样的装扮的这些装饰，比如说衣服，这个围巾之类的这些装饰我已经选好了，那我这个model其实就够了，我不考虑下游任务的话，我这个model是齐全的。我的这个新的模型的网络架构，我也知道要往里面加什么了。无非就是说我还需要把他的加进来的这些部分的参数设置成一个比较好的参数，那怎么设置呢？你可以是加载进来的，你也可以是通过给他一些数据训练出来的。
	好，我们再来把这个逻辑捋一捋，让大家好好去理解它。有了auto PFT model之后，其实我们刚刚也在这个介绍里跟大家讲了，它其实是希望更简化的去处理问题，就是通过这个task type来处理问题。通过task type和这个PFT type，它能够自动检索出来我现在需要用什么样的这个model。然后这些model又会自动的去找它需要用什么样的一些config，是这个逻辑。
	我们现在再来看这个PFT config里面的这个task type的话就比较好理解了。首先这些类型跟我们前面看到的auto model其实是完全一样的。我不知道大家这个名词上陌不陌生，其实你细看的话，有英国的，有这个sequence，也有token classification的，也有这个问答的，包括一些特征提取的那这些其实是由task type其这个字段或者这个key来描述的。但是每一个具体的大的模型里面，它可能还会有一些细更细微的一些task type，可以去做一些细分。
	然后这些模型最终要怎么样去训练，其实是对对应的我们开始已经学过的transformers这个trainer这样的一个抽象，我们学过用trainer来训练，trainer要训练的话，其实需要定义几个很重要的输入。一个是我要我要在什么样的模型上面去训练。第二个就是说我要用什么样的训练的参数来训练。当然第三个就是给他数据了。现在我们首先搞定了通过什么样的在什么样的模型上面训练，这里我们给他了足够多的选择，有各种各样不同的PFT的模型可以给到这个trainer直接去训练。所以换句话说，这个transformers的这个trainer它是能够接受PFT的这些模型作为它的输入来进行训练的。
	我们举一个简单的例子，让大家感受一下，就比如说我们已经定义过transformers的这个模型了，那假设PEFT也有模型，就比如说有Laura这样的一个模型。那Laura这个模型它需要的重要的几个内容，一个是这个model本身，一个是这个Laura的config。而这个model本身其实是包含了这个，你可以理解这个model其实就对应着transformers里面的一个model。而transformers里面的model其实就是那三件套都有啊，有它基础的那个模型网络配置和它的token niza。那么token either就不需要在PFT这一个更高层次上去定义了。因为它里面的这个model已经包含了，已经能找得到了。
	这个递归嵌套的逻辑大家可以去理解一下，你可以理解transformers的这个auto class，它的智能体现在你给它一个对应的model的name或者path，它能找到一个大模型，包括这个大模型的配置。那PFT它的智能体现在刚刚那些功能它都有啊，因为它是继承了这些设计和功能的，它都有。但是他要智能化找的是说我给他新加进去的这个外挂adapter，它也有一些配置。这些配置是什么？通过整个auto PFT的model，用这样的一个智能来实现。就比如说Laura model，它就需要有一些Laura的config包括这个如果我要给它加多个不同的apt还可以取一些名字。
	那那在这里我们看一下，假设我们要去用PFT这个库来高效微调，使用lora来训练一个模型应该怎么做？首先这个PFT就跟transformers一样，直接通过这个from PFT就可以从PFT库里面去导入一些对应的重要的config和方法。这个Laura a config就是我们刚刚看到的给lora这个PEFT的类型，它是预定义好的，实现了这个lora论文里面的一些内容给lao ora这样的一个PFT的这个方法，它有哪些超参数都行。在这里就有点类似于大家回想一下上节课我们去用这个BNB的config，它里面就已经实现好了这个Normal flow four，就是NF4这种数据类型，它已经预实现好了。同时它也实现了双量化。然后这两个还能叠加起来，它都已经预实现好了。这个就是PFT带给我们的很多的好处，就是你在论文里面看到了很多很新奇的技术，它已经通过一行代码就可以用起来。
	对于这个lora也是一样，lora里面的一些经典超参数，比如说这个Laura的这个rank，然后它的这个Laura阿尔法，它的这个比例因子，然后包括它的这个Laura它到底要外挂在原来的哪些模块上。就比如说这个QKV，就是我们开始看到的这个论文截图里面写到的他的WQWKWV。然后包括它的这个输出的部分和它的全链接层，就是它的这个前馈网络，就我们开始说的粉红色的那个部分feed forward network，当然它的注意力也可以，我只是把这个注意力这一层删掉了。然后我们知道做pot这样是这个是一个在深度学习网络或者说叫深度神经网络训练的时候，一个非常经典的超参数。就我在训练过程当中，我可以把一部分的这个神经元直接给它关掉，相当于就随机化的去砍掉一些通路，然后来增加一些随机性，使得我的训练过程更鲁棒。
	这个罗尔有对应的超参数，包括这个BIOS的一个使用和它的最终的任务类型。这里我们写了它是要用来做文本生成的，所以我们的这个task type写了对应的任务类型。当然lora肯定不只支持这一类的任务，后面我们也能看到Laura用于其他的任务类型，所以我们捋一捋，就是通过PFT它有各种预定也好的config就像我们使用量化的技术一样，我们的PFT也有各种各样的技术，而这些技术在PFT这个库里，它就已经预定义好了。通过导入一个Laura config，你可以去把这些参超参数给它设置好。但是我要把这些超参数设计好之后，我最终是要训练它。
	那怎么训练它？这里我们有一个很经典的方法，也是这个PFT库的一个最佳实践的方法，就叫做get PFT model。你可以理解成这里的get PFT这个model，输入的这个model就是我们的原始的transformers的这个模型。比如说我们原始的transformers是一个OPT67的模型，或者是一个whisper的这个v two，那它就是那个原始的模型。但他只需要把原始的模型给你，就像大家想象就像这个QQ秀一样，原始的有一个人物角色，这个config就是你买的那些各种各样的装扮，然后你只要把这两个合一起，通过这个get PFT model这个神奇的方法，它就帮你把这个Laura config里面设计好这些内容，给它组装到那个网络里去了。
	这里出来的这个model其实就是一个PFT的model。然后这个PFT的model它支持一个打就是一个很常见的方法。因为你PFT的核心就是你不用训练全量的参数，那你最关心的不就是你现在训练的参数占全体参数的比例，对吧？所以它有一个内置的方法叫print trainable parameters。比如说这里我们把这个rank调到了8，它就只需要训练原来的0.12%这样的一个比例的参数，就可以完成它的Laura的这个指定。当然你可以把这个降的更低，就是你不要去对于其他的这个，比如说你要把这个rank再降一步，或者说你要把Laura的这个模块数再增加，都可以对它的这个trainable mohamed's变成一些变化。
	好，那么接下来其实就是实战的部分，刚才讲的这些可能都是一些概念，他那个概念很重要，这个设计理念大家尽可能的去理解。更多的可能就是你如果理解不了，就在我们的实战的代码里面去体会一下。然后今天有两个实战的demo，一个是OPT6.7B的一个文本生成的demo。我们会直接使用这个PFT库，然后来实现它的这个lora这么一个机制适配。第二个就是OpenAI的这个whisper large v2，我们做了这么久的文本类的任务，我们今天也做一个云类的任务，并且是跟中文相关的人物，让大家去感受一下，怎么样去训练一个这么火的。因为本来vester又是一个会收费的API，大家如果真的对于这个语音类的这个任务有需求，那么你在自己的硬件上面最终训练出来一个可用的精度，你也就不用调它的API，就可以直接去在本地使用这样的一个模型了。好，那我们来看看。
	这个代码应该已经push到这个github上。对，大家可以看到这里有一个PFT的目录，目前传了两个，分别是这个PFT lora OPT6.7B和这个PFT whisper large v two，是这两个。然后这里有一个VS per large v2的1个音频文件，是我们来做测试的，是我自己录的一个声音，大家也可以自己录一些这个音频文件，但是我们实际是啊好，然后在讲这个之前我们看一下，在同样的这个目录下，整个项目目录PFT下面有这两个文件，就对应着我们刚刚在github上面看到这两个文件，我们重启一下。
	稍等一下我的网络连接。
	好。
	好，终于加再见了。好，我们看一下，就是我们会用到之前学的，我们循序渐进的去让大家把之前学的一些东西用起来。第一个就是PFT two，这个库是需要去新安装的，这个BNB是我们上节都已经用过的量化技术，所以对这里得提一下我们的requirements更新了。所以大家得去每节课最好都跑代码之前去跑一下这个requirements这个项目，这个文件依赖的拍摄文件，我们上节课安装了一些像量化相关的一些内容，AWQ，GPT q之类的，包括这个BMB。这节课我们加了一些新的跟音频有关的一些依赖，包括这个PFT之前就已经加进来了，这些库都是大家需要安装的。我看今天还有个同学，他的代码里面报错居然是没有装transformers，这些应该尽早的在去安装好啊，这些pyto依赖包好。那假设我们都装好之后，我们现在开始去看怎么样去快速的去实战。然后这里我们通过代码来额外讲一些transformers的一些设计和加速，包括这个access rate这个库，我们已经装了这个库了，那这个库其实它的有很多好的用法，就包括这个代码其实里面藏了很多细节。
	第一个细节这个大小还需要再放大吗？应该差不多了，然后又收起来。这个大小大家能看清楚吗？
	假装可以，我看大家没有问题。好。这里大家注意一下，我们细心的同学会发现这里有两个变化。第一就是我们没有使用这个auto organizer，也没有使用这个auto for这个因果推断的这么一个子类的模型，为什么？其实大家细心的我不知道大家都没有有没有去读过我留的这些超链接。这个链接就是我们没有讲OPT这个模型这个模型其实是meta AI这家公司对标GPT做的一个模型。然后在这个model card下面其实有写过，包括他用了什么样的训练数据，怎么样去在预处理预处理的这个地方有写到。
	应用开发课的时候，其实当时我有讲过这个tok nizar的一个工程方面的一个技术，当时提过一嘴，就是GPT two和这个GPT four，或者说GPT two和这个GPT3.5开始，大家使用的这个tok ized的技术in bedding的技术就不一样了。但是GPT two的这个token ized的这个技术，也没有说它很落后，至少说它的速度上还行。比起这个hugin face实现的最粗暴的auto technet来说，它还是有一些优势的那这里我们能看到这个预处理部分其实有写过这个the text，total ized use GPT two by level version of bite pair encoding for unicode characters。然后它的词表是5万，然后输入的这个是2048的2K的这么一个头啃的上下文。然后这个信息其实挺重要的，这一类的信息其实大家用的多了，慢慢就能找到一些相关的概念就能串起来。然后他这里用了GT2的token ize，然后GPT two的这个token ize，原则上来说是比这个auto token ized要快的。然后我们教大家怎么样去理解，然后我们看到这个GPT two的token ither可以通过什么来看呢？Up again, please, the. 
	Transforms are A. 
	搜索。
	GPT two的这个token iser，然后它是在这个OpenAI的GPT two下面，这样看应该就能看看清楚了。OpenAI的GPT two是一个transformers这个库下面实现的一个具体的模型，这个具体的模型它有它的config，它有它的模型网络，它也有它的token nizar。而它的这个token ized比autotote anozer更快，然后有一些别的优势。
	OPT是跟他用了同样的tokenizer，所以我们这里可以选择直接用auto token ized，也可以选择用GT two的这个token，neither这俩都是可以的。然后如果你选择用这个auto together，他会选择这个，你可以认为就是有个兼容性。如果他做的好，他甚至会去找到这个GT two的token ized。但我实测下来，他不会，他会用这个你可以理解这个最最baseline的这个token iza来做相应的编码和映射，然后用这个GPT two会更快，然后你要用GPT two的这个cognized，就会用到我们之前的一个依赖，就是这个accepts，这里accelerate。然后类似的这个OPT for这个casual的LM也是类似的一个逻辑。
	就是OPT这个模型它也实现了一些，也解开给大家看一下，就这些概念应该怎么查。因为我们不太能把所有的API和这个概念都给大家讲，我们尽量教一些方法。那么OPT这个模型它也实现了自己的这个LM，这个就跟我们刚刚上tokenizer有auto，有这个GT two，那么model也是一样，我们有最最baseline的基础的因果的模型。当然每一个具体的模型，它为了它的性能的优势，最大化放大它的性能，他也可以继承那个auto的model，再去做一些派生。比如说这里的这个OPT，for care这个LM就是这么一个概念，它是基于基础的LM去做了一些继承。好，那么我们就回到这个代码这儿。好，然后conflict自然就还conflict，这个没有什么太多的滑头，因为它没有太多性能的区别。
	好，那我们现在就实际的来加载一个67亿的这么一个模型。
	这个模型有点大，会稍微等一会儿，可以看一下这一侧的GPU的这个情况。
	loading. 
	看起来有点慢。
	今天网不是特别好。
	再重启下可能试试。他的这个jupiter notebook或者jupiter lab，它有时候在海外的链接确实会不太稳定，但这种情况也挺少见的。
	神奇。
	我们先往下面看这个对应的代码，等它加载一下，看这个机器的原因。那么通过这个加载，其实理论上我们把model和这个token niza通过from trade from tree train这个方法，可以从hugin face上面给拉取到本地，然后加载进来。加载进来之后，我们知道我们现在要用这个lora来进行这个低值适配。然后使用lora的过程当中，我们如果这边开始加载了，终于右边的GPU也有反应了。不知道这个GPU的这个字大家能不能看到，可以再放大一点。
	把这个拖过来一点，这样能看到一个GPU的一个实时情况。加载了第一个模型的分片67亿，加载到一块T4上确实有点有点慢，不过它加载的时候我们就已经使用了这个barbat了。这个是from free train的，就已经自带的，相当于BNB内嵌到了free from这个free tree里面的方法。上节课里面讲过，就在这个quantization BNB里。好，终于加载一下，加载进来之后大概7GB左右的一个显存。然后我们看到这边实际消耗也是差不多，六千多兆。然后这里就要用到PFT这个库里的第一个神奇方法了，叫prepare model for into eight training。简单来说就是我们要用PFT来进行高效微调。
	上面的部分，我们其实是把模型加载进来了。加载进来之后，我们的模型里面有各种各样的模型的参数，具体的一个的参数。然后这些参数其实有的精度是int 8可以的，有的精度可能是FP16，有的可能是FP32。然后如果我们用了这个node in eight bit这个方法，首先用这个方法就意味着其实你在用BNB这个库，所以他对BNB是有前置依赖的。然后你用了BNB的这个load in a bit这个加载方式之后，理论上肯定是希望全部都加成了8 bit的这个权重。但有可能有的它就是没法变成int 8，它变成inter 8会出事儿。一些特定的权重我们不去展开讲。那么这些没有变成inter 8的，跟那些已经变成全部都是inter 8的，它就变成混合精度了。一个比较好的方式就是把那些既然有些就没法变成inter 8，那我就把它变成双精度的，这个flock的这个float的32位的这么一个双精度的浮点数。这个是一个常见操作，并且通常都会去做的一个操作。
	第二个就是说在这个输入就input的这个embedding里面，嵌入一个forward hook。然后这个其实是也是在这个高效微调之前的一个界定操作。包括用这个gradients的这个check point，这个梯度检查点去启用这样的一个设定。你可以简单理解成这是一个常规操作，不去深究的话，就用它，相当于给你的这个模型进行一个预处理的一个工作就好了。然后一定如果你要用int 8来进行微调的话，或者说这个babbit来进行微调的话，使用这个方法是很有必要的。相当于把你的模型洗了一下，做了一些处理，这里我们直接去运行它。然后我们能看到我把这个转换成了这个GB，之前有一个是这个MB的，是为了跟这儿对齐，然后我们看到这里有稍等直接。
	打印出来。这个大小是不用换行的，然后这个就是我们的这个model，就经过这一堆处理之后的这个model。然后这个model最外层套用的是这个OPT的，这个for care的这个model里面是加载了一个OPT的model。然后这个OPT的model，这个decoder，然后这个eva的token，position这些就它内部的一些网络结构，你可以理解成，然后这个网络结构，也就是这个模型本身的这个可就我们平时讲的好。这些大家其实不一定把每一个都打印出来给大家，这个在notebook里面展示过，但如果大家对这些感兴趣，都可以自己输出来看一看。对整个神经网络的这个architecture有一个了解，因为我们不太会去再深入去讲到里面去了。大家如果感兴趣可以通过这些方式，找到一些关键的词，可以再进一步的去深入研究。
	然后接着我们再看，相当于模型的可有了，然后这个模型也基本上把它的权重已经做好了，给我们拿来做int 8这个精度来做训练的这个权重了。但是这个模型还没有把lord a加进来，我不知道大家捋一下现在这个状态，就是有一个模型了，他洗了一遍，从原来的精度变成int 8的精度了，甚至为了训练，还把一些这里的一些准备工作也都做好了吗？那接着我们要把loa这个模块再加进来，就比如说我们刚刚在这个课件里有讲，通过这个P这个库，我们可以导入Laura config，然后再导入一个get PFT model的方法，那这个其实就对应着我们开始课件里看到的这个流程，给大家看一眼这个课件。
	大家想象一下我们的Laura conflict是什么呢？就是一个这里的左上角PFT config。当他确定好之后，它就应该是右边的特定的一个config了，对吧？然后特别特定的一个configure，假设我们现在选的就是Laura，这个PFT config g不就是这个Laura config。然后这个Laura config加到原来的这个model里，它就变成了一个PFT的model。
	那我怎么获取到这个PFT的model呢？有一个方法叫get PFT model，其实就这么一个逻辑，挺简单的。所以这段代码其实是一个PFT的经典使用的套路，就是我们从PFT里加载一个你要使用的方法的config，再加载一个get PFT的model，然后你要用什么样的PFT的策略，你知道它有哪些超参数，你就去设置这些超参数。然后这些超参数也有一些经典的实验结果的值，然后把这个conflict设置好之后，通过这个get PFT model这个方法就得到了一个把这个加载进去，那就是把我们的这个新的LORA的apter加载进去的一个新的model，我们可以运行一下，运行之后，最后有一个输出。我们到底这个Laura a加进来之后，Laura的这个模型要训练的参数在原来整个OPT6.7B这个模型的参数的百分比。Get PFD这个model会消耗一定的时间。然后这下面这个是给深入研究的同学的一个参考，就是内部的一个实现。
	到底怎么算出这个train val l，其实可以通过底层的这个框架里面的这些一个的参数，它会有一个特定的字段，这个字段叫做它是不是需要这个request gradients，就这里这个值用来判断它是不是一个需要去训练的参数。这里我们其实是可以通过调整不同的超参数来改变需要训练的这个参数量的。就比如说我们知道通过lora的论文做的实验，我们知道这个rank最小取到8是能接受的那假设我们想要它的效果好一点，给它取到16有1点冗余。假设我们资源多，那当我把它设置成16的时候，显然参数量就变成两倍了。这个大家应该能理解，就是大家想象Laura的这个你看到了。
	Loa s的，然后应该是pt，这个是它的文档。这还有个dogs，dogs PFT. 好，我们搜一下loa，单看这幅图就明白了。Space organization user. 不对，这里。
	这个图。
	这里对这幅图应该很经典。我们刚刚改的这个R是这个地方的套餐，然后这个R显然是完全影响它的trainable parameters，就是因为真正的trainable parameters就是这些A和B。那么你改了这个R当然是成倍数的这个关系，这里是很明显的。但是你要把它改的再大一点，像论文里面32也是可以的那就0.5%。大家这里这就是大家可以去调的一些超参，这些可以去做对比。然后这个BIOS通常建议在刚开始是给它设置为然后这个drop hot刚刚讲过了target modules，这个target modules就对应着我们的整个模型里面QKV3个不同的在transformer layer里面这个QQV的权重矩阵和它的输出的投影，以及feed forward in and out的权重，就对应着我们这里可以看到的。在这样的一个典型的经典的transformer的encoder的结构里面，QKV主要指就这个WKWQWV，然后这个FCNFC out就是指这个feed forward这个全连接，它那个FC其实是全连接的这个缩写。然后这个前馈网络其实也就是全连接网络一个缩写，这个out是指这里，我记得之前我们讲lora那一节课的时候，给大家详细的去提过这样的一个事儿。
	好。行，然后我们再往下看啊，设置成这个8，让我们往后走。0.12%，接着我们开始处理数据了，因为我们要对它进行微调。我们都知道lora是在一个特定的数据集上，然后去重参数化或者说低质适配一个小的模型出来。这个小的模型是由B和A组成的那如果是ADA罗A就是这个SVD的分解，就是左右期向量再加上一个期值本增值来决定的那这里我们使用了一个数据集，这个数据集是英文版的名人名言，我们可以在这儿提前打开了，是用来干嘛的呢？就是大家可以想象一下这个OPT6.7B67这个模型其实还是挺大的了。然后这个模型如果我们要拿来做事情，是可以做不少事情的那现在假设我们先要让他就像这个哆啦A梦里面有一个记忆面包，对吧？就我能让他背书记知识，然后记下很多名人名言的话，像这个文史哲类的考试可不可以？
	这个测试集其实这个数据集其实就拿来干这个事儿了。首先OPT它自己没有用到完整的训练集，因为它的这个训练集其实写的很清楚。在这个部分training data，类似于GPT的这个训练集，用了这个书，用了各种各样的common coral里面的爬虫的数据，然后用了一些CC的新闻等等。这个是它的训练数据，和我们现在要用的训练数据是没有太大关联的，所以这个是第一点。
	第二点，现在这个训练数据我们希望能不能快速的让OPT6.7B这个67的模型学会里面的一些内容，技术里面的一些内容可不可以？因为我们给他的任务类型是一个因果的这个任务类型，生成的这样的一个任务类型。现在其实我们就想做这样的一个尝试。好，我们先下载这个训练集。这可能大家如果下载有一些网络问题的话，可以通过镜像站来解决，那就用本地加载的方式来加载就好了。
	有个同学问那几个module的名字在哪里找到的？这个同学问的是哪个module？Adapter吗？这个如果是问的算法，它的加载，这里都有给到链接的。大家去看一下这个官方文档。对，这个就没必要我再抄到课件里了，有可能会有接口的更新。
	好，我们加载好了之后，我们看到这个名人名言，英语名人名言有三列数据，分别是名言、名人标签，对应的这里名言标签。比如说这个爱因斯坦说过，对吧？这是我们待会儿要做的一个测试数据，two things，就这一行。
	好，现在我们用之前就已经被反复用过的这个方法来给我们快速的展示一下数据集。这边是court，然后有对应的名人author和他的这个标签，我们这里主要用count这一列就好了。收起来，这个方法是非常常见的一个方法，用了data sets里面这个class label和sequence，方便我们去查看这个文本类的数据。
	好，现在这个数据集原始数据有了，我们应该很熟练这个pipeline了，对吧？有了原始数据，然后需要把它的这个原始数据tok nice之后，变成我们可以用的token nice的dataset。然后用这个data set的map方法去针对我们的整个数据集去应用一下。
	那这里，我们刚刚说到了，我们只使用这个quote这一点，暂时不使用名人和标签这一点。所以我们只需要用这个token either。这个token ither就是我们一开始加载的这个GPT two的token ized，用这个token ized去针对输入的名言进行一个token ized的这么一个操作，可以批量处理。通过dataset的这个map方法就可以实现，变成这个token ized的data set，这里就2508条，就全部都应用好了，很快。然后通过这个data collector可以在。实际的这个训练的时候，批量的去处理。好，处理完。
	然后我们在微调模型之前，最重要的是设置这个模型的训练参数，这里我们设置好几个关键的训练参数，这些训练参数或者说这些超参数都跟咱们的实际的硬件资源有关系。我觉得比较重要的可能大家需要关注的就是，第一，这个base size它直接决定了你每一个step要往我们的显存里加多少的数据进去。然后这些数据都需要被加载到显存里。如果这个东西太大了，设置的你可能就会OM，就是你的扩大就会报错，告诉你这个显存不够了。这个是一个关键参数，是一个大家值得去调整的。然后不同的这个模型它的这个learning rate不太一样，其实是可以去查一查，网上都有啊。然后这个其实是一个挺挺经典的实验值了。
	这个FP16混合精度的这个训练，可以去打开这个通常也是直接设置为true，logan step，是我我多长多少个step去记录一下。然后max step和这个number train a pox这两个通常是二选一。然后为了演示的方便，我通常会使用这个max step。因为它可以让我确定的知道多少个staff之后我的这个训练就完成了。但是大家如果要模型效果，都是最好去使用这个number in a pox，就是去训练多少轮，就把这个数据遍历多少回，要全部都丢进去训练一遍，这个是ABOX的这个概念定义好了这个training arguments之后我们再定义好这个trainer，这个trainer也是算former里面的一个trainer。然后这个设置好啊，用这个模型，这个模型就已经是加了Laura的这是我们在这儿看到的加文罗A的这个模型了，我们可以打印出来看一眼。
	大家看跟上面明显不一样，我不知道你们这个如果熟悉的同学可以把它做可视化。这个是添加lora模块后的模型，一堆的PFT model for Carry LMPFT巴拉巴拉一大堆对吧？然后有base model，是它的那个PFT里面的base model，和我们上面这个形成了鲜明的对比，就是因为它增加了一堆的旁路，用我们的lora的模块加进来了，通过这个神奇的方法叫做get PFT model，给它对应的config，它就能够把它变成增加了lora旁路的这个模块。并且我们给lora config里面设置的一些超参数也都进来了。
	真正的原始的OPT，我们在这个位置给大家高亮一下。大家可以看一下，OPT for Carrier LM，然后OPT model，OPT decoder, 然后embedding这一层我们没有做。Lora lora我们坐在了QKV，大家记得吧？K project how we project a cure project the centre, 包括output这个project activation方向我没有去做处理，就这些我们都给它加了对应的lora，这个是它对应的这个model，大家有兴趣可以去研究，我们把这个model也传进来，然后训练集是刚刚map过后的这个训练集。定义好这个trainer，这个trainer就是transformers里面的trainer，并没有什么特别的。然后。
	这里会有一个推荐的一个配置，是为了减小警告的。我觉得可有可无，看大家有没有这个强迫症也好或者什么的。大家看到这里有一个use cash，有很多的这个模PFT model，就PFT的这个model或者说auto model，都可以去都会去让大家把这个东西在训练的时候设置成force。然后设置成force之后，就不会去报这个对应的告警，包括他自己也提到了，因为你的greedier check pointing，就是我们前面的这个prepare那个部分设置了gradient check point。是为了训练的时候的一些方便，他在训练的时候是不能用这个cache的那现在我们把这个训练的时候，如果加上这个use cache等于boss，这个告警就会取消掉，因为他不太会用这个cache了。这个就见仁见智，我觉得这一行警告放在这儿，我也OK对他自己的trainer会去过滤掉这个部分但是它需要在这个传进去之后，然后再设置为这个use cache。就是你就能够就相当于你在train之前，把这个model的use cache给它关掉，然后你再去训练它，就不太会有这个告警了。好，我们训练100个step，看看100个step能不能让它有一个还不错的一个效果。
	稍正好还有两分钟，我接杯水，大家有什么问题可以提一下。
	刚刚卡掉了，我重新回来了。我看一下大家提的问题。我看看。
	有很多同学问这个比例因子指的是啥？应该是问的这个问题。罗尔阿尔法这个比例因子应该是问的这个问题。然后是这样的，首先这个注释是ChatGPT加的，所以他可能翻译的不是很准。他他YY出来的这里有一个是这个逻辑，大家看看这幅图就明白了。
	就是Laura的这个理论片的时候其实提过，the out有点问题，刷新一下，把这个门锁进去，大家看能看到现在这个图，对吧？就这个图里面其实我们知道通过低质的分解，然后我们用BA去代替这个德耳塔W然后实际上这个BA去代替delta w的时候，这个BA还会再乘上一个scale，就是这个所谓的缩放因子这个scaling。那这个scaling它是怎么来的呢？Scaling就等于刚刚看到的。这个na阿尔法除以这个R我这么献写一下，让大家理解一下。就是你可以理解这个B我们有一个B乘A，当然不能直接这么写，应该是矩阵相乘的这个matt mail，这个B乘上A就相当于我们这里看到。
	的。
	这个B乘上A。然后这个B乘上A之后，并不是直接就等于我们的W它其实有一个scaling这样的一个所谓的缩放因子这样的一个值。而这个scale我先把它切成back down，而这个scaling它应该是等于我没有记错的话，应该等于我们这里的lora的这个阿尔法去除以这个rank，应该是这样的。在论文里我没有记错的话，大家可以再去看一看。所以它是这两个值的意思，我不知道这个表达清楚没有。
	然后我们看到这个模型，不是这个模型，这是没有加罗A的模型，模型在tiner之前有一个模型对吧？在模型里面有有有权重也有偏置，大家看见了吗？有这个with也有这个偏置bias。这个b BIOS是什么东西？我们要知道在讲最早还讲任何模型之前，我们就讲过一个东西叫在这个地方。这直接就写到这儿，叫做。关于mara。
	就是有这么一个东西。我们知道对于任何一个模型来说，其实都可以抽象成Y等于WX加B那么W是我们的权重，这个B其实就是我们的这个BIOS。那这个BIOS到底要不要去做这个微调？这个BIOS等于none，就是不去调它，把BIOS相当于给phrase冻结起来。
	如果我设置成Laura only，就这个地方有好几个可以设置的。大家如果不知道它有哪些参数可以设置，我们应该教过大家怎么看文档，API的文档Laura config，看见没？Laura conflict里面有一个这个。搜一下这个。这里有写BIOS string，写的很清楚。跳到我的插件里。我的科目崩掉了，我靠，尴尬，稍等一下。
	Restore一下。
	我们刚刚在这个位置，Laura的config里面有一个。Bas跳跳过去，它会告诉你可以取哪些值。Can be none or Laura only, 对吧？然后if all or no running，the corresponding base will be updated during training. 
	这个应该好理解了，对吧？我的bus我可以开始就动上，因为我不知道我的我的训练数据，我不确定好不好，多不多，够不够，所以我把这个偏置可以动上。这个是不会犯错的一种做法，至少他不会变得更差。换句话说，当然你很有自信，你对你的这个微调的这个技术手段数据都有一定的把握了。你可以再去把它设置成only，或者说设置成Laura only的意思就是Laura的这些模块的bias是会去训练的，会去迭代它的。
	All的话就是all，就字面意思就是关于这两个超参数的含义。就是这么个意思，我不知道这个说清楚没有。看还有什么问题。验证及可有可无吗？是这样的，就是验证集就算首先验证集这个玩意儿是为了给我们这些训练模型的人一个参考。然后这里就跟上次量化一样，没有给大家去加那个所谓的评估。
	这里我们重点要学的是这个PFT的这个config就这堆config和怎么样变成一个PFT的model。然后我们这儿给的这个演示的流程比较简单，没有去加这个验证集，加了验证集就跑的慢一点。那大家上课就会看我在这儿跑模型，意义也不大。
	当然你要加这个验证集当然是可以的，我们之前也教过怎么加，对吧？其实就是把我们在数据处理这个地方，node dataset去给它split一下。但因为我们这个事例，其实它没有太好的办法去给他打标签，所以这是我们没有加验证集的一个原因。
	如果你把这个作者，这跟你的使用场景有关。假设我现在使用场景是，我想做一个应用，这个应用就是我很会讲名人名言的这个套话，就是你懂有一种场景就是他是英语的版本，你把它想象成中文的版本就好理解了。就是每个人跟你说话，你都不会让话落在地上，你都能接一个很优美的话给他回过去。然后这个话就是名人说的那特别好啊，是这样的一个场景。当然你也可以做一个联合的训练，你把名人的这个quote加上author，就是这个名人是谁，你也都拉进来一起训练。那拉进来一起训练的时候，其实这个OPT6.7B它就会在它的生成结果里会带上这个名人，那你就知道这个名人是谁了，好消息就是你就能够还告诉对方，我没有让话落在地上，我话说的特别漂亮，我还告诉你我这话是哪位名人说的。这跟你使用场景有关。
	我们因为已经教过怎么样去做数据集的划分和做评估了。这个notebook我们就做的简单一点，就只是教大家怎么用nura。所以这儿你是可以调这个split的这个方法的。当然whisper那个是为了，因为whisper那个更偏实用一点，还不只是教大家怎么样用lara，因为这个其实是更单纯的。我相信大家习惯了我的notebook，就知道有的notebook，尤其是第一个就是教这个pipeline，教这个SOP，教这个流程。后一个可能会更偏向实用，你是可以拿着它去做实用性的改造的，那么是有两个梯度的区别。
	好，然后我们接着说回来，说到这儿超参的这个部分大家应该明白了，对吧？对，这个同学举的例子很好。对，就子曾经曰过，就是这种场景大家懂的。
	还有同学问这个training arguments。这个BF16其实上节课注释里有啊，FP16这个是指用混合精度来训练。就是你会发现我虽然用了int 8这个精度来做training了，但是我不是所有的参数都是int 8。所以其实它会，这个涉及到另一个另一篇paper，其实我把它下载下来放到平台上了，但我还没想好现在要不要花精力去给大家讲，因为我们总的学习时长是有限的。就是有一个技术大家可以看是一篇paper，有兴趣的同学可以去研究研究，叫做LLM的。对，就这篇paper，就是int 8 training，这篇配方叫做LLM，就大语言模型的这个八位量化8 bit matrix，就是八位这个位就是这个位运算的这个位，然后给transformer去用的一个在八位精度上面做训练的一种技术。
	我们刚刚知道了，不是所有的模型权重都能变成因特，有的还是得保留成浮点数。实际在运算的时候，就是一个混合精度的运算。混合精度的运算就是很慢，我们上节课讲量化的时候就讲过哪里慢，对吧？那么FP16这个东西就是指我就是要用混合精度来做训练，然后我FP16设置为true，我印象当中应该是可以在这个accelerate这个库上面好像可以支持一些加速，然后这个详细的这个FP16，我们也可以通过类似的方式去看它的这个含义。
	我没有背一下每一个参数的全部的含义，我只记了一些我大概会用到的training arguments。这些问题大家都可以通过我现在给大家展示的这个方法去查。Fp 16，whether to use fp 16 bit混合精度的precision training instead of 32位的这个training，就能加速，也不用用更多的显存。好，那我们就往后走了，因为还有这个whisper。
	这个同学问的很好啊，loss为什么会越来越大？这个可以通过加这个验证集去验证一下。这个也是我们后面留的这个家庭作业，本身对于一个67亿的这个模型，其实这个数量的训练集是很少的。然后现在的训练步数太少了，也不说明问题。你可以去完整的训练三个epoch，看一下这个loss的变化。但确实这个名人名言本身就很多都是套话。
	然后我们要把这个lora模型保存下来，这个应该大家都学过的，就是Steve free train，当然你用trainer的这个save model也是可以的。然后model sim free train跟这个from free retrain是一个对称的方法，保存下来。哎。Model DIR model ID. 
	他没有去复习，这就是之前训练的这个model adapter的这个model。然后它的这个lora好像还会自带一个对应的解释，就PFT这个库生成的一个命。然后这个是它的对应的lora的这个部分的模型对应的这个模型的权重。好，咱们就先把这个OPT6点CB讲完。
	然后如果我们想要去使用这个Laura的这个model，也可以可以加载回来。就比如说你可以再开一个别的notebook去加载你保存的这个路径里面的这个模型。你也可以直接在这儿，因为你的这个trainer里面就有model，或者直接使用这个model。然后这个方法应该大家都很熟了，就这种自回归的生成方法。Laura model就是我们现在训出来这个model，它的参数已经变了，跟我们一开始在微调前看的这些参数肯定不一样了，然后。运行一下，这没有付钱。
	运行一下。然后这里其实就是直接找任任意找了一句，就刚好爱因斯坦的这一句话，就是有两种事情是没有限制的。The universe，my disappointment in you. Um i'm not sure if you are quoting something or not, but I neglect is a count from the movie. 
	不说一大堆，这一段话其实是我们看到这个。其他原话是这样的，就是the universe and human stupid，就是宇宙和人类的这个愚昧，然后是无休无止境的这么一个状态，然后也是很难去探究清楚的一个状态，就有点讽刺。他其实通过这个training还原decode回来之后，没有完整的去说这个human的，或者说说这个人很愚蠢。对，但是他说的是针对于你的对于你很失望另一种表述。然后我们可以去运行它的过程当中，我觉得这个是做大语言模型或者说做这种深度神经网络会发现很不一样的一个地方。就是你看我没有换这个model，然后我们去运行了一遍之后，它的这个输出结果是不一样的。Two things，然后这个地方几乎就已经把原文给还原回来了。
	所以第一就是大家需要理解为什么大语言模型它不稳定，有幻觉，有各种各样的原因。就是因为他在这个运算的过程当中，它的结果并不是那么的稳定。尤其是很多同学也提出来了，这个training好像是过拟合了。然后是不是真的过拟合需要第一，我们把训练集拆分成两部分，一部分是我们的训练集，一部分是我们的测试集，这种方式来做。但刚好我们现在做的这个应用，它不是一个特定有特定标签，特定label的这么一个应用。所以你直接去划分这个验证集没有什么没有什么特别好的一个标签，或者说一个特别好的指标去验证他说的对不对。因为本身名人名言这事儿也没有一个所谓的对错。
	但是通过这样的一个事例，其实是一个最简单的一个事例，让大家理解怎么样去用糯软。第一，它好在以前的67B6.7B67的一个参数的一个模型，你是压根就没法训练的那通过这样的一个pipelines，就像我们最开始去学习怎么样用这个transformer的这个pipeline，怎么样用这个transformers的这个trainer一样。这个notebook的这个pipeline是想告诉大家，怎么样去用这个PFT，尤其是PFT的lora，它的几个关键步骤。一通过这个prepare model for int eight training这样一个方法，可以把我们的模型做一定的预处理，第二个要为我们的这个lora设置一定的超参数。然后通过get PFT model，可以把这个模型转换成一个PFT的model。然后这个PFT的model有一些特定的方法，我们可以包括打印它的训练的参数等等。然后这个PFT的model是可以直接给到transformers的trainer去做这个训练使用的，然后我们通过群方法就能够完成这个训练，是这么一个流程。
	那么whisper可能就会又有一些不同，这个奥特林是我们可以把这个输出打印出来，这个out其实就是我们输入的是这个。这样作为一个上文，上文给到模型之后会生成一个下文，下文有一个长度，这个会被我们的token either解码回来，得到一个值，跳过特定的token，这跟都跟之前一样。好，那那这个是lora的一个标准的使用流程，算是它的一个必要操作的话，这应该是它最简单的一个操作步骤了。我们看一下，假设要把这个lora用到一个更实际一点一点的场景，那么两千行我们知道这个数据集很小的，只有两千多条。2510条的数据对于一个67亿的模型来说，肯定是很小的。假设我们现在要训练一个大一点的模型，那它可能就不太方便我们实时去跑，但是它比较实用，会怎么玩呢？那就涉及到我们要讲的下一个示例，叫做OpenAI的这个whisper large v二这样的一个模型。这个模型其实它是用来做语音识别任务的，就是automation speech recognition，就是我们的自动语音识别这样的一个任务。那为了实现这样的一个自动语音识别的任务，OpenAI也做了一个基于transformer的模型，这个模型也是非常简单粗暴，然后就能达到一个很好的效果。
	那我们接着来看一下怎么样去使用它，我们把这个显存关掉，有个同学问na以后显存占用还是13G，这是一个好问题。这个其实是底层框架带来的。你可以理解成现在的这个GPU的显存是被明面上看是被PFT和transformer这两个库占用的，实际上是被patch ch占用的。更实际一点，其实是被库达这个更底层的库给占用的那如果我们做的好的话，是不是应该不用了，就把它回收了，显存就释放了，这个是一个比较直接的最佳实践。但实际情况就是在显存的回收这块，目前没有特别好的解决方案，比较让人难过，没有特别好的解决方案。有一些实验性的方法，或者偏向开发端的方法，不是生产端的方法。
	就比如说现在这个13GB将近13GB被谁占着呢？被这个model占着，其实是被我们这个model加training trainer给占着的。就它占了的，因为它里面会加载dataset，然后这个model也站着的，它里面加载了模型权重。那么按照我们正常的这个GC就是垃圾回收的逻辑，应该是把这俩删除掉，delete掉，然后显存就应该回收了。如果我们是内存的这个分配的话，一般是这样的。但显存不是就你把python这个层面上的一个变量删掉，在库达这一层它不一定响应得到。那实际情况你想要闪怎么办呢？
	大概率是第一，按照我的经验，橙色flow和拍拖去应该都这样的，你把他们删掉了，然后TN sor fo w和pyto ch应该各自都有一个API，是叫做什么NPT CUDA之类的名字。就是把CUDA的这个没有用到的显存给它清空掉，但它并不总是有效，尤其是在交互式的环境，所以最粗暴的方式就是大家经常看我在操作的，就是把它关掉，然后把它这个kernel给干掉之后这儿自然就没了。因为它的进程没了，就是整个这个kernel l关掉，就相当于我现在打开这个页面的进程被关掉了。那它的内存显存自然就全部都释放了，虽然粗暴，但它现在有用。然后我们的文件也都持久化了，所以问题不大。我们再看这个whisper，那这个whisper我把这个参数就拎上来了，方便大家自己去调。第一它用的是OpenAI的这个west per large v二这样的一个模型，我们也是给大家看一下它的模型，这个模型的训练集叫跟之前有点不同了。Hugin face. 对。
	对，然后这个whisper是一个apache 2.0的license，然后支持很多种语言，它训练集也很大。然后我们能看到它的large v two是15亿15亿的一个规模，我们用的就是这个版本。然后它的这个前处理和后处理的这个方法，我们在代码里面也涉及到，我这儿就不展开了。这个是一个vesper的一个processor，就可以理解成就是音频处理的一些方法，你直接用就好了，然后包括他的一些special token，我们讲bert的时候也有一些special token，我们这儿就类似的每个模型都会有自己的special token这个套路，然后这一行代码context的tokens can be set according ing我没有用，就我们也有用到boss的decoder ID，用这个代码里也有，然后我们用的这个task type。
	就这个任务是一个叫做转录的任务，transcribe的问这样的一个任务。转录的意思就是说我做语音识别，就是我给你一段音频，然后大家的直觉就是语音识别。就是我给你一段音频，然后我把它转成了文字，就像大家都在用的这个微信的这个语音识别。然后有的同学问怎么在下载模型到本地前，就知道模型占用空间的大小在哪里？看上节课有一页PPT专门讲了怎么算的，有同学回去复习一下。对，然后这个转录的意思就是其实换句话说就是你想象一下转录是个什么概念，就是记下来然后抄下来的概念。只不过它的模态变了，输入是音频，输出是文字，输出是这个text，是这个意思，这个叫转录这种task type这种任务。
	好，这个是关于我们的whisper的一个描述。包括它的训练数据对模型就比较大了。它是在应该是68万个小时的音频数据上训练出来的一个大模型，所以理论上这个模型已经很强了。这个模型它经受了长时间的各种各样的音频数据的训练，它本身就是一个多语言的一个模型。在我们在上面的这个模型列表里能看得到，这english only或者是这个Martin ingo就是多语言的。那么从large和large v2看得出来，都是多语言的。
	然后我们现在的point是什么？我们的点是什么？我们是说你特别牛逼，你擅长几十种语言，甚至上百种语言，但我不需要这么强的模型。第一，它的运行成本很贵，15亿的参数对吧？第二我现在就想要有一个中文的语音识别的模型就好了。然后这个模型可以小一点，用柔软来给它适配成一个小一点的，行不行？肯定可以，今天我就下载了，应该得有2 30GB的这个音频。就下面这行代码，大家到时候如果去训练的话，也得当时的很久。
	就common voice这个数据集挺大的，然后里面的中文语料也挺多的。我印象中我看一下，应该是有给这个。language. 有一个超链接是关于这个。这个data set。
	我直接搜一下。他是某Z基金会弄的一个数据集。然后这个数据集规模贼大，大家可以去搂一眼它的官网其中中文有1230个小时，它是一个非常活跃的一个音频数据集。大家可以看到它最近一次更新是12月27号，也是今天然后1点57，不是东八区的时间。就是他每天都有人在网上面贡献不同国家的御姐，是一个到此时此刻都非常活跃的一个数据集。甚至你想给他贡献这个随机也是可以的，你就在这说话，他就录进去了，然后你的声音就变成了这个数据集当中的一部分。然后moza foundation它也有对应的一些协议，他不会把它拿来干一些乱七八糟的事情。
	这个是关于这个数据集挺有意思的，也是挺主流挺先进的一个关于音频识别的一个数据集，你也可以在这上面下到各种各样国家的语言。那现在我们是把这个把中文给下载下来了，然后作为了我们Laura要在中文，就在整个中文这个数据上面去把语音识别做好。然后能不能尽可能用一个小一点的模型，然后把这个whisper的能力在某一个特定的语种上给它复刻出来。就干这么一个活，那我们来实际试试他会怎么做，那这个跑不完，因为我印象当中我跑了100个step，就是一两个小时，所以大家实际下来得花一些时间去跑一跑。然后这个数据集也比较大，可能第一次下会花点时间。因为我在海外服务器是200兆的一个带宽，也下了得有半小时。
	大家可以评估一下，有个二三十基地，就是它的数据，我们可以看一下，就这儿就可以把这个数据，因为它是语音识别，这个任务跟刚刚不一样，这个任务就非常好打标签。然后还有它对应的matrix，就是你想语音识别多简单，就是一定有标签的。然后这个标签能帮助你去评估你的模型现在的水平怎么样，因为你没识别对那个字就不对。
	我们在这儿能看到，他会去下载这个数据集，这个数据集的名字叫moza foundation。Common voice就是我们刚刚看到的这个数据集这个数据集。然后这个数据集我们把它这个训练集拆成了training和validation。这样你在训练的过程当中就可以去做评估，这也是我们的一个homework，预告一下就这儿，homework一为中文语料的训练过程增加过程评估，观察train loss和validation loss的一个变化。然后这个validation loss用什么matrix来评估呢？可以，第一，因为它的训练速度其实已经比较慢了，可以不在训练过程当中去全量的做这个matrix的一个计算。我们在find tune QA这个notebook的时候跟大家讲过原因。
	你可以训练完成之后，再用特定的metrics去计算它的一个实际水平。在某一个特定的指标上。那么这个评估模型在后面留了对应的这个代码，大家可以试一试，对于这一类云识别任务有一个经典的metrics叫做mwer，就是word error rate，应该是叫词的错误率和这个词的错误率是hugin face evaluate这个库已经支持的一种指标。所以你可以直接node进来，就跟你去node这个准确率accuracy是一样的，你加载进来之后就能跑。但这里友情提醒一下，我的这个bedside设置的已经是到一个比较极限的一个数字了，我的best side设置到了64，然后这个64的basset基本上就快占满这个16GB了。我们可以实际运行一下，这儿分出来了，然后打印出了第一个训练集里的数据我运行了吗？重新启动一下，确保这个正确。
	这边会从本地加载数据集，然后去做一个划分，然后会好输出了，这个是common voice的一个标准的字段，这个字段我就不再用那个show random elements打印出来了，大家也可以在这儿看得到，就是hugin face的hub里面有一个功能，应该叫做data set view。就这儿有一个按钮，它能更好的去加载各种各样的数据。我把这个拉大一点，大家就能看得到这个是一个完整的common voice 11这个版本。它的这个数据集的license n是CC0的1.0。然后大家能看到它这有各种各样的的字段，我们并不是每个字段都需要用到这个里面的内容是什么呢？其实就是一堆的音频数据，我可以听一听，我不知道大家能不能听到这个音频输入。稍等。
	We have听不懂对吧？不是英语，这应该是个俄语我猜。它有个字段是区分语种的。我印象中。我们可以找个英语的。
	The track appears on the computer tion album craft works. 所以这个是很多个国家的语言，他还在不断的去做各种各样的叠加，包括中文，中文它分了中完整的china，然后香港和台湾，然后这个就是我们刚才看到第一条，性喜温暖润湿气候且耐寒，包括不包括香山公园和八大处公园内部的景色。其实它还会有一些噪音，这个噪音是挺好的。就是你一个模型没有这种噪音的话，其实它的真实使用的时候没有噪音，它的这个抗噪能力鲁棒性就差了。
	这些字段并不是每一个都齐全的，比如说H性别之类的那我们现在要用来干嘛呢？我们其实不需要性别什么的，对吧？我们要用的字段很简单，我们其实只需要这个audio跟这个sentence这两个就好了，就是音频输入和它的内容就够了。剩下的字段我们目前用不到，如果我们用得到的话，跟你的这个应用有关系。所以我们预处理数据要怎么去处理，我们一个一个来看，我们到这儿了，把这一段加载出来了，信息温暖润湿气候且耐寒这么一段。然后我们实际只要用这个audio和这个sentence就够了，所以我们这个常规操作加载一下tokenizer，然后这个feature extract，这个特征提取器，包括这个processor都是音频里面需要用到的。
	我们暂时不去展开我们的这个课程内容在这儿通过一个remove columns，这个是这个首先这个common voice，它是一个data set dict这样的一个数据结构，这个数据结构它支持去移除一些字段，就跟你拿着一个excel，你删除某几列一个逻辑，把一些我不要的删掉，删掉之后保留audio和这个sentence。我只留这两列就够了，这个是我需要的内容，然后我们看看实际会还需要进行哪些处理，我们现在拿到的这个数据集跟whisper真正训练集是不一样的。这个还是一个大家得整明白的逻辑。
	我们没有拿whisper自己的训练集再去做loa，拿了另一个的训练集。然后大部分的loa都是这个场景，就是正经你要去做应用的时候，你一定要找一个你要用的那个场景里面的数据集。就像我们现在拿着这个common voice一样，就是你如果要有一个特定的应用场景，然后这个lora本身也是多模态都可以用的，不管是文字的、音频的、图像的。假设你现在用了一个特定的场景，你要去说话，你就可以录一堆的音。就包括我们待会儿要去试的那个测试数据，就是我今天录的一段这个音频，我说的内容，我的这个音色发出来的内容，然后。呃所以要用自己的这个数据作为你LORA的训练数据。
	这里就有有一个细节就是我们的这个whisper模型，它和我们现在用的common voice这两个数据集，它的simply rate，关键指标采样率是不一样的，所以我们需要进行一个降采样。采样率不一样也很简单，就是vesper这个68万个小时的数据不可能都达到48千赫兹这么高的质量，所以它就只能降采样了。那么我们就把这个common words降采样到16千赫兹就好了。好，16K然后处理一下。处理完之后你会发现这个地方是它的实际编码的这样一个信息，会有一些变化，但这不重要。往下我们把上面的这几个操作给它放到一起，变成它的一个预处理的这么一个方法，这个都是常规操作了，我们就不再赘述了。
	好，然后给他做一个data collector，去把它有一些不必要的空的部分给它补全，然后定义好这个data collector。然后这会儿这有一些细微的区别，这个区别是什么呢？就是我们知道前面都是因果模型，对吧？
	但我们现在做的这个是一个什么？是一个典型的sequence to sequence to sequence的网络，就是编码器解码器的网络。为什么这么理解？你想象一下语音识别的逻辑是什么？就是我们讲encoder decoder网络也好，sequence to sequence网络也好，这一类网络它擅长的是什么？就是一个编解码的能力。你的音频输进去，然后被encode编码了。
	编码之后就变成了每一个词都变成了一个vector。然后这个vector在那个向量空间当中，理论上它是有一个特定的一个语义。然后这个语义它的这个特征向量或者说他的高维的vector，你说的这些所有的内容，它都有一个跟它比较相近的一个在模型里面的值。就像我们平时做相似度，做各种方式去去检索的时候一样的。然后编码器就是把各种各样的输入，不管是中文输进去的，英文收进去的，法语收进去的。反正我训练的时候那个高维的那个空间里，把这些语言都融合在一起，有一个特定的语义。然后这个特定的语义大家是统一的，就像我们之前讲word是一样的。这个是编码器要做的工作。
	解码器要做的工作是说，好，这个语义我知道了，我要把它解码成一个特定的语言。那这个时候就把它还原成人能看得懂的内容，这就是一个典型的encode decode的结构。就像我们最早讲注意力机制，机器翻译也是一回事儿。Encode是一门法语，decode是一门英语，只不过都是文字写的。而现在是输入是音频，输出是文字，这个只是一个编码方式的tok nize tok nize这个阶段的一些处理，这样大家应该就能理解为什么会使用这个sequence to sequence。然后在auto model里面还有这个speech sequence to sequence。
	因为你的机器翻译的这个序列到序列的模型和音频识别的这个序列到序列的模型肯定不一样，所以他就用了这个speech sequence to sequence这样的一个模型。然后同样的使用这个babbit来进行加载。然后d device map设置为auto，这个是一个accelerate底层的一个参数设置，为这个时候能进行加速，混合精度的一些加速。这个地方是我们刚刚看到的，在这个vesper v2的模型这个model card里面有写的，我们这儿不赘述了，是它的一些参数，但这个加载要花点时间，我们看到这儿的显存就开始被拉起来了。
	这。
	大家能看到这个显存的增长吗？买一个简单的算术题告诉大家，这个1点55亿的一个whisper large v2的模型，以bar bit的精度加载新应该会需要多少显存。这同学应该熟悉的应该瞬间答出来，对吧？就8 bit就是一个字节，然后一个字节，那就1.5T就够了。
	然后pi touch我们上次也在量化那些notebook给大家看过拍套ch通常会预留个两百多兆的显存。我们可以看看这儿一些1.7G，其中有两百多兆是这个配套及预留的。然后同样的套路，我们使用这个prepare model for international training，给它洗一下，给模型洗一下，选完之后变大了对吧？大家看到这儿从增加了300兆，为什么会增加300兆？因为有些不是int 8的，被转换成32位浮点数了，肯定就变大了，这个好理解的。然后同样的Laura config，这个地方我就没有在每个参数都去写了，我们只针对这个QV，目前简单一点这样，因为主要还是训练太花时间了，大家也可以把这儿再调整，然后去做一些这个训练。25‰，0.25%。
	然后这个地方我专门注释了。首先这个模型你看着只有15亿，其实挺大的，15亿。然后第二咱们要想把这个whisper这个open I的这个whisper large v2真正用起来，这个是一个生产机可用的语音识别模型。大家不要小看这模型，只不过你要把它训练好。然后针对这个模型我的建议是这样，首先我这儿是用了max depth，是为了能够走完一次执行。然后实际上大家要用的时候还是像这两个，是我刚刚提的用这个number train e pox，然后包括这个evaluation strategy，把这两个注释给解开，然后解开之后给这个训练的过程当中去对比一下它的validation loss，就是我这儿写的，然后让他实际的去跑一跑。然后我们把训练集也都区分出这个训练集和验证集了，是可以直接用的。定义好，定义好之后这还预留了一个大家可以去用的，就是我们的这个call back，就call back的这个机制，回调的机制就是我们在trainer里面是可以加call back，这样才能去保存一些中间结果，这儿我就不赘述了，大家可以去看看，其实就存一些中间的内容，然后这个地方使用了sequence to sequence的trainer。
	因为大家要记得这个网络结构变了，就是几个分叉路了。这个我们专门在课件这个重要性的级别里去做了区分的，我们再看一眼这个玩意儿。是三个是不一样，是不这个级别上大家是不能共用的。大家都是auto model的，一个父亲都是auto model，但是我的网络结构上有大的区别的时候，我得用不一样的这个子类的model了。那既然用了子类的model，它的trainer当然也都是对应的这个trainer了。
	所以这一点大家需要注意，就像我们这个地方的含义，自然也要用不同的trainer了。所以我们看到这个地方用了sequence to sequence的trainer，sequence to sequence的training arguments，这个也很好理解，sequence to sequence trainer。肯定训练方法不一样，然后网络结构都变了。然后既然网络结构都变了，训练的超参数肯定也不一样，就这么个意思。好，这就我刚刚说那个参数，如果我们把它设置成这个force的话，就不会有那一堆警告了。这个train是比较花时间的，我估计今天肯定没时间给大家演示完了。这个训练得100个step，应该就得要一个半小时，然后通过这个model的c retrain的方法可以存下来，就下午存下来的这个有点卡。
	卡住了。这儿应该能看到显存已经拉起来了，已经快13.5G了，因为basset设置成了64。卡了，稍等一下。好，这儿只要UI主摄好了，我这应该就能剪进去了。
	还不行。
	张印。在这儿看吧，models这个里面有这个vest large v2，就我的这个visual studio code里面能看到。然后有对应的这个节省文件和它的这个read mi的文件，这个read me文件也是PFT自己生成的，然后这个是下午生成的这个模型文件这儿能看到它的一个训练，是100个steps，是因为这个地方写的max steps。如果咱们把这个写成number train e pox，那你只能想象它的example，它的这个样例数应该是有39000多个，我没记错的话。大家到时候实际执行的时候，能看到加载的时候有这么一个数据，然后训练完之后保存下来。
	然后大家还记得怎么样去执行这个语音识别的任务。其实我们之前第一个transformers的课程就教过了pipeline，大家还有多少同学记得pipeline然后用feline就可以直接来做这个实战，我把这个关掉了。这就这个先停止了，我们用一个下午已经。好，先停掉。好，停掉之后，把这个。这个部分的代码捞下来。
	我们重启一下哎也，不用，估计得重启一下，不然显存就没了，不够用了。然后这个地方有我的一个测试音频文件，中文的我们可以荡下来看看。这已经打不开了。
	Data audio. 
	看能不能。
	直接播放，看有没有这个维修studio的插件能支持播放音频不行，那还得下来下到本地。
	好，下好了。这个。Quick time player. 
	你可以听一下，这是一段测试，用于whisper large v2模型的自动语音识别测试。这个是我下午录的一个音频，然后这个音频很有意思。第一是我的声音，所以训练基地肯定没有。第二这一段里面我故意用了中文和英语混合，里面提到了这个west large v2，这是英语，但是又有中文。看它的这个识别怎么样，就应该重启了。重启之后我们大家还记得要怎么加载回模型吗？我们在这儿还需要把。Node model的部分，找到node model在yeah。这儿。
	然后。
	其实。
	是夹在他。这就我们训练出来的这个模型。应该没有被覆盖掉。
	我看一下。
	Is not a local folder, not a valid model identifier. 本地没有这个路径吗？我搂一下models whisper large v two ASR。In ba. 
	这可能是被我覆盖掉了。这个试验可能只能大家训练完了之后再试验一下了，这可能是不小心被覆盖掉了。是大家如果直接拿训练完的这个模型去跑test这个audio，都不用去保存。
	就像我们开始做的这个实验一样，用这个trainer点model，可以直接去试一下。这个是我下午直接试过的，然后可以得到一个，对对对，这里有一个对应的输出结果。对，这个是用这个训练出来的。咱们的刚刚看到在中文的语音音频的这个数据集上训练了100个step之后，它能够把刚刚我念的那一段话成功的识别出来。当然可能这个识别出来之后，这里有一个空格，他没有打而已，但其实效果已经很好了。对，然后大概是这样的一个逻辑。好，那我们看看有什么问题。
	对，还有一个小的要补充一下，上节课在讲这个量化BNB的时候，应该是少给大家做了一个参数的一个合并。就当时我们有提过可以用NF4的这个精度来加载使用这个BMB，这个forbid宽带有NF4也有这个int four，然后也有这个双量化，这个双量化是用的use double quint，当然我们还记过一个把q lora的所有量化技术都用上来加载模型，就是可以把这些都用上，包括这个混合精度。刚刚还有同学问这个，是这个BF16设置为true，其实这是一个意思，它底层用的都是BNB，然后quant的这个类型用F4，用这个double count。这个其实就是我们看到的这个q ora的三个经典技术。好，这里再跟大家做一个说明，然后这个代码也更新到github了，好看大家有什么问题，然后我们今天的这个分享就是到这里。
	能做个应用，把youtube英文发音的转成中文字幕吗？肯定可以，而且youtube现在就已经支持这个功能了。有没有专门中英文混合语料的数据集？现在这个数据集同学就是我专门给大家做过展示的。这个数据集就不只是中英文，有N个文章。你在下载这个数据的时候有一个超参数，有一个参数。你看你发现没有这个data name，我去弄的这个data set的时候，data name还有个叫做language a language就这儿，这是可以换的，你可以换成english，也可以换成别的。它就node不同的数据集了，只要你的硬盘存下来了就可以了。
	所有的hugging face上的大语言模型都可以用PFTV调吗？这个我不敢给你回答，因为hug face上几万个模型我没有一个一个看过。对，但是只要他能用transformer，然后加载进来是一个model，然后它的网络正常，没有这个乱七八糟。因为你要知道每个人都有权限往下跟face上推一个模型的，包括咱们，然后推过去是不会有校验的。就像这个问题，就像github上面所有的开源项目都是好项目吗？肯定不是，对吧？然后只要你是一个正经能运行起来的，然后auto model，那你大概率用lora是通用的。因为lora本身是一个通用的方法。
	怎么知道要训练的模型是自回归的，还是sequence to sequence？这个很早讲过，这里有标签的同学。我把这个。放大一点，然后这些地方都有写。比如说我们看这个模型，它是一个写的很清楚，就你找到模型了，它它得有这样的一个页面，那这样的一个页面里面它会写清楚的，比如说他会写whisper is a transformer based encoder decoder model。首先你看到这段话，你就知道它是一个sequence to sequence的模型了，他写的很清楚了。然后如果还不知道他下面会写，他的代码里就写的很清楚，比如说from transformer informers，然后要去加载它这个模型，怎么加载呢？它就写的很清楚，怎么样去加载？
	这里都会有，然后你就可以拿着这个东西去搜。就比如说他用的是叫做穿a whispers for conditional generation，条件生成对吧？你去这个地方，so. 
	这不就有了吗？
	这里就有。下面就会有一些详细的内容告诉你是什么。甚至因为vester它自己定义了这个自定义的这个model，你甚至可以用这个，就像我们刚刚用的这个OPT和GPT two一样，不用这个sequence，sequence也是可以的，用他自己的这个也是OK的。
	看看还有什么问题。不微调就不能识别吗？不微调可以识别，我们不微调你就得用这个完整的vester。我们刚刚讲了这个LORA的场景的，这边就不再赘述这个同样的问题了。
	怎么找到支持理解中文的模型，也是从标签能看到或筛选吗？是的，一个好的模型它当然应该把这些写出来。就像一个好的开源项目，它会告诉你这项目是干啥的。你像这儿他就有血吗？这不就有吗？这个同学看到现在这个吗？
	Sequence to sequence的模型一定要用这个架构吗？不是这个东西，是这个意思。这个sequence to sequence就是指encoder decoder这样的架构。也这么讲也有点绝对了，就是序列到序列大概率就是你就是encoder decoder了。目前也没有第三个类型了，对。
	我没有找到一个特别好的例子来给你举例。其实你的这个问题他也不是说茴香豆的茴，你写这个回可以，写那个回也可以，但它其实指的都是茴香豆，对。因为这个sequence其实就是指的这个RNN的结构。然后你硬要说的话，他们有交集。但当我们谈论这个自然语言处理的时候，他俩通常是等价的。因为他们的交集在自然语言处理的这个语境下，他们的这个交集就几乎，你可以这么理解，就是他们的交集如果你要打上一个标签就是自然语言处理，就他们的交集在自然语言处理这个语境下就是一样的。所以当我们讨论自然语言的时候，sequence to sequence就是指encode decode的架构。
	但如果你要说sequence to sequence，能不能表示别的架构？比如说你硬要搞一个2个RN兑在一起，然后他们不是一个编码器，解码器的架构也能搞，它也符合所谓的sequence to sequence。但你如果搞了两个，它又不是编解码的结构，它能是什么结构呢？对吧？看大家还有什么问题吗？
	然后今天因为我们元旦应该是会放假，所以那天是不上课的。所以我们准备了这个wasp的模型。这个模型是很值得大家去深入玩一玩的，这个模型它实用性还挺高的，这个数据集的规模也很大。然后大家我我的建议是大家都好好玩一玩这个notebook，然后把这个如果有一定开发的这些同学，把这个模型移植到你的某个场景里去，然后试试做成一个应用，我觉得是很有意义的。
	如果我要提高识别地理名词和特定名词，例如车型。你说的很这个前半段是对的。如果你要识别特定的，比如说地理名词，或者一些特定名词的准确率，那你找这些词肯定是好的。这个是最简单粗暴效果好的方法，但是指标是不变的，就这个东西指标是不变的，这个指标是不用变的，因为这个指标就是用来评估语音识别的，跟你说了什么内容没关系，我不知道表达清楚没有。它的这个指标的意思就是你说的话跟你识别出来内容不一样，你就错了。至于你说的是什么话，你只要标注，别搞错了，这个指标都是有用的。
	10点20了，大家还有什么问题吗？没有的话，我们这个。这个今天就到这儿，经典的应用很多。你想象一下所有的输入法不都在用这个例子吗？然后还有的同学问这个转译，就是咱们不能把这个玩意儿只当成一个waster，它不只是一个可以用来实现转录功能的一个模型，它也能实现翻译。所以因为它本身就是一个多语言，所以你是不是可以做成一个像这个科大讯飞的数这个什么录音笔？
	就理论上来说，你现在做成一个应用，里面套一个whisper的q ora的这个lora的模型，那你可以对接99种语言作为输入。如果你不去做这个lora的话，首先它能对接99种语言。如果你做了ra的话，你就在你的场景里把那几种语言都保留能力，就都加到你的训练集里，保留他的能力。因为ra就是这个意思。然后那你在各种场景里面，你只要语音输入，不管是手机上的这个微信的输入还是什么别的方式，离线的或者在线的输入进来之后，你都可以翻译成一个目标语言。然后这个目标语言还可以文字的形式呈现给你，也可以再套接一个语音包。比如说open IDTTS，那他就能翻译成一个对应的这个就是有人把你的这个翻译的结果念出来，我不知道这个表达清楚没有？这个效果差不差的，自己比较一下，然后也有论文。可以把会议录录音直接转成会议纪要了。当然当然是这样的，声纹和语音识别是两个任务不一样。
	对这个效果差不差的，这个我建议去看一下这个论文里面有贴的。然后也可以自己试一下。好，我们要不今天就先这样，我建议大家这个元旦抽点时间去把这个vest这个模型可以好好玩一玩。然后把正好通过这个过程，把LORA的这些API把参数给熟练。然后我们后面还会再去用这个ChatGLM在上面去做这个Laura。
	但为什么要讲vester？是因为首先它是另一个模态。第二，这个模型它挺先进的，并且它遵循了这个simple is the best。它的模型架构不复杂，模型架构很简单，就是一个把transformer用到了语音识别这个领域上的一个最佳实践。也是OpenAI的一个很好的里程碑作品。只不过他没有TIGPT那么惹人的关注。
	然后基于它也可以做很多应用，尤其是作为这个应用的输入端，你后面可以把比如说你想象一下，你可以把它作为这个输入端。然后很多你搞不懂的语言通过它变成了正常的文本。正常的文本再去接与这个文本的相关的这个大模型，他是能赚钱的好吧。
	然后vesper算不算大模型？这个算不算大模型这个事儿我们以前讨论过。对，就大模型的特点是什么？十几亿的这个参数算大吗？这个问号我们不去做这个下定义。
	对大模型跟正常模型的区别，我们之前讲过理论课，这主要就是跟每个家庭有关系。如果在家庭里有1000张卡那这个GPT也不是大模型。对，如果你只有一张卡那可能十几亿也是大模型了。
	对，好，然后我们今天就先到这儿。还有一些同学有问题的话，我们就在群里再提问。关于这个准确率不高可不可以？所有问可不可以的问题，这个同学都可以问GPT4，就不要问我，因为这个问题实在是很难回答，可不可以相关的问题就是你要非要问我，就说都可以，肯定都可以对吧？因为没有什么可以阻挡。是的，肯定都可以。对，只是说需要你去做事儿，你不能想想的话，肯定就不可以干的话就都可以。对，然后干的话也没那么复杂，就是要干的代码及其事业都能帮忙生成一些。
	好，我们今天就先到这儿，大家多跑一跑。这些模型的代码就慢慢就有感觉了。然后他会跟你以前的工作方式不一样，因为你会有很多时间确实是在等待，然后在等待模型的训练。那你就可以小批量的用小模型去试。比如说我们前面用过的一千多万的OPT的1.3B，现在我们用了6.7b，whispers不需要几百亿，语音识别不需要那么大，十几亿就已经很大了，已经效果很好了。语音识别的时延性要求很高的，他不能等那么久。好，我们今天就先到这儿，大家有问题我们再再沟通。好，谢谢大家。