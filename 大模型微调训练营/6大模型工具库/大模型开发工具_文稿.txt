	好，我们经过了两周的理论课的学习，我看很多同学都在问什么时候能上手写写代码，然后实战的搞一搞。所以我们把ChatGPT的理论课的部分往后放一放，因为后面会涉及到使用这个RLHF之类的一些训练方法，包括一些指令微调。我们先把之前讲过的一些内容，通过transformers和PFT这两个hugging face开源的代码库，把它们先用起来，所以我们做了这样的一个顺序调整，现在我先做一个小的调查，就是今天现在参与直播的同学，如果你从来没有使用过transformers这个库，你在评论区里打一个二，我大概了解一下这个比例，如果使用过的话就不用管，我看一下这个有多少同学是从来没有使用过的。
	我这个比例不低，比我想象中的要高一些。那行，那那我们就今天稍微讲细一点，大家有什么不明白的话，也都可以多问一问。不过整个transformer这个库是比较好用，并且又简单上手的，那我们就正式开始了，好。
	今天我们讲hugin face的transformers这个库，这个库其实是也开源很早的一段时间，很早就开源了。然后也逐步的变成了一个越来越主流的一个机器学习或者说深度学习相关的一个库。我们今天就分成三部分。
	第一部分是跟大家快速的介绍一下transformers这个库是什么，它的名字取得也很巧妙，叫transformer，跟我们的很有的神经网络的框架transformer多了一个S，这个取名也有一定的原因。它的核心功能模块现在一直在扩展。我们今天会讲它最主要的也是说，如果你是一个transformers这个库的新人，一定会先用到的三个模块，也就是pipelines，token ized和models。当然它还有很多其他的库，那些都是他通过四五年的时间，这18年底19年开源以来，逐步的迭代，增加的一些功能。包括跟这个hugin face的hub，也就是所谓的大模型时代的github，这个hug face hub有一些联动，以及整个hugin face的开源的eco system，它的生态会有一些什么样的连接。我们都知道整个纹身图的这个板块，stable diffusion hugin face也有一些自己的参与。包括我们在这个应用开发课程里面使用到的一个叫做radio GUI图形化界面的一个开源库，也是hanging face开源的。所以整个这五六年的时间，这么一家在美国成立的法国人的公司，抓住了整个大模型时代的浪潮，在软的这一部分以及数据的这一部分做的非常好啊，我们会整体的介绍一下这个transformers。
	第二部分会教大家去做一下这个大模型开发环境的搭建。一个是咱们自己触手可得的一个GPU的开发环境，第二个就是说如果咱们现在还没有GPU，比如说你现在自己还没有买GPU，或者说你还没有在这个云服务厂商去出一个GPU，也有一些临时的方法，就是这个google的clap，只要咱们能够科学的上网就能够。我印象当中，是有24小时还是48小时的一个测试时间，甚至可以给到TPU。然后今年我记得hugin face的团队和google的团队也做了合作。Google club现在也预装了这个transformer school，能够直接导入。
	第三部分就是我们来实战了，也就是今天下午在群里面发了这个github的开源项目的地址。就咱们整个课程，整个大模型微调的课，都会在那个开源项目里面去迭代我们的代码文档。所以大家如果还没有去看那个项目的，记得去关注一下，star一下，folk一下。因为你要去自己去做修改的话，都需要你fork到自己的这个github仓库里，再去逐步的修改使用。
	好，那我们就正式开始这个transformers，这个库是干嘛的？首先它一开始是一个python的库，然后现在也有这个GS的库了，然后它最早是一个为机器学习服务的，所以我们很多同学一看到这个库就很懵，叫做state of the art。我们经常听到的一个词sota，就是当前最好的机器学习machine learning的组件。
	然后面向的这个是机器学习的框架，是James py toch和tensor flo w。TensorFlow可能大家是听到最多的，这个是google 15年开源的一个神经网络的框架，最早服务于这个CNN，后来像强化学习，包括这个IN都有一些知识。Pyto ch也是一个非常主流的，并且现在可能在市场占有率更高的一个神经网络的一个框架，像我们现在看到的大语言模型，有很大一部分都是基于patric去实现的神经网络，包括transformers。它自己对于PyTorch的支持也是最好的，TensorFlow的支持没有py touch那么完善，当然也在逐步的完善过程当中。
	然后jack其实也是google很早就开源的。我印象当中应该是我作为这个google developers expert，会有一些内部的free release的邮件。应该是在19年的时候，还是20年初，我就已经收到过google的团队发的这个James的一些早期版本的0.1版本的一些邮件。然后jack其实是想做一个更轻量版本的，更快速便捷的一个神经网络框架。
	但anyway说回来就是说transformers是主要支持这三个神经网络框架，而这个S这个transformer的这个S其实取的也很巧妙，包括他后来自己的一个演变，我们就会看到其实大语言模型我们都知道，大部分的大语言模型都是在transformer的这个神经网络框架上面去修改、叠加改造。那么transformers这个S其实就体现在大量的大语言模型的框架。在transformers这个库里面，其实它整合了一堆的大语言模型，并且它你可以认为它预定义或者说它内置了实现了这些大语言模型，所以方便你能够快速的去使用它们。像他自己提到的，这里提供了数以千计的预训练的模型，并且也支持很多种语言，然后也支持很多种任务。这些其实都是对于新人，就是我们所谓的想要学习大元模型或者NLP相关的同学是一个非常好入手或者说上手的一个工具库。当然如果你想要做更进一步深度的改造，transformers也有提供一些customer model这些功能，就是你去一些自定义模型等等，整个这一套工具库技术站是比较丰富的。
	当然从早期transformers这个库刚刚发布以来到现在，其实它已经开始从自然语言这个偏语言文本类处理的一个工具库开始扩展。扩展到什么呢？像我们最有名的多模态，他现在也在涉猎，包括我们今天的这个事例里面就会有一些比如说VIT或者DETR这种偏向于transform mer在视觉领域的一些模型，包括经典的计算机视觉的网络，比如说各种CNN的网络，read net等等，包括我们看到目标检测，我们有使用这个DTR，在音频处理里面，比如说我们的各种音频处理有，最典型的我们要对音频这个信息进行分类，打标签，或者说对于音频进行一个语音识别。那这些其实也都是transformers现在已经支持的一些任务或者说模型，而这些模型为什么能够被广泛支持？有一个很重要的原因就是因为他们都是神经网络的模型。所以只要能够支持神经网络，能够去正常的运行它的inference推理，那么这些模型就能够被很好的部署使用。同时它还支持了很多训练的功能，就比如说它最近在推的这个in的模块。
	好，那我们就看一看，这个是他的一个整体介绍。它除了刚刚提到的，他自己实现了各种预训练的模型以外，还有一些什么样的好处包括它的易于使用，还有两还有一些好处。我们这边给大家整体介绍一下。
	一个就是说一个好的开源项目非常重要的一件事情就是它的活跃度。而hugger face是一个你可以几乎在上面找到最新研究成果的快速集成，这个是非常厉害的。我们看到有很多的新的论文，不管他是真的有开创性的一个创新，还是说他只是做了一些局部的一些网络结构或者说参数，或者说模型模块的一些组合这样的一些尝试。这些研究成果都很快的能够在transformers上面找到它的实现，这个是从它19年18年底的时候，就有这么一个底蕴在这。因为我们知道那会儿bert这个google发布的大语言模型非常的有名，并且有很多人在bert上做了大量的更改。
	那么hugging face的transformers也是最早去实现这个开源版本的birt的一个库，包括GPT two也是类似的。同时它的社区非常活跃，有很多的人在更新和维护它的服务。就比如说我们现在看到了javascript这个版本的transformers的库，然后他也对于一些高阶的用户，他也允许去各种各样的去定制和调整模型。因为它往上能够直接到部署这一层，甚至跟它的这个hugin face的hub去做对接。
	往下它也是能够对接PITOCH TensorFlow这些神经网络框架库的。所以这些库的API它是能够直接操作的。我们甚至能够在代码里面看到有大量的python的代码，比如说怎么样去引用py touch的这个neural network，就是这个神经网络层这一层级的API，在这个API层面上去操作各种tensor，这样也是可以的。所以它往往下面去走，也是很灵活可定制的。同时还有我们刚刚讲到的，它已经不再局限于是NLP的应用了，还扩展到了其他的模态。
	刚刚看到的bert 19年初18年底发布的这个开源的这个版本，在这个transformers上面其实是有非常多的应用。我们现在看到这幅图，其实是清华的这个NLP实验室。他们在应该是在去年还是前年整理的一个基于bert做相关研究的一个，你可以认为是一个族谱。那在这个族谱里面，其实我们会看到各种各样的预训练的序列到序列的模型。这些序列到序列的模型，他们底层其实大部分的神经网络的架构都是基于transformer去实现的。但是这里我们看到像bert它本身就是一个双向的深层次的一个transformer的网络。并且除了单纯的一个but以外，还有各种各样曾经在其他领域做机器学习和深度学习的一些研究技术，移植在了bert上面。然后诞生出了很多其他的bert的这个衍生品模型，包括我们这儿看到的一些多任务，跨语言，加上这个生成的。因为bert本身它不是一个自回归的模型，它更多的是一个自编码，所以是更强调理解上下文，去做文本补全的这么一个模型。
	也有人在上面去做生成，包括去做这个知识图谱，跨模态，对接这个encode decode的架构，去生成像T5。But这样的一些既有理解能力又有生成能力的这样的模型。包括后面很有名的这个switch transformer，当然GPT可能是另一条线，不过transformer也很积极的去跟进了OpenAI的GPT的研究。所以我们能在这个hugin face上面看到GPT two，这样在当时看起来不算是特别有名的模型，但现在可能很多人会回过头来去研究它。所以这些大模型你都能在transformers库上面找到它，并且不需要你做太多的改动。你可以通过两三行的代码就把这个模型部署起来。当然前提是你要有足够的资源就可以部署起来，所以我们能看到这个transformers s其实是通过或bert的在学术界广泛的研究，使他自己的这个库也得到了一个快速的推广。
	从时间线上来看，我们也看得出来这个是star history去追踪的整个hugging in face的transformer这个库和它对接的底层TensorFlow和PyTorch这两个库的一个star数量的增长。当然我们能看到这个是把这个时间线直接平铺开来。我们看得到在18年底19年初的时候，transformers其实开源之后一直增长的很快，并且以比较快的速度超过了排套ch我们把从项目开源的时刻到到今天为止，他们把时间线叠在一起之后，我们看得到其实从刚刚开源开始，transformers就比牌照企业要成长的更快，并且有一个非常快的加速度，还在增长，尤其是大语言模型出现之后。现在也会有，很多的非NLP，或者说不是AI的研究学者也开始使用transformer，因为它非常好上手。今天我们第一部分的这个代码，其实就想告诉大家，怎么样能够让一个甚至不会写代码的人，只用两三行的代码就能实现各种各样的AI应用，这个就是transformers的pipelines带来的价值，非常好用。
	除了transformers库自己这个SDK以外，其实hugin face这家公司他们有很大的野心。就比如说这个hugin face hub，在左下角我有给出他的这个，网站的地址，这个应该也很很多同学应该访问过。这个hugin face hub大家如果没有访问的话，打一个这个五，我看一下有没有同学没有访问过这个网站。这个high face hub可以算是一个搞AI的军火库，因为里面有大量的开源协议，很友好的模型，数据集，甚至是inference API。简单来说就是它直接部署好了，你可以去上面直接用。然后我们经常会去看到的大元模型的这个天梯榜，LLM的leader board也是在hugin face hub上面的一个influence API，有有同学，所以大家都用过对吧？都都去访问过这个网站，我看没有打五的好。
	我们就看到，其实在hugin face的这个hub上面，这个数据一直在增加。我们看到这里有超过30万的模型，这个模型不只是大元模型，包括我们刚刚提到的各种模态的模型都在这个hug in face hub上面开源了。以及我们经常在新闻里面听到的，国内的这个大模型公司也基本都在hugin face上面托管了自己的模型。同时我们看得到刚刚提到的这个LLM leader board，包括这个stable diffusion，包括stable video d diffusion，还有我们看到这个sameness m forty。
	这个应该是meta开源的一个多模态的一个开源版本。就是它能够支持你用麦克风，用语音，然后文本进行输入，然后转换成不同国家的语言，以及像类似于语音的生成，让它不只是从文本输出，也能从音频输出，这样的一个开源模型。整个space就是hugger face hub上面一个托管平台，然后hugger face也通过跟AWS，现在应该也开始跟微软的azure合作，托管了大量的AI的模型服务，使得我们可以不需要自己拥有资源，或者说租赁资源就能够去访问这些AI的服务。
	同时还有很多同学都在问，数据哪里搞？我们看得到其实有超过5万个数据集，在哈根face hub上面开源是可以直接使用的，而且是各种各样的数据集。有我们之前讲过的很多美国的高校，也有一些非盈利性的机构，比如说Vicky，还有一些公司司，一些研究团体都在上面。有各种各样的这个数据集是可以直接访问的，当然我的印象当中这个哈根face hub的访问现在好像是不是特别稳定，有一些镜像站，国内有一些镜像站，大家可以访问他。如果你的这个网络访问有一些问题，我们回头可以在群里整理一下，看怎么样以一个比较稳定的方式去访问这个很有用的一个军火库。
	除了我们刚刚提到的transformers以及hub以外，就是我们看到这幅图的左边有一个笑脸在他们公司的logo，中间是一个transformers，22年6月份的时候，他的头上是一个hugin face的hub。把transformer里面已经实现的，已经预定义好的各种各样的模型，通过hugger face的hub以及space的这个模块，变成了一个个可以用的服务，对外提供了influence API。除了这些以外，还有我们看到的data sets，还有它的这个模型评估，包括它的这个模型加速，这个模型加速也跟我们后面要讲的微软的做，分布式模型训练有一定的集成。包括它的优化。
	整个左边这一块是它去年或者说将近一年半之前的一个状态。那到今天为止，其实它还在不断的去做扩展，包括我们后面会讲到这个PFT，包括这个safe tensor也是它现在已经集成到了transformers里面，diffuse sers。就做很多的纹身图相关的以及做视觉的相关的一些开源库。
	所以我们能看得到，也就这一年的时间啊哈根face的生态系统还在不断的去做迭代，成为了一个其实TESOFOP py oh jx这些神经网络框架已经被隐藏到后面了。这也是很多同学在问要不要把patch ch学的很深入，要学到什么程度？我觉得先不用急的一个原因。因为通过更高层次的一个API封装，其实你现在已经越来越难看到下面这三个神经网络框架的API了。更多的是通过上层的API在进行操作。甚至他也在有意的去做一些统一的接口的封装这个操作或者说这种模式，就跟南茜想要去把很多大语言模型的服务商，或者说API的提供商给隐藏掉一样。就是谁能够离我们的用户，离我们的开发者更近，他显然就会拥有更多的关注和流量。
	现在其实我们能看得到他跟face的生态系统还在不断的增长，包括它的agents。好，刚刚其实是对哈根face有一个简单的了解，大家应该知道它大概是个什么东西了，然后它具体怎么用，好不好用，到底预定义了哪些模块？我们来简单看一看，当然待会儿也都会去实战这些模块。
	第一个就是我们刚开始提到过的一个词叫做pipeline，就流水线管道，pipe line其实是hugin hugger face的transformers主推。包括你看它的所有的这个tutorial文档，read me都会去讲的一个很重要的一些。因为这个API可以算是它最高层次的一个封装了，并且它就像一个一个可一个帽子一样，把很多底层的API封装在里面，套在里面了。然后你通过使用pipeline，可能你本身并不了解AI或者不了解大语言模型，或者你也不了解什么是CV计算机视觉，不了解什么是音频处理相关的这些技术。但你只要能够去熟练的去使用pipeline，你几乎就可以把刚刚我们提到的各种各样的AI的模型用起来。
	怎么用？我这边做了一个简单的整理，首先我们看到目前pipeline预定义支持的4种模态，从音频的到计算机视觉，自然语言处理，包括这个多模态或者我们叫跨模态，这四种不同的模态它都有一些已经预定义支持的任务。这个任务甚至你不需要去指定使用什么模型，只要你有一块GPU，甚至你可以在一些比如说在一些音频处理，或者一些图像处理上，你可以使用CPU来运行。因为它不需要那么大的神经网络，你就可以直接去运行这些AI的服务了。
	然后你看到最后这一列叫做pipeline的API，你只需要指定你现在到底要做一个什么样的任务。比如说是这个音频的分类，比如说图像的分类，目标的识别，甚至图像的分割，QA，或者说翻译文本摘要等等。这些东西都是可以一行代码去生成一个对应的服务的，这个是非常夸张的。其实我们之前在讲这些所谓的AI agents也好，或者AI native的APP也好。就是因为像transformers这样的库开始出现了让我们很多的开发者不需要理解AI就可以去使用AI，像pipeline API就是这样的一个设计目标，并且通过这样的一种设计方式，逐步让大家能够去享受到这个AI的便利。
	那具体来看，把pipeline API打开来看，它到底是一个什么样的东西？首先pipeline API我们刚刚讲到它是一个封装，那到底封装了什么，其实我们经过前两周的学习，已经大概理解了这个大元模型。首先得有一个模型，然后这个模型的输入，我们都知道它不是任意的输入都可以的，它有一些输入的要求，比如说有这个embedding的要求，比如说这个需要需需要输入的是token，而不是原始的文本。甚至我们往细了看，它输入的这个in bedding的向量，它的维度也是有要求的。比如说GPT3.5、GPT4、GPT3，他们输入的这个in bedding的向量，它的维度甚至都不一样。
	那么对于pipi API来说，我们往这个抽象层次往下降一层的话，可以首先把这个pipeline API的调度看成三个大的模块。最中间的当然是我们的模型，它需要去在这个hugin face的hub上拉取一个模型。拉取到这个模型之后，跟这个模型的前处理和后处理去做一些对接。当然这些对接的模块不是由这个pipeline来实现的那是由它调用的底层的一些模块来实现的。然后整个hugin face的这个transformers的核心就是以这个model为一个中心。然后不同的model它有自己的前后处理的需求，但是我们待会后面也会讲，它有一个叫做auto class的东西，有一些智能的路径识别和一些模型的识别。让它能够通过他自己的库找到它对应的tokenism er和model。
	回到p ypo n，我们可以看到整个抽象最终就变成了一个像流水线一样的东西。我们刚刚提到的所有的任务都可以以这样的一个抽象去描述它，不管是音频视频还是我们处理的文本，都可以把它变成一个从最开始的输入，通过前处理，以一个模型能够接受的这个形态和样式给到模型。模型处理完之后，我们再根据这个任务需要的输出样式进行一个后处理，就可以拿到最终我们的结果。就比如说我们在这儿用其实可以用一行代码，我们把这个结果存到了这个派别，然后我们看到这里有一个什么样的概念，就是transformers，这个是一个python的SDK，所以我们应该大家都是至少会python的对吧？如果不会python可能就得去稍微学习一下了。这个python的导入，从我们的这个transformers，这个python的SDK里面导入这个pipeline这么一个module。
	通过我们开始有看到各种各样的任务，这里是一个情感，你可以认为是一个情感分析或者说情感分类这么一个具体的task，具体的任务。然后这里我们其实是缩写了这个task等于这个参数。因为它默认会把这个task作为它的第一个参数。所以如果你想要极简的去使用它，你甚至可以。
	直接传入一个任务类别，然后这里的这个pipeline，sentiment analysis其实就变成了一个，你可以就把把它理解成变成了上面我们看到的这一串，这一串的输入就是我们能够人能够直接输入的这个内容。比如说我这写了一个吉尔萨白和臻了。那这个其实就是我们对外暴露的AI服务，可以直接跟用户去作为交互的一个接口了，就是我们的自然语言文本。那它输出的内容其实就是这个任务的一个标签，就比如说我们情感分析，通常会有两类或者三类。比如说他是一个积极的情感，消极的情感，或者是一个中立的一个情感，这个跟他具体使用的模型有关，我们其实可以看到整个导入一个pipeline，然后用pipeline这个抽象的接口去指定一个特定的任务，然后变成了一个pipe的一个实例。然后这个type的实例就变成了我们可以我们上面看到的这样的一个流水线，它可以接受我们需要的任何的输入。只要是跟这个任务相关的，那他就可以直接返回一个我们最终可以处理的结果。
	就我们这里看到的下面这一行，就是这个警告，下面这一行有一个label negative一个score，可以简单理解这个score它的一个置信度。然后我们待会儿在实战的时候会去看啊，其实这里的这个置信度还有一点小问题因为我们这里使用的模型其实是一个没有在中文上面去微调过的一个模型。这样的方式就实现了一个情感分析的一个应用了。其实很很夸张的，你要知道放在三四年前要做一个这样的任务，其实会写很多的代码。类似的假设我们要实现一个智能问答。我们知道现在使用ChatGPT之后，大家好像对智能问答的机器人，或者说这种连续对话的机器人的要求，或者说用户体验都直接拔高到了GPT4这个层次。
	但是咱们往回看，或者说咱们就算在今天看，也很难有一家公司能够做到ChatGPT以GPT4作为这个大语言模型引擎的这样的一个体验。所以大家直接去使用transformer的这些QA应用的时候，可能会感觉很错。就为什么好像他没有那么聪明？其实是因为确实是因为ChatGPT或者说OpenAI这家公司的这个技术还是很领先的。并且他们用了更大的算力资源去实现这样的一些任务。并且我们还知道ChatGPT今年的一些大的进展，包括多模态的一些进展，这些其实都是需要背后有很多的算力做支撑，还需要做大量的AI工程化。整个AI大语言模型，包括多模态的发展，其实都是在向大模型加pipeline这样的一个组合去演进，因为单一模型现在也很难做完所有模态的事情，我们也不可能马上训练出一个多模态的大模型，万亿级的，然后它能解决所有的问题，它也不经济。所以更多的情况可能是有很包括像GPT4有这个MOE的这样的一个架构，有好多个大禹模型。并且会去对接一些OCR的模型，对接一些图像处理的模型，然后通过pipeline再把大家串起来，只不过这个pipeline可能更多的是一个抽象的语义，不是我们这里指的这个API。
	假设我们使用这个transformers的pipeline去实现一个智能问答，那这个也是它预定义的一个任务，这里其实是使用了一个轻量级的bert，然后在Scott的我们知道这个数据集在我们前面的理论篇看过很多次了，是stanford一个测试集，一个benching mark在这个上面去微调的一个轻量级的bert。然后我们用它去实现一个QA的应用，也很简单，给它一个question，然后给它一个context，一个上下文。那这一类智能问答，其实它的核心就是从context里面去抽取出一些有效的信息，然后来回答我们的这个question，回答我们的提问，这个也很像我们的这个RAG的一些应用，RAG现在基于GPT4，或者说基于一些比较模型参数比较大的一些大模型去做的。RAG通常就是把我们的向量数据库或者说我们的知识库里面检索出来的内容作为我们的context，然后再把用户的问题和这个context去做整合，然后作为回答。其实RAG的原型很早以前在大模型的QA这个领域里面就已经有一个雏形了。这里举了两个例子，一个是我们问这个rap，就相当于我们问这个github的rapper叫什么名字，然后他的上下文给了一个相关的信息，这个名字叫做higg face的transformers，这个肯定很好回答了，因为其实这个context几乎就已经回答了这个有问题。下面我从维基百科找了一段摘要放在这儿。
	就是我们这个中华人民共和国成立的时间是1949年的10月1号。然后在哪里，成立了什么样的一个叫什么样的国家？然后问的问题是中国的这个首都在哪，它提取出来了。这个其实还是能说明一定的问题的，并且它的score还比较高。
	然后类似的除了我们刚刚提到的文本类的问题或者说任务以外，语音识别现在也能够支持了。但是它有一定的前置依赖，我们在LLM quick star这个课程的开源项目里面也有提到需要装一个很有名的音频处理包，叫FM pack。应该如果做过音频处理任务的都知道，这个包几乎是一个事实标准了，那么需要去预装一下再用。如果是linux的用户，可能用APT去装一下这个FM pad，然后再用python的pad去装一下它的python层面的API，然后我们要去实现一个自动语音识别，有一个目前来看被广泛使用的，大家都认为挺好用的一个OpenAI的模型叫vester。然后这个vester也有开源版本的实现，这个vest small，一个更小版本的vester。然后我们通过这样的一个模型，大家可以看到我们刚刚其实都指定的任务。当然我们也可以显示的去指定出这个任务要用什么样的模型去完成。这儿其实聪明的同学应该就已经发现了，我如果在这儿指定了模型，那可能它的自由度就没有开始那么高了，因为就不再是由transformers这个库去选什么样的模型来支持这个任务，他选的肯定都是API对接完整的。
	如果是由我们来指定这个模型的话，那我们就得考虑这个模型到底支持哪些任务。这些通过hugin face的hub上面模型的这个主页都能看得到，包括我们这里写的这个模型的参数OpenAI斜线westers small，这个其实就对应着hg and face hub上面这个模型的主页。那我们把它的这个测试文件也下载到了我们的课程项目里。所以如果有同学有这个网络访问的问题，关于having face也可以直接通过我们的开源项目就能获取到这些测试数据。
	这里其实是一个很有名的演讲里的音频，让它识别出来i have a dream one day，这个我就不再念了。类似的图像分类，我们知道图像分类在上一波深度学习阶段其实是非常火的。大家都在使用tensor floor，包括这个tenn sor flow service pyto HOYX来做实现。现在也可以通过这个pipeline一行代码去实现这个image classification它使用的是一个已经训练好的一个的一个模型，这里去做了一个识别，但可能它的标签还没有做那么好啊，这也是我们之前有一节课讲这个CNN的find phone需要在最后去把这个label ID也好，或者说全年阶层把一些特定标签给训练的更好，有一定关系。这里其实是一个叫做好像叫什么舍舍利猫还是什么山猫，一个长得有点像像猫，但是又比较胖的一种动物。
	我们刚刚讲的这个pipeline它的一个顶层的抽象，它是一个前处理，然后模型后处理。那前处理具体怎么做的，这里其实有一些基础的同学都会知道。我们在包括在前面的读论文的这些内容当中也都知道，原始的文本，就是人能够看得到的这些文本，现在几乎都不能直接作为我们模型的输入了，一定会变成这个evidence，embedding的这个向量给到我们的模型。
	为什么这么做呢？从技术实现或者说从效果的角度来说，通过embedding这一层，我们可以统一很多的知识表达。举个简单例子，我们都知道world十年前的九年前的这么一个论文和他的这个工具有一个非常好的效果。就是把我们人看得到的这种各种各样的自然语言提取出语义，然后降维到一个相对来说低维度的一个空间。比如说word work，它的常用的经验值512位。比如说这个GPT4，它的evading ADA的这个02版本1536位，那它其实知识体量是非常大的。我们知道encoding这个技术就是我们的编码技术或者说嵌入技术有一个迭代过程，从早期的这个one hot这种热编码，然后逐步通过word vest这样的一些embedding技术、嵌入技术，开始能够以极低的维度或者说相对低的维维度提取出足够有用的语义。并且通过这些提取出的evading向量之间的一些操作，能够去证明这些语义被有效提取到了。
	最有名的就是king queen和man woman这个例子，它是跨语言的，能提取到这个知识。也是因为通过emb ign这一层，我们才能打破很多语言的这个障碍。因为我们如果单看输入的这个row text原始文本，我们其实很难能够理解。比如说这边是一个man，这是一个男人，或者说经典的我们读的注意力机制这篇论文，这个是一个area，这个是一个重，就英语跟法语，明明人看到的这个单词都不一样，或者说甚至他都不是这个锡文，不是这个英语字母，而是英语加上中文。但他们表达的语义其实是一样的。
	所以整个evading这一层最大的一个意义就是在怎么样能够消解掉这么多年以来，人类的语言学发展出来的各种各样的符号。这些符号都是为了代表这些我们看到的这些语义我要是上过这个应用开发课的同学也知道，当时为了讲这个embedding，我们还提出了很多哲学上的一些形而上的一些考虑这些其实都是举一反三，一通百通的。因为整体来说，我们的语言的发展就是为了表达出我们这些不同的人看到这个世界，看到的内容，然后找一个词，一个符号去代表它。那embedding其实就是想做这个逆过程，就是把这些不同符号、不同文字代表的语义提出来，最后在一个统一的向量空间里面去表达。
	就比如说我刚刚提到的word，或者说OpenAI的这个ADA in bedding这个模型都是想干这个事儿。把同样的语义变成了一个嵌入空间的向量之后，在这个空间里面我们其实每一个词最终都可以变成里面的一个。向量，但是变成这个向量，我们不可能让它无限多。因为1536位，如果我每一个词它都是一个向量的话，这个也很夸张。因为它其实理论上可以有无限多个token出现的。
	这个token就是指我变成这个向量之后给他标的这个标签，这个ID那么那怎么办？比较好的方式是说我还是会去做一个词表，然后这个词表里面就是指假设我们现在要把这个中文互联网或者说中文的所有书籍全部都变到一个向量空间里，然后我需要做什么步骤呢？我第一步其实是需要做分词，我把这个词分出来，就像这里的英语，我也需要做分词。不过英语可能一开始他有这个空格就分好了。那分完词之后有很多词的意思其实是一样的。我可能会把它变成同样的这个token，这个是有可能的那如果你做的粗糙一点，那你就不去做这个处理，你可能就变成两个不一样的token。但无论如何，最终可以编出这样的一个词表，就在我的这个向量空间里，相同的向量有一个统一的ID然后这个ID其实就表示下面的这样的一串向量。
	比如说这里就是一个七维的一个空间里的向量，10120232607、2003等等。不过这里的101和102可能它是一个特殊的字符，还有一些别的含义。但简单来说就是通过这个步骤，我们可以把原始的文本变成一个统一语义的向量空间。
	这个向量空间的维度可以由我们来定义，它可以是一个一开始人工设定的，也可以是一个超参数我们学出来的。然后在这个空间里面，每一个向量都有它特定的语义了。而这样的一个向量才是我们的模型可以接受的输入。
	所以到这儿我们就发现有一个新的东西，他把我们的前处理部分给替换掉了，他叫token nier。有的把它翻译成分词器，就可能不够准确，因为它更多它的价值是把输入变成一个我们模型可以接受的token，它不一定是个词，而且分词也只做了token ized的第一步的工作，我们刚刚有讲第一步是分词，第二步是映射。就是把分出来的这个词映射到embedding ing的空间里，变成一个我们的embedding的向量。然后这个向量在模型里面通过我们之前学过的不同的模型，有不同的网络结构。然后这个网络结构会去做一个前向的处理，前向的传播。
	这个传播就会变成从X经过一堆的Y等于WX加上我们之前讲过各种各样的模型，变成最终的那个Y，然后那个Y就是它的这个输出概率。这个概率通过后处理我们可以变成，比如说有的最后这个概率是做分类的，它可能就会有一个label标签。比如说我们刚刚看到的这个情感分析，它就会有一个正向或者反向的标签。比如说他是一个图像处理的，它也会有标签。比如它是一只猫，它是一只狗，它也是对应的label。
	当然我们也可以去做生成，这个生成就像基于这个transformer的GPT，它可能是一个我们叫自回归的一个模型。他可能这个模型就是为了基于输入的X去预测下一个它紧紧挨着或者它下文的这些词应该是什么词。这些词有一个概率，我们取那个概率最大的，或者说像RHF它是概率差不多，我会再跟人类价值观对齐，anyway选出来这个概率最终是能回成一个词的，变成一个词。那这个词怎么样变回那个词？就是我们刚刚看到的在向量空间里，我有我的哪个向量跟我现在输出这个概率的向量最近，我可能就会去选它了。这里就会有各种各样的一些技术手段，包括基于相似度的，基于各种各样的手段，我们待会儿也会去看到像bert里面使用的这个top k，也是一种方式。但无论如何，不同的模型有不同的后处理，但不同的模型它们的前处理是有一个大体的框架可以被抽象出来的那这个框架就是token ized，先分词再映射，变成一个invade e向量。后处理根据你的下游任务不同，会有一些不同的后处理手段，这我们就不展开到合适的这个阶段，我们会逐步的去给大家讲。
	然后我们看到整个transformers有一个pipeline，它很很牛逼。因为它既高层抽象，同时又非常好用，两行代码就能去完成一个任务。我们刚刚看到的这个底层抽象展开之后，有一个totalizer，有一个model，它又怎么样去使用呢？这一点transformers做的很好啊，他就让大家记住，如果你只是一个只是使用不需要去进行各种预训练的同学的话，他就通过两个方法就能够使用了。
	一个叫做from free train，就是来从预训练来的。我们都知道像我们用github的同学会给了一份代码库，然后就可以跑起来，对吧？Deploy或者run起来。
	如果你是一个AI的开发者，你需要从hugger face上面去拉取一个模型。那么你也不用去使用一个gate这样的指令，用transformers这个库某种层面上就代替了gate的使用，那怎么样去clone使用这个from free train，你的token ither可以通过from pre train能够拉取到本地来，你的model也可以通过from pre train拉取过来。甚至他们的这个from pre train都可以通过这个auto tokenizer和auto model这样一个统一的抽象类去完成。这里面填的就跟我们刚刚看到的这个task里面填的是类似的，这里就填特定的模型，这个模型就是已经在hugin face hub上发布的模型，这个流程就很像github的rapper了。
	你在github上面开源了一个项目，你可以get home这个项目。你在hugin face hub上面发布了一个模型，那你就可以通过from retrain拉取这个模型，那就会拉取它的权重配置文件。如果是token niter，还会去拉取它的词表、vocabulary, 以及它的一些各种各样的Jason就是我们刚刚提到的映射关系，怎么样把这个token ID映射到对应的这个商品来，又怎么抵扣的回去等等，都可以通过它来进行拉取。
	类似的假设我们完成了预训练，我们做了修改，我们针对自己的小的数据集，发现原来的这个词表没有这样的词，我就是需要添加token，那你可以去改这些token ized和这个model。因为你改model其实就改的是模型权重。那改完之后也可以通过一个统一的方法叫做save pre train，就是保存下来。然后这个方法，当然你不能使用这个auto totalizer和auto model了。因为你要存的就是你改后的这个特定的实例，所以你改了那个实例，你就通过这个实例的save pre train和这个save pre train的，就对应的token iza和model都使用这个save pre train也可以存到这个本地，或者说上传到这个hugin face上。不过如果你要上传，就跟github样，你要配一些东西，github你需要去配这个key，SSH的key等等。那hugin face也有对应的这个key，你可以去配置。配置好之后，它就会自动帮你上传或者存到本地。
	对于token either来说，如果你还在本地有一些新增token的操作的话，当你去save的时候，通常会生成五个文件，这五个文件都是一些很重要的文件。如果丢了的话，可能就不一定能够restore，不一定能够加载回来了。就比如说我们这里看到的这个tocom ized的到Jason这么一个文件，是它的一个原数据文件。然后我们的因为这些文件都比较大，尤其是这个models，所以我们的a LLM这个quick star这个项目里，github的项目里，我没有把这些东西传上去。但是大家可以通过运行这个notebook，就我们在这个github上面开源的这个notebook，运行下来是可以保存的，你就可以在你的本地有这个文件了。这样github的代码库也不会变得很大，大家还可以自己去这个实验，这里还有一些其他的，我这就不一一念了。包括它的这个分了普通字符和特殊字符，这个我们待会儿会去讲，包括它的词表以及我单独存放的这些新增的tokens，单独存放这样也方便。未来如果发现我加了token对于我的这个embedding的空间有影响，我给把它挪掉。
	包括这个model，也有这个模型配置文件和它的权重的文件。这个patch是使用这个，那么tensor or flow是使用这个check points。大家如果写过这两个框架的代码，应该就比较熟悉了，或者就不再赘述了。
	那么再简单讲一点点最后关于这个model和token ize还有pipeline的这么一个概念。就我们刚刚提到的这个后处理，其实有一个很重要的部分就是关于模型这个概念。大家可以去仔细去想一想，就是我们在讨论这个模型的时候到底在聊些什么，我们看一下，不管是早期的attention这个注意力机制这篇论文讲到的sequence to sequence，或者说叫这个encoder decoder这种结构。还是说后面的transformer这篇论文讲的这个通讯也是oilily的，他自己提出来的这个transformer的这个结构，还是后面我们又讲了GPT1 bert等等的大语言模型。其实这些说回来大部分都还是基于transformer的。
	那么为什么叫transformers？因为大部分都是基于transformer的，所以我们可以把transformer这部分甚至单独提出来一个transformer的一个network这部分是可以复用的。某种层面上来说，这也是大家看到很多having face上面的这个hub上面的这些主页模型主页有好多不同的模型。其实它就是在改后面的这部分，它的我们以前那时候叫backboard，就是它的主骨干网络，主干网络没有变。现在可以说它很多的模型，它的transformer network是没有变的。但它的隐藏层，包括它最终的这个head这个头就有点很像我们以前搞CN的时候，最后那几层用来决定你最终的这个任务是什么类型。所以一个完整的模型和一个transformer的网络其实也是有不同的。
	而大部分的时候，现在transformer的network是很难会去改造的，因为要去动它就更难，但是去去动这个完整的full model，更多我们大家都在聊的是去动这个后面的部。而我们刚刚看到的这个model input和这个embedding，其实是我们token niter会去负责的一部分。而pipeline，其实它是把整个这条线都给封装起来了。所以大家可以想象得到，pipeline未来一定是就像一个我刚才提到的军火库一样，不断的有人去实现新的模型，然后实现新的任务，然后去对接pipeline的API，就能够开箱即用的去使用它。对那这个是transformer的一个我们的一个初步的介绍，当然可能还是要通过上手不断的去使用，大家有一个更深的体会，看到这儿为止，大家有没有什么问题？没有的话，我们就来讲一讲这个环境的搭建。
	有个同学问怎么识别用户输入的自然语言对应到不同的后端pipeline？这个问的挺好的。首先他没有这个能力，他不会识别你输入的自然语言。它完全取决于对于你这个输入的那个任务是什么。它会有一个你可以认为有一个默认的模型，就是一个特定的任务会有一个特定的默认的模型参数，那个模型参数能支持什么就支持什么。就比如说我们刚才说情感分类，它其实不支持中文，但我输入的是中文，那它表现的就不好。
	我们待会儿在实际的代码示例里也能看到，词表的文件是指词嵌入的文件吗？词表的文件不是，我们待会儿可以打开看一看，大家就知道了。词表的文件就是那个词本身，那个自然语言。然后他跟偷窥ID有一个对应关系，方便我们最终能够解码回去。
	大家还有问题吗？没有，我们就先继续往后讲，今天实战的部分还是有一些内容的。
	有个同学问pipeline具体能做的事情在哪里能看到吗？以及task这个写法有要求吗？总不能写中文。这个同学问的也挺好的，所以你看你要看我这一页写的很严谨的。首先这个叫API，所以你就得按照这个写，它不是任意的，这个是API，这个是transformers的pipeline，这个模块的API然后这个API它就规定了有这些参数是支持的，也就对应的这些任务，所以它不是随便写的。像这里我们不同的任务，它有不同的描述，但它还有对应的API里面的一个指定的key，如果你乱写，它是不认识的，它不支持这个task。
	有个同学问这个查询此表文中的此表文件中的索引，得到向量再去实现值。不是的，就是词表是词表，词表就是一个记录，就是记账的，记一个账本一样的记在那儿的。它跟我们的inviting技术没啥直接关系，对它就只是一个用来记录的。
	关于invading是怎么回事？确实我们这个课你没有讲太多，因为这个课覆盖的内容太多了，每个人的基础不太一样。但in bedding大家可以稍微去搜一搜，大概了解一下，你不一定是怎么回事。
	有个同学问什么梦见我了？这个解答不了，不是周公，没有这个水平。我们先接着讲，待会儿大家再一起提问，尤其是看到实际的代码，大家可能会更有感触一点。
	首先这个项目，大家上过微调的这个训练营的同学可能第一次直播，这个训练营不一定每个同学都用过github。之前我们应用开发那个训练营还会一直教大家怎么用github。但是我觉得大家基础的去了解一下，github是一个很有用的平台，上面有各种各样的开源项目，就包括我们这个训练营的开源项目，也欢迎大家赶紧去star和fork，你才能够去接着在上面改。然后这个githa的项目上面，我们会每周都会有更新，包括我们新的这个代码和我们的一些文档，甚至我后面会把我们的这个课表，像这个应用开发一样做一个schedule，里面就会有课表。未来如果我们有一些优秀的同学的作业，我们也会放上来LLM quick star，在我的这个个人开源项目上下面，欢迎大家去关注一下。
	好，然后具体来看，这个项目里面我们目前这个阶段需要用到的这个开发环境里面的一些必要的包。一个是我比较推荐的这个mini contra，是这个安娜康da的轻量化版本。相对来说轻量化的版本，然后它本身也是一个python的一个虚拟环境管理工具，也支持各种各样的这个操作系统。
	对，大家通过我们的这个开源项目这边打开一下，大家应该能看到这个界面变成了浏览器。我看一下，我这边还没有切过来。对。网络流行语。刷新一下。对，已经调过来了。好，然后这个其实就是对开源项目，然后这开源项目的地址也比较简单，就我的个人下面的LLM quick star就可以访问了。
	然后在这里面，目前我们传了一个简单的开发环境和by lab的一个后台启动的配置。有发现有的同学还不知道怎么启动它，能够以这个常住的方式去启动。我们今天的notebook也在这个里面，就我们这已经提前打开了hugin face的这个transformers下面有两个去拍的note c，我们会在这个notebook里面也加了很多的注解，让大家能够直接在notebook里面完成学习。
	一个是这个pipeline，一个是这个pipeline的the month，我们待会儿再来细讲，我们先来看看环境，刚刚看到的这个mini contra，大家通过这个github上面的项目就能够做跳转。我这边都有加一些超链接，我这边逐步打开给大家稍微讲一下当然关于GPU驱动这个我就不讲了，这个应该很基础了，就驱动怎么装。对，然后这个mini的可以通过你自己是什么操作系统，你就下载对应的这个操作系统。但我不太推荐用mac，因为mac的这个GPU这一块比较麻烦，我们后面去跑一些大的模型不是很顺畅，还是比较推荐这个linux的系统。或者说如果你是windows的这个系统，然后装了一些GPU的消费级能卡，比如说什么4090之类的也行。
	然后通过这个安娜康达，其实是啊这个mini康达是能够比较方便的去隔离不同的python的开发环境，那比如说linux，像这个我自己因为使用的是一个海外的服务器，所以网络就还好，不是什么太困难的问题。其实就这几行代码就能够完成它的安装了，去建一个当前用户目录下的mini康的三目录，去获取到它的安装包，然后重命名到这个目录下。这个是一个SH，一个需要脚本，这个需要脚本里面才会实际的去执行这个安装，装到three这个目录下，然后这儿可以去重写一下你自己使用的这个，需要，像我会使用这个ZSH，那就可以去initialize一下，它就会在ZSH的这个配置文件里面去刷新。那这个其实就把python的这个环境管理可以管理起来了。第二个就是我们要讲的jupiter lab，jupiter lab也是一个很重要的，并且现在挺主流的一个交互式开发的环境最早从它的apple notebook到P到就拍lab，逐步的迭代过来。就拍lab现在也跟这个google collab，其实也有去跟这个主派的lab做一些借鉴应该用过的同学都知道它的这个好用。
	那么这个链接跳转过来，因为你刚刚已经安装了mini康大了，所以你直接使用这个康大就可以去安装to拍lab，count install，类似于pipe install，counter install之后，就可以安装这个to pt lab，就装好了，就这么一行代码就装好了。装好之后，接着我们需要去安装这个hanging face，hanging face现在也推荐使用这个康da来直接进行安装。我们看到这边跳转过来的链接是install with counter，这边也有对应的目录。如果你不习惯，你喜欢用pip来安装，也可以使用pip安装。不过pipe它也有让你去用这个虚拟环境我建议大家就直接使用康带那用康带安装，安装好之后，这里有一个细节需要注意。就是康达安装好之后，我们又装了transformers这个库，但是transformers这个库它并没有强行绑定。你要用pyto CH还是tensor floor，所以它不会帮你去装这个底层的神经网络的框架。所以你还是需要自己手动的去装一下这个PyTorch，就我们这儿看到的。
	他应该有写。那应该这个链接刷新了，简单来说就是你在pipe in store touch就好了，我待会儿也可以把它加到这个github的read me里面，再使用一行pip install porch，把PY去掉。因为这个pipe就已经是python的这个包管理工具了，就把它底层的这个装起来了。如果你使用你要使用这个tenn sor floor也是一样的，就install一下这个tenn sor flow GPU。
	然后最后就是这个FM pack，如果咱们需要去使用这个音频处理的一些任务，还需要去安装一下这个FM pad。然后这个package如果你是在linux系统里面，你用APT这个操作系统级别的包管理工具去更新一下它，然后再upgrade，把它更新的这些source，它的资源包再更新一下，这个upgrade是升级一下，然后使用银行APT in store，FM pack就可以装起来了。那这些东西做完之后，其实你的环境就已经具备了我们跑整个这节课例子的这些能力了。我们回到回到这里。好，然后这个是在咱们有资源的前提下，我们都把各种包装好了。但如果实在比较麻烦，现在还没有GPU怎么办呢？这里也提供了一些google的clap的地址，我们可以再去访问一下这个地址。
	大家在课件你能看得到，这个hugging face的box transformers notebooks下面，有这样的一个算是文档的一个跳转。我提前已经打开在这个地方，这个方式来看就是我们在课件里看到的这个地址，notebooks，这里它是官方提供的一些notebooks，然后这个notebook里面我们都能看到这里有一个collab这个studio lab印象当中好像是微软做的，没有用过。那我们能看到这里有一些各种各样的例子，然后比如说我们打开一个quick two of the library。当然这个需要科学上网，就是这个collab的访问，但它能提供给你资源，连接之后大概率是需要你有一个gmail或者能够访问collab的一个用户地址，然后你这可以去connect去连接连接，你可以连接你，甚至连接你本地的这个资源，也可以用他的免费提供的资源，直接点一下就好了。我印象中这个之前的stable division也有提供类似的flab，然后就可以在这儿去运行了。
	各种各样的这个transformers的库，它能够安装，比如说transformers这个库本身，其他的data就它要用它数据集相关的一些使用，这里还会有一些大家就可以把我们的课程项目里面的这个notebook拷过来，它是能直接用的。因为它是兼容的，这个是ipad no book兼容的。所以如果咱们没有资源，这个是一个临时方案，可以在这儿去使用。像这里有写，其实就这一行，我刚刚说的，需要去安装一下touch。好，那这儿我就把这个环境这个部分，先跟大家快速过一下，看大家还有什么问题。没有的话，我们就可以正式开始了。大家关于这个开发环境的搭建还有什么问题吗？没有的话，我们就正式开始学习，怎么样去实现我们的transformers库。
	如果是租服务器是在服务器上安装吗？同问，是的，还有个同学问是从哪里进入的collab？这里我们在课件里有这样的一个UIL，这个是哈根face自己提供的，然后这里下面有很多的，其实就是把他自己的这个官方手册用collab实现了一下，可以跳进去，你也可以不跑他给你的这些例子，直接把这个collab里面。因为collab是对接的google drive，大家如果用过谷歌的网盘的话，你可以把它拷到这里面来。就这里，但我就不打开了，因为google drive里面有我的一些私人文件。课件你没有看到UIL在这个位置。这一页大家可以看一下，我们的这一页左下角是有的，就是在课件里面我现在播放的这一页。
	然后租服务器的话，我是我也是租的服务器。然后我在这个服务器上有这么一个，就这里这个是租的一台服务器，然后在服务器上的安装，大家可以看到。对，有，还有同学问海外服务器有推荐的吗？这个事情是这样的，我这边也在跟，因为我之前跟咱们的结课时间的同学也都在讨论，怎么样让大家能够租到一些好用又便宜的服务器。我这边还在跟华为沟通，有可能的话会尽快。也许最快的话就是一月份就能够让大家直接使用有折扣的华为云，然后在大家个人的账户上一个折扣码的方式让大家去使用，并且不局限于我们课程。反正那个折扣码你就能够一直用，具体打几折我还在跟华为沟通。
	然后这样的话大家可以用国内的这个华为云，然后又可以用它的海外的机房。就跟我现在一样，我用的就是一个每周的机房。那这个机房就可以直连海外的网络，就不太会有太多网络的问题。这个具体时间，包括这个折扣，我这边会确定之后再跟大家去做同步。创建虚拟环境的python版本推荐是多少？推荐是3.10或者3.11都可以。
	刚刚漏讲了一个部分，就是我们的jupiter lab。这个jupiter lab大家安装好之后，再通过这一行代码让它生成一个jupiter lab的配置文件，然后在配置文件里面修改两个比较重要的配置项。如果你本身已经比较熟练了，你就可以不以root用户来操作这些安装的部分。如果你本身是租的服务器，你也无所谓，然后你的防火墙设置也到位，那你也可以用root用户，这个看个人习惯。
	然后如果是你用的root用户的话，就panel AB处于安全角度，有一个参数叫low root，你需要把它注释给这个给他这个注释干掉，然后写一个true。然后还有一个很重要的参数，很多同学都配完了，就是访问不了远端的lab。因为它的这个service APP到IP这个地方，需要给它做一个修改，让外部的网络也能访问到这个dupatta b然后实际启动的时候，我们用no hap这个简单的后台启动的，需要命令就够了。然后有几个参数去指定你在什么端口启动，然后使用什么样的密码，把这个地方替换为你自己的一个密码。以及你的就拍lab启动之后，它的根目录在哪里，这个也可以去做调整，这个是重定向输出，把你的这个本来就拍的lab的日志重定向输出到这个no hub点out这个文件里。大家也不用担心自己的no hub点out被大家看到了。
	因为我们的get ignore已经忽略了这个文件，所以你本地的这个文件是不会上传的。如果你的这个有什么输出比较敏感，我们也看不到。关于linux的命令操作和服务器配置，我这边的建议是直接问ChatGPT。因为ChatGPT关于这种就为什么运维人员这一轮大模型很焦虑，就是全是一些碎片化的小trick，这种东西是ChatGPT最擅长的。所以如果你有哪个命令不熟练或者怎么的丢给ChatGPT，它一定会给一个非常好。我的直观体验是比stack overflow的回答要高的质量高得多的这个回复是很好用的。
	个人的台式机得本地搭建一台一套linux虚拟机搭建环境吗？你的台式机本来是什么操作系统？如果你本来是linux就不用虚拟机了。如果你是windows的话，因为我其实已经有快十年没有用过windows的系统了，只用过mac的系统和linux系统。我确实不太清楚windows现在对于大模型的支持到底有多好啊。
	我是知道好像从win 10还是win 8以来，它一直在兼容这个linux的share，但我不知道兼容的效果有多好，它的那个power share还是什么的。好，我们就直接开始跑这个实战的部分了，大家有问题再问。首先如果我们把去拍的lab后台启动起来之后，会有一个这样的界面。我的建议是大家去get clone。比如说我在这个我看一下，比如说这台服务器是一台就是我这个jup at lab的服务器把。然后这台服务器，大家可以看得到，我们使用这个mini康达之后，安装好之后，然后还有一个命这个命令行大家有各自的喜欢我就不做强行推荐了。那像我的话使用了这个ZSH，然后又用这个visual studio，这个VS code，然后你可以通过它的remote explore去连接很多远端的服务器，那连接了服务器之后就可以像本地一样去使用。
	这里我们开始有提到这个no hub启动，这个是mini contra装完之后，然后你通过mini康达的这个初始化in IT，contain in IT ZSH，它就会加这么一段。然后我又比较怕这个环境里面可能会有python 2，那我们会做一个别名。其实就这些，基本上弄完之后，你就可以开始去操作了，去去玩你的这个开发环境了。
	然后同时我们还有一个很重要的指令，叫做no hub去启动这个two Peter lab。那我的建议是大家get clone，就get clone了这个项目之后，就在这个项目的目录下面去启动这个Peter lab。那么你的jupiter lab里面，它的根目录就是我们的这个项目目录。然后这个no b点out是我本地文件，大家可能下来是没有的。这个我们在这儿也看得到，本地是没有这个文件的。然后我们在ignore里面有过滤掉这个no hub点out这个文件，这customization的部分，然后像我们今天的这个hygd face transformer的目录下的这个models，也是被我们过滤掉的。所以大家不用担心自己的模型，自己的token either和自己的no hub点out日志会被传到项目里面，这个没有关系。
	好，假设你已经通过主拍的lab启动起来了，那么你就会有一个这样的界面。在hugging face的transformer里面有这样的两个notebook，一个是pipeline，一个是pipeline advanced。不知道这个字体大家看得到吗？需要再放大吗？不需要，现在一来不需要用V100和A100这么贵的。如果你的这个你要自己花钱的话，我觉得甚至你用一块T4的卡都可以先跑起来，后面用V一版，因为T4现在很便宜，提示出来很多年了。对等我们后面真的要find team的时候，大家可以用V100，那A100就有点贵了。对。好，有的同学说再大一点我就再放大一个。
	这个notebook，熟悉应用开发课的同学应该都都知道这个风格。这个notebook我会尽量做的，通过它就能直接学会要教的内容，也不用在课件里面反复去跳转了，这个是我们刚已经讲过的pipeline API支持的，当然我相信未来它还会有更多。所以我附了一个hygd face的官方文档，在这儿，大家应该能看得到。通过这个可以看到他最新支持的一个情况。这里是他这边写的一些，就右边有一个目录，音频的、计算机视觉的，自然语言处理的，尤其是多模态的，它还在不断的迭代，这儿我就不再花更多的时间了，我就往下看具体怎么用。
	首先因为可能大家一开始不熟练，我在尽量每一个应用下面都附上了它的模型主页。模型主页就这个这里大家能看得到，就它使用的是哪个模型，让大家能够习惯去hugin face上，这个hugin face的hub上去看看，尤其是习惯这个页面，它其实就是一个github，不过是功能更强大的github在这个页面上会有很多模型的信息。大家如果不习惯看英文，可以装一个什么google的这个翻译的插件就完了，就有中文了。然后我们回到这儿来，就比如说我们开始看到的示例，这个pipelines。第一个这个notebook主要是为了面向这些没有太多AI经验和基础的同学，让大家能够激发兴趣，就两行代码一个应用，两行代码一个应用确实挺方便的，但里面也会有一些pipeline API的一些参数的使用。
	一些小的技巧第一个我们在课件里面看到的示例，就是我们去做一个情感分析，比如说这儿我们重新加载了整个note ook，那这儿还会有一些warning，一些提示，我们看到这儿，他给的提示其实挺有用的，大家可以去看一看。第一个就是这里写到的no model supply，就是我们没有指定模型这个地方，它默认其实含义是任务等于这个sentiment analyses，那你如果不指定，你就这么写也OK。然后实例化了一个pipeline之后，这个pipeline里面的参数就是这个模型可以接受的这个入参，是整个前处理部分的入参。比如说我这写了一个今儿上海可真能，然后它的输出是一个negative和一个score。这个negative就是指消极的，就是我们这儿看到的情感分类这一类文本分类任务里面的这个标记。
	当然它的这个transformers的warning里面还有一条我觉得也是很重要的，需要跟大家提醒。虽然你用两三行代码就能去启动一个这样的应用，一个AI的服务。但是在生产环境里面还是需要以指定模型的名称和版本是比较好的，直接调用很容易不稳定。因为transformers的版本一迭代，或者说它背后的那个hub一点，有可能你的这个效果就不稳定了。
	因为模型参数变了，那么通过这个pipe我们实例化之后，他也可以去不断的调用，测试更多的实例。比如说我觉得这家店的蒜泥白肉的味道很一般，这个是一个经典川菜，negative也是消极的。这两个看着好像他他都很成功的把这个情感识别出来了。但是我们再看下一个示例，这也是很多同学感觉这个大模型好像输出不太稳定什么的。其实确实是因为咱们要对背后使用的这些模型有一定了解才行。
	我们刚刚看这两个示例，好像这个都识别挺准的，score也挺高的，感觉是模型不错。但是我们再看，这里这个例子明显它就有问题了。我写的是啊，你学东西真的好快，理论课一讲就明白了，希望每个同学都是这样的。这个东西它识别出来是negative，是消极的，这个很不合理对吧？我们这个明显说的是很积极表扬的话了，怎么会是negative？
	然后我们再去刚刚那个主页里面去看一看因为我们没有指定模型，所以情感分析它默认使用的是这个模型。这个distil bert，这个是分开看啊，distill bert其实是一个bird这个模型的轻量化版本，然后一个英语的版本，所以它本身它对中文就没有做太多训练。所以你可以理解成前面这两个他也没有猜对，或者说他也没有分类对，所以是恰巧撞上了，但是我们可以把它换成英文，你可以使用各种各样的方式把这段话换成英文，对吧？Your nerve thinks really quickly, understand theory class as soon as IT talk. 
	他这回猜对了，positive，然后score 0.99，自信度非常高。其实这个自信度才是靠谱的。大家看到这个地方的0.85感觉很高，对吧？但其实在生产环境里面，通常0.85的这个thresh hold就已经把它干掉了。我们通常使用的执行度通常都是高于0.9甚至更高的执行度。所以其实从这句话如果大家有经验的话就能看出来这个很很不靠谱。即使如果这个地方它是positive也有点不靠谱。正儿八经他把语义整明白了，然后去做情感分类的话，0.99，包括我们刚刚看到有一个叫做今儿上海可真了，我们改成today ang hai really cold，0.99。
	这个是一个小细节，也是大家如果真的没有AI的这个背景，你又想用，还是要去看一看模型主页的介绍。就这些模型它到底应用场景是什么样的。一个最简单的就是它到底适用于什么语言，这个是一眼就能看清楚的，怎么看呢？我们刚刚这有看到标签，各种各样的标签，这个标签就是指模型的这个标签，这里写的是english，它只支持英文，然后不支持中文。我们后面会写支持中文的模型是什么样的，hype eline的模型会下载到本地吗？是，因为他就跑在你本地的资源上，然后我们也可以批处理的去调用模型推理。就比如说我不能每次都是pipeline对吧？
	我丢到一个list里面，我的这个text list里面，我这三句话都丢进去。大家能看到这个蒜泥白肉，也判断的很很准确了，都0.99，score很高。这个它主要是英语的这个语料决定的。只要你输入的是英语语料，这个情感分析分类它还是能搞得定的。因为我们给的这三个事例，他的情感的急性还是比较明显的，他不是那种很模糊的感觉，大家也可以自己去改一改这个事例，去试一试。当然我们可以通过pipeline API调用更多预定的人物了。除了刚刚看到的情感分类以外，就比如说token的这个classification，就我们讲的这个标记，这个其实是很早以前搞NLP的人都会去遇到的很重要的NLP的任务。现在都现在都已经几乎被这个端到端给包在里面了，都不太会直接去用了。
	像早一点，我记得我14年那会儿做这个条件随机场，那会儿的基于这个马可夫链等等去做自然语言的一些模型的时候，都需要对各种各样中文分出来的词去做NER和POS，就是我们的命名实体的识别和词性标注。NER就是说能不能把我这个一段话里面真正有价值的这个内容给它找出来，打上标记。这个E就是这个entity词性标注就是给这些词再打上一个特定的标记，有的可能是标记它是这个动词名词，有的可能是标记它是什么作用，这我们可以再看一看，同样的我们可以不指定这个，但大家会发现warning里面都会写这个NR任务用的是哪个模型，这里有一个默认使用了哪个模型，bert，大的bert，反冲的在这个上面的一个模型。这里模型主页我也写了，大家有兴趣都去访问一下这些主页了解。
	然后我们把这个classifier，这样的一个新的type跟上面的不一样，这个是新实例化的一个pipe，把它实际化之后同样的我们可以输入一段话，叫hugging face is the french company based in new ork city。Hugin face这个公司是一个法国公司，当时是在纽约的航空公司，那我们看到他打上了这个首先实体识别出来的一些磁性具体的词都已经做好了。这个start和end是指把这个词，因为他要做这个token classification，他把这个词首先找到他定位，02 hugin face，HU，这个跟还给分开了，这里就感觉很奇怪，hugging居然被分成了两个，然后怎么办呢？怎么会这样呢？就是因为它默认就是首先使用这个模型，它会尽可能分开，然后默认的时候他没有去做这个实体的合并，那你也可以去加这个参数，这是非常实用的一个参数，叫group的entities，就把这个实体合并，那我们再去执行。就能看到就合并的很简单了，一共合并出来的group就有organization，然后这个MIC我确定他一个什么样的标签、location，然后识别出来的word hugin face，合并了french、new york cities，然后还有一些词stop force就被跳过了，一些停止词就跳过了。在这儿本来也都没有好吧，然后他其实就是把这三个合到一起了，这都是一些基础的NLP的知识，如果大家不明白，也不用担心，因为现在也用不到了。
	基本上除非你要做很底层的数据处理，这个可以以很方便的方式去实现NR并且大家要记得这个地方的模型都是可以换的，并不是说我使用了这个bert就不能用别的模，我就不能做中文的NR了，其实都可以。假设我要做QA，也有使用这个question answering这样的一个场景里面，我们同样的可以去做QA，比如说开始我们举的这个例子，一个是问中国的这个首都，一个是问这个hugin face transformers，这个rapper的名字，都能够得到对应的答案。这些事例大家都可以去再研究，改一改对应的这个示例输入和模型，让大家熟练一下怎么样去用这个pya eline API和对应的hugging face的这个models，summarizing也是一样的，这里可能会参数稍微多一点，并且会涉及到一个比较有名的模型了，叫T5 base。它跟这个是我们第一个显示指令的模型，T5的这个base也是很多的，我们看到paper里的一个鸡肋模型，然后这里有一个小细节，也是一个tricks，就是我们到底那个model的参数要填什么，对吧？你可以理解成我们看到这里有一个URL，这个hugging face到CEO，这个后面就是我们要填的内容，就跟github它的这个仓库的名字是一样的。这是什么？就是它顶级是T5杠base，你就填一个T5杠base就好了。然后因为它是一个summarizing的任务，文本摘要的任务。
	那这个文本摘要的任务，它本身我们对于这个生成的摘要可以有更多的参数去限制它。一个就是指你最少生成的摘要要有多长，最长的这个摘要不要超过多少，这样的一些参数，当然还有更多的参数，大家可以通过这个文档去做一些查看，我们就不做这种应用层面上的API讲解了。比如说这里我们筛了一长串的内容去讲，讲了一堆，然后他来总结，他讲这个transformer是第一个sequence transaction model based entity attention，就讲transformer这个网络，第二个是大语言模型是什么？还有讲大语言模型是一个非常深的深度学习的预训练的模型。在大量的数据上，这个其实是summarizing一个任务。然后这个音频处理，大家如果想要去做音频处理也很简单。这里有这个音频处理的相关的一些demo，大家可以去试一试。
	刚刚有讲过，需要有一个可选的音频处理的数据包叫FM pack。需要通过这个方式在操作系统层面上去做安装，安装之后再去安装一个python的FM pack和FM pack。Python装好之后，你可以重启一下这个地方，就可以去把这个notebook重启之后，就能够运行安装之后的依赖了。
	这里我们选择了这个任务叫audio classification，然后使用的是这个super b harbert base super BER，这个super b是一个很有名的一个数据集，然后ER是它下面的一个情感识别的一个情感类别识别的这么一个数据集。然后我们这里其实是传了一个higg face hub上面已有的一个音频测试文件，这边给大家看一下。这边就直接荡下来了，因为它我们把这儿删减一下。其实就是这里的一个文件，我们刚刚看到的都是models，但其实hunting free上还有很多的data。就比如说我们这里的这个要去做音频处理的文件，就在hanging face的这个data size里面有好多好几万的这么一个数据集。那么刚刚荡下来的这个音频我们可以播放一下，大家能听见吗？应该。I have a dream that one day. This nation will rise up, live out the true meaning of the street. 
	非常有名的一段演讲，我们把这段音频，其实classifier，我们这里是想跟到大家说的就是说这个pipeline，你输入的这个可以是本地文件，就你要去让他做处理的。可以是本地文件，也可以是这个hugin face hub上面的一些文件，只要能够访问或者说一些其他的UIL，它是能够拉取下来的，那这里我们直接输入了这个线上的一个face hub data sets里面的这个音频文件。让他来进行处理，然后让他做的是一个音频的分类，就对我们的这个情感做一个识别，他是开心的还是不开心的，还是生气的。那我们进行运行之后大家可能去运行的时候会稍微比我要慢一点点。其实我之前已经运行过了，所以这些模型其实我都已经到本地了，如果你的网络不是很快的话，可能执行这些事例会稍微等一等这个模型下载。
	然后这里我们看到就是刚刚这段音频里面，happy stand各占了一半，然后中立的生气的是几乎没有的。这个是对刚刚的这个音频的一个情感的一个识别。然后如果咱们实在是下载不了这个high face dataset上面这些数据，我们在这里也把它当到了项目里。这个MLK点FLAC大家可以直接运行下面这个，但需要你去单独定义，所以把这一段摘出来了，这样可能好一点。这个上面你如果能够成功的把模型拉取到本地，然后运行定义好这个pyi eline，然后这里你拿不到data上面数据就用我们本地的就好了。这边出示一下。
	类似的刚刚那段话，我们也可以调用这个语音识别的这个模型来快速的实现，比如说我们用这个OpenAI的westers small来识别刚刚这一段，我们就直接使用本地的这个版本了，就这儿这个项目里的这个版本，我们可以看到它的语音识别其实做的已经非常好了，就你丢任意的这个音频，当然这个音频格式有一定要求，如果你的音频格式不对，这里不行。大家可以看到刚刚那个视频里面，那个音频里面，它的这个内容基本就被识别的很准确了，应该没有错误。
	I have a dream that one day nation will result。一大堆。然后这里还有一个小的trick小的技巧教给大家。就是说我们看到这里有一个hanging face的模型，这个模型里面还会有一个标签，叫做inference and the points。这个标签是什么意思呢？就是指这个模型它在hugging face上面是部署了一个实例的，你是可以在这个页面上去测试它的。就比如说这里我们看到有一个influence API，这个部分其实就是咱们的这个刚刚看到的OpenAI的vesper small，它可以在这儿运行，就跟我们刚刚用pipeline来运行是一样的。然后你可以在这儿上传一个文件，比如说我们刚刚下载下来的丢上去，那我们可以播放一下。
	I have a dream, but one day this nation will rise up, live out the true means of the street. 然后我们在这儿执行compute，那这个就跟我们在刚刚notebook上面去执行是类似的一个流程，他会loading，然后再把这一段音频实际的去跑一遍。然后我在想刚刚那个音频的情感类别，应该是观众的声音，体现了happy的这一部分。大家能看到这个模型在CPU上运行了2.3秒，i have a dream that one day this nation will result。识别的是很准的，然后你也可以让它输出这个节省的格式。类似的其他的一些模型，大家如果想要做测试，也可以通过这个方式来快速的实现。计算机视觉也有一些模型，现在是ging face支持的，包括这个图像分类和我们的这个呃目标检测，我这边就不花太多时间去讲CV的内容了。
	反正通过一样的方式，我们通过这个指定task，然后这个是一个测试的图像文件，也是在having face的这个data sets，我们可以就这只小小，我不知道叫舍利猫，好像是叫这个名字，我记得。然后同样的，如果我们无法使用这个远程的文件的话，也在本地叫狼猫。好，我当时查了一下，有这个本地的文件，里面有对应的咱们的这几个测试文件，包括我们的这里的这个cat chunk，还有我们的下面的熊猫，都有本地图片，可以去做一些测试。当然如果你要做测试，也可以上传到这个目录下，或者你的服务器可访问的地址里。这里它就识别出来是这个大熊猫，这个标签就识别的很准。
	然后类似的如果我们要去做目标检测的话，这里有一个前置依赖包，需要去安装一下。这个TIMM，也是我们刚刚在ecosystem里面看到的，我这边已经安装过了。可以去加载这个DTR这个模型，安装好之后，一样的我们可以去做这个目标检测了，识别出来。
	这里是一个开始有一个这个cat是它识别成了cat因为它的标签里没有这个玩意儿，也很正常。我们开始提过，这种视觉类的需要咱们再单独去训练label的。如果是这个猫和狗的话，因为这两个类别比较明显，就是大类它就能够很准确的标出这个位置，这个是它的bounding box，就是这两个框框到底在哪儿，我没有把它渲染出来。
	好，这个部分其实是很新手的，就教大家怎么样能够通过pipeline这个API很方便的去使用各种各样的AI模型，确实就两三行的代码就能玩起来然后留一个小的homework，就是希望大家能够替换这个上面的设计当中的一些模型，然后对比一下不同模型在相同任务上面的一些表现。然后我们可以在having face的models里面找到适合你的模型，怎么找呢？我们知道刚刚要填这个task，那么task其实就对应的这边的类别标签。比如说我们要去做这个QA，这里就有各种QA，就比如说这里的lama two，比如说这个Roberta base，这些其实都是可以你直接把这个复制下来，就变成了model你要填的内容。然后你的question answers就是你的task要填的内容，那就可以去试了，很方便。这个大家多去玩一玩，还是很有意义的。
	然后第二个就是比较核心的了，就是我们的这个pipeline到底包括跟我们后面的微调也有一定关系。就这个pipeline到底是内部怎么玩的，我们刚刚其实讲过它的整个内部的流程是一个从原始的文本肉text到输入的这个token ID，然后进入到了模型推理，然后输出了各种概率或者说最大释然估计，最终生成的这个结果给到我们。这个所谓叫py pels进阶主要就是这三部分的内容，给大家看一下。第一个就是说前面都是以任务为主，各种各样的pipeline的任务。那下面我们希望能够让大家理解一下，其实pipeline更多的还可以去跟大模型去做对接，就跟我们的现在的大模型去做对接，来完成各类下游任务。以大模型为中心了，以这个model为中心，然后通过这个auto class，也是以model为中心去找到对应的这些预训练的模型和预训练的token lizer。
	那我们接着就来看一看，我们都知道经过理论课的学习语言模型其实主要有两类，目前用的多的。一类是自回归的auto aggressive的这种decoder only为主的，像GT3，它的训练目标，模型的这个目标函数就是一个条件概率，通过输入上文的这个概率，大家回想一下我们讲这个语言模型的一路迭代。从早期的这个机器学习统计的这种到神经网络的这个语言模型，到后面的大禹模型，也都是这个贝叶斯公司，就是根据上文，尤其是这个GPT，根据上文上文的这个条件概率的相乘预测下文下面要补或者下面要生成的这个文本。最大的概率可能是哪个？然后那个概率可以跟已有的这些我们的token去做一些对比，然后去找到跟他最像的那个字，然后再给它decode回来，就变成个字了，这个都是自回归的模型。
	那么以这个自编码的，我们以这个bert为主的，它的训练过程就像完形填空，你训练的时候给了他一个或者很多个sequence，句子有的，然后15%的句子，大家应该都记住这个套餐了，因为bird太经典了。15%的这个几率，这个单词我说百分这个句子当中有15%的单词是用掩码它遮住的，mask的这些word这些单词，有80%的概率是随机死，10%是正确的原文，10%是一些错误的或者说不重要的词。是通过这样的方式来进行训练的。所以这个就意味着其实这两个模型的使用场景不一样。就比如说我们刚刚看到的标签，GPT two这样的一个auto regressive的模型，它一定是优先去支持这种文本生成类的任务，tech generation的任务。那么hugger face其实也是很早就实现了GT two，然后我们知道很多大元模型，那会儿比较著名的在19年的时候就记不起二和bert。那么我们使用首先使用这个pipeline去指定GPT two，我们能看到这里还有一些不一样的提示，这重新运行一下，重新restart一下。
	好啊，然后我们我们把它做到文本生成任务里面，大家可以想象一下，这里我们开始用这个prompt这个单词，让大家或者说这个变量名让大家有一点感觉。
	做文本生成GPT two时代的时候，假设我们现在给的这个上文，我们刚才讲大语言模型给的上文是hugin face is the community based open source platform for machine learning。Hugin face是一个基于社区的开源平台，然后是面向机器学习这个领域的，给的是这么一个上文，然后让他用GPT two把它生成下文，看它会生成什么样的呢？它其实生成了下面是uh IT is an open source，a platform for much learning that supports simple program and small clusters of distributed systems that open IBMA patch, Spark and cloud front cloud front, 那就是它能够生成对应的内容，但是感觉好像质量不是很高，对吧？
	通过GPT two这个比较小的模型，我相信大家的资源应该都能运行起来我们可以去学习这个pipeline的各种各样的参数，尤其是在文本生成这个领域。比如说第一个比较重要的参数就是我们的这个number return sequence，运行一下大家就知道了。那么通过设置这个return sequence，其实就是指我们的GPT two，可以，就跟大家调用GPT的API1样，可以让它返回多个不同的结果。然后不同的结果有它各自的实然估计，这里也是一样的，我们可以返回不同的sequence，是它生成的这个内容，你可以在pipeline的这个定义里面去设置，在生成器这个整体上去设置，也可以在每一次具体的生成的这个任务上去做设置，这里我们看到number return sequence，设置为二，可以的。然后同样的我们也可以去设置文本的最大的长度，就比如说你要把你的这个服务变成一个最终对外的一个服务了。如果你的用户可以无限制的去生成，甚至逐渐逼近你的这个模型的上线，其实会消耗很多的算力。有一种做法通常是去设置这个最大长度，这个是一个比较硬的一个设置方法。它会带来的一个好处是节约算力，坏处可能是会有一些截断，那这个截断可以通过一些简单的分子检测处理就能够知道了。
	就比如说这个i don't know about，那就说了一半的那你可能就不返回这一部分内容，就返回前面的内容，这样也是可以的。这里就是一些你要把你的这个单元模型变成真正的服务，可以注意到的一些细节和一些特定重要的参数。我们开始还看到中文支持一直都不是很好。但其实哈根face上有很多中文的模型已经都做的不错了，比如说这个bert，bert自己也有一个中文的base的模型，我可以搂一眼，这里面有写的很清楚，叫bird base chinese，被下载了，上个月下载了三百多万次，然后自己也有这个inference API。那通过这个API你也就知道了它的mask应该用中括号的方式来做标记也有一些对应的example，什么哪个巴黎是法国的首都我们待会儿会在事例里面通过这个mask来跟大家讲一讲这个tokenizer然后这个是hugin face团队开发的一个专门去做文本补全的这么一个模型，语言是中文。
	好，那我们可以看一下，如果我们使用这样的一个模型，first base chinese, 然后任务是fill mask，完形填空大家可以简单理解，那么我们可以看看很有意思，这个地方我们要使用top k了，因为它模型逻辑不太一样，没有用这个return的这个sequence，不是一个不是一个文本生成的任务了，那这样我们选择了这个人民是什么可战胜的top k等于一，这里写的是人民是不可战胜的，很有意思。我们再看另一个例子，美国的首都是什么？对吧？大家刚刚还记得这个example里面写的是法国的首都是巴黎，我们再看一眼，在这儿，这个例子我们可以复制过来，巴黎是什么国家的首都？首先我们看到这个运行的结果不是很顺利，美国的首都是mask这里是一个问号，可以再执行一遍，确认一下美国的首都是一个问号。没有，他的势力这么好，即使他的这个势力就是一个什么是什么国家的首都，这很有意思。其实我之前举这个例子的时候，并没有想到他这个例子里居然就有什么首都的这个问题。那假设我们现在问他的这个事例。
	巴黎是法国的首都，这个其实是一个大语言模型应用的一个很有趣的trick。
	也想跟大家讲，当我们把这个mask放在一个sentence中间的时候，其实它的效果会好很多。其实这两个问题没有本质差别，而且我相信这两个问题的训练集里的频数也相差无几，并且甚至可能更多。但是我们明显感觉到这个mask放在后面是不太行的，放在中间是比较好的，并且我们可以让它多生成几次试一试，比如说这里我们让它top k等于3，对吧？把把后面三个也输给我们看一看是怎么回事，美国的首都是问号、句号、感叹号。就是当我们放在末尾的时候，通常就会出现这样的问题。然后大家也可以去想一想，因为这个musk其实本身它是一个抽样，然后在训练的过程当中抽样去掩码的。然后你在两头本来就会触发一些首先概率不高，其次就是这种表达方式，在他的训练集里的出现频率也不高，所以他的这种问法就不是很能够拿到一个你想要的结果。所以如果你想要把你的这个表现好的话，要么就是把你的问法去做调整，这个也是去使用单元模型的时候，这个mask尤其是bert为主的这一类模型经常会需要去调整的一个地方。
	那同样的我们可以看看别的这个变化会不会对他有影响。比如说连续的mask，设置一下，很有意思。连续的mask首先我们刚刚一个mask会带来什么问题呢？就是因为美国的首都它不是一个字，对吧？所以会造成一些不必要的概率，那我们可以把它弄成三个，三个其实一开始想的是华盛顿，但是很有意思，他给出来是纽约，然后最终一个句号，这个大家要细看一下，就每一这里是一个三个mask，然后每个mask对应这里的一部分内容，它的自信度，然后它的这个词以及它的这个token ID，这些都是孤立的，top k一都孤立的，然后最终得出来了一个最大的结论，美国的首都是纽约。
	这个是使用burk类的时候，大家可以去好好玩一玩的。然后也也不是什么很很黑科技，大家多用一用就能够理解的。但这里重点是想体现出这个部分，给大家来做一个思考。
	包括当时放到这个尾巴上为什么效果不好？因为在训练集里我们开始提过有一类token都是这个文本。还有一类token是特殊的token特殊字符，就包括我们的mask其实本身就是一个特殊字符。那么一句话的头和尾通常不太会放普通字符。这个musk虽然是特殊字符，但它是代表的普通字符，它有特定含义。
	但还有一类特殊字符，它是不代表文本的，它只是用来表达一些特殊作用的。比如说这里看到的COS和这个SEP，我想大家还记不记得这个COS的SEP？这个其实我们在理论篇的时候有讲过到底是个什么东西？为什么填充之后这里出现了这两个玩意儿，之前我们的这里都没有，怎么回事那我们接着往下看啊，怎么回事儿。就跟我们的tokenizer和我们的这个decode有一定关系了。
	我们前面讲过pipeline很好用，它通过一个统一的接口，你只需要指定task就能完成各种各样的AI任务。然后如果你想用到生产环境，希望你去指定它的model以及它对应的版本，这样能够让它的结果更加的稳定。然后如果我们想要更进一步的去使用transformers，然后让他能够去为我们的微调，为我们的新的数据集去服务的话，就需要开始介入toka ized和model。那这里我们先第一次比较浅的先给大家介绍一下怎么样去用它，这里会用到一个新的抽象叫做auto class，这边有写，其实很简单，我们前面也讲的这个auto class，就记住两个方法，现阶段就够了。一个叫做from free train，一个叫做save free train，比如说我们想要继续用上面这个中文的模型来研究这俩到底是什么东西，让让大家再回忆一下，看看token neither它的这个编码过程到底是怎么回事的。
	首先auto class重点其实第一要放在这个auto上，为什么叫auto？是因为我们都知道要去玩一个大语言模型，它的配置文件数据特别多。就比如说我们这里看到的预训练权重，包括这个相关的模型，有非常多的内容，包括配置文件词汇表怎么弄，其实它是通过这个auto class以一个统一的key，这个key现在我们理解就是哈根face上托管的这个模型的那个页面。它现在叫model car，或者就是这个模型的页面，这就是一个唯一的路径。这个路径就像这个模型的UIL或者ID一样，通过这么一个东西，让你去管理它周边的一系列的模型和token niza。比如说我们这里要去加载一个已经预训练好的bert base chinese，就是我们上面玩的这个模型，它也在hugin face上面有这么一个页面，就要去玩这个模型的话，我们只需要填入这么一个per face tidings，然后就可以加载这个模型了。
	我们先来看一看，先加载它，名模型的名字叫bert base chinese，然后加载到一个是它的token ized，一个是它的model。那加载之后，我们刚才有同学就在问，到底这个编码是什么意思？我们来看一看第一编码它是跟着模型走的，大家还记得我们这部分要放在哪儿去了？应该是在我们的这个课件里。再给大家搂一。大家记得这个模型跟token iser是配套的。
	就像我们开始讲GT4，他用的这个ADA embedding 02，然后我们的这个TPT3可能用的是更早期的一个embedding，bert有自己的这个embedding，怎么实现的？由to ither来实现的，那具体怎么实现呢？我们来看一看。就比如说我们要来做这个imagine，要把这个原始的文本变成最终模型输入的这个向量，它是怎么玩的，我们先来浅看一下，首先我们得要能有一个token lizer，就是通过这个model name就能找到这个token。那这个to ither实例化model也实例化之后，可以我们运行一下。也叫。然后我们理解一下整个tokenizer encoding这个编码的过程，其实就是两个步骤。
	我刚才讲到的第一个就是我们的分词。奔驰有很多种策略，最简单的策略就是一个单词，就word based就是一个单词，或者说一个字一个中文，一个词就我们要分出来的一个token，当然这样的话其实丢失了很多语义的信息，所以后来你会发现有各种各样的，包括这个character base，就是我刚刚才讲的这种完全切散的。也有word base，也有这种free space短语based，还有这种基于词典的等等。但这些，其实现在都都已经掩盖在这个token net里面了。
	简单来说就是当你想要去做预训练的时候，你的或者说你想要去做微调的时候，你下载的这个预训练的模型里面就已经实现好了一个token lizer。那他就已经实现好了他自己的分支策略。这个时候你通常是不用去改这个分策略的，你更多的可能是把新的数据用它的分词策略。分词之后检验你的新的数据里有没有一些token不在它的词表里面。如果不在的话，你就确定需要把它添加到词表里面。比如它很重要，它是一个实体，那你就加进去。如果不重要，那你就忽略掉。其实是这样的一个策略。
	我们现在就实际来看一看，我们把刚刚说的这个自动化的步骤打开，看它具体是怎么做的。让大家理解这个流程里面token nize到底干了什么活。其实就两步，第一步分词，这个是一个character based，就是一个单词，一个字符，应该叫一个字符或一个中文字，分成一个字。分成之后，第二步其实就是把这个一个月的值转成对应的token ID，也就是我们经常在使用GPT API的时候，会讲这个token有多长，其实就是这个token有多少个，完成这个映射这样你就能看到就每一个字，美5401，国1744都有自己的一个token ID这些有腾讯ID的自然都在词表里面。然后实际情况下，我们可能更多的会使用这个encode方法。就完成了这一个两步的操作了。
	你没必要写两遍，对吧？这是一个很没必要的操作，那我们把它取名为叫做端到端的这样的一个方法，应扣的方法生成出来。大家会发现有些人就会发现多了两个的ID，前面有个101，后面有个102，中间的部分才跟上面的拆散的两部一样，为什么有这个新增的部分？有应扣的自然就有抵扣的方法。
	有同学在问那个词表是干嘛的？词表就是一个token，ID对应一个对应的人类看得懂的自然语言。Decode就是去读词表，然后把它映射回来的。所以词表跟embedding的关系就只是一个decode反映射回来的关系，然后我们decode一下就知道了，我们通过decode原来的两步拆解和端到端的能发现一个细节，两步自己做的时候，这个token ID没有这个101和102。而这个端到端的in code去增加了这个101和102，而恰好101和102就是这个CLS和ACP，也就是我们上面做的这个示例想让大家思考的内容，为什么有这么一个玩意儿，然后为什么前面没有问题，后面那个出现了CLS和SEP？大家可以回头再回想一下理论课，这边时间关系我就直接跟大家讲原因了。
	第一就是多段文本的时候，我们需要知道我们真正去训练的时候，不会一次只丢一句话，会丢很多段话，或者说一个batch的sequence，就是一批语言。大家还记得bert这个论文里面，对，bert这个论文里面的截图，这幅图我不知道还有多少同学有印象，大家仔细去看这幅图。Mask的sentence a mask sentence b，这都是我们训练的语料。训练的语料里面有CLS，有token一直到token N、SEP，token 1一直到token m所以我们其实这里的这里想要告诉大家的这个sequence batch其实跟这里是一个意思。然后CLS标志的是我们在bert这个token nize里面的过程里面，COS是开头，就你训练语料这一批训练语料的开头，SEP separator是分隔不同这个句子之间的一个分隔符，当然如果你后面还有就是这个SCP，后面可以再接一些内容，这样就把它整体变到一段里了，就变成了可以给整个中间这部分，其实就是bert的核心的transformer的网络，全部丢进去一次性进行训练，而不用一个sentence一个sentence的去进行训练。
	到这儿我不知道大家看明白没有，这一部分是我们的encode部分，然后encode完了，整体才会丢给我们的这个大语言模型连接在一起输入进去，这样就是可以有一批sentence来进行训练，然后我们可以试一试编码多段文本，这个其实就是两句，我们放在一起跟刚刚略有不同，刚刚是一句话。使用in code。这里我们就能看到形成了这样的一个编码后的那大家熟悉的话就知道这一堆的token ID就是这会有个101102102对吧？
	然后实际的这个使用的时候，我们还会使用什么方式来进行这个操作呢？其实除了刚刚的in code decode去做这个模型层面上的操作以外，如果你只是简单的要去去进行一些访问，甚至都不需要看到这个底层，还会有一个更简单粗糙的一个方法，就跟pipeline的实例化一样，这个token ized本身就可以接受一个batch的sequence，然后可以返回更多的封装好的内容，都比这个编解码还要更高层次的一个抽象。这个是一个python的一个语法堂，就是指tok niza本身是可以被直接调用的，虽然它是一个类的实例。然后就能一次性完成文本编码和特殊编码的补全。
	返回的这个结果包含三个部分的内容，我们刚刚看的这个encode出来的结果是叫token ID，对应着我们直接调用to nizar的这个方法的input ID token ID。但我们都知道，它有很多很多个句子。这么多token第一个要解决的就是哪一个token是哪个句子的这是有一个箭头，做分类，实际没有怎么办？通过这个token type ID来做这个归属的句子编号，然后我们还学过注意力机制，对吧？就我们到底这段话里面哪些词比较重要，我们的QQV哪些key是值得关注的，这个attention mask是来干这个事儿了，我们执行一下，我把它取名叫这个in bedding的batch，输出一下。其实就这三个部分的这个字典这个比较难看，这里用这个部分来输出一下它的结构会稍微好看一点，这样大家能看明白，就把这个词典稍微枚举一下。那这里能看到是两段话，这是一段话，然后这是他的token type ID，然后这是他的一个attention mask，这我们就往后走，然后这个大家整明白之后，最后一部分内容是我们自己要去训练的时候，需要添加一些新的token，要去添加新的token，首先我们得知道已经有是已经词表里面有哪些了，所以第一个我们可以通过token nier的vocabulary，去看一下现在有多少个词在我们的词表里。
	这个vocab出来的是一个字典，python的字典，通过它的case可以指方为它的key，然后通过这个可以知道这个有多少个。这个是python的基础语法，我就不赘述了。然后我们要添加常规的token和特殊的token这两种情况。然后常规的token其实就是我们讲的正文里面有一些实体，原来的指标里面没有，那怎么看原来的词表里面有什么呢？我们通过这个vocabulary其实就能看啊。
	然后怎么样去看呢？因为它是一个字典，对吧？所以其实就是把词表那个文件，我们可以通过这个方式去便利出来，就比如说took neither vocabulary是一个词典，通过这个也是一个python的语法堂，它内部的一个迭代器，可以查看此表的一部分内容。比如说查看十个，你也可以全部两万多个都打开。当然你也可以选择，就假设你已经save到本地了，这里有对应的这个vocabulary的text，可能要加载一下。这个应该能看到这个就是每个token ID对应的一个内容，就是你可以理解这个行这个line的第几行就它的ID，这样可以省略一部分的空间，那么这个方式我们就能看到此表了，假设我们现在要加的这个新的token。我想了一下，这个可能是他原来指标里没有的，叫天干地支，那我们要怎么检查呢？可以自己去看，对吧？
	但实际我们都学过这个python，知道有一个东西叫集合，可以通过集合之间的差做差这个操作，就知道他们有没有这个词表里面有没有这两个词了，我们做一个差，如果有的话，这个做完差应该是空的因为没有，所以这两个new token还在在这个集合里。但因为我们用了这个操作，所以这个new tokens已经不是一个列表了，变成了一个集合。但是我们需要接受的是传入的是一个列表，所以把它重新变回一个列表，通过add tokens加回去。然后我们的常规的这个token，如果要加回去，它是直接加到那个词表末尾的。我们刚刚看过那个vocabulary的那个文件了，对吧？一个TXT的文件，它会直接追加到末尾，相当于就会有新的token ID给这两个词运行一下。不好意思，感冒了。那我们可以看到总的这个vocation ary的key，就从21128增加到21130了，之前都是21128。
	还有一类特殊的token，就比如说你有一些特殊用途的token，你现在看到的这个mask一个中括号mask。假设你还有一些特殊的，比如说这个分隔符，你非要有一个特殊的分隔符叫new special token，那也可以加。但是这个我建议是审慎的操作啊不要随便去加。
	假设一样的这个流程，只不过它是一个key value的形式了。因为它是标志了这个是一个什么类型的特殊操作符，这里我有列出来，就是预定义的特殊用途的BOS，end of begin of unknown saper，the painting mask等等。如果这些你发现这些特殊词表当中的词都满足不了你的要求。还有一个类似于others的这么一个类别，是transformers支持的，它的key叫做additional special tokens，你可以放这里面我们执行一下。好，那把它添加进去，你可以看到21131，现在词表里面就有新加的这三个token。
	然后我们这些都操作完了，需要把它保存下来，怎么保存呢？可以通过这个save方法就能够保存，也很简单，这边就保存到我们刷新一下。大家看到这里就是本来是4个小时前，下午的刷新之后，这就是刚刚保存的，5秒和2秒分别对应的上面的token either是先执行的，是这个是去拍lab，每十秒会自己刷新。5秒钟之前，15秒钟之前，然后这个model要比它晚两三秒，执行完，就会有这么一个时间差，就会把它的这个token either和模型都保存下来，那这个其实就是pipeline和我们的outclass的一些基本使用，希望大家通过这两个notebook能找到一些感觉。
	先把我们的transformers用起来，然后大家也会发现其实这个transformers的接口并不复杂。对于我们想要简单的使用和一些初步的做一些小的调整参数的这个设置是很简单的。好看大家有什么问题？哎。
	看大家有什么问题，我们来问一问，我顺便接个水。感冒了，最近天气变化比较大，大家注意一下。
	我来回答问题，修改后的模型保存后怎么使用呢？重新加载本地的模型吗？对，就是你save了，接下来下一次不就可以from吗？可以的。然后你想你从hugging face上面拉下来的模型，也是别人save下来的。那我开始讲过，这个seve如果你配置了这个transformers的这个配置文件，你可以推到这个hugin face的hub上存下来。
	还有个同学问下载的模型可以指定路径吗？可以，这个也是一个配置。我下节课放到这个课件里。这个同学问的问题就是我从这个transformers从hugin face hub上面下的这些模型，可以放到一个指定路径，可以通过配置文件来修改，不用放到这个当前目录下。
	如果你的本地磁盘比较小的话。Pipeline API不需要totalizer，auto class需要cocooning，zer是不同的应用场景。不是的同学pyi eline API你可以理解，就是用你不需要改这个模型。我们讲auto class其实已经开始逐步跟大家讲，如果你要改这个模型要怎么办？拍拍API就是用不改。
	中文的分词能自定义吗？是这样的，就是我刚刚通过这个例子就想告诉大家，这个tokenizer跟model是绑定的。然后你如果选定了一个预训练的模型，它对应的neither通常就定下来了。如果你只是使用的话，但如果你想要对它进行微调，那么通常它的分词策略不太会去改更多的可能是去改他的词表。但如果你要连它的分词策略都改了的话，那他可能很多以前的预训练权重就会受到影响。就比如说你的分词策略改了，那token不就变了吗？就原来那个词是那两个词合在一起的，你改完分词策略那两个词分开了，那分开了之后token ID就变了，结果也就变了。我不知道这个解释，有没有说明白。
	除了python可以加载这些模型，其他编程语言可以加载吗？有性能上的区别没有？是这样的，你要想象一下，python其实也只是一个DSL1个领域特定语言，它没有加载，真正加载运行这些模型的是py touch的TensorFlow。那你就要想象一下PyTorch的TensorFlow有没有其他语言的实现，其实没有太多，或者说有一些语言在更表层做了转换，但这个地方一定会有一些兼容性问题和性能损失的，所以还是用python，也没那么难。你看这个代码一共才几行，对吧？没有那么难的。稍微学一学，你都搞AI搞大模型了，这个python不需要很复杂的。
	有个同学问添加了此表之后，必须要做resize token in beating吗？这个不一定，看你的这个添加的新的token对原来的in bedding有多大影响了。你这个问的问题挺好的，后面我们做微调的时候会涉及到。就是相当于假设举个例子，你新加的这个词，好巧不巧他就是在一堆很稠密的原来在这个位置中间，他在他跟大家去凑个热闹。那这个时候我们的模型推理出来的结果刚好可能结果就会被他影响了。原来靠近了正确答案，现在靠近他了。那这个时候可能就会去首先对这个evening整体也许会做一些调整，做一些retrain或者做一些find to。包括我们讲的各种so soft proms，不是其实也是在做这些相关的一些工作。
	Python的book自己安装的吗？之前直播没看，同学去看一下，就拍lab自己装一下。
	推理业务场景是不是只能模型常驻显存，需要一直占用GPU，有些算力厂商能按计算任务的运行时长。对有没有可能每计算一张图片这种方式就是每次都要加载一张对比。这位同学你问的问题就是是这样的。我给你举个例子，你想说的是我没有那么高频的使用，所以我能不能寄件收费，而不是包月收费。这玩意儿就跟你去网吧一样，对吧？你网吧去的多了，你就会选择这个充卡包月，甚至你自己买一台电脑。你网吧去的少，你就选择按时付费，对吧？这个是收费模式的问题，这完全取决于你用的频率高不高，而不是说哪种方案更好。
	老师，请问华为云我们是不是选择按需使用就够了，还是包月更合适？我的建议是如果你想要你，你是一个每天都会去学这个课程的同学，你就抱怨。如果你只是想要试一试，那我觉得现阶段你都还不需要开这个GPU。因为我们还没有到这个微调，可能要再等两天。就是你就精打细算，因为你已经算到这个这么细了。理论上我印象当中华为云的包月和按需付费是价格差了一倍好像。
	有没有一些小一点的模型？这个四五百兆就已经很小了，同学就四五百兆的模型已经很小了。
	有个同学问没法下载hugging face上面的模型，有什么办法解决吗？去找一下那个镜像站，hugging face的镜像站，king face有镜像站可以访问。对，但我也不清楚这些经商在稳不稳定。
	架构模型是一个专业术语，希望能直接识别出来，而不是识别成架构模型这两个词。所以是不是要从头做tokenizer，从头异性恋。这个看你自己的分词策略是怎么做的，就是看你现在用的这个模型是怎么弄的，然后可以直接弄到词表里。Token ID是每个模型自定义的，会有不同的值，对吗？所以有些词汇涉及不到才要添加到词表。
	是的，可以在项目文件夹看到之前运行pipeline下载的模型吗？不可以。同学因为我的这个项目里面是不会上传模型的，模型太大了。然后这个项目的目录也不包含我的模型目录。就理论上来说，如果你访问hanging face的模型有困难，我相信你访问github也有困难。
	最后回答三个问题，10点17了，看大家还有什么问题。确实一到实战，可能大家各种各样的问题都会出现，依赖安装在哪儿？这个同学你这个上下文有点缺失，我不知道你的依赖安装是什么。
	看大家还有什么别的问题吗？
	Embedding和词表有对应关系吗？我理解一下这个问题。Embedding和词表有对应关系吗？应该是我看你这个问题想问的到底是什么。如果是说这个词表的token ID的话，那它肯定是有关系的。因为它最后映射得要有个ID，那他有一定的关系对。
	但其实这部映射是通过读配置文件就能找到的。
	大家可以到时候去运行一下，看看这些文件就明白了。就比如说我们加的这3个AD的token，这三个文件它的ID在这儿，新增的token，比如说我们的config文件里面有写这个模型基础的一个模型的文件。
	包括我们这里这个special token的映射关系和token ized的这个技术文件。这些大家看一看就有感觉了，就没有什么抓没有抓手的感觉了。
	对，这个大家硬盘还是稍微准备一下。硬盘现在也不贵了，SSD都降价了。我印象当中现在这个课程当时有提过，需要大家准备至少500个GB的硬盘，因为会下各种模型，但是这硬盘挺便宜的，你下载下来之后可以做各种各样的AI模型的测试，不挺好的吗？
	康达是支持windows的，所以是可以用的。
	In code的逻辑不是去找ID。这个同学我不知道是不是中途进来的，如果是的话，可以看一下录播，或者看一下我们前面讲的那个逻辑。Encode是两个步骤，您扣的也没有那么神奇，就是两件事儿做两个事儿，分词加映射。是的，因为今天模型都很小，我开始说了，就刚刚有个同学问华为云GPU这事儿，今天的模型甚至都不需要用GPU也能跑起来。好，我们今天就先到这儿，大家有问题再在群里沟通，大家也早点休息，然后这两天多跑一跑这些事例，换一换不同的模型和输入感受一下，后面我们会再上GPU来不断的去加深。然后大家如果环境没有搞好的，也可以用这个collab去试一试。然后hug face的镜像站也找找。如果实在网络环境太差的话，好，那我们先这样。