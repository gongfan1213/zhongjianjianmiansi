	Hello, 大家可以听见吗？刷新一下。
	我这个手机怎么看不到画面，大家画面是正常的吗？能看到这个视频里的画面吗？
	出来了，声音也正常，对吧？好，那我们就正式开始。今天的算是我们这个大模型微调课程的下学期第九节课我们按照进度来说的话，应该到q Laura了。然后我们也终于经历了大模型的早期几年的发展阶段。从bert我们都用过这些模型，大家可以回顾一下，从我们最开始的理论篇跟大家讲解AI的发展到大模型到初代大模型。比如说这个bert，GPT two这些都是我们在学习transformer这个库的时候，大家都使用过pipeline，推理部署过，调用过他们。我们也去使用了一些很新的模型，比如说vesper large v2这样的一些在刚刚发布不到一年时间的这些新的云识别的模型，包括一些图像分类的模型，我们大家都用过了。目前我们应该接触过的这些大模型，多多少少的也有个五六个了，但是我们一直没有去用q ora这个大家都听过的很香的一个技术，也是才发布几个月的时间，怎么样用Q罗A来进行微调。
	同时在十月份的时候，也就是二三年10月份的时候，十月底在CCF的年会上，质朴发布了第三代的ChatGLM6B的模型，也就是我们今天的主要要跟大家介绍的要微调的这个模型，叫做chat GM36B。其实整个ChatGLM在去年一年的时间迭代了三个版本，更准确一点来说其实是半年的时间。因为第一代的ChatGLM这个6B的模型是3 4月份发布的，十月份的时候就发布了第三代，所以是非常快的一个迭代速度。
	大家如果关注过微软一些进展，微软最近在我们在深度学习阶段叫模型蒸馏，或者说怎么样用一个更小尺度的，比如说10分之1或者1%的模型参数，达到一个原来一样的性能，最近微软有一些进展。然后我们今天其实就是想说怎么样让我们的微调的课程里面，让绝大部分个人的开发者或者小的公司用1块16GB的显卡，就能够去体验到大模型的这些好的新的生产力，通过Q6A我们可以实现。并且我们其实今天的整个实战部分，大家会感受到，我们的从这个库的使用开始，一步一步的已经让大家开始越来越熟练的去接受这一套大模型时代的开发库。比如说transformers，比如说PFT，其实它的新概念API并不复杂。更多的是我们循序渐进，由浅入深的带大家理解了这些不同的大模型他们之间是什么关系，这些概念是什么是什么样的含义。所以既然讲到ChatGLM，我们就不得不提这个GLM就质朴也是一家很年轻的公司，他们自己在这一年多的时间快速的迭代，发布了GLM的整个大模型的家族，也就是这最近一两年的时间。那么GLM他这个大模型和GPT，或者说这个以GPT为主的OpenAI的这一系列模型有什么样的一些关联，或者说他们对比着来看有什么样的一些特点。
	然后同时我们知道所有的这个大模型，现在几乎都是用的transformer这样的架构，我们以前也学过了，有不同的transformer，有这个encoder的，有decoder的，也有这个encoder decoder都有的。那GLM是一个什么样的大模型？它和我们已经接触过的这些bert OPT比起来，它又有什么样的特点？然后基于GLM这个模型，支付AI又做了哪些不同能力的一个探索，这是我们今天的上半节课想要给大家一起来交流学习，让大家能够了解这个国产的目前算是国产里面热度最高的一个大模型，那他是一个什么样的模型家族。
	然后下半节课可能会偏实战上手一些，今天我们会用q ora这个PFT已经支持的微调技术来微调我们的chat GM36B这个大模型。使用的数据集也是质谱。他们在使用P20这个prom ti方法的时候，推荐的一个数据集，一个广告语言的生成的一个数据集，叫ADGEN。那么今天的重点其实是教会大家使用q lora这个技术，然后能够用q lora除了微调chat GM36B以外，也能去微调其他的模型。当然我们整个课程里面，下学期就接下来这四周的时间，chat GM36B会有很重的篇幅。今天主要是要教大家怎么样用q ora来微调它。
	可能下节课会把重点放在，假设我们微调好了一个chat GM36B的模型，怎么样能够把它变成一个应用，不管是部署成这个web的形态，还是部署成一个API，甚至是跟我们的年线和向量数据库去进行一些联动，变成一个RAG的应用，下节课我们可能会重点解决后半部分的内容，就是一个微调好的chat GM36B如何在一个私有化的环境里面去变成一个应用。这节课我们先解决前半部分的问题，就是怎么样去微调一个chat GM36B好，首先既然提到了质谱AI的GM大模型家族，我们就不得不提他的这个团队。就是是谁做了这样的一个大模型？这个团队其实来起源于清华大学，这个KEG和这个DM这两个组。一个是叫知识工程knowledge engineering group KEG，一个是数据挖掘data mining。大家如果关注github上面质谱的一些开源项目的话，几乎就会发现所有的这一类模型都挂在了THODM这个组下面，就是清华的这个data mining这个小组下面，在group下面。然后TTHODM他们也发布了很多的大模型。
	就像我们前面看到的，除了今天的主角chat GM以外，也有这个code jx，就是对标着OpenAI的代码生成的模型。OpenAI有一个code生成的一个模型，代码生成的一个模型。还有一个叫做cock VLM，这个也是一个快速迭代的视觉语言模型。在今年3 4月份的时候，ChatGLM6B第一代发布没多久就发布了这个visual GM。后面我们会有一个发布时间图，那visual GM其实就是以ChatGLM6B为基础，然后又做了一个clip的一个对齐，做了一个小一点的10亿级别的一个视觉模型。然后让这个微信GM就是让他能看懂图片。然后这个cog就cognitive，VLM其实是它的一个迭代，是它的下一代的版本，也是在今年的下半年发布的。然后web GLM其实是在KDD这个顶尖的数据挖掘会议上发布的一篇论文。然后同步发布了一个工具，这个工具也是在大家想象一下，其实整个切ChatGPT的发布也就一年多一点的时间，web GM其实在ChatGPT发布没多久，甚至是发布之前就已经在开始的一个工作。
	怎么样能够让GRM这个大语言模型拥有，就像我们现在知道有各种各样的给大模型联网的能力，比如说这个病，new bing的搜索引擎，比如说这个web pilot这样的ChatGPT的插件，所有的搜索引擎都在往大语言的模型上面去做对接。那web GLM也是一个类似的一个实验，就怎么样用web的知识来增强大模型的对于新知识的一个信息的一个缺失。然后通过新的搜索引擎的知识加上大模型对于这个知识的理解，然后变成一个问答的系统，这个叫web GLM。
	然后GLM130B应该是整个质谱最重要的一个大模型，因为它是一个最佳实践。某种意义上来说，它应该就相当于质谱对于OpenAI的GPT3.5这样的一个存在，他算是拳头产品，然后他也是在大陆质朴去做to b或者政府的营销的时候。非常重要的一个千亿级的大模型。然后GLM130B也验证了GLM这个大模型，这条路是可以走得通的。只不过要去训练一个千亿级别的大模型，会有很多的坑需要去解决。然后类似的下面两个模型就是一个纹身图和纹身视频的这个大模型了。
	然后最后有一个很有意思的叫agent tuning，就是怎么样能够把大语言模型，就像我们这个开发课讲的很多内容。其实这部分就怎么样把大语言模型当成一个大脑。然后这个大脑他有思考的能力，有理解需求，拆分问题的能力。他又能够借助外部的很多工具，就比如说他能借助刚刚提到的搜索引擎，它能借助这个天气查询的这个API平台，可以借助各种各样的已有的，不管是这个SDK还是像python的这个软件包，或者说开放的第三方的平台，都可以为他所用。然后用大元模型来调度他们，最后把所有的这些应用统称现在有一个比较泛的称法叫做AI agent，就AI的智能代理。
	Agent tuning其实是在这条路上有很多，包括整个除了agent tone以外，还有这个agent的benchmark，就是在质谱的它的研发路线上，然后他通过这个agent benchmark设立了八个不同的关于agent这一种不只是裸的大模型，而是加了各种工具的能力之后的这个agent做了八个不同的benchmark，待会儿我们有时间也可以看一看，所以整个这又是一条新的路线。但跟我们今天要讲的稍微。关联度不高，简单提一句，我们首先既然要讲质谱的大模型，就得从根儿上去理解一下这个出现频率最高的这个GLM到底是个什么东西。因为我们听听过太多GPT、bird这样的一些模型了。但深入往根上去找，这个GOM是一个什么呢？它其实是一个叫做这篇论文的名字，其实发布的时间也不是很晚，很早以前也就是最近一两年的一个成果，叫做一个基于自回归的一个空白填充的通用的语言模型，是一个general language model，然后跟我们经常看到这个GPT不太一样。因为GPT是这个general free train transformer，核心是transformer。而这个GRM它重点是想说这是一个语言模型。
	然后这个语言模型的重点有两个。第一个是它是一个通用的语言模型，不过他现在还做的没有那么通用。简单来说就是比如说GPT4已经做到可能一百多种语言都可以做的很好了。但是GM的130B可能重点还是在英语跟中文双语，不过他中文可能会更有优势一些。第二个就是它的使用方法，它的使用方法是用这种自回归的。
	Auto regressive这个词我们其实在理论篇，就第一周第二周的时候出现的非常多。然后这个auto regressive它主要是做什么呢？它是做这个blank in fairly就空白的填充，这个也有点像我们最开始讲bert的时候提到的训练方法，大家回想一下bert是怎么训练的，首先bert它是一个alter encoder为主的一个模型。它重点是encode的部分，用来做语义理解的部分。就把输入编码成各种各样的embedding的向量，然后再去做完形填空。它的完形填空是挖了很多的mask然后通过上下文的理解，然后把上下文当成它的条件概率去预测这个掩码，这个mask的这个部分应该是一个什么样的字，换句话说应该是一个什么样的logit，就这个地方的概率分布应该是什么样的那这个地方最大概率是一个什么样的字，找出这个最大概率的跟它相似的那个字词表里面，就是我们就猜就是那个字，这是一种方案。但如果是预测很多个多个字的时候，就我们在应该在第二周还是第三周的时候给大家做过这么一个notebook。就是让让我们的这个大元模型去做句子的生成的时候，那这个时候他就不能简简单单的是按照概率最大来贪心的去找是一个什么词，因为它是一段话，那这一段话可能会拉通了来看，然后最终把这N个词做成了一个分数。
	然后拉通这N个词的算术平均的一个值，最后去预测出它应该是什么样的一段话，所以并不是这么简单的去能够反映射回去它应该是什么字。但是我们的GLM不一样，GLM它更偏向就是从这个auto regressive大家就能看得出来，它更偏向像GPT1样。它其实是一个主要是以我以生成为主的，以decoder为主的这样的一个结构，待会儿我们会去看它怎么样去做的这个预测，然后他怎么样去用这个of。它其实用到了一个空白填充的这个任务上，或者说空白填充的这样的一种训练方法上。我们待会儿会细拆讲怎么样的一个方法。
	但是从我们对于大语言模型的分类来说，我们就能发现其实它跟之前我们学到的三种都略有不同。比如说bert，它的这个训练方法，它的网络结构可能大量使用了in code部分的结构。而GPT更多的是使用了这个decode部分的结构，放弃了前面的这个需要去串行计算的这个encode的这部分。T5可能是一个sequence to sequence这样的一个结构，那GLM它有什么样的一些不同，我们待会儿会细讲，所以它是一种不一样的类型，所以从这个视角来看，大家要这样去理解，它的类型不同，但是为什么需要提出一个GLM？这里就有一个大的背景，我们知道已经有这三大类了。
	然后这三大类又有各自擅长的地方，比如说我们学过知道了bert擅长这个语义的理解，因为它能看这个上下文，而且是双向的。大家还记得bert的这个B其实是双向的，能看这个上下文，所以他对语义的理解更强。然后他更适合说做一个语言专家，能充分理解你的说的是什么什么需求。但是对于下游的任务，他不一定做了大量的适配。然后它在下游的任务表现上需要一个一个的针对下游任务的数据集再去做微调。但因为这样的一个架构，使得这种架构我们不考虑成本的话是非常好的。因为就相当于大家各自有分工，然后做各自擅长的事情。Bert就本身作为一个中间结果去把语义理解能力做强。
	下游的各种当stream的这种数据集你给到我，我再去做这个训练，那么我在下游的各种任务上都能表现的很好。但它的成本比较高，就它的效果好，成本比较高，为什么呢？这个事情就相当于你是一个武林高手，你数十年的时间都在修炼内功。然后你修炼内功的话，你到你开始真正练武功招式的时候，你已经30岁了，然后这个时候你选择了某一个门派的功夫。然后也许到四五十岁的时候，你会变成一个非常厉害的顶尖高手。但是你只会那个门派的武功，你可能花了50年的时间只学会了那个门派的武功。用那个门派的武功你会非常厉害。
	但是换了一个门派的功夫的时候，可能你就完全抓瞎了。你没有对下游的任务有这么好的泛化性能，你更多的是你你理解你的内力很强，应该这么讲，然后像GPT，它可能就是说我内力没有这么强，但是我会类似于像我想不到有什么好的武功招式，他他是一个内力不强，但是他看了很多这个叫武功秘籍。看了很多武功秘籍之后，他能大部分的这个招式都能给你来一点，比如说能来个五六十分，但是不一定能给你做到非常高的分数。
	当然后面的GPT2、GPT3在不断的迭代，再补充这部分的这个能力，那如果回到自然语言这个处理的任务上来，其实就是有两类任务。我们如果把内力比作自然语言的理解能力，就是语义理解的能力。那这一类任务我们就叫NLU，natural language understanding，就是我能听懂自然语言，理解自然语言，这是一类能力。还有一类能力叫做有条件生成的任务，或者说无条件生成的任务，这是另一种，你可以理解是招式也行。这一类任务其实是GPT擅长的，是bert不擅长的。因为他没见过那些套路，他不知道怎么生成，但他能理解你的这个意思。
	那么GLM是想说能不能有一个很厉害的模型，它在差不多同样的消耗的情况下。举个简单例子，就是我是一个我想在50岁的时候，我能精通少林武当峨眉的三种功夫，这个就是GLM想提出的一个愿景，就是相同的模型大小数据条件。就我同样的训练集，我能不能在这三种任务上都比他们干的好啊，内力又高，然后我的武功招式会的又多，有点像虚竹，一开始练力练得很好，突然有人把各种招式传给他的感觉，他这个是他的提出的一个愿景。这样的一个愿景，他希望能够证明这个事儿是能做到的，并且他也在不断的去做这样的一个推进。其实大家如果跳出来看，这个事儿是有机会做到的。因为transformer我们学了之后会发现，它是一个非常暴力的，用资源来吃数据的这么一个模式。现在所有的大模型的，比如说24年的重要的研究方向，其实就是在怎么样用更少的资源去尽可能不降低精度的情况下，把大模型给部署起来，或者说能不能把大模型的这个基础模型再瘦瘦身，这些其实都意味着GRM想要做到这个事情是所有人都想做的，因为现在太贵了，我去开一个窗。
	太闷了。对，说明这个事儿是非常有机会的，那我们看看他怎么做到的，他做了一些什么样的变化。具体来看，其实就是我们下面能看到这幅图，能看的比较清楚一点。其实说到头就是他到底是怎么训练的，能做到他想要达成的这个愿景。我们争取用两页这个课件，能够把这个事儿简单的给大家梳理一遍，讲明白。大家可以重点去看有哪些不同，有哪些变化。
	第一个就是他的这个训练过程大体可以分成下面我们看到的这三个小图。这三个小图解答了他的训练的流程，我们首先看到的是左下角有一幅图，X1、X2、X3、X4，这个大家应该很熟悉了，这个是我们输入的一个文本，就我们给的这个自然语言。然后我们要去训练它，怎么样让它能够去空白填充去训练它。
	经过bert的这个训练，大家理解了，我们首先知道要引入一个东西叫标记，mask这样的一个标记。在bert里面它其实就是直接操控，就相当于在在比如说在我们的大家做过的英语完形填空里面，bert就是我挖了十个空，这十个空分别是十个单词，那这个单词会有不同的概率去填充填充上。比如说错误的词，随机的词，或者说正确的原来的这个词。这个是but bert的这个训练方法。然后它有一个比例，就是我会抽样的这个比例大概是多少。
	而GLM它略微不同，它的标记首先就比bert要复杂一点。它除了有这个mask以外，就是我们看到左下角有个m musk以外，他其实还提出来了两个标记，一个叫S一个叫E，分别表示的是start和end。那这两个标记有什么用呢？大家可以简单想象一下，现在你把这个学习的过程就像像人在学英语一样。就假设你现在是这个GLM，你要学习语言，你要学习英语，你是怎么学的，怎么去理解应该这个地方用什么词。这样大家为什么说好的语言环境能让你学的更快？其实都是类似所有人都在研究到底人是怎么把语言学会的。那么GIM的提的这个思路很有意思，大家也可以看看这个思路对于你自己学习新的语言有没有帮助。
	他第一第一他是说我会掏很多的空，但是我还会去记录这个空到底掏走了几个词，这个跟bert略微有一些不同了。就比如说我们看到这儿掏了两个空，一个是X3，一个是X5和X6。这两个空在论文里面有一个专有的词叫做span，它叫做跨度。第一个这个span其实是套了长度为一的一个词X3。第二个span掏了两个是X5和X6，那相当于就把原文输入的这个input拆成了两部分，一部分就是它的所谓的叫part a就是原文加上掩码的部分源码。只是告诉你这地方被掏了一个洞，未来会要填进去，所以那个位置是需要补充内容的。然后part b就是掏出来的那部分内容，就相当于标准答案那么分成了两部分，所以左边这左下角这幅图让我们理解一下，就是数据的预处理原文然后抽抽抽样，找各种空。然后这个空抽出来之后就把原文分成两部分，一部分是原文加标记，第二部分是我们看得到的这些被抽走的这个部分，那么这两个东西拿出来之后，他要怎么样去用呢？
	我们就看到中间这一幅图就很有意思，中间这幅图其实是我们讲transformer的时候就有提到。在标准的transformer这个网络结构里面，它其实我们的这个网络，它的decoder这一部分要预测后面有哪些词。但是它是没有位置编码的，所以transformer自己去做了这个位置的编码。
	那同样的GIM其实提出来之后，他也需要去做位置编码。因为他抽走了，他有part b有X3有X56，有这样的一些信息。但是他也需要把这个位置记下来，不然就是没有意义的。
	因为大家知道part a它是连续的一段话，所以它的这段话的前后的位置关系是保留了的。因为我就知道X一后面是X2，X号后面是一个my mask，后面是X4，后面是这个mask。但是我们看part b它其实是没有这个位置信息的，所以这里他提出了第一个所谓的一个创新点，叫做一个二维的位置编码，去表示这个跨度间和跨度内的位置。就是我们看到的这个红框的部分，这个部分它是怎么去表达的，聪明一点同学都看出来了对吧？我们把part a就用蓝色来表示了，然后我们把part b里面每一个span每一个跨度用不同的颜色来表示。比如说这个黄颜色的部分是我们的五六这个跨度，然后我们的这个绿颜色的部分是我们X3这个跨度。然后它有两个位置，两个位置编码position一和position 2，那么这两个position其实是两个维度的事儿，这个维度大家理解一下，学过线性代数就知道它它没有相交的，它完全是两个含义，所以它可以叫做两维的这个位置编码，这是没问题的。只不过它不是这种欧式几何里的XY，这两位是有特定含义的两个维度。
	那么position one这一维的这个位置编码是什么意思，你其实看一下就能知道。我们在做这个token ither的时候，都会知道某一个词要属于哪一句话。这是类似的，我想知道现在的这个span对应的是原文这个part a里面的哪一个位置。所以我们看到原文这个X1X2MX4M在剩下的这个原文part a的这部分被编码成了12345。而这个X5X6加上它的这个特殊标记start，他们有一个叫做五的这样的一个位置编码。这个编码其实就对应着我们在part a里面的这个位置。因为我最终还是需要在训练过程当中知道这个地方本来应该是什么字，我们可以用来做监督训练的。那么类似的后面这个3，其实就表征着我的这个X3，其实对应着part a里面的这个3。
	然后下面的这个position to是什么意思？其实也很好理解。我们看到这个先看这个绿色的部分，那绿色的部分有一和2，然后我们的黄色的这个position 2有123，那就很清楚了，就是我在这个span内部我也有好几个词。因为我不是我不是像bird里面每次都摘一个词走，我可以载好多个词，我可以摘一串，但这一串在part a里面就是一个mask。然后在我的这个part b里面，我的每一个span内部我也需要有顺序的。那这个顺序可以通过这个position to来做表达，所以你能看到一二所有的start在part b里面都是一就所有的这个start这个标记都是这个position to标的，都是一，那就很好去处理了。
	我们经过了前面的学习，都会理解这个token ID的一些特点。然后大家如果完全没有上过手的，可能会理解起来有些吃力。我相信训练过的微调过的一些同学，应该对这种位置编码什么的应该是有一些感触的。
	把我们的这个end的这个标记就加在这个最后，就经过中间这个部分是GLM的一个网络，然后其实也是用了这个mask的attention，如果通过这个方式大家就能看得出来，整个输入其实我们就做了一个调整了。原来的输入掏空，掏完空之后然后把掏出来这部分做成我们的part b然后part b会有自己的位置编码，通过这个位置编码我能够找回到在原文当中的一个位置，也能去表达我掏出来的这个span内部的这个词的顺序。接着我们就能看到第三部分，第三部分是它的一个另一个创新点，可以这么理解。那这个创新点要怎么去理解呢？就我们看到，大家想象一下transformer这个网络结构，我们最早讲过是一个从注意力机制开始讲起的，然后后面有了这个self attention。我们讲他自己的这个self attention，其实就是一个把输入经过两个矩阵，得到了一个WQ和WK的权重矩阵。然后这两个矩阵的训练的目标就是为了去找在这个query里面它的key到底是哪些词，然后最后把这两个找完之后，相当于得到了一个注意力的一个attention with这些都是最早讲过的一些概念。这个tention with其实就是我们来表征在这个query里面哪些X是最重要的。
	针对特定的这个问题，或者说针对特定的本身的词的指代关系，最后又去乘上了一个WV，这个WV就是X本身，这是一种注意力机制的实现方式。在GOM内部也是一样的，它也用了一个self attention，然后有query有key。但它的query和key他把它可视化出来之后，是为了让大家更好的去理解它。
	我们我们看这个querio。其实这个query和这个key你直接看，因为这个输入是一样的。X1X2MX4M然后SX5X6、SX3，这个东西就是我们的所谓的QQV里面的那个输入V或者就是我们输入的本身真正给到模型里面的那些token。就是我们每次token nize之后会有一堆的token ID那这堆token ID其实最终就长这么一个玩意儿，这个X一是指代的那个token ID你会更好理解一点。然后有这么一个输入之后，我们的part的这个AB就被移到了这里来了。大家看一看这个query，这是part a，这是part b，然后part b里面有不同的span，分别对应的是不同的我要去预测的这个mask里面应该生成什么内容。这里就很很有意思，就是我们看到其实整个query和key他要解决的是什么问题？
	在自注意力掩码它的这个创新点上，他做了第一个在part a就是我们的这个part a蓝色这部分，大家仔细去看一看，内部其实是可以所有的这个都是可以互相看得见的。像我们大家最早学习attention mechanical的时候，所有的输入的这个我们叫in这个encoder。Decoder的这个结构里面，所有的输入的这些法语和对应的英语单词都是互相可以看得见的。通过一个注意力的这个矩阵能连接起来。
	然后到了self attention这个网络提出来之后，这个transformer这个网络提出来之后，他不需要做翻译了。但是他需要知道这一句话内部哪些词跟哪些词的指代关系是很重要的。所以我们单看这个part a这个部分，其实它跟transformer的标准的self attention是几乎完全一样的，我不知道这个大家能不能理解到位，然后这个是part a的部分，所以它内部可以互相关注。但是大家要记得transformer是这样的一个结构，GPT不是GPT，其实是直接用的。大家想象一下，就是这里需要大家有一点理论的积累。但是前面都讲过了，如果你这会儿听得有点搞不明白了，就复习一下前面讲的理论课的部分。
	我们回想这former的结构里面，它的encoder就是现在的part a这种结构，互相能看得见self attention，还有多个通道muti head，那么它的decoder的部分其实是一个mask的self attention，它它取的名字叫self tension mask是另一个意思，大家别搞混了，在这个transformer里面的decoder里面，它其实是用了一个mask的机制。他这他的这个mask是指我的每一个输入都会把从当前要预测的后文都遮住，看不见，然后只能看得见前面的这个部分的内容，那就有点像我们假设假设完整的原文是拉通的这一部分的话，那part a就看不见后面的这部分，那就有点像我们说的decoder的这种结构了。但实际上不是，实际上的原文就是左边这么多。所以通过part a的这种方式，它有一点好处是说就是我的原文，我所有的原文，大家可以想一下，musk其实是知道这个背后是什么含义的，只不过他现在被掏了一个空而已。那通过他的A，其实它使得完整的一段话是存在的。每一个mask虽然被掏走了，但是我可以有它的这个概率，或者说它其实是占了这个坑位的，那么通过这种方式，整个part a的这部分它内部可以互相关联上下文，就是原本的这些上下文的信息是充分能够拿得到的，我也能拿得到逆序的信息。比如说我站在X4，我想要回看X1，我是可以看得到的。
	我不是一个消了一半的倾斜的矩阵，我看不到，我是能看到完整的上下文信息的那这个是第一个点。然后第二就是说你要想象整个训练集里面，我这一轮是把3和56掏走了，下一下一轮可能是把一和4套走了。所以经过不断的训练，其实我本来也是在不断的理解这个上下文。这是从多轮的里面去理解这个part a的好处，通过蓝色的这个part a整个模型的上下文的理解能力，捕捉能力会变强。
	但是这里有一个重点，就是它的A很强。它就算能跨不同的a poc去看到不同的这个mask就摘出来的这个mask不一样。但是在当前这一次训练里面，他是看不到这个part b的，因为帕帕特B是他的训练任务，它的优化目标，它就是要通过这个模型去预测这些mask的部分到底应该是什么词。所以我们聊当下的这一组数据的话，它是看不到这个part b的。所以我们能看到严格的划分了分区，这个是part a这部分它的一个训练方法，以及它的一些好处。
	那pat b有什么好处呢？就我们看到pat b它的这个标记是黄色和绿色两个词。它跟我们的decoder的这个结构就有点像了。它基本上就是完美的一个，也不叫完美，是复刻了这个transformer的这个结构。
	Transformer就是一个掩码的结构。我的每一个大家看到S这个位置，他能看到前面的所有的部分，以及我在B里面的前面的这个部分。就比如说我的X6，我是能看到X5和这个start这个特殊标记的，但我看不到后面的内容。我的这个part b的这个绿色的这个X3的位置，我也是能看见前面的X5和X6的，或者说我的这个part b的第二个S我也能看见前面的内容，但是我是看不见X3的。这个就是经典的transformer网络里面的decoder的这个结构了，大家可以去回看一下那部分的网络结构然后这个训练方式也是为了让大家理解整个part b它是做了shaffer的，是要打乱顺序的，也是能够方便他去在不同的语序上下文里面都能够去生成很好的文本。那通过这个方式，首先大家理解一下，就是这是一种新的训练方法。然后这个新的数据处理流程我们也都讲明白他怎么去处理的了。
	通过这样的一个方式，第一他用了一个二维编码的创新。第二个就是说它的标记符它也做了创新，不再是一个单独的mask，而是标记了我的每一个spend位置和它内部的一个顺序。同时也因为这个二维的编码，其实我也知道了这个span的长度，就相当于关于我知道这个mask这个地方，应该是填几个词合适，就这个时候应该是话多一点还是话少一点，你别这个musk这个坑地方本来应该只说一两个词，说了一长串的话，那就没有意义。所以它其实连这个span的长度，它也都放到了它的优化的目标里。
	也是它的这个函数本身里面会带的信息，通过这个方式去生成的，通过整个流程的描述，我相信大家知道了它的一些特点，它的一些特征。然后至于这个特点好不好，其实我们这儿拍脑袋也没有用，都是看最终的训练结果的那我们把这个流程给大家讲明白，这里其实大部分的逻辑都已经讲通了。但我们回过头来看，还有一个点就是是我们故意放到后面来讲，让大家不会乱。既然他要去抽样，他要抽这个part b要抽这个X3出来，要抽X5和X6出来，那怎么抽呢？他以什么样的方式来抽呢？难道是随机的去抽吗？总感觉随机的抽不太好，那怎么去抽呢？
	在paper当中，在GM这篇论文当中，他用的是一个泊松分布的方式去抽样的这个原文。然后在播送分布里面有一个超参数number的取的是3。这个三的意思就是说你可以简单理解成首先这个泊松分布它其实它的这个lambda这个三简单一点理解，跟大家简单一点理解就是这个拉姆达直接就跟我们的这个span的长度是相关的。而这个泊松分布本身它的特点，就这个分布本身它其实就是描述在一个固定的时空当中，某些事件发生次数的一个概率。其实就跟我们想要去学习的时候，有些常见的语句词汇，我们出现的频率高、次数多，我就优先多学一些，其实是这样的一个类似跟人的学习比较像的一个学习过程。而这个nmda等于三的这个超参数也就决定了这个spend的长度。
	平均值是3，就围绕着平均挖的这个span是3，在附近去做这个分布，大家了解波动分布应该就理解了，那么这个是在训练过程当中的一些小的细节，但这个细节也直接决定着他的语言理解能力好不好，就他的NLU这个任务好不好。我们一步一步去捋这个逻辑，那么用了泊松分布来抽样，然后nmda等于3，那它就能够如果我有一些词被不小心删掉了，或者抹掉了，或者没有了。因为我在训练过程当中，我就是能够预测那些控制什么词。所以我的这个模型GOM到目前为止，他的语言理解能力或者说自然语言的理解能力，NLU这个任务一定是做的比较好的。因为他的训练整个训练集就是干这个事儿，但凡我只要在这个人类的语料库里见。一次或者见过两次，理论上你就只要重新说一个类似的说法，或者这个语义主题，差不多我大概都能预测出来你没有给我的那个信息，那个mask是什么内容？这个是由他的训练所决定的。
	但是这儿就出现了一个，细心的同学就注意到一个点。这样的一个方法它很好啊，然后nmda等于3，大概也都能够符合我们的一些常见的场景，在自然与理解这样的一些任务上。但是对于生成的任务就不是很友好。因为为什么呢？就是这些mask我们现在是从理解的角度来说，我把空填回去了。
	但是我如果我是从生成的角度上来说，这些mask也是我生成的内容。你不能做一个模型，老是只能生成长度为三四这样一个长度的内容，那它就不是一个好的文本生成的模型。我不知道这个大家能理解吗？你的模型训练就全是短文本，短文本在NLU的任务上是非常精准的。因为你想完形填空各种各样的这个NLU的任务，都是说几个关键词没了。然后我通过训练这个模型，我能把关键词预测出来，然后我能够解决这类问题。但是生成任务是说，领导就说两个话，你总结一下对吧？那你总结得不能两三个词，你得多总结一点。
	那对于这种生成类的任务，GOM做了别的处理，这个我跳了一页，我不知道你们这边网速有没有跟过来，跳了一页。第二页下面的这个逻辑没变，这个流程没变，但是他在处理任务上做了很另一个优化，另一个新的创新，怎么创新呢？我上面这个已经解释过了。
	短的跨度兰姆达等于三的这个泊松分布的训练，适合NLU的任务，因为LU任务就这个特点，但是为了我们要做一个GLM，是想说它同时能够兼具NLU和生成类的任务，那怎么办？作者提了一个清华的团队，是他们提了一个很好的方法，叫多任务的一个预训练的方法，muti task free training，通过这样的一个训练方法，能够很巧妙的让他既能够学习NLU相关的这个任务，同时还能够去完成一些生成类的任务，怎么做到的呢？就是把我们的这个两个事儿，两个任务PPT没有切换吗？切切换了上面的字有换。这个同学我就刚才想说，有的同学可能看的不仔细。这个pipeline没有换。为什么这个pipeline没有换？是因为训练目标没有这个公式上面都没变。你可以理解成区别是什么？
	还是这套架子，还是这套pi pelle，但是换了这个超参数，然后换了的这个超参数，你可以理解成，我们刚刚提到了，我们去年这套流程，在NLU这个任务上重点是什么呢？重点是这个泊松分布拉姆达等于3，对吧？这个拉姆达等于3使得我们在取这个就相当于给它打上这个码的过程打码打码的时候能够比较好的符合NLU这个任务场景。但是在文本生成类的任务的时候，我是需要生成很多文字的，生成很多内容的。这个时候我的跨度如果是三就不合适了。
	所以这个时候简单来说就是我们知道最终这个训练过程是有一个目标函数的。然后我们的整个训练过程就是不断的去通过神经网络的这个前馈网络跑一遍得到一个值。然后这个值和训练集里面的这个标准答案去做一个度量。从反向传播再来更新这个模型参数。这个是一个抽象的可以通用的一个优化范式。但这个范式我们知道它的目标函数其实是可以变成有多个目标，只要这多个目标最终能收敛。我不知道这个大家能不能理解，多个目标是能收敛，就相当于你是一个聪明的学生，你能同时去学数学和英语，然后你的脑子都能学得过来，因为你脑子容量足够大的，模型参数足够多。只不过你看你在用语文的能力，所以用数学能力的时候调用的是不同的模型权重，就跟你用的是你大脑当中不同的神经元一个逻辑。
	形式化一点，用数学的方法来讲的话，就是你把你的优化目标拆成了几个穆小的目标函数。最后他们去做了一个加和，当然中间可以通过超参数来调节权重，然后把我们的这个文本生成类的任务变成他的第二个目标。然后整整个的这个目标函数是他们的一个结合，然后去共同优化这个文本生成的人物。
	这个时候问题就来了，价值不变，那变的是什么呢？变的就是我们的这个span的数量和长度，简单来说就是这个part b，我的这个要变了，因为我短文本不适合我这个文本生成类的任务，不管是有条件的还是没条件的。如果是这样应该怎么变呢？他做了两个级别，就是文档级和句子级的任务，分别是针对不同情况下的这个文本生成的任务。简单一点，就是现在ChatGPT都有的能力，大家也都用过的，帮我写日报，帮我写作文。这个其实对应着原文里面提到的这个叫文档级的一个任务，就是我们这儿看到的第一条，文档级的任务。Document level就是在我整个文档级别。
	这个文档级别要怎么样去训练和采样呢？也简单，就是你想象你现在拿到了一篇英语的完形填空，他的训练是一个均匀分布的采样，就是很平均了。因为他不知道长文本生成你会从哪一段开始要求他生成，所以他用的是一个均匀分布，那从均匀分布里面去采样，均匀分布里面采样，那就问题来了，他采样的这个长度应该是多少？它这个地方就很有意思，它的这个取出来的这个就整个取出来一整块。你假设想象成这个X1X2到X6，直接就把X123取出来了，甚至把1234都取出来了，甚至把123456全部取出来，但这个有点夸张，这就没有输入了，然后12345或者23456。把整个原文长度的50%，甚至到100%。因为有的这个文本生成就接近100%。
	但是他们原文里面是提到将这个到100%大面积的原文内容直接摘出来。摘出来之后，按照我们下面的这一套讲的这个方法去做这个训练这也是为什么后来我不知道大家有没有关注，上周一个很大的新闻，就是纽约时报去告这个OpenAI其实就是这个训练方法，我相信GPT肯定也用过的。因为这一类方法从这个从应该从体我从这个but开始都在用就这种文本填充的方法，长文本填充的方法。所以当时纽约时报就说这个OpenAI生成的一些内容，跟纽约时报里面的这个内容几乎是完全一样。大家可以去搜一搜那篇文章，OpenAI现在被纽约时报给告了，要求索赔几十亿美金。就是因为这个训练方法就这么训练的。
	我直接把原文当中的绝大部分做了一个mask然后拿来做训练，那就很有可能，因为他有可能他的前文条件概率的前文就是一个文章标题。比如说这个donor trump便成功出任说打败了什么民进党当了总统，或者说巴拉巴拉类似这样的一些标题性的文章。那后面的正文肯定就是一整块mask之后去训练，那这个效果肯定好。因为一些这些做这个叫什么记者写的这些文章，几乎都在做报道类的一些事实性报道。那你整块去做这个训练和生成，效果肯定会好很多，这个是一种。
	还有一种是句子级的，就sentence level。它就是第一它的采样，因为你是句子，它至少要保证我是完整的一句话。就我的这个mask不像之前我在一两个token，我是保证我每一个mask都覆盖的是一句话。大家就想象有一个一篇英语文章，我每一个mask至少都覆盖了一句话。当然一句话达不到这个总体的采样的数量上的要求，所以我一次simple就是去采样。原文当中的15%，然后这15%可能有上下浮动，因为刚好可能就差了那么几个token，都是完整的一句的话，而这些话把它mask之后，用同样的方式来训练。
	这两个simply的方法就比较适合用来做文本生成类的任务了，一个是句子级的生成，我补一句下文或者两句下文挨在一起的，我给你补充回来。还有一种就是我直接帮你写小作文。那这两种方法变成另一个目标函数，然后这两个目标函数共同变成整体的GRM要去优化的那个目标函数，然后我不知道这个跟大家表达清楚没有，所以整个GLM其实核心就是这样。它的训练方法，它的网络结构，然后它的不同两种类型的任务。NLU和这个text的generation的任务，其实就是通过一套统一的架构，然后不同的超参能够完成，在一个模型架构相同规模的模型参数的情况下，在不同的任务上下游任务上都还表现的不错，这个确实是有创新的。我们不得不说跑个分对吧？
	GOM到底怎么样？在这个是GOM这篇paper，就GLM130B是大家比较熟悉的一篇paper。我开始写的那个GLM是有一篇更早的一篇paper，就讲GM这个架构的，这个也是来自于GRM这篇paper里面的一个对比，所以你能看得到它有各种各样的下划线的这右下角的这个GOM，包括这个large的版本。
	针对这个文档的版本，sentence的版本，你可以做细分的优化，也可以把他们拉到一起优化。你可以整体有一个大的目标函数，也可以小的目标函数单独优化出一个GLM，这个都是可以的。所以GM本来本身天生就适合去干这个，你用不同的span的长度和mask的这个方法，就是能在不同的下游任务上去做训练。然后如果你模型规模大一点，你还可以把它做成多目标优化，一个统一的价值产品。多目标优化都具备这样的能力，这个是它这个网络架构天然带来的一些好处，通过这个实验结果，我们能看到，在大部分的任务上，GLM不管是这个base的版本还是这个large的版本，基本上都超越了这个bert bird，几乎成为了大家的贝斯曼。然后唯一的这个例外就是在一个WIC，就是词义这里词义去消歧的消除歧义的这个任务上，GOM的这个base，GM的这个base，它是比bert的这个base要稍微差一点，其他的都是有超过的。
	我们看到GLM的这个应该是下面的其他的这个我们看到其他的这个WCARTE之类的这个版本，其实是比bert明显是要有一些的进步的。然后它的模型规模也没有很大。这个是GRM这个网络提出的时候带来的一些新的很有意思的一些挑战。然后我们可以看到在GLM的这个模型里面，听起来好像很顺畅。就是几种不同的采样分布，然后去设计了3种不同的mask然后用一个同样是用通信的机制去训练这个网络，训练完了之后，他还能多目标的去做优化。
	那怎么样去做训练？因为我看最近这一两周大家开始上手做这个模型训练之后，都会有很多的问题。其中有一类问题问的最多，就是这些超参数到底应该怎么设置？有没有一个金标准？告诉我learning rate应该是怎么设置，我的这个drop pot要不要设置，然后我的其他的一些参数。怎么设置？因为我相信大家问出这个问题，肯定是自己在上手去做一些训练了，甚至是去看过documents里面去介绍这些参数的含义，但是就是不知道应该怎么用。这里我们正好就借着讲这个GLM它的这个预训练的超参数，跟大家对比一下，这个很有代表性。
	首先大家看到这里有三列，三列不同的超参数，但是都以GLM开头，先解释一下这三个模型，GLM base好理解，就是那个基础类的模型。GM large是一个更大版本的，我们从它的模型架构的参数就能看出来，就是我们前面这几行number of layers就是一个陈述，一倍的这个一倍深，它更深有24层，那个只有12层。然后这个隐藏层的尺寸也要大得多包括这个前馈网络，大家还记得transformer的结构的话，去想一想它有这个muti head attention，然后通过这个layer lam之后，是我们的FFN feed forward network这样的一个结构。然后它的这个FFN也要更大一些，attention head要多一些，就attention的这个头要多一些。
	前面这个是表达了模型的不同，这后面还跟了一个几乎跟他这个网络结构是一模一样的这这个是一个什么东西呢？这个其实就是揉bert a这样的一个网络，只不过是他们团队训练出来的这样的一个网络，可以这么理解，就是在做这个比较的时候，为了公平起见，因为bert确实是一个老的模型，18年的模型，而GLM是一个20年21年的一个模型，那你不能老指着前面的这个去跟他比，所以Robert a其实是一个bert的一个增强版。你可以简单理解这个RO就是robust这个鲁棒性的意思。Robust这个bert的版本会效果更好，然后性能也更强。GOM在同样的数据上去训练了一个robot的A，然后来做这个对比，是这么一个意思。
	那我们就来看一看他们的这个训练超参。首先这三个模型大家应该理解了，他们的这个超参数有哪些变化了。我摘了一下这个warm up steps是一这个变化的值，大家能看到明显有6K、8K30K。然后那这个warm up steps是干什么的？我们在训练模型的时候也会设置这么一个参数，它是一个什么样的一个值？大家想象一下这个学学了我们的梯度下降，在TensorFlow的视频课里我就跟大家讲过了这个梯度下降的这个过程。就跟下山很像，就下山要去找一个下山的路，下山的路，我们谁下到这个山脚越快，谁就是英雄，谁是一个好模型。然后下山的过程当中，如果你的路线选择不对，你走到了一个死胡同里，你就停在半山腰了，你就下不去了，你可能还得往他往山上走，在山上的某一个分叉口找到一条对的路才能下山，是这么一个逻辑。
	好，那么这里就有一个很重要的概念，就是我们的这个步长，我们的learning rate就是我们的步长。因为我们在每一步的时候，通过一批数据都会知道有一个方向。这个方向就是我们通过反向传播，通过我们的这个链式求导法则求了这个几阶导。然后我们知道这些模型权重，每一个模型权重的这个值应该减多少，应该加多少，就是我们的方向。只不过因为我们是一个特别高维的，所以模拟成一个下山的这么一个逻辑，而不是一个线性的向左向右的逻辑。
	这个过程当中，这个learning rate的意思就是相当于我这一步要跨多大。我不知道这个大家理解吗？就是我看了这一批数据，这一个batch best size给到我之后，我训练完了。然后我的某一个特定的参数我需要去做调整的时候，我应该调整多大。这就是narrate rate决定了我的这一步有多大，那么这一步有多大呢？
	Narrow rate我们可以设置一个值，但是这个learning rate设置一个值之后，它是固定的，那那是不是就不太好，对吧？因为你可以想象得到，你一开始走的时候大步向前，因为你知道下山的方向就是在那条主干的大路上，越往后走，越往后走越不好找。那这个时候就反而需要把这个learning rate调小。所以我们可以理解这个learning rate本身应该是一个有变化的过程。然后通常来说越到后面可能会越来越小，它会有一个衰减，是这样的一个状态。但是如果你到了这个后期，就是我们所谓的经典的训练过程当中，陷入了一个局部最优解到了一个鞍点里面了，你可以想象成就是你其实没有到这个最优的一个状态，你只是局部，就像你到了一个盆地，然后你到了一块盆地之后，你怎么走，这块盆地都是平的。你往左走，往右走，往哪儿走，好像这个loss都没有怎么变化了，你就感觉你好像已经到了最低点了。
	但其实这个时候反而需要突然一下把这个the rate再拉高一下，让你一下跳出这个盆地，再往四周看看，说不定又能走到一个更低的位置去，是这样的一个意思。所以learning rate并不是一个固定的值，它要随着我们的训练过程去做调整，大的逻辑是由大变小，到了一个阶段之后，有的这个optimistic优化器会突然去调整一下这个learning rate，但它可能不是直接调那个值，而是对我们的最终的这个delta去做一些调整，这个是关于learning rate的点。那么warm up的意思就是说，我一开始learning rate设置了一个固定值。那我在训练过程当中开始的这多少步可能他都不稳定的。所以我一开始比如说这个6000步的意思就是我前面6000步我不是用的我设置的learning rate，而是一个从小到大或者从大到小的这么一个过程，它是逐步到达平缓的the rate，这个是warm up steps的一个含义。
	然后通常来说，如果你的训练的总的部署增加了，那么你的warm up的steps都需要做一些相应的增加。尤其是你的数据规模和模型起模型参数的规模比较大的时候，你可以写死一个warm up steps，你也可以写一个百分比。就比如说warm up steps是在你所有的总的训练步数里面的1%、1‰都可以，这是一种方式。所以我们能看到几乎随着这个模型的增大和训练数据规模的增大，因为这个Robert a的训练数据规模比较大，它的这个warm up的steps是在增加的。然后这个batch size通常是能越大越好啊，你可以理解成你能够把你的这个显存吃满不报的情况下，被sih肯定是越大越好的。
	这就相当于你有个侦察兵对吧？你现在的就是要下山的这个场景，你能同时得到64个侦察兵给你的消息，他们说有大部分的都是往那走，那就好。但如果你的比赛设置是一，那你这回遇到的侦察兵水平不行，给你说了一个反方向，你这一步不就白迭代了吗？甚至融落入一个深渊对吧？甚至落入一个鞍点，所以这个是base size的含义。你的best size越大，那它肯定是越好的。前提是你的资源够用的情况下。那么这个YDK刚刚有讲，其实就是一个权重衰减的一个过程。然后它是在非常大规模的大模型上面，就我们刚刚说的这个learning rate可以拉大拉小这个VDK也可以用来最终解决这个拉小的问题。因为最终改的那个德耳塔是在这个权重上的那对于一些很大的模型，到后期需要金条的时候，它他的为了防止过拟合，或者说这个其实就是我刚才说那两种场景可以通过v decay来做一个调整，然后我们一共要训练多少步？
	我们有一个叫做max steps，就一共要训练多少步，可以去直接指定部署，也可以去直接指定你需要把这个训练集去轮询多少回。就是我们已经用过的一个参数叫做number train，epos，就是一个epos，就是一个完整的训练集的训练。那你这个地方值设置为一，就是把训练集完整的训练一遍，设置为三就把训练集训练三遍。然后每一遍训练的时候，你还可以给一个随机化的种子，重新打乱，重新排序，重新抽样。Max steps就是跳出这个数，我直接就是说我要训练多少步，两种方式都可以用来指定我总共要训练多久。Learning rate的DK也是一样的，就是我的训这个学习率也可以衰减的。随着我的这个训练步伐，然后的这个learning rate decay，跟我们用的这个learning rate的这个调度器有关。这个衰减函数我也是可以设置的，有线性的，有cosine的都可以。
	然后还有一个词，还有一个叫做梯度的一个裁剪，这个是用来干嘛的呢？就是你可以理解成我们模型大了之后，就是我们讲的有效精度的问题。模型大了之后，你的这个权重值都是一些小于一小于一的这个值乘完之后超过有效精度了。那么在计算过程当中，在这个反向传播的过程当中，就可以对梯度进行一些裁剪，以免造成一些不必要的梯度的爆炸的问题，这个是梯度裁剪这个参数的含义。
	当然这里还有一些他们用到的特定优化器。比如说我们看到的这个adam，这是一个很著名的优化器，是一个自适应的一个优化器。他其实就用了这个优化器的一些经典的实验值。
	然后有的同学我其实今天在群里说这个learning rate设置为这个十的负9次方，然后好像就不太好了。对，一般不太会设置成这么小，就十的负3次方到10的负6次方。然后问这个经验值的话，可以去搜一下。第一是收这个PITOCH和tensor floor，他们关于optimized这些优化器本身就已经实现好了，那它的默认值是一些经典的实验值，这个是理所当然的，大家想要到的那么去看看。第二就是你更有兴趣的话去读一读当年大家都会看的那些paper。就是我在讲TensorFlow的时候还介绍过一些优化器但现在这个优化器已经都不卷了，这事儿已经卷完了，几乎就卷到头了，那么就直接用就好了。
	这些优化器可以去看看paper里面关于它的一些经典实验值的一些取舍。就像我们今天讲na的一些经典的超参数应该怎么去取一样。大家还记得我们讲lora的理论课的时候提过rank是一个超参数，设置成应该是16就差不多了，太大了也没意义，徒增这个训练参数的总量。然后你设置成四都还行，不会特别差，但你设置成一可能就不太行。
	这些其实都是整个大模型或者说深度学习的这个技术迭代过程当中，有一些或者你想炼丹，这个炼丹有些炼丹炉越来越成熟了，有些开关手动的就不调了，就按照那个值去用了，因为要调的东西太多了。现在比如说我们用QLA的时候，像这些优化器的这些参数，都不太会去深入去给大家一个讲。只是说最近大家问的太多，还有很多没有这个深度学习训练背景的，我们来介绍一下。
	但是学习的路线方法其实就是这样的，原来老的这些超参数几乎都慢慢定下来了。但是你要说比如说我要OpenAI跟anthropic两家大公司要打打到最后其实还是老师傅炼丹的这个核心科技。就是他在一些特定的数据上，他就是知道要去调那个套餐。这个超产可能就是所谓的小时候听到的经典故事。就是有个师傅找出了工厂里面某个螺丝钉，他出错了改一下值100万，只不过现在这个螺丝钉变成了神经网络里面的某一个参数而已。但是不是每个人都要去当那个老师傅的。
	我不知道这个比喻让大家理没理解到，因为这个老师傅的技术，它不是一个可以直接教的技术。它是你不断的去修机器，不断的去针对不同的大模型和数据做过训练之后，得出来的一种甚至一种经验、一种感觉。他没法在某一个数据集或者某一个模型上就教会了，就是得熟能生巧。
	好讲这个GLM讲了很久的时间，但希望通过这1个小时让大家真的整明白这个GM的模型、训练方法、架构以及它的特定的价值。后面我们还有三周多的时间，都会大部分围绕着它来展开。所以把这个讲明白对我们有好处。既然有了GOM，那GOM130B就好理解了，他其实就是一个1300亿的GLM一个最佳实践。并且大家如果去看阿KV上面的这个论文，就M130B他们在去年10月份发了V2版本的这个paper，GM130B其实是22年底的一个文章，然后在二三年的十月下旬，他们更新了V2版本，更新了一部分的数据，包括这个实验的性能数据。
	那从这儿就能看得出来，首先GLM130B它在跟一些什么模型在比较？GPT3，OPT、blue palm这些都是一些经典的大模型。OpenAI做的这个GPT3OPT是facebook在后面推动的一个开源版本的GPT，他们是第一代的谷歌的大模型。现在我们听到的这个Jimmy就是谷歌刚刚发布的这个劈材发布的这个gm背后就是这个pm two的一些改进。我们能看得到GLM130B其实在整体的表现还是很好的，并且他跟这个GPT3比起来还是有一定优势的。这个是GM130B能拿得出手的一个成绩。
	然后第二个就是它跟这些同样的千亿级的大模型国际舞台上去比较的时候，有一些差异性的优势。第一个就是说我们看到在目标函数这儿大家用的都是GPT，这个都用的是GPT的结构。但是GM130B用的是一个blank in filing，就我们刚刚提到的空白填充，还有这个MIP是他在GM130B提出来的另一个方法，你可以理解成我们刚刚讲的前面这一部分blank infilling，就我们刚刚讲的前面的GLM那篇paper的整个训练方法。MIP是另一个目标函数，它可以再把前面那整个目标函数变成一个，比如说权重90%的，实际当中我记得好像paper里提的是95%，具体的这个值我不记得了，大家可以看一看，相当于再把MIP和blank in filling这俩目标函数都在合到一起去做一个共同的优化。然后这我就不再深讲了，这个是它的一个在目标函数上面的不同。第二个，他自己提出来了一个layering zone的方法，叫做deep loan，像训练效果什么的，还可以在paper里面有一些介绍。第三个就是他的训练的语言，主要的语言，我们能看到GPT3和OPT175B就是我们用的OPT6.7B的1751的版本，主要还是英语作为它的训练语料，当然现在像这个GPT3.5、GPT4早就已经是多语言了。那么bloom 176B是多语言的。
	然后GM130B在22年下旬的时候，就前gbt还没发的时候，它是一个双语的，也还是有一定优势的。白领狗的英语和中文的，然后它的训练的这个方法，FP16的，他们支持这个混合精度的bf 16，然后在量化方面它做了一些更好的优化。你看到其他的都是inter 8，它能做到int 4的量化的效果。因为我们已经教过大家怎么算这个显存占用了，艾特芭比英特4，你同样的模型就是要贵一倍的显存显存占用要多一倍。所以因为他用了英特斯的量化，所以它甚至能在四四张3090或者8个1080钛上面去部署起来，这个是130B，不是6B所以3333090就能部署于一个1300亿的模型已经很香了，不需要A100。然后英特斯的量化相对来说还比较稳定，它做到的可以能够以一个比较相对来说没有什么损失精度的一个前提下做到英特斯的量化。对于一个千亿级的模型来说已经很牛逼了。我们之前的英特斯量化都是在几十亿这个规模，千亿的话模型参数太多了，它要存的这个次数更多，你的精度到int四这个环节就会有大量的精度损失。我不知道这个大家应该都理解了，都讲过的知识了。它的实验结果也能看得出来，在这个zero shot，在这个叫LAMBADA nuda这个语言模型的比较上，zero shot的意思大家都懂，我们能看得到它是明显有提升的。
	最高的这根柱子就左边这幅图最高的这个柱子GLM130b by directional和GLM130B的单向的on或者叫什么uni directional on directional这样的比较。它的bad directional是效果最好的。超过了当时的这个pub，就第一代的谷歌的这个大模型。然后它的从右边也能看得出来它的英特四版本，就G1M的这个英特4版本是这个五角星，黄色的局色的五角星。然后它的这个英特斯版本的精度几乎也没有太大的损失。这个是他在不管是在量化前，跟当时最好的大模型比起来，或者说公开的大模型比起来，还是它的这个量化的精度损失，都还是一个蛮不错的一个状态。或者说都是一个蛮不错的一个可以用的一个很好的国产的大模型了。
	然后他的预训练过程其实遇到了很多的挑战，它是一个1300亿的，跟GRM当时的这个通用的的方法不太一样。GLM当时训练的，大家还记得刚刚paper里面的内容，都是几十亿，或者说几亿这样的一个规模。1300亿是他后面这篇paper很重要的一个进展。
	那么1300亿怎么训练？其实有很多的问题。就是你可以想象这个参数变多之后，就相当于你在的这个山，它就是就像你打游戏的时候，你在这个山，你人眼是看得到这个山应该往哪儿走的。
	但现实情况是你假设你把它模拟成训练模型的过程，你就可以假设你不是一个正常的人。你是一个盲人，你说一个盲人要下山得多难？然后你一个盲人要下山，你当然他可能大家都不是，我也没有这个歧视的意思，我只是描述这个困难度，就是如果你是一个视力看不清楚，然后你要去完成这样的一个挑战。并且这个山还特别复杂，它就像是好多个群山，你得下山，不是从这个山就能走到山底的，你还得下山再上某一个山。大家如果爬过那种大的山，比如说峨眉山，这个松山之类的，少林寺的那个松山，你就会发现你上山就不是说你看到那个山就能直接上去，你得先爬一座小山，然后再下山，下山之后走到另一座山的这个山腰，然后再往上走，是这样像环环路一样上去的那下山也得还得下来。那你想象一下，如果你看不清楚，你下来你会很难。
	那么实际你如果这个模型越大，就相当于那个山越复杂，然后它就是不好训练，那怎么办呢？就是相当于训练过程当中一定会出问题。那通常会有哪些问题会出现呢？
	其实就是这里我列到的一些问题有硬件资源的问题，也有训练过程当中这个梯度爆炸一些常见的问题。然后你要想不梯度爆炸，其实就是你不断的去调整那些超参数了。就像我们刚刚提到的，比如说梯度裁剪的参这个超参数，比如说这个学习力的这个超参数，还有就是这个模型架构本身如果设计的不够好，可能在中间过程它会造成一些非法的一些值。你可以想象成就是有一个侦察兵告诉你应该往那儿走，然后你又没有那个拐杖或者什么天眼，一步下去就深渊没了得重新开始训练，就像你游戏存档一样，回到上一个check point，重新来，所以这个是比较麻烦的，然后内存溢出或者说显存溢出也是比较常见的。
	还有就是像刚刚我们看到的GM那篇paper里面有写他的那个robot a其实就是没有把这个训练的步数拉到跟bert当时一样长的，它资源原因他训练不了那么久，就像我们现在想要在一张T4的卡上面去q lora训练一个ChatGLM36B，今天也给大家看了，你训练一个epoch，在我找的这个数据集上可能就要十几个小时。那你训练三个a pop就是两天两夜过去了，两天两夜对于一张T4来说不算什么。但如果你是针对的这个，你租用的是一个A100的集群，那可能就是非常贵的价格了。所以这个是某种层面上的很大的限制，那么GLM其实它是回应了这些挑战，但我就不去讲它怎么解决的了，这个又是另一个很复杂的事情，他的实验是看得出来这个梯度增长，其实loss反映的就是梯度增长的不是很顺利。到一个阶段，1500步的时候就会出现这种loss值的飙升不稳定，像OPT的1.5B其实也是一样，然后bloom的这个176B也会有类似的问题。那么真实的这个训练过程，其实它还是比较稳定的。它的loss是一个比较稳步下降的一个状态，但具体怎么做的，大家有兴趣可以去看一看GM130B的paper，这不是我们的这个重点，然后它的量化后的精度损失也比较小。
	这里有写写这个int 4，大家可以看一下，int 4 inter 8FP16，int四几乎没有什么下降，很很低的一个精度的损失。然后针对不同的GPU类型，它的速度在推理侧也做了一些优化。其中就使用了这个fast transformer这样的一个方法，包括使用这个deep Normalization来提速。
	简单来说就是怎么提速，就是你要做大量的矩阵运算，那谁能在这个矩阵运算的过程当中不丢失精度，或者几乎不丢失精度，又能提升运算速度，那不就能提速，对吧？你可以在硬件层面上，像今天在群里给大家贴的这个图，用tensor call。比如说英伟达自己从这个V100发布之后，这个vota架构之后，就疯狂的在干这个tens call。Test call就是为矩阵运算做的硬件，它在硬件级别上就非常舒服。如果你不能做硬件加速，我们都知道可以软加速，对吧？在软件层面上去做加速，但是软加速的缺点就是一定会有一次转译的过程，那这个转译就会有开销。但是如果你能转移的跟硬件适配的足够好，那么它也还是比较快的，比你什么都不做要好得多那么这个是GLM130B，但因为咱们其实都拿不到现在最新的这个版本，所以大家大概有一个了解就好了。它的int 4的量化版本其实大家看得到，最小只需要8个2080钛就能跑起来。
	然后除了刚刚提到的GLM130B以外，其实我们还能看到还有很多的一些GM的衍生品其实就是我们刚刚讲的对标OpenAI的，比如说这个web GLM是KDD的一篇paper，大家有兴趣可以看一看。简单来说就是把，web GPT这么一个对标的产品去做了一个GM的版本。Web GPT可能现在大家都不一定会去看到他的文章了，但是他现在都还在OpenAI的官网上，他曾经是21年的时候，21年的时候OPI发的一个你可以认为是ChatGPT的原型，就是让GPT3有这个web的能力了，然后去网络上去获取信息，这里就把GPT换成GLM就好了，所以你可以看到架构图讲的很清楚。
	其实就是让我们的GLM能够去获取到网页信息，网页上面各种各样的paragraph，爬取到的信息，查出来的信息之后，去反冲出一个检索器，这个检索器再去跟这个生成器一起去训练，训练之后相当于就能做问答了。那这样整套系统就可以做成一个，你提问题之后，我可以根据你的问题去网上找答案。找出一些相关答案之后我能够把这个相关的答案和我的知识一起去生成一个reference。然后这个reference再给到我下一个生成器。这里是两个模型，生成器在基于你的这个reference去生成一些答案。当然有多条答案，多条答案会有不同的信息，然后这些信息最终也可以打分，打分最后去选一个。
	这个流程大家我相信看过，然后也做过一些相关研发同学会有一种感觉很像RAG，就我们现在提到的RAG这样的一个检索增强的生成式的这种应用模式。RAG其实是它的进化版本，下一代的版本。因为你去想RAG干了什么事情，RAG其实是把这里的几个模型合到一个大模型里。就现在我们用RAG为什么要用GPT4，不用差的模型，就它的能力不行，它它无法同时兼顾这个reasoning和这个reasoning你要去做寓意理解，然后做这个目标拆分。然后做完目标拆分之后，需要去决定要调度哪一个tools。
	然后从tools里面拿到的observation之后，你还要去判断这个observation是不是达成了你的所有目标。大家可以去稍微了解一下RAG，不过我们后面也应该也会做对应的应用，就把ChatGLM封装成一个R仪器的应用。我们那个时候也可以给大家再详细的讲一讲，但可能chat GM6B不一定能像GT4表现那么好啊。然后那个时候我们就可以给大家看一下这个RG大概是什么东西大家可以做个预习，这web GM它有一个对应的这个demo。他在KDD的这个学术会上，他做了这么一个demo。相当于我们有一个浏览器，这个浏览器背后不是接的搜索引擎，而是接的web GLM这套。然后你问他问题的时候，他就能够去像刚刚看到的检索出一些reference，通过reference就可以去得得到一些答案。
	所以大模型的这个我这就跳过了，所以大模型的这个技术发展特别快，你看刚刚那个是KDD2023的一些结果，但是我们现在看来好像也觉得不够fancy了，反正就这样。但其实要做出来那个效果是很难的，就是相当于现在很多RAG是站在GPT4这个巨人的肩膀上。如果你想象当你没有GPT4的时候，你要做出一个类似的东西是难度非常高的这也是为什么现在OpenAI这么值钱的原因，就是现在我们看的习以为常的一些demo，你会觉得很简单，但其实要做出来很难。
	而这个visual JM其实是从这个ChatGLM6B针对对话做了优化的一个小的60亿的GM的对话模型上，有62亿。准确来说的话，然后通过leap，你可以简单理解成就是让这个ChatGLM这个6B这个62亿的模型，它有各种各样的语言的概念了。然后这些语言的概念背后其实就是一个一个的向量。然后这个向量让它跟一个视觉图像的向量也好，或者说高维的抽象出来的图像的向量去做关联，搭起这个所谓的桥梁，让他能够去理解一些就你给他一张图，他能理解这是一个什么意思。看图说话，就我们小时候一二年级做的这个事情。然后它的这个模型也不大，因为整个6P通过这个int 4量化，基本上就可以在大部分的消费级显卡上部署了。但是它的初代版本效果一般，现在几乎也没有再主动的去推这个版本了。
	但它也是一个很里程碑的一个作品，并且也就才发布了半年多一点，大家有兴趣可以去关注一下，就他当时给的一些这个demo，比如说描述一下这个场景，他写了这个泰坦尼克号，这个是他的图像视觉的能力，能够看懂这个图像内容。然后同时你再去接着问这个上下文，这部电影的导演是谁啊？这就体现了ChatGLM的对话能力，就他能记住前文这个是一个demo，他能记住前文。你问的这个泰坦尼克号，他回你的这个事儿是一个电影，他会告诉你这个电影的导演是谁啊，这是GLM6B的能力，然后这个聊天的这个场景是chat等等。
	你然后类似的比如说在GPT4的发布会上有这两张图片，为什么奇怪？因为他有这个语义理解和图像理解的能力，他会知道这幅图里有什么，然后有什么之后才能去推断这样的一个场景。很奇怪，因为一个熨斗的服务不太会把这个人挂在汽车后面。然后右边也是一样的，蒙娜丽莎的这个复制品，其中把蒙娜丽莎换成了一只猎犬然后这个cock VM是他的下一代的这个版本，也是唐杰老师团队自己发的。他其实就是开始去研究这个多模态了，简单来说就刚刚我们看到了维修GM是开始尝试一个小模型。几十亿的，希望能够把语言理解能力和这个视觉的能力兑在一起。
	但是后来我们都知道现在有这个所谓的视觉语言模型，visual的这个language model。那么这个cog VOM也是一样的，它就是有一个100亿的一个视觉的参数的一个模型和70亿的一个语言模型。所以它其实是一个17B就我们看到右下角，在各种基准测试上，它像这个六边形战士一样，打败了很多老对手们。咱这个扣个VM大家有兴趣也可以去体验一下，是可以直接去部署的。
	这个我们就做一个相当于开眼界的一个介绍描述图片，他做了对比，主要对比了这个nova 1.5版本和mini GPT4这两个版本。那么能看得到在这个示例上，前面两个是有问题的，主要问题是说识别问题，配餐饮料勺子什么的，勺子什么的没有识别。对，然后mini GPT4他的问题是把这个镜面反射的内容也识别进来了。所以他说有四碗饭，你看这四碗饭，四碗虾就不太对了，右侧这个刀叉也是不太对的。然后coke VM17B在这个表现上还不错，还有就是他跟GPT4V的一个对比。
	但大家就仁者见仁智者见智，就是问有多少栋房子在这个卡通的图案里，GPT4是说三个，因为这里有一个小的三角。我觉得这个可能是找的一些非常好的case。大家能了解到就是我们的visual language model，我们的视觉语言模型一定是24年25年的发展重点，coco VM也是二三年下半年发布了，所以包括GPT思维也是下半年发布的。我们接下来能看得到整个大语言模型多模态会是一个更快速发展的一个过程，因为大家其实资源会更聚焦简单来说就是它识别出来还有一个小的角落，应该是提醒人，其实那可能也是个房子巴拉巴拉的。但单纯从图像表征来看也不一定，所以这个就仁者见智者见智了。
	然后code GX2，对标着这个OpenAI的这个code x，然后是原本的code GX的第二代版本，也是KDD二三年的一篇文章。他在ChatGLM2的基础上，做了代码的一个预训练instruction ti，我们后面讲chat相关的一些技术的时候，会再引入这方面的理论的知识。然后通过chat GM2这个平台在上面去做代码的预训练之后，可以去做代码的生成。所以你可以想象这个就像乐高马积木一样的。那现在chat GM36B出来了是吧？那么code GX是不是可以有第三代，甚至code GX的能力会直接内化到chat GM36B的这个code interpreter，也就是我们的代码解释器的能力都有可能，所以整个基础模型迭代之后，很多都可以迭代。
	Code GX2它的特点是，他算是国内最早期做这个事儿，就做代码生成这个事儿，然后还做了产品集成的，它集成到了VS code jet brain，这两个应该是市场占有率最大的IDE里面大家有兴趣可以去了解。这个是一个它的一个demo示例，就在visual studio code，这个VS code里面，它可以去装它的插件，装完之后解释一下函数是什么。其实所有包括像github的copilot都在干这活儿。然后现在有很多创业的团队在做这个事情。
	刚刚讲了这么多模型想用怎么办？对吧？又又没有资源，还有face是一个好地方，我们在这个THUDM就是清华的这个data mining这个group下面，它在hugg face上同样有它的这个主页，然后也有这个space。我们讲过space就是在线已经部署好的，别人提供资源的一个demo示例，可以上去玩一玩。夸克V2M然后这个code jx，包括这个GM130B，都可以在上面去体验。
	然后我们刚刚讲的这一系列模型，其实都是去年一年甚至就是3到10月份这半年发布出来的。所以你可以想象二三年的中国大模型有多卷，这个质朴从GLM的第一chat GM第一代，到基于它的图声纹的这个就视觉理解的visio GM一个半月，又过了一个月，chat GM two就逼出来了。然后又过了一个月，这上面的代码生成的code gx出来了，code GX2出来了。然后长长文本的版本，chat m26B32K的版本出来了。紧接着又发了两个benchmark，一个叫做agent benchmark，就是我开始提的怎么样去评价以一个大模型为核心的agents的水平。反过来相当于就是评价这个agents，就是在评价这个大模型的能力，包括针对这个大模型长文本的理解能力的一个评测，non bench，后面做了这个检索增强的这个扩散模型就是diffusion，就我们纹身图很火的这个SD那一套，然后十月份又做了多模态。
	就我们刚刚讲的CVRM是十月份的版本，然后十月底做了chat GM36B，所以整个去年这半年是一个狂飙的一个状态。那么今天我们要用的也就是chat GM36B这个模型，chat GM它也有线上可以体验的环境，就这个1300亿的，大家可以去了解。右下角是它的这个链接，我不知道大家能，我知道bug了，右下角好像都会被我的人的这个脸给挡掉。大家在课件里面去看一眼，课件里面的这个右下角是有这个内容的，有一个UIL，所以很多的这个UI都看不见，从课件里面大家应该能捞出来。
	这个是质谱AI的ChatGLM的主页，里面有各种各样已经预制好的模板。简单来说就是他把各种prompt都已经设置好了，然后ChatGLM其实就是在GIM的基础上去做了简单理解，就是做了基于对话的一些优化，然后这个对话的优化使得它可以像人一样跟你聊天。不然我们学习了大模型就知道它就是个条件概率。这个条件概率就是你给我一个输入，我给你一个输出的概率，然后我再根据一个后处理的方法，能把这个概率decode解码成人能看得懂的文字，就这么一个逻辑。但如果我没有针对chat去做优化的话，我是没法像人一样跟你聊天的。所以chat的能力是一个独立的能力，跟GOM，就像GPT跟ChatGPT它这是两回事儿，这些类似的技术，我们后面会给大家去讲一讲，就怎么样把一个GPT变成一个ChatGPT，怎么样把一个GM变成一个chat GM。
	这个是最后一步。就是我们先教你有了chat GM怎么去高效微调，然后怎么把这个高效微调的模型用起来。再到后面我们会去讲怎么样把这个GM变成chat GM。当然不一定跟他们用的一样，只能说从公开的这个技术手段和数据集里面，我们可以去大概的掌握这个脉络和技术。当然不一定调出来跟他一模一样，但是这个技术是发展脉络，并且大概率你只要花时间精力去调，还是能调的好的，只不过这个数据质量会要求很高。
	然后第二代的这个模型其实就是在chat GM的这个26B上面做了一个迭代这个上面的主要的迭代的第一就是我们从benchmark的角度能看到它有提升，这里我们看到好几个像MMLU、GSM8K都是一些几倍的提升，非常夸张。然后同时，它的上下文从最早的第一代的两K扩展到了8K它的默认版本以及它的长上下文版本32K。然后主要这里用了一个叫flash attention的技术，你可以列成就是把token又进一步压缩了，还能做这个medical attention，使得它的推理的就因特斯量化之后能支持的更长，同时推理速度还能提升，然后这个协议也更友好一点了。在开发这个开发课上也做过一个讲解，深度的讲解。
	就是那么nama two和chat GM的这些模型的协议是什么含义？简单来说就是你别拿它去干一些违法的事情，在国内还是允许的，商业化的话你提前告知，那chat GM3其实是更进一步的一个提升。我们能看到它的主要的比较就是首先它有它自己的前一代的模型，chat GM two 6b base，他自己下面这一行是chat GM36b base。还有一个当前最好的基线，所谓的sota它挑选的几个测试集也好，benchmark也好，是全面超越了这个缩塔，所以很好啊，当然这个都是在同规模的，都是在100亿以内的上面去比的。非常突出的一个进步这也是我们要用chat GM36B的一个原因，你会发现这个很残酷，就是你在6 7月份，你在chat GM two 6B上面折腾了半天训练出来的一个模型。可能现在chat GM36B不用训练，都比你训练的要好啊，就是这样的一个意思。
	其实也好理解，因为质谱这么多人这么贵，花这么多钱，又有这么优质的数据，使劲在迭代这个基座模型，和你这个玩票似的空闲时间去迭代这个模型，肯定他们要好得多。它的数据更优质，然后它的特定的训练任务上面，也是专门去调过的，所以大家要去理解它。但是我们如果学了这个技术，就是为什么要讲理论课？要讲这一部分？就是这个理论课学完之后，然后你了解了这个技术。其实如果你的这个技术你是真的理解到位了，你在三代的模型上再去调，你还是能有额外的增长的，而不是说你就死守着这个第二代的模型了，你学到的是这个微调的技术本身，而不是说你学到了某一个特定的模型上面的几个参数配比，然后在一个特定数据集上你调成什么样了。这也是我说咱们做模型微调这个技术跟咱们去做什么，后端开发一个APP，开发一个web server很不一样的地方。
	因为你可能很多程序员，我的理解，从我招人管人的这个角度来看，很多研发是到处拷贝这个代码，把这个项目代码考来考去稍微改改，就完成了自己的工作。但是这个事情如果你要在模型微调上面去做的话，很容易摔跟头，你甚至都不知道错到哪儿。因为你得整明白那参数是什么意思，你才知道要怎么去调。
	然后chat GM3它也有新的一些能力增强，除了刚刚看到的基线提升以外，它又继续对标着GPT的迭代。因为我们知道GPT4出了这个function calling，也出了这个code interpreter，那这里它的two工具模式其实就是对标的。六月份去年6月份，OpenAI出的这个function calling的这个功能，甚至它的demo都是一样的，就是查询天气，它内置了这个天气查询的API，对它能直接查询天气。然后code interpreter也是一样的，它能够去把GOM当成一个代码解释器，能运行这个python的代码。所以我们能看到他就是说画一个爱心，然后生成了一段python的代码，然后运行了这个python的代码，然后出现了这个爱心，这个是chat GM3，它也有这个网页版本，这radio的这个网页版本的一个聊天框，这些都是后面我们会去部署的时候，给大家有机会长这个长线的去运用。
	现在我们还是重点聚焦到微调怎么做？讲了这么久，居然都已经37了，我们抓紧时间，怎么微调？其实PFT这个库我们已经用了应该算是用了有一两周了。大家PFT它的核心其实也是基于transformers这个库，专门去做了一个相当于在它上面在封装叠加了一层更高层次的一个抽象。但是无论他怎么样去抽象，我们去做微调模型的这个pipeline要不断再给大家强化。这里再强化一下，其实就是三步，第一步准备数据库，第二步训练模型，第三步去把这个训练好的模型去用起来做模型推理，其实就这三步，无非是这三部的细节会有一些不同。你换了数据，换了模型，换了微调的方法，对吧？
	那在我们这儿，今天我们要用的这个q lora，其实理论部分应该已经铺垫的很好了，甚至前面也用了这个BNB去跟大家讲怎么样去做这个q lora的前置工作。就是这些配置NF4的这个数据类型，double count这个双量化的和混合精度计算的配置。其实现在就是再往前一步，把这个配置好的PFT的模型拿去做微调。这个是量化，我们这节课讲的就是上周两节课之前讲的就是模型的量化。在transformers这个库里面，from free trade里面就能去做这个量化的加载。上节课我们讲了lora，要去实现。
	Lora其实不管是在whisper还是在OPT上面，核心就是要用PFT把我们的模型洗一遍，不知道大家有没有印象，prepare for 1，这个是一种洗它的方法，也还有很多其他的方法取决于你要用什么PFT。那这里我们就要用这个prepare model for key，KK beat training这样一个方法来对它进行预处理。预处理之后相当于这个模型里面的一些参数，大部分是NF4，还有一些打上了标签。因为如果我们要用混合精度计算Q罗A这篇论文理论课我们讲过，它是存的时候用NF4算的时候，还是要回到BF16的，或者回到别的精度的。它是做了双双量化之后，单纯是降低显存开销，但计算的时候在int 4在在NF4上面去计算是啊行不通的，所以它还会重新加载回一个更高精度。
	那么这个部分到底要把哪些模型参数拿来做？Lora, 这个跟q lora就没关系了，这是纯粹的Laura这部分的知识。就是我们这个Laura或者说叫做PFT的adapter，到底是要针对我们的模型的哪些部分去做训练，做微调。上节课我们也看过了，你可以针对QKVQ project和我们的这个v project，或者k project就是projection才对，不应该叫project。然后就它的映射，QKV的映射，或者说针对这个FFN，就我们的全连接层，我们的前馈网络也可以做激活也可以去做。
	比如说我们的不同的大模型，他们应该针对哪些模块去添加adapter。这个其实是transformers PFT已经预制好了的，很多的这个相当于帮我们解决问题了，大家可以直接用的。然后Q罗A的超参数应该怎么调？纯的话这个大家都已经用过了。最后有一个模型推理，就怎么样去加载这个QLA的模型，然后这就是我们今天整体的这个流程，大家一定要把这个流程给记下来，因为这个流程记下来，你几乎就能训练所有的模型，微调所有的模型，只是其中的某个环节你要去调节。然后大家再去回想我们第一节transformers的微调课的时候，那个quick star应该是就是这个的再简化版本，里面没有我们的这个PFT的量化的部分。但其实都是一点一点往里加，分布式也是在这个上面再去加一些内容，大家去由浅入深逐步去理解这个，一步一步跟着节奏来会感觉学起来很轻松的，好吧。
	那么再简单讲一下这个数据集，这个数据集是叫做advertise GG，这个间值就generation，就是广告生成的一个数据集，然后他在hugin face上也有人已经上传了，如果大家hugin face的访问很顺畅，那么就可以直接用，直接用就好了。然后它的输入是一个经典的content和这个summary的这么一个结构，什么意思呢？就是他的场景是这样的，你可以想象成你在淘宝、京东或者拼多多上买东西，你会有很多的这个类别标签。然后这个是它的输入，它的content，然后它的输出是什么呢？输出就是你假设你是一个销售，那就是你把那个场景想象成你在线上逛有销售其实是给你推的这些广告推荐或者直播，线下的商场。你就是有个人，那这个人通过这个content，这个content可以是用户给的，用户说我要什么类型的，我要裤子，我还是要衣服，我是要这个条纹的还是要什么鱼尾裙，这个是用户输入，我作为一个销售，我可以给你去做推荐。
	这个是advertise generation这个场景，这个数据集的场景，也是一个非常常见的场景。就sales销售这个场景永远是最赚钱的场景，也是最先应用新技术的场景，这个场景就很好理解了对吧？就是你你有一些需求，然后是标签化的，然后我给你的输出就是针对你的需求给你打了一些广告，这个是数据集。
	但如果大家很多同学这个hard face访问不方便的话，github上面这个TIGM清华质谱的这个HHM。然后他们自己在第一代的这个模型里有这么一个链接。就是他们当时用于第一代的这个6B的模型的P20的微调，用的也是这个数据集。这个数据集他们放在了应该是放在了，对，这里有写，放在了google drive或者是清华cloud。这应该是个FTP的一个文件处理，文件存储。我今天下载了一下，也能下载，就看大家的这个情况了。下载之后就从本地来加载就好了。
	然后我们去处理数据的时候，大家用token ized这个地方就用chat GM36B的token ized。这个前面教过的，token lizer和模型是配套的。然后其中这里有一个超参数，或者说这里有个参数我们待会儿去讲，叫ignore label ID，比较重要，专门贴了一页让大家去理解一下。
	然后这个就是我们讲的用PFT的adapter，核心就是我们的适配器。我们的这个旁边要加的这个网络，应该加到哪儿？那这里首先通过Laura config我们去配置了这个适配器本身，然后要加到哪儿呢？现在在PFT这个库里面实现了各种预定义，你告诉我你是什么模型，我就告诉你你应该加到哪儿比较好啊，然后我们待会儿也可以去详细讲一讲。
	然后最后有一个就是关于怎么样去去计算这个step是有多少步，这个我看群里有很多同学在问，其实很简单，我之前是在课上跟大家口述教过，念过一遍。这儿我直接把它写下来，大家记下来，放在课件里再好好记一下，记一下，就跟教大家这个大模型要用多少显存一样。这其实是非常简单的一个计算方法，但是很多同学可能不知道背后的原理，所以一定要把原理搞明白。很多事情就真的是一下就能看明白了。
	那我们这里简单跟大家讲一下，首先你要去训练多少步，有两种方式去指定。一种就是直接写max steps，我一般演示的时候会用那个参数来指定。然后max steps就是你硬指标，你就告诉他就去年那么多步。但更多时候可能大家会看到用这个number train a pox，就我这儿写的这个要训练多少个air pods，那么训练的总的步数，其实上面这个公式是万能的对吧？你训练了多少步，其实就是你每个每一次跑完一遍完整的训练集要用多少步，然后乘上你要跑多少个epoch，这个大家能理解。去年总步数其实这么算的那现在问题就变成了，因为number train import x是我指定的，我跑三遍，跑十遍都可以，跑一遍也行。那么每个epoch我要多少步，我需要算这个。那这个怎么算呢？
	其实也简单，就是首先我们知道这个数据集，你要用什么数据集你是清楚的。比如说现在我要用的这个数据集，训练集跟它的验证集分别的数字是这样的，训练集有114599条，其实就是11万个11万多个，example有样本样例，所以这个number train examples其实就是114599。我下面也把这个公式带进来给你写了，总的我这跑我要把这个训练集跑一遍，就把我的所有样例跑一遍，所以就所有的样例就是分子。那我一次一个step跑跑多少样例，这个我得算。所以分母就是我每一步跑多少样例，分子就是我总共有多少个example。那一除就是我这一个input可要多少个step？这个应该是很简单的，这个公式大家应该看得明白。
	那么我这一步跑多少个呢？这里会引入一个新的参数，大家可以先不管这个gradient accumulation PS，先看前面这个batch size，就是一批跑多少个，理论上来说，当我们没有后面那个参数的时候，我一批跑多少个就取决于我的best size。比如说我best size设置成16点，那就是我一次跑16个example，那假如说我现在或者说我娶个好一点的数字，假设我一次跑1万个一个ample，那我现在114000，那这个除下来不就12个step，就能跑完一个epoch，对吧？但实际取不到那么大，实际可能就取个16、32、64，取决于你自己的显存。
	那现在这个gradient accumulation p是什么意思呢？简单来说就是一个很形象的意思，就是大家看电视剧就是找这个侦察兵去说，你去看看敌军什么情况，豹敌方扎营安营扎寨两个团。然后这个时候有一个很经典的台词叫再探，我不知道大家看没看过，就让那个探子再去探一下新的情况。就是我现在先不做决策，我让这个侦察兵再看，再去看看，我先不更新我的这个梯度，那这个gradient accumulation PS就是指再探几次，这是我我用这个beside这一批数据，我训练出来了一个德尔塔，我不去更新，我再来一批数据，然后跑出来另一个德尔塔，我还是不更新，我就积累。比如说我的这个gradient accumulation steps，我设置成了三次，那我其实就是要跑三个batch之后，我把这三次的delta放一起一次性更新，这个大家能理解对吧？
	一次性更新那这个一次性更新的好处是什么呢？就是你要知道更新这个参数，更新这个权重它也是有成本的那我多积累几次这个best再去更新可以解决很多问题。就比如说我可以解决我的base size，我就是因为显存太小不够大。那通过这个方式其实某种程度上是把base size变大了，因为我的权重其实是没变过的。我跑了三次数据，我其实用的是原来的同一份权重，相当于某种层面上把best size变大了一点。
	第二个就是我也可以节约时间，所以这个公式也就好理解了，在gradient accommodate steps这个参数如果非不是取得一的情况下，那他这里取的如果是3，那相当于一次迭代一个step。完整的就是训练过程中一个style就跑了三个batch，所以你就除以三就好了，这个同学怎么还在问这个问题，为什么不直接增大倍size？刚刚在讲，没有听，你显存会爆的。好，我这就不再解释了。
	下面这个公式其实就是把我刚刚说的这个逻辑带进去，大家可以去验证一下。就我们跑一个air pod，然后example是11.4万，besides假设16 greedier iginla step，假设4，我们可以算出来，它一个EPOK要跑1790个step，那如果你去调整这些参数，自然这个也都会变，这些也都放到了notebook里面，大家可以去试一试。最后就是这个超参数，我们前面都提过了，我这就不再赘述了，我们用到的像这个warm up的racial learning rate，然后我们的这个optimization用什么？然后我们用不用这个混合精度来进行训练等等，然后最后有一个微调前后的一个效果对比，就是我们这儿看到的，就是我说的，大家可能会感受到自己花了很多时间微调，但是基座模型迭代了，你好像工作被吃掉了。但因为我们是学这个技术本身，大家不要太纠结。因为你会发现还是有迭代的空间的，要么就是这个数据其实还可以再去做一些精细化的一些设计，从这个对比来看，假设我们现在看到输入都是没变的，同样的一个输入，在同样的数据集上面，针对不同的版本的chat GM6P去进行微调之后，它的这个表现是什么样的？
	在这个第一代的chat GM6B上，我们能看到我们给了这个输入微调前的这个输出这个7IGM6B第一代它可能不能理解它的这个任务场景，所以他直接去做了一个格式上的调整，大家可以看到类型，然后这个显瘦风格简约图案撞色，相当于把这些井号键做了一个拆分，拆分之后去做了一些归类，然后他他是这样的，他完全不能理解。但经过这个数据集的lora微调之后，他能理解其实他需要的是描述性的文这个文字。好像一条简约而不简单的连衣裙，采用撞色的印花点缀，打造文艺气息。
	玩玩苏利亚队，然后交M26B，我们能看得到，第一它有变化，它在微调前它其实知道这样的一个输入，它是有下游任务的表征的。所以他开始有这种像广告一样的这种说法，但说的还不够好啊，就是断断续续的，有点像一个一个在吐这个字一样的一个一个往外蹦。比如说这款裙子版型显瘦，简约文艺风格。不是不不是你要听这个，你想象一下这个要是拿人的嘴念出来不好用的。微调之后他可以讲这款连衣裙简约的设计，撞色印花点缀，丰富了视觉。但说的有点不是特别的连贯，就感觉是堆砌文字。
	这是我训练了100个step，下午做到的一个迭代，大家可以搂一眼。首先我们看到输入，我想象得到应该chat GM36B自己就已经在微调过这个训练集了。所以它它的描述已经很接近了。但是通过你的微调之后，他会把这部分的权重再强化，所以它会更像是能做这个特定的广告销售这样的一个模型。因为chat m36B它也不能只是说做sales，所以我们能看到像江36B的这个微调前，其实已经说的很好了。
	连衣裙是女孩子们最爱的单品之一，采用撞色圆领设计，偶尔说一下嘴。然后微调之后讲的这款连衣裙，简约的圆领设计。大家会发现他会把散的就是这个，因为它上面的输入就是散的一个一个的tag，通过微调它的这个明显的，因为本身你给的训练激励就是一堆的标签，然后给你一个完整的句子。通过微调它会明显发现把各种散的标签，把它的顺序摘出来之后，会集合到一起，变成一些完整的语言。而且完整的语言尽可能上下文是比较贴合的。
	就比如说我们能看到他开始讲简约的圆领的设计，凸显出修长的颈部线条。因为圆领跟颈部是连在一起的，它不像微调之前一会儿讲是女孩子爱的单品，一会儿又讲圆领设计简洁大方修饰脸型，然后又讲到衣衫，又讲到袖子，然后又来一遍简约的款式设计，它就很乱，它就是没有去针对性的做sales的这个方向的微调。而下面你能看得到，它这个讲衣服，然后又讲袖口和裙摆都是压褶的。这个设计有层次感，有艺术感，然后整体的什么样的彰显出什么样的气质。所以这个哪怕是100个step在NVIDIA的T4这样的卡上，所谓的只能拿来做推理的卡上，也能去微调一个60亿的模型，甚至是用这个Q罗A来微调的，效果还是不错的。
	所以大家也不用觉得好像大模型就真不是个人开发者能玩的了，其实也不是很多的技术，它还是很能够不断的迭代之后，能把一些以前做不到的事情，就能让个人开发者也都能做到了。那我们实际来看一下这个代码。首先是这个代码其实在开课之前已经提交上去了。大家可以看到这个github可以去folk或者说去同步一下做一下。然后我们看我有个很好奇的问题，就是到时候可能我回头也看看后台数据，我看到只有folk 79个人，star 81个人。所以是不是咱们其实几百个同学，只有不到100个同学在真正跑代码？如果是的话，这个得赶赶一赶进度。
	我们看到PFT里面新增加的这个叫PFTQ lora chat GM，这个是用来做训练的。然后PPFT的这个chat m inference是专门摘出来把我们微调好的模型加载起来做推理的。这是两个不同的notebook。然后我们这也加了这个超链接，你点完就可以跳转到这个inference这边来，那我们现在去实际看一下。
	这个代码我们这个hugin face的主页，THODM这里加了一个reference链接，大家有兴趣可以去看一看。然后开始提到的space，可以在这儿去体验，这里回去玩一玩。他们还发布了很多的models，包括这个dataset，后面有一些我们也会用到。那么回到这个QLA本身我把一些全局变量摘出来了，方便大家去理解比较重视的一些变量。
	这次我还没有加验证的这个流程，我们是准备下节课会把整个notebook的形式，会把它做成一个python的一个PUI的文件，然后用PY的文件来启动一个长期的训练，然后把整个notebook，这个也是一个家庭作业，大家也可以提前做一做。就怎么样把我们的这样的一个notbe交互式的环境，这些代码整合成一个PY的文件，说叫做train点PY的这么一个文件。然后在里面把合适的这几个步骤封装成对应的函数，然后我们就能够快速的去更换不同的数据集来进行训练，这个可以作为大家的一个作业，下节课我也会做一个对应的实现，这里是一些重要的一些参数，是如果我们要去提取出来的话，是可以去做一些特定的操作的。
	在这儿我们先看一下最重要的两个模型和数据集。模型是使用的THUDM的chat GM36B，看这个的话大家就能理解了，其实就是用的我们在这儿的model这里就用的是这个模型。数据集用的是这个数据集，就我们在都在hugin face，一共有train和validation，已经帮也分割好了。然后这个data viewer大家如果没用过的话，可以用一用，挺好用的。就是方便你能够查看这个hugin face上面的数据集特征。包括它的输入的一个长度的一个分布，输出的一个长度的一个分布，然后这些东西叫这些东西我们才是实际跑一跑吧很好的显存有没有被占用。
	没有，我们加载这个数据集。
	可以看到，下载下来之后114599，然后验证集是1070。我这里没有用验证集，大家也可以把它加上。
	怎么加验证集？大家应该都明白了，在training arguments里面加一个，然后这个是专门用来展示样例的这段代码已经出现过很多次了。我们看到这个是输入的content，summary算是我们要训练的输出，比如说这个类型裙子材质蕾丝、公主裙设计上不不不标准答案相当于然后token ized，chat GM36B有自己的token ized，需要去按照它去加载。然后这里有一个叫做ignore label ID，这个其实是下面我们会用到的，在tok nize function里面会加到的一个值，设置成了-100，什么意思呢？就是说有一些token ID我们大家都学过了前面的课程，知道token either要干的事情就是分词映射，最终变成一个token ID那有些token ID是没有什么实际用处的，这样我们看得到，需要填一个值。那这里我们设置成-100，是通常在你可以理解成就是我们会不管是什么模型都有一个token either。这个token either都会有一个词表，这个词表和这个token ID是一一映射的。而这个-100通常它就不太会有人去用，它是一种就跟我们写代码里面的一些占用的关键词一样。
	而-100就是一个大家都默认不会用这个词，所以它就反而成了一个用来做忽略的一个ID，也常常被用作一个也算是一种行业惯例，或者说大的都会用的一种默认值，就这么一个意思，我怕很多同学不知道这个概念，把做做了一些注释。这个执行了，看看。加载一下这个token ier，然后我们定一下这个token的token ize这个function，它要对我们的每一个example进行处理，然后把我们输入的这个内容，我们的这个content变成这个question，变成问题的形式。然后答案就变成这个summary，变成这个答案的形式。然后还需要去对这个文本进行一个in code。我们把它展开给大家再看一看，token nice到底是怎么工作的。其实这个流程我们很早就讲过了，这一个notebook就希望大家能再完整的回忆一下，到底训练有哪些重要的流程。
	Token nice需要先encode，转换成对应的这个token ID，然后去构建完了之后返回的是一个这样的形式，就input ID和这个labels的这样的一个形式。我们可以再去看一眼，这个就处理好之后的形式了，-100就是用不到的，需要忽略的这个labels。然后这里我们用这个dataset的map方法就可以去针对整个训练集，这里只用了训练集去进行处理了，这些都是学过的，我们就不再纠结了。
	然后这里再介绍一下，我们需要对数据集再进行一个相辅，就是打乱。你可以理解成之前的这个有一个顺序，十多万有一个顺序，我就把这个顺序打乱一下，打乱怎么打乱呢？也可以给它一个随机化的种子，这个种子我随便设置的是8，就是大家可以随便去调这个值。
	然后sufder之后，我们还需要进行一个flatten，这里有讲这个逻辑是什么？简单来说就是这一步是为了打乱，然后再去删除这些不必要的索引映射，让它不会乱。因为之前有索引，但是它的索引失效了，因为我打乱顺序了，关于这个的解释，后面也有给这个相应的链接。但它的功能就是这样的一个功能，但是大家也不用担心打乱之后这个好像效率很低了，这里不用担心这样的一个问题，因为整个transform CS里面本来就有一个批量处理数据，在训练的时候，批量数据处理数据的一个类，就叫做data collect。很多同学问这到底是个什么玩意儿？你可以理解成它就是用来把我们训练的时候，我们不是有batch size吗？还需要从训练集里整一批数据变成一个batch，就是通过这个data collector来实现的。不同的数据，有的数据用默认的data collector就能处理了。
	像ChatGLM它要的这个形式略微不同，所以需要自定义去处理一下。主要就涉及到，因为你要自定义处理，你想象一下处理什么？一个是长度，长度不一样。因为你要处理这些数据的时候，无非就是padding，你需要padding或者截断。那padding的话你得告诉我最长有多长，这个参数要给到。第二个就是ignore的这个label ID你得告诉我，这两个写清楚就好了，然后他会去进行这个处理，这里我们实例化，传进去这个pad的token ID就好了。
	然后训练模型，怎么训练呢？这里就是我们学过的内容了。上次我说只差一步我们就能进行q ora就是这里。首先BMB这个transformers已经设置已经预定义好的这么一个config，就已经实现了q lora里面的这些量化技术。所以大家能看到，这是我们在讲transformer量化，就是上上节课的时候就教过大家的。我们把这几个黑科技都用起来，然后这里这个参数也很重要，就是我们提到了它纯是用的NF4这个量化类型。
	在存算的时候，我们其实是有很多种选择的，包括今天有个同学用的这个P40，b flow好像不支持会报错。因为它底层就是不支持它的孤单，可能就没办法拯救它了。硬件上不支持酷大也没办法，那他可能就只能使用FP32或者FP16了然后这个是我们在Q罗拉理论篇都介绍过的技术，我这儿就不再赘述了，加载一下模型，DS map，这个是用用auto这个参数可以享受到accelerate里面的一些加速，也都是讲过的，大家如果觉得这都陌生，赶紧回去复习去，才八十多个人，这个跑步代码没有问题。
	然后我们能看得到其实应NF4的这个量化类型去加载这个六十多亿的chat GM3，所用到的显存就是三千多兆，3700多兆，不到10个GB，3.5GB左右。然后我们需要对这个模型量化之后的再进行一次像黑魔法一样的洗一下，对吧？那么q lora需要使用这个prepare model for k beat来洗。我们之前用int 8来做这个微调的时候，用了一个叫做int 8。我不知道大家有没有印象，我们看看给大家回忆一下，对比一下OPT6.7B的时候，我们用过对称的一个方法。
	呃。这里叫做prepare model for int eight training，这个其实就是让我们的PFT库支持用int 8这个精度来进行training。洗一下就PFT这个库的神奇之处，它帮你做了很多这种dirty work，杂货类似的。
	Q lora它有一个prepare model for KB at training，大家如果有兴趣可以再去看一看，这些都是已经讲过的概念了，我们给他操作一下这里有一个警告，这个应该是因为chat GM36B自己的这个网络模型里面，用了一些老版本的这个format，就是检查点文件的format。大家不用管。我相信质谱后面应该也会迭代的，等他们迭代之后，你去拉取它的新模型，就不会有警告了。但什么时候迭代，这个可能得看他们，但不影响咱们使用。
	那么经过这个洗完之后，变成了k bit的一个model。这个k bit的model这里要讲一个新的小技巧新技术了。就PFT我们讲了，其实lora就是给它加一个模型，旁边再加一个小网络，然后把要调整的这个德尔塔放到小网络这边来实现的。那么到底这个要加到哪些模块？上节课我们其实是给大家展示了什么模块都能加，对吧？就是敞开了让大家先把PFT玩一玩。
	这节课我们教你的是他到底应该加哪个，那么有一些经验的，比如说这里给了PFT库，这里有一个constant点POI文件，这个文件里面就是你可以认为是一些经典的实现经典的改造。我们在这个库里面可以看到我们现在用的这个都是一些常量，那这些常量，我们现在在用lara对不对？再用这个Laura，那么我们就去找lora，这个常量叫做transformers model to lora target modules mapping。名字取得非常直白，就是你现在是一个transformers的模型，你要使用lora这个方法来进行微调你的目标要调整的模块，模型里面的什么模块都预定义好了，GPT two的话是这个c attention，contest attention，然后bloom是query key value，然后OPT是Q和v project，那如果我们现在用的是ChatGLM对吧？这里也是query key value，大家还记得GOM这个模型，我们刚讲了很长时间，那个self attention mask的那个论文里面的截图，query key value在这儿需要去加一个lora的adapter来进行微调，所以我们就可以通过这个方式导入就我们刚刚选中的transformers modules，transformers models to nora target modules mapping, 我们现在在微调chat GM就给它拆GM。
	如果你在微调别的模型就去微调别的模型。如果你微调的那个模型的key刚好不在这里面，那你需要去了解一下这个模型其实是基于什么模型改的。比如说这个mr，它会有一些变体，但是这些变体如果还是用的这个架构的话，你就写m miss tro也是问题不大的。就像假设你现在在微调的是ChatGLM2，那你也可以用这个，因为他们没有什么大的变化，模型架构没怎么改，主要是改了这个训练方法，然后那你就直接可以沿用的。就比如说明天GM4发布了，它只要模型架构没变，你还是可以这么微调你可以看一下他查出来的这个就是query key value这个query key value就是你的low。
	Laura的这个apter要去调整的这个模块，然后这里是一些lora的超参数，上节课讲过了，我就不讲了。然后你需要通过这个get PFT model，这个也上节课讲过的这个黑魔法。我们的PFT里面有一个get PFT model，就可以把一个原始的模型和一个Laura的configure结合到，相当于把量化后的基础模型和这个lora adapter变成一个完整的PEFT的模型，大家不知道是啥意思的。
	去复习，然后打印出来它的实际要训练的参数总量1.5‱，trainable的这个参数是1.5‱，因为整体的这个参数是非常多的，62亿，然后我们实际训练的不到100万。当然你可以把noa的这个参数做调整，它会有变化。这个都讲过的，rank、阿尔法dropped out，dropout不影响，主要是rank和阿尔法不影响。然后这个训练的总步数怎么算的，这里刚刚讲过了就不提了。这里我留了两套，主要是一个用来演示的，一个是用来做这个实际训练的时候需要的。然后我把一些注释掉了，大家可以看一下，就我们刚刚提到的这个best size和这个gradient examination steps，把这个加上。我给大家的超参数都是一点一点去给大家提的，没有一次性给你们教很多东西，希望大家能逐步的消化吸收，然后后面就知道这些参数怎么用了learning rate，这里是使用的这个epo x来决定训练几轮。
	这个learning rate schedule type用的是线性的。我们刚刚看到GOM的paper里面也有提到可以用cosine，这里用线性的就可以了，六十多亿的还好warm up可以写steps也可以一样的写比例，就是预热的这个比例占整个steps的一个比例。Login steph和这个保存日志的一个策略等等的都讲过了，然后我们可以用这个，当然我们这个演示就用下面这个也没问题。他就是写死了，我就跑100个这个step，然后我也不用评估什么的。如果要做评估的话，大家就把上面的这个数据处理再加上这个validation的数据就好了。我们看到这有两部分，把这个data set也用token ization的跑一遍，用map方法跑一遍，跑完之后传到这个里面来就可以了。就在实际跑的时候，在这儿有一个参数可以加evaluation dataset，这之前也都是用过的那我们先看一下，就假设这儿我们定义好啊有一个trainer开始训练。这个差不多100个step在NVIDIA t4上面，应该是会跑将近1个小时左右一个小左右的时间。然后下午的时候我跑出来这个模型，在这个是在demo里面。
	对，差不多就1个小时。从第一个checkpoint到最后一个check point有1个小时的时间。我跑完之后，同样的我们可以通过这个trainer下面的model的save free train的方法，就可以把它保存下来。保存的是这个adapter，大家记得存下来的是这个adapter。
	用lora这一套方法去存模型的时候，它只需要存adapter。因为它原来的基础模型是没改的，是没有动过的，为了让大家理解这个事儿，我看上节课讲过，但是很多同学不信或者没没整明白，我专门把这个influence做了这样的一个区分，这里我就停下来了，就不训练了。他占用的这个GPU的。我们就用这里已经训练好的。看一下显存释放没有。好，我们看一下这里他用的是models demo，就是我现在这个路径下的给大家确认一下，没有耍赖。
	PFT models，PFT就是我现在这个文件，它下面有一个models有一个demo，有一个models有一个demo，有一个THUDM chat GM36B，就是我这接下来的这一串6B，在这个里面是我的PFT model的路径，换句话说这个是我的adapter的路径，我看看，然后同样的，我之前是怎么去q lora的，然后怎么样去加载这个量化的模型的人都照搬过来就好了。但这里要注意这个base model，大家细看这个base model和我q ora这边的model其实是一样的。在在这儿，就这个model这个model和我这儿加载的base model是完完全全一样的，都是从hugging face上加载的。原来的这个模型参数没动过，大家看这完整的加载了七个，甚至也没用它的半径度全是用的这个NF4的方式加载的。
	加载进来之后，这儿可能我稍微缩写了一下，这是用的torch的float 32这个格式，这儿用的是BF16，这个是加载它的一个格式，不重要，训练的时候会调整过来的。你可以理解成然后微调前后的一个效果对比，这个就能看得出来了。这个是input对吧？然后这个token ized，是也是用的hugin face上面的chat GM36B的token ized。因为我没有改过token ized，我没有新增词汇表，词汇表里面动过，那么我们可以把这个打印出来，肉眼输入，然后这个地方是微调前的，你可以理解这微调前的base model没动过。
	然后chat GM3或者说所有的chat GM6B都支持一个chat方法。那这个chat方法其实大家看官网都有啊，下节课我们会去细讲怎么去应用它。Chat方法然后传入的一个token iser和一个query，然后执行一下。
	有点久。
	好了，这里有一个微调前的一个版本，就是从hugging face上刚刚拉下来的，那这个有点不同，大家细看首先model这个model是啥呢？是PEFT的model，这PEFT的model怎么来的呢？From free trade从一个base model再加上我的PFT的，它就可以加载出一个这个就相当于我这儿训练好的，训练好之后相当于我们看在哪个位置在这儿，这有一个q ora的model，其实就是这个模型，这是一个完整的模型，它是基础模型加上LORA的adapter，然后参数也都有啊。就是既有基础模型的，又有lora a的adapter，这个模型的参数的在这儿，所以我存的时候它只会存adapter，因为剩下那部分没必要存，它没变化。那么这里也是一样的，通过这个方法我就可以加载回在刚刚那个训练的notebook里面的那个完整的模型。然后用chat方法可以看一下，就微调后的。对base model是一样的，所以你能看到它是有明显的变化的。然后你也可以用这个方式只保留一个adapter，然后你就能够去还原回一个q ora之后的完整模型，至于这个模型要怎么样再去做下一步的应用，然后怎么样去做更进一步的一些改造什么的，我们会放在后面的课程。
	今天就是要教大家，第一搞明白什么是GLM，然后chat GM36B跟前面两代是有一些变化的，我把它对应的这个输出也放在这儿了。大家甚至要去实际去输出的话，就把这儿从三改成2，甚至把它干掉。就第一代把他们这些前辈的模型搂下来之后，运行一下就能实际看到它的输出了，代码都不用怎么改。第二就是把QLA的这个流程，这套经典的怎么样做QLA的流程搞搞清楚，然后接下来这个方法是可以不断的去运用，不同的数据集，不同的模型都可以按照这个方式去做。好，这个就是我们今天的上课的主要的一些内容，看大家有什么问题。
	Target module是一个列表，是否真正可以传递多过一个可选值？没有看懂这个问题的同学你再写清楚，我没看懂什么叫微调过程同时进行训练。
	这个同学说的报错，一看就是你没把Laura adapter加到那个量化模型上。我不知道你改了什么东西，还是怎么回事。你这个就是他的报错信息已经说的很清楚了，微调以后会不会出现遗忘的问题？会，所以看这就看第一你的基础模型好不好，就你基础模型架构好不好。第二你的数据怎么样。第三你在微调过程当中，你的一些超参数的一些设计等等，会不会造成一些问题。比如说你的训练集，其实训练集的影响是最大的。如果你的训练集就是1万条一模一样的话，就一直告诉他一些信息反复的在那洗，那他的那个值就会被刷的很奇怪。
	Chat GM3微调之后性能变好的主要原因是什么呢？是因为你的微调的数据集都是针对下游任务的，这个还这个不理解，我一直在讲fine too，我们的语言模型针对下游任务去繁重，是一个非常经典的模式。不管你怎么迭代，你只要是要把它落到一个具体的任务上，你在那个任务上面的数据集去微调这个模型，它在这个任务上就会表现的更好，这个事情就跟你有这个新工作，有试用期一样的。你这个试用期里面，你就在学这些工作要做的这个主要的内容。然后好的模型就是三个月试用期就能上岗了，差的模型就三个月试用期就被开掉了。
	多目标会导致参数多，为啥G2M可以小规模参数做到很好的效果？直觉上是这样的。但是有没有可能之前的几千亿的参数压根就没用满没用好呢？就大量的神经元是浪费的，对吧？其实是这样的，就是太浪费了，现在的神经元的用法太铺张浪费了。
	还有同学问这个target model module，这个是个列表什么的。去翻上一节的作业和上一节的这个notebook，去理解一下那个target mode是什么意思。那个本来就是用来去给这个基础模型加这个旁路网络的一个参数，那当然可以有多个。上节课我们不是加了N个，这个同学是不是上节课没跑过代码？
	这里。为什么上节课要全给大家列出来，就让大家知道这参数什么意思。然后这个OPT6.7B本身是一个它的规模比较大，是用来给大家展示技术的，而不是拿来做最后用的端到端的微调模型的，好吧。针对content和summary是怎么处理的这个问题没有看懂怎么处理的？其实代码写的很清楚，这个同学是在问这个处怎么处理，是在问什么问题？
	如果这个注释都看不明白的话，那得再补补课了。推理的时候，lora和基座模型是不是参数合并了，推理速度上没有增加。这个看怎么理解，就是你用QLA，然后你加载它的时候，第一它的精度变低了，你消耗的显存总量变少了。第二就是你通过这个方式微调，你的微调成本很低。第三就是那个推理过程当中新增加的参数量是1.5‱，所以它没有增加多少推理的成本，因为你的精度变低了，所以你原来没法做的事情可以做了。就是你要理解成他不是在说原来我可以做，现在我要做的更好。而是说原来我做不了，现在我可以做了，QLA是解决这个问题的。我们到十点半结束。
	Gradient accumulation p增加的时候，显存不增加是否正确？是的，你几乎可以这么理解，它应该是不会增加的。但是这些问题我都希望大家最后能动手去实践一下去玩一玩，对这个实践出真知，经过自己验证的技术和知识是最牢靠的。
	看大家还有什么问题吗？关于这个大家都去跑一跑，就是所有的大模型也好，这个部署也好，多用一用遇到一些现实的问题去查问题，解决问题，比较能快速成长和进步。对，如果只是听看，那就跟听这个评书相声似的了，那也能听个乐对，但是可能技术不一定真的学到自己手上了。
	看大家还有问题吗？对，然后还有关于他跟module那个问题，大家不懂的去看看刚刚我给你们看的那个OPT6.7B，然后去看看这个consistence，去看看就理解了。然后我之前就讲过，为什么要先讲理论篇，不讲理论，这些key就跟天书似的。我估计这位同学看见这个key头都大了，什么QAQ project什么东西，对吧？经过前面的讲解之后，我相信你们应该能理解这key是什么意思。不理解的就是前面的知识学的不牢靠，得再回去学习。
	Chat GM的token是如何切分的？毕竟中英文差别挺大的，这个不是我们的微调重点，可以你上上网稍微搜一搜就好了。对，这个分词的技术，中文分词的技术很多年了，有很多的技术流派。
	然后这个auto organizer里面也有一些预的预预定义的实现，chat GM36B它有它自己的实现。但那个不重要，你可以理解成这个事儿，就跟就跟咱们现在在学开车，你要考驾照了。你问我这个V6和V8的引擎，它这个转速到底有什么区别？这个六缸和8缸，为什么这个八纲要快一点？这个八纲快在哪儿？其实是一个意思，虽然不一定恰当，就是它不是现在我们学这个技术的重点，你用就好了，难道你还要去自定义token er吗？不会的，你现在还没到那个时候，你还没有到有一天需要你自己去修发动机的时候，你先把这个驾照拿到手。
	然后关于token nize的这个原理和逻辑，其实我们在讲transforms这个库的第一节课就讲过了。然后就跟我刚才举的例子一样，就是已经教大家这个发动机的组成原理了。但是教了组成原理不代表每一个车的发动机，我们都要教它是怎么组成的。是这个逻辑，你可以去拿着这个原理去看啊，chat GM36B他是怎么样分词的，然后其他是怎么样去分词的，好吧，看看大家还有什么。
	十点半了，那行，要不我们今天就先到这儿，大家有什么问题我们群里面再问。我们确实今天拖堂了半个小时，讲的比较深入。这个GIM多花了一些时间。大家如果有关于这个代码的问题，可以回头实际跑一跑，遇到问题我们再交流。一定还是得手上多练一练，才能把这个事儿给整的特别通透，不然的话就总感觉云里雾里的。但实际跑一跑你就会发现，就跟我设计的这些notebook也是有这个意图的，有的是参数全部放开让大家玩，就是让大家不要遇到这个参数整不明白就先去试试。越然后到这个节点就是开始收敛一些，有的参数会跟大家讲应该怎么设置，然后有哪些最佳实践，有哪些默认参数，有哪些参考。然后后面也会有这样螺旋式的让大家去不断的熟悉这些库、这些概念，然后这些数据集、这些模型就会方便很多了。行，那咱们今天这样，咱们周日再见，谢谢大家。