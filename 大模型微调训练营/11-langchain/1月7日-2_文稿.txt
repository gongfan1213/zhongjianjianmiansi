	好，大家都上来了吗？
	这个确实是刚刚还在说基础设施，我们国家这个网络要访问海外确实是一个比较困难的事情。
	行，那我们还是把刚刚这个事儿收一收，我们回到REG这个知识上面来，还是回到RAG这来。我们接着刚刚讲就能欠跟这个IG应用的一个关系，RAG这个检索增强的生成式的这种应用，南茜是一个很中心的状态，连接了三种不同的技术。Embedding model就是把我们的文本变成高维向量，也就是咱们在transformers里面用过的这个token，neither它的一个工作。然后向量数据库它的出现也完全是因为这一轮大模型的技术的一个出现，而伴随着一个的存储形态。我们知道数据库是一个已经有很多年历史的一个技术了。那数据库为什么一个这么基础设施的很底层的一个基础设施的技术，还会现在有新的迭代？其实也是因为上层应用牵引着我们的存储形态，存储介质的一个变化。
	我们知道在web时代有大量的数据需要被存下来。那会儿大家存的都是存到像mysql，或者说这个分布式的这种多组多重的这种mysql l集群里的，或者说其他的一些关系一些数据库。因为我们在web这样的一些应用里面，更多存的是一些关联关系，我需要去做这种多表的这个join查询。
	然后我用这种关系型的结构，我可以把这些字段都结构化的存的非常的你说符合集范式都行，完全是贴合着业务来走的那现在的向量数据库的出现也是一样，我们整个大禹语言模型处理的其实输入是这个自然语言。但所有的自然语言通过token inter之后都变成了高维的向量。而这些高维的向量在经典的关系性数据库里面其实是不好存的。
	大家去想象一下，这个mysql我印象当中应该是在5.6还是5.7版本的时候才支持。在这个mysql的关系型数据库，它的一个单元格里，你可以理解成像excel的单元格一样。它的一个数据存储介质，一个储存储单元里，才能允许存放像节省这样的数据。但是如果你存一个非常大的节省还是会出问题。
	对于向量数据库来说，它要处理的数据全都是高维的向量，就一个一条数据或者一个数据，它就是一个几千维的一串数字。这些数字你说放到传统的关系数据库里面，那你把它放到一个特定的这个数据数据单元里其实也不合适。因为没有这种基础数据类型。我们写代码都知道，高维的向量通常放在了数组之类的这种数据结构里，这种结构不算基础数据类型。那如果不是基础数据类型，要存到基础的这个向量关系性数据库里是很困难的那向量数据库就是为了解决这个问题而提出来的一种新的存储形态。我这个数据库就是用来存向量的，就是用来处理这些embedding的结果。不管是输入embedding还是我的计算过程当中有一些结果我需要存下来，最终它都是面向这种高维的向量来提供方面的存取存取服务。所以我们看到南茜也能很好的去对接他的这个关于向量存储的和读取的这样的一个生态。
	我们看到RNG里面的这个4和5就是一个典型，当然大模型就不用说了，南茜本来是一个语言的链条，连接的就是大语言模型。那一个经典的rng应用，其实就可以被这八个步骤给抽象出来。用户提出问题是自然语言，这个自然语言通过南茜发送到一个特定的in bedding model。这个embeddable model如果是像open I的evening model，大家用过就是一个线上的服务，跟你调GPT的API1样。如果是我们用的transformers，在hugging face上面下载的那就是那些特定的token ized通过这个embedding model，把我们的原来的自然语言的question变成了对应的向量。
	这个向量在RAG里面，简单来说就是向量数据库是我们大语言模型的第二大脑，它可以存储已经回答过的这个问题和对应的答案，也可以存取一些存储一些外部的知识，就比如说我们的搜索引擎查询到的知识，比如说我们最新发生的一些新闻等等。总之就是在训练集之外的一些信息。或者说为了提升，因为大模型处理速度很慢，为了提升我们的响应速度，存了一些相应的内容，答案等等在向量数据库里。然后这个答案，通过我们的这个特定的检索方法，也是一个rng里面的R，我们可以给它检索出来。就比如说在这幅图里面，我们看到第五条这条线上面写着top k的相似的这个答案，相似的向量其实是啊因为它是一个QA，我们把它叫答案也OK的相似的前K个跟它相关的向量检索出来。这个过程其实我们在教transformers的这个微调的时候也讲过，就是我们讲这个句子的生成的时候，要生成连续的好几个词。那好几个词都有自己的logit，这些logit都是一个一个的vector，那这个vector最终要解码回这个内容，那这些前K个相似相关的结果从向量数据库里被拿出来之后，会再统一给到大语言模型，相当于作为一个参考，由大语言模型来完成总结、润色、翻译等等各种各样的事情，都可以通过你的第六条的prompt来做决定。
	就比如说我们的open x translator，它这就是个翻译，比如说auto GPT，那这个prompt可能写的就很复杂等等。但无论如何，一个REG通过这样的一波流程之后，就把用户的问题变成一个你要最终使用的大语言模型配套的embedding model，处理好的vector，这个vector去向量数据库里看看有没有相关的信息可以提取出来，有的话给到大模型作为参考，大模型最终整合出一个答案给到用户。冷却就完成了他的使命，把这三个不同的技术流派、技术栈连接起来。当然除了这三个以外，能切还能连接一些其他的一些技术模块，这个我们就有机会再会给大家提。但这三块是最重要的。
	既然提到了这个RAG的应用，也提到了它的一个标准的流程，八个步骤。那除了RAG以外能签还能做什么？它的基础模块都是一些什么样的抽象？我们可以看看在这幅图里面，其实是非常底层的一些抽象，通过这个流程图的方式给大家做了一个展示，最上面的这个左上角的这个红色的大语言模型，里面竟然就有私有化的OpenAI，也有开源社区的having face等等。
	那么除了大模型以外，大模型要用起来最重要的就是这个prompt，这个prompt我们知道它叫提示词。但一个真正的应用其实就是要不断的去复用你的提示词。然后对于大模型来说，prompt又是它的一个输入model的input。所以在整个能源里面，其实prompt做了大量的设计，就比如说它有各种各样已经预先实现好的proved的模板，这是我们可以直接去参考学习的。第二个就是prompt如果要被复用起来，针对不同的大模型，那它也能够去做到无缝的复用。只要大模型本身对于你的这个设计好的prompt能响应出一个好的结果，它就能够进行复用。那么有了prompt，其实还有一个大模型的输出，叫做power。我们后面会去讲，就是我的大模型输出的结果怎么样能够规范化，变成一个我想要的一个格式，然后除了这两个以外，还有一个很重要的功能就是memory。
	我们都知道学了大模型都知道，其实大模型本身是一个概率模型。你给我什么输入，就相当于给了我一个条件概率的前置的条件，然后我自己这个模型是一个分布，你给了我前置条件，给了我这个分布，我就知道他最终输出的这个最大概率应该是一个什么样的结果。但是这是一个单轮的一个，或者说这是一个一轮一个轮回的一个事情，round one结束就没了。
	那为什么ChatGPT能跟你聊天？是因为OpenAI它实现了memory的功能，他能记得住之前你跟他的对话内容，哪些是你说的，哪些是他说的那对于一个裸露的没有经过任何处理的大模型，它不具备这样的能力。所以需要由应用开发者自己来完成这个记忆的功能。这部分在南京里面叫做memory，有了memory之后，我们看到就能做各种各样的大语言模型的应用了。也就是我们左下角能看到的这个agent，agent就包括这个应用要被实际运行起来，还有一个它的运行时，它运行时最典型的内置的实现就叫做agent的execute。然后大模型它自己只有一个大脑，但是你要真正把它变成一个智能的应用，它需要连接各种外部的工具。所以年限也实现了预定义了。很多tool kids都在agent这个大模块里面。
	那我们还提到有一个模块叫data connection，数据的这个连接转换怎么样体现data connection呢？其实我们这里看到的这个红色的四个模块，包括这个chain和最终的应用，这些流动的线条上面都是各种各样类型的数据。这些数据其实就是data connection这个模块要解决的问题。因为在不同的阶段，比如说原始数据可能是存在各种各样的不同的数据媒介上的。有代码，有文档，有视频，有音频，还有不同的网站它的存储形式形态。格式都不一样，它最原始的形态变成一个蓝线框架可以处理的统一的结构抽象。然后变成向量数据库里可以存的向量，最后能查出来再为大模型所用，整个链条上流动的就是data connection要解决的问题。
	而欠这个抽象要怎么理解呢？就我们刚才讲的是这个管道里面流动的内容是数据，这个管道可以被定义成券，就我们看到这一根根管道，那管道为什么能够，为什么需要管道呢？那很简单，通常就是我们可以想到的，你一轮对话解决不了我的问题，我需要多轮的对话才能解决问题。甚至我这一轮对话里面跟第二轮对话里面，我的大模型它的prompt都不一样。这个大家应该能想象，就想象一下ChatGPT你跟ChatGPT聊天的时候，你们每一句话，你的prom是不是都不一样。那你这些不一样的prompt，确实我要把它记下来，就我有memory的能力能把它记下来。但同时我要把上一轮的信息给到下一轮，应该怎么给？这个其实也是一个问题是直接的就把这个信息原封不动的抄下来，还是怎么样去处理，这些其实都是欠需要能够回应的一些问题。
	这还只是一个对话的场景。假设我们是像alt GPT1样，它的整个运行流程里面，大模型的定位是完全不同的。有的时候他是在思考问题，应该被怎么拆解，在做OKR，有的时候他是在解决具体的问题，就是我拿到了一个KR我应该怎么去做，我应该要向外求有哪些工具，向内应该怎么样把这个问题再细分拆解，去做对齐拉通。
	最近有一个电影很火，就把这个互联网黑化。但其实大模型它可以分成很多个不同的角色，每一个不同的角色通过你给他不同的prompt，他就能够有不同的角色定位。然后通过这一堆的基础模块的组合，最终我们可以做各种各样的应用。就包括RAG，文本摘要，聊天机器人等等等等，包括这个代码的理解生成，都是可以去做的。只不过像RAG是当下最典型的，因为它的实现方式，它的起步是最简单的那我们来看一看具体的能签的模块第一个要讲的就是这个model IO的模块model IO顾名思义就是模型的输入和输出，model inputs outputs。那么模型的输入输出其实我们可以怎么理解，就是为什么南茜是一个在当下这么备受关注的一个框架，是因为整个南茜其实提出了一种新的概念和想法，你可以理解成南倩他提出之后，至少我自己的理解，他是希望让大部分的写代码的同学研发能够认识到你学到的很多编程语言，其实它也只是一个过渡产物，它是一个短期的历史的产物。
	这个事儿其实也不用去争辩对错，它就是一个实际发生的事情。为什么？大家回看一下现在你学习的这一门编程语言，你最擅长的这一门编程语言，或者说研发的框架，它才出现了多长时间？它的生命周期有多久？然后历史上有很多不同的编程语言开发框架都消失了。为什么消失了？
	这个其实是一个很灵魂的拷问。简单来说就是软硬件的迭代，互联网技术的迭代，应用的需求的发展，使得很多的技术它就是被历史淘汰了不管是我们刚刚讲的基础设施的故事，还是说你包括像汇编语言，这些底层的指令，我们现在也都不会用了，都被编译器的自动化的编译给干掉了。所以大量的这些中间产物，就我们给这个机器对话的这些编程语言，长期来看，未来都不太会存在这么多语言了，因为它一定会被替代掉。
	你可以想象成把大语言模型当成一个新时代的编译器，这么理解可能会好理解一点。就是你如果有一个更远大一点的愿景的想法，去想象十年后的编程是怎么回事儿的话，你就能理解了。就像我们现在不太能想象当年的那些程序员是怎么写汇编指令来编程的，因为它效率太低了。但是你十年后再看今天的我们，怎么会有一个研发拿着产品经理的需求去一个一个的去写这个页面呢？你会觉得很扯，你也不太会接受一个研发拿着一个运营的需求去数据库里给他拉报表，写分析这些。其实这些工作你说起来感觉很fancy，效率好像提升了，但它其实也是有更平替的方案的。这个平替的方案其实就是南线和大模型未来可以做到的一些事情。
	就是怎么样把以前很多重复的，没有必要的这些不管是代码生成还是文档的生成一些工作由大语言模型来帮你做，其实就这么简单一个逻辑。而这个过程要去理解它，就是有一个很核心的一句话我也一直在讲的，就是你要理解怎么样用自然语言来编程，这个事儿其实是一直在发生的。大家去回看编程语言的发展历史，从非常早期的这个汇编指令，到后面的CC加加，到后面的java pad，其实整个代码就是包括python。为什么这么多人这么多用户？就是因为它挺接近自然语言的了，然后他对人无限的宽容，因为它是弱类型的语言。你就算没有一开始定义好这个变量，你直接写了也没关系，他不需要你提前去声明和定义。然后如果你没有给他说清楚它是一个什么类型的数据类型，就是它是一个字符串还是一个整形也没关系，解释器会帮你做这个事儿。这些其实都是这个编程语言在不断的迭代，让编程语言更像人，就是让人更轻松的能跟机器交互。
	自然语言是我们天生使用的这个语言，如果能够用自然语言来让机器理解你的意图，那这个事情是很夸张的。所以你想象一下未来其实就算还有很多门编程语言还存在的，因为这个轮子已经造好了。但至少我们会拥有一个相对统一的一个入口。
	这个入口可能就是一个基于大模型研发的应用，或者编译器，或者IDE，他接受的就是你说的自然语言，然后你给他的可能就是一个maybe假设可能你就是一个产品经理的角色，你给他的就是一个PRD文档，你给他的就是一个高保真的设计图，然后他直接把对应的代码生成给你。这也是现在最热的一个创业方向了，我们回到这件事情来理解，就是这个刚刚讲的都是很远大的理想。那我们现在做到什么样的程度了？我们要怎么样能够一步一步达到那儿？现在我们拥有什么样的技术？我们回过头来看这幅图就很好理解。就是这幅图前中后三部分就分为了模型的输入、模型和模型的输出。
	模型的输入我们刚刚提到了自然语言，像自然语言一样来编程。我们学过编程，写过代码的同学都知道，有一个很基础的抽象叫做函数。函数抽象了一个特定的功能或者业务流程。而在这个函数里面我们都会知道有一定的输入参数变量，那些变量决定了我的这个功能，针对不同的变量，它可以处理出一个统一的结果。
	我们再用这样的方式去理解提示词模板就非常好理解了。就比如说我们现在看到的，左边这幅图，它其实有两个阶段，第一个阶段是上面X和Y分别是两个变量，下面这个部分其实就是一个提示词的模板。最后我们给到大语言模型的叫做提示词。然后中间的这个步骤，他们俩合在一起，这个步骤叫做format，也是能签里面的一个方法。就针对提示词模板这个抽象，你可以定义一个模板，定义它的一个变量。通过format方法传入这个变量具体的值就可以变成一个真实的prompt，它就都是真实的值了，都注入进来了，动态的，然后再给到大模型大模型也有两种类型，一种叫做LLL就大语言模型。
	这个大语言模型是比较相对来说没有那么高级的大语言模型。因为它没有对话的能力，它可能只有生成的能力。换句话说，他没有身份的抽象，他只能说你给我一个输入的内容，然后还你一个生成的结果。就像我们前几周使用transformers处理的很多几十亿的规模，甚至几亿规模的模型，都属于这个LLM。
	这个chat model其实是针对对话去做了优化的，专门去做了对话的能力提升的大模型。就比如说OpenAI的这些GPT的模型，或者说这个ChatGLM的模型，它都是针对对话专门做过优化的，所以它有角色的概念。叠加了我们刚刚讲到的memory这个记忆的能力的话，那他就能记住哪些话是什么角色说的。如果这个模型更强一点，同样的角色还可以有多个人，比如说这个assessment a assistance b，他们扮演了不同的角色，这些抽象的最终在应用层面上就会被体现成不同类型的agents。
	这个是模型的部分，模型拿到了prompt最终会输出一个结果，比如说who does一大堆，然后我们希望它输出的结果是一个统一的格式，因为下游可能还要针对这个输出的结果再去做一些处理。这个时候我们可以有一个抽象叫做模型的output poser，就根据你的pozza r的类型。把你的模型输出结果变成一个我预期的一种特定的结构，不管是Jason的形式，key value的形式，还是一个比如说其他的这种字符串模板的形式。咱们如果用过这个python的话，应该就会很熟悉这个format的这个功能字符串。好，这个是一个最基础的一个抽象，通过它我们能够把大模型的输入输出做好一个定义。
	有了这个抽象之后，我们要进一步去使用大模型，怎么样去扩展？第一个就是大家可能大部分都会用，也都会接触到的，也是很多大厂都在第一时间做出来的这个迭代。就比如说牛逼大语言模型在这个第二代的病的搜索引擎上可以联网了。
	那就是通过这样的一个方式，我们想一下，这幅图一个大语言模型的应用，它得有这个提示词模板，当然更重要要有这个大模型。但是这个大模型的输出结果，我们通过format通过这个format得到了front，通过大模型得到了输出，通过pother把这个结果变成了一个特定的格式。比如说变成了几个重要的关键的给搜索引擎的关键词。那这个搜索引擎拿到这个关键词又有一个搜索的结果，这个搜索的结果可以再填到我们的下一个模板里下一个模板再经过方面变成第二个阶段的大模型要做的这个内容，然后这个结果再给到我们的用户，这是一种非常常见的应用形态。所以你会发现通过model IO我们就组成了一个最基础的单元，就是我们的提示词模板大模型。它的输出model IO3个部分又是一个整体。
	像我们刚刚看到的这个图里面，三个部分是一个整体，它就像一个最小的一个单元，一个函数一样，然后这个函数你要去理解它，提示词模板这部分定义了函数大模型执行运行的这个函数，相当于把这个函数实际的跑了一下，像我们去run一个代码一样，run出来的结果就是我们的这个out of partner。好，除了联网以外，我还想把数据持久化，这个时候我们就可以引入这个向量数据库。这个向量数据库也可以放在大模型的输出这个位置，可以把直接把这个大模型生成的结果放进去，也可以把这个搜索的结果放进去，也可以把搜索结果和大模型的结果整合之后的内容放进去。其实这都是OK的，这就比较简典型和经典的实现了。
	我们把这个demo展示到时候放到一起，我们先往后给大家讲，那接着就是这个chance，就是怎么样去把刚刚说的这个最小单元能够在更好的抽象和复用，因为我们刚刚感觉还是有点散。就是有model IO了，然后这个model IO它有prompt，有model，有puzzle。但是这三部分比较抽象的概念叫model IO那有没有一种实际的概念把它框起来？就像我们写函数，能不能把这个函数规范的写一个什么define，或者写一个class之类这样的一些语法，把它统一的规范起来，有一些语法堂的设计。那这里其实欠就是这样的一个功能，我们最好的理解chance其实就是乐高，这个是我自己想到的一个比较好去理解它的概念。大家想象其实乐高是一个很神奇的存在，它你说它复杂，它能造世界上各种各样的玩具，甚至还能加一些复杂的模块进去，之后还能坐火车、坐桥等等。但是你说它的基础组件也很简单，就像我们右上角看到的，就那么几种，单个的，然后两个的、三个的，有没有三个的我都不知道。可能三个的可以由这个叠加起来，就是它的这个是一个巨奸的，这样应该有三个在我们这儿看到的1234，这种不同类型的模块，再叠加一个颜色，它就组成了构成这个乐高世界的基础大。
	那么能线其实也是一样的，大家去想象一下，我们要做一个复杂的大语言模型的应用，也要找到最小的单元。那最小的单元是什么？我们看这幅图就很好理解，我们要把这个大语言模型应用的乐高的最小模块找出来，其实就是聚焦到这里看到的大语言模型的这个一front输入和这个大语言模型本身，也是我们这里看到的这部分我们红框新框的一个框。
	在model IO里面，你可以理解成作为一个大元模型的应用。我希望每一次你的生成结果都能被我服用起来。就像我们开始看到open a translator的时候，它有一个翻译，然后有一个译文的结果。然后这个过程如果能够被一个统一的方式定义起来，那未来我不管是干翻译，干这个任意语言对的翻译，不只是英文到中文，可以全世界几百种语言互相翻译，还是做这个文本摘要，还是做识别，都能够被一个统一的抽象给划出来。然后划出来之后有一个定义。这个其实就是最基础的欠的产生LM欠它的目的其实就是为了什么呢？整合一个语言模型和一个提示模板，然后把他们的这一组，因为这个其实就是配套的。
	大家想象一下，就你不同的语言模型，它的模板可能是不一样的。因为它的响应机制不同，你同样的一个模板你换三个不同的模型，那它生成的这个质量是不一样的，这个我们学过了，前面的理论篇都知道。因为你训练的时候，你的这个提示词使用的不一样，它的响应机制也不同。我们在学soft prompt这一类微调的时候讲过它的原理，所以提示词模板跟大元模型通常是配套的，那这个配套的抽象是可以被拎出来的。因为即使你换了一个模型，它也还是需要提示词模板。只是这个模板具体的这个值，可能就不是这么说的，可能他不会再说does x like y and y，他可能会换一个说法，但表达的功能还是这个意思。
	那么AM券在南茜里面要怎么样去实现呢？我们可以看到这个代码其实非常简单，从人群里面我们需要导入模型和提示词模板。所以你能看到上面这一行导入了OpenAI下面这行导入了提示词模板。然后通过OpenAI这个open I其实就是一个GPT3这个模型的一个实例化，那我们可以直接就把这个LLM，就我们刚刚这条线里面的上面那条线，因为GT3它没有这个角色的能力，实例化了一个大模型，就跟我们transformer里面加载了一个大模型一样。然后我们定一个提示词模板，就比如说这个提示词模板是要给公司取名字，然后它的变量是input variable，只是一个产品的名称，就基于产品然后取名字。那最后要运行起来，怎么运行呢？有一个很好的run方法，也是能欠里面经常出现的一个用来执行的方法，欠点run。然后我们通过AM chain把这个大语言模型和这个提示词模板，这个prompt是这个提示词模板构造在了一起，那么它就可以去运行，运行之后相当于就完成了我们刚刚这个链条。
	把这个X其实就这里的product，然后负就是这里的性能卓越的GPU，然后通过把这个性能卓越GPU填回到模板里，其实给到大模型的输入是给制造性能卓越的GPU的有限公司取十个好名字，并给出完整的公司名称。那你下一次要运行它的时候，你可以把这个GPU换成汽车，换成什么都可以。那这个方式就很像我们在调用一个函数的时候，给一个函数的参数一个不同的值，它就会像运行这个函数一样。这个chain点run就像我们在跑这个函数一样。其实背后是雕龙了大模型，大模型拿到这个prompt给出了一个结果，是这样的一个流程。
	除了我们刚刚讲的这种最简单的方式以外，我们都知道真正写代码的时候，不可能一个程序里只有一个函数，它有很多的函数。我们学了编程也知道有很多的经典的程序设计方法，面向过程的、面向对象的，面向切片的那我们首先想最开始学的大部分的同学都是学的面向过程的，因为它最简单，一个流水线。类似的在南券里面，我们能不能把每一个1MM chain就像一个函数一样，然后做一个面向过程的一个设计？这个CQA就是我们要干这个事儿的，它就是把一个调用一个AMT的输出作为另一个的输入。
	然后在这里面还有一个更简单的抽象，就是simple sequent chain，它就是一个单链条的，就像大家想象一个工厂的流水线，就只有一条流水线，这条流水线它的上游的输出就是下游的输入。而sequential chain n它更通用一点，它可以有多个输入，多个输出，就相当于这个流水线里面输入有多条线，输出也有多条线。你可以选择在每一个节点上取得是上游的哪几个作为输入，然后下游的这个输出，然后说下游他要拿到你的这个结果，他也可以不处理，直接再递给更下面的人，这个都可以。那sequential券其实就是一个我们刚刚讲的simple sequential券，其实就这样的一个模式，2个LM券。他们一个串联的结构，像上一个的输出结果作为下一个的输入。
	所以这里你大家就能想到为什么要puzzle了。因为AMG b他对于你的这个上游的输出我是有一定要求的。就像我们一个比如说工厂，然后在照精准的消费电子设备，那他可能上一个人他得把这个元器件螺丝给上拧拧紧一点，才能给到下游的人，他才能能去安装，这么一个意思。如果你们按照这个方式去拧紧，那下游再去装就会出问题。对不上那就没法作为我的下游的输入了。
	我们举一个具体的例子，那就比如说第一个AM chain他是一个创作者，他需要根据一个标题来写出一个戏剧简介，就比如说这里它的输入这个上面部分就是它的input verb，它的input variable的一个具体应用的时候，我们给了一个值，叫做三体人不是无法战胜的。那么模板就是你是一位剧作家，根据戏剧的标题为该标题写一个简介，标题是巴拉巴拉剧作家。以下是对上述细节的简介，这是一个prompt，通过这个format的过程就变成了真正的prompt大语言模型生成的结果。在这儿他写了这么一段话，就是我们的戏剧简介。那这段话就会作为下一个M千的输入，好理解。那么下一个输入就在这儿，我们看到有一个psychosis chain output，这个sops chain就是我们刚刚定义的这个chain，它的输出作为了我们第二个欠的变量，你是一个纽约时报的戏剧评论家，根据剧情简介撰写一篇评论性剧情简介。就是上上一个券直接输出的，结果没有做format而没有做这个part。
	以下是来自这个戏剧评论家的评论，他有一个评论，三体人不是无法战胜的，是一部不是一大堆，那我们串起来看，其实就是一个这样的结构。第一个券它的变量是title，这个title加到这个prom template里面之后，变成了真正的prompt。通过第一个欠生成了一个结果，这个结果变成了第二个欠的变量，syn opposes，然后变成了真正的第二个券要的这个prompt，这个prompt通过给到第二个大模型，大模型生成了一个评论，他们整体组成了一个simple sequential chain的抽象。
	而这样的一个抽象，如果我们要自己写代码是很恶心的。大家想象一下transformers的优势是让你不用去管这些模型的定义了，就是模型是怎么定义的，网络是怎么写的，你都别管了。你就通过transformers的这个from free train，一行代码就加载进来了。但加载进来之后，你要把它用起来怎么办？对吧？你要写各种各样的方式去实现它的推理处理器。那能欠需要帮你解决的就是你别管这个怎么去调用他们怎么样去管理这些提示词模板，他来帮你去做好这一层的封装。
	所以比如说我们要实现刚刚的这个流程，在南茜里面就从它的chance模块里导入这个simple sequences chain。那这个simple sequences chain要实例化，就传入预定义。好的，这两个圈是offices和review。这俩就是我们就跟我们刚刚取名字一样，就是两个基础的AM chain，就像这个乐高一样。
	大家想象一下我们最开始用model，最小的模块有prompt tempt，最小的模块拼在一起变成一个AM千。那就像一个乐高里面的小房子或者小人一样。然后通过simple actual change又把它们串在一起，变成一个小的流水线，这个小的流水线的overall chain也是预定好的，然后它的rain也通过rain这个方法，三体人不是无法战胜的，这里就会写。然后因为我把这个verbs这样一个参数设置，为了true，它会把中间结果打印出来。
	上面的这一段就是我们的synapses chain的输出结果，就是我们的第一个AM chain第一个大模型基于它的这个prompt生成了一段剧情简介。第二段的这个结果就是这个review chain生成的一个戏剧评论。就通过这么几行代码就能实现了。
	那如果是多输入输出的这个串联调用，我们也可以指定。比如说有两个inputs给到第一个check。然后第一个券输出的结果可以一部分直接给到最终，就相当于输出了最终输出了一部分给到这个AM券，第二个目前它来做处理，这样也是OK的。
	就比如说我们这里使用的这个sequential chain，然后我们就能够把首先我们输入可以增加一个时代背景，不止是title，输出我们刚刚的输出是因为我们打印了中间结果，所以我们才能拿到中间的那个部分，就是那个synaptic is券。大家想象一下，刚刚那个流程里面第一个chain的输出，我们是没法直接拿到的，我们拿到的是最后的这个输出结果。大家想象一下，就这幅图，我们拿我们只能拿到review，nop says我们其实拿不到的。因为流水线你可以想象你这个手机的流水线，你是拿不到半成品的，你是拿到最终的成果。因为是simple sequent券，就这么定义的。那么sequential chain它是说你可以指定中间哪些成果你也是可以拿到的。就比如说这个output variable里面定义就是我们最终输出的内容，我们把cynon pis也放进去，这样就能把synopses作为一个我们最终可以直接拿到的结果。所以我们看到通过这个方式定义出来的这个multiple，就是多输入多输出的这个overall change，它其实就是secure chain的一个实例化，它是最终拿到了我们的synapses，也拿到了我们的review，当然输入也都能拿到，就是这样的一个抽象，也很简单，定义它。
	然后我们除了刚刚看到的这些欠以外，其实还有很多就是我们面向过程，我们有最基础的我们的面向过程。那么内嵌还有一个很巧妙的抽象叫做transform chain，它是干什么的呢？你可以理解成这个玩意儿就是把应该这么去打比方比较好。就是我们能券是把所有的应用都做到了一条流水线上。但是你要有一个前提是你如果想要去参与进去处理这些内容，你得首先能进到这个工厂，变成流水线上的一环。
	在刚刚我们的代码里面，你会发现这个南茜，他在处理过程当中其实有大量的交互是跟大模型的，然后有大模型的提示词模板，然后这些结构也都是存在这个特定的chain里面的那如果你现在定义好了一个原来的python的一些函数，但这些函数它处理的对象其实跟男性里面抽象的这些数据类型不一样，那怎么办呢？它相当于做了一个壳，这个可就方便你把各种各样pya的函数套到这个传送群里面一样。就相当于以前你具备各种技能。然后穿swarm chain就跟南茜的这个工作服一样，通过transform圈你就穿上了南线的工服，那你就能够去跟其他的线串在一起了，我要这么描述，大家能不能理解，除了面向过程以外，我们既然把它当成一个编程语言，那if else是一个非常重要的能力，就我们能做条件判断。这个LLM的router chain其实就是用来做这个事儿的，它叫路由，就通过我们给大语言模型的prom template，里面去买好很多的这个条件，你可以理解成这两个欠AM欠AAM欠B其实就相当于我们条件判断里面的衣服怎么样，就做一个事情，else就做另一个事情。那么条件写到哪儿呢？条件写到rota chain里面。但是你的这个then和else这两个具体实现的逻辑，分别放到了MCNNA和m chain b里面，我不知道这个大家能不能对比着理解到位。
	那这些要实现他们其实核心就是可以去学习南茜是怎么写这个AM loto chain的prom template。这其实就相当于有点类似大家看优秀的人怎么写的代码，就跟我们现在学习优秀的人怎么写prompt是一个逻辑。然后这里有一个小的homework可以可选的。大家可以看看就是我们刚才讲的这些知识，就这些欠的知识都可以被串联起来，最终可以整合到这样的一个，你可以认为一个简单的应用。这个简单的应用其实就是把这三种券，就我们的transform，LM router和我们的这个sequential chain串联起来。因为LM chain是最底层的，就不用讲了。M router chain和这个transform chain，还有我们的这个sequential chain，其实是一个非常好的工作流，我们通过这样的一个抽象的工作流，其实就能实现一个什么呢？
	既能做文本摘要，又能做翻译聊天的这么一个助手，或者说叫agents。那完全就取决于你现在的这个input。这个input可以是文档，也可以是你输入的内容。那你你要处理文档的话，通过transformation是可以处理的。然后处理完了之后，然后根据这个条件判断去看你现在需要翻译还是需要做摘要。
	我举一个简单的例子，大家能够去想象的就是可以有这样的一个场景，就是最终输出的都是中文。然后如果你现在输入的这个文本本身已经是中文了，那你可以就直接进入summary这样的一条通道。如果你现在输入的不是中文，那你就可以进入translation这条通道，然后让它变成一个。翻译的一个应用。当然你也可以在翻译之后再接上一个summary的一个AM券。那就相当于把AM券A再接到这个AM券B后面，那这样也是可以的。这个其实就是可以排列组合，就像咱们去玩乐高一样了，我们就不再赘述了。
	但是通过这个描述，大家可以想象，其实用自然语言来编程写代码是一个未来应该是一种新的发展趋势。因为它使得大家学习编程的这个是应该叫什么这个ROI，或者说学习它的这个效率变得更高了。我觉得是一个很正常的一个发展过程。就跟现在大家再去学英语，和我们小时候学英语的方法比起来，现在方法肯定更高效。我们都知道现在需要一个更好的语言环境，只是背单词是一个很低效的学习方法。但是至少在我小时候，我的这个小学和中学小学和初中，都还是以背单词背短语为主的。
	那学编程也是一样的，大家想一想之前我们去怎么去学习门编程语言，反正就是先看看语法，然后刷一刷这个little code之类的，可能甚至连课都不刷，就是刷一些很乱七八糟的这些东西。但这个过程其实是很没有，只是效率是不高的，就相当于你可能这一辈子终于学会了好几门不同的编程语言。但是这个编程语言本身它的生命力也不一定长久。
	第二，其实你学习编程语言，本质上学的是一个所谓的编程思维。而这个编程思维未来如果能够跟大模型结合好，那其实这个编程思维通过提示词模板也是可以体现的。然后提示词模板再加上大模型就可以变成N种编程语言，这个很好理解对吧？因为大语言模型帮你去学语法，你要学的只是编程思维了。那语法这事儿其实就很卷，咱们没必要花那么多时间学习语法，更重要的是学习这个编程思维，是这么一个意思。
	那么memory，就我们提到的记忆的功能，刚刚我们提到这些其实都是一个简单的抽象，有了这个编程的能力了。但是如果我们想要实现聊天对话的能力，还需要memory memory在人群里面的这个价值。就是我们的这个跟人一样的，就是你得有一些这个呃，就比如说举个最简单的例子，就是工作当中，领导前天或者上周交代给你的任务，他不一定每天都来问，但是当他问起来的那一刻你得知道，不然就会出麻烦。Memory就是这么一个存在。
	同样的既然是记忆，它也有长期和短期的。短期的记忆可能就放在你的内存里面，你的buffer里面，只要就相当于你有一个小小一点的存储区间，它的特点是你一下就能想起来，但是它能记的内容很少，这就是很经典的计算机设计，就跟L1L2多级缓存一样。那么短期的这个记忆通常就放在这个应用程序的热存储里面，通常就放在内存或者显存里面。还有长期的记忆就是我们的烂笔头，领导去年交代的任务，那你可能是写到了某个文档或者某个笔记本上面，那这个就可以放到香料数据库，或者一些其他的长期存储的这个地方，但无论如何存下来了。
	那现在既然存下来了，那我们就得回答另外两个问题，什么时候存的，什么时候用？那么什么时候存呢？无非就是我们在这里写的right这条线，写入的这条线就一根。那这条线呢就是我们看到在这个生成一些结果之后，我们为了记住，就像我们领导交代了任务之后，我们为了记住它，我们会把它写到一块能够存放的地方。就比如说从生成的结果里面，然后得到了这个answer，就得到这个答案，会写到这个memory里面。
	然后什么时候用？可能是用户或者领导提的某一个问题跟他相关，比如说他就问你，去年我们的这个目标达成怎么样了？这个时候你得先记得住去年目标是什么如果你记得住，在你的短期存储里一下就掉出来了。你记不住，你就得去翻一下笔记本，看看你记下来的去年领导说的目标是什么。这个过程就跟大模型去运用memory是一样的，它就会有一些关键词指代词去取，那你这个时候想象一下，这个去年目标不就像我们RAG1样，这个memory不就像一个向量数据库，对吧？去去从这个去年目标检索一下，在这个memory里哪个跟它是最相关的，然后把它给到我们的大元模型，它这个大元模型里就不只是question，还有从这个memory里取出来的一些过去的对话或者信息，给到这样的一个券。那这个券拿到这两部分的内容，然后给到我们的模型，模型再生成一个结果，再存下来，所以RAG其实也可以套到这个图里面来理解，只不过这个memory是一种更特别的更具体的抽象。所以不管是RAG还是其他的系统，只要增加了memory，我们其实都能把这个系统的能力再做进一步的扩展。
	有了这三部分的能力，其实我们就已经能做一个有记忆能力的大模型的应用了。来想象一下有model IO，有各种各样的chain，包括条件判断的、转换的、顺序的。然后m chain又把model o做了进一步的封装。
	Memory其实就是做了各种各样的记忆的能力的一个抽象。包括我们刚刚说的对话式的，我可以去帮你存下来，像文件里面就愈实现了各种各样的的对话式的memory，有的是只记录最近五轮的对话，有的是只记录谁谁谁的对话，有的是通过单元模型把过去的几轮对话做一个summarize。相当于就做了一个摘要提取都可以。那这些，其实就是咱们八仙过海各显神通了。跟着应用场景来设置自定义的memory系统。像我们刚刚讲到的x memory，x的券，能够做出apps的system。
	最后讲一下这个data connection，就是我们的这个数据连接到底是一个什么样的过程。我们刚才讲南券是搭了一个很复杂的管道，可以做各种各样的应用。管道里面流通的其实就是我们的各种各样的data，data在这个流通过程当中又有几个不同的阶段，然后不同的形态。整体来看，其实就可以抽象成这五个关键步骤。
	第一个步骤其实是处理从数据源到南茜这样的一个步骤，叫node加载。然后我们看到左边其实是有各种各样的数据来源，比如说github b，youtube discord，然后各种office的软件，包括网页、twitter这样的社交媒体之类的那这些不同来源，不同格式的数据都需要被南迁人用起来，统一到南茜的框架里来。所以，就有一个抽象叫做document load ders。然后这个document也是南线里面关于数据最重要的一个基础抽象。你可以理解成所有的数据最终都变成了document，就是所有数据来源数据都变成了document。然后这个document需要通过一步转换transforms，才能够进入我们的嵌入模型，我们的invading model。
	这个也很好理解，就像咱们自己已经处理过这个数据了，就知道你一个太长的文本，就比如说一本小说是没法通过一个token nize的过程变成一个向量的。因为一本小说的信息太多了，这个in bedding model tocom ized它也是有输入上限的，你不能无限制的给我这个输入变成一个向量，我提不出它的这个语义的这个嵌入模型会失效。所以说从这个流程就像切割，尽可能切割成跟我们的目标应用一样的颗粒度的这个原文，然后原文通过image变成一个向量，向量存到存储里，变变成这个向量数据库里的数据，真正要用的时候再检索出来，大概是这样的一个流程。
	具体来看其实在南茜里面已经实现了各种各样的document loaders去应对不同的数据格式和来源。就比如说我们刚刚说到的这些CSV，HTML，包括Jason等等，还有左下角的是不同的数据源。比如说archive上面的论文，哔哩哔哩上面的字幕，fig ma上面的一些设计稿，github上面的一些代码，tens flow data set里面已经实现好的一些数据集一都在右下角，这个南券的官方网站上，都有它的已经实现好的预定义的loaders，方便咱们一步加载数据源。
	加载进来之后，我们就要看到这个最重要的抽象，叫做document。这里是去理解南茜去接收所有的数据，加载到这个内部的形式。最重要的两个抽象，一个叫document，一个叫base loader。Document是加载之后的存储抽象的所有东西都叫document。Base elo ader约束了这个加载过程，就是咱们通过这个class base loader左边这部分就能看到两个函数，一个抽象方法叫做load方法。这个load方法是需要具体的loader去实现的那它无论怎么实现，最终返回的都得是document的列表，就相当于不管你自己实现了一个什么样的loader，加载什么样的数据，最终都得变成document的列表。而这个document就是右边我们看到的这个document里面就是一页一页的这个内容，就像咱们的PDF1样，然后还可以实现这个look up的一些操作。
	然后我们刚刚讲到的第一步叫load，第二步叫transform。有的时候你的处理数据量太大的时候。其实这两步通常是合在一起的，不然你加载进来相当于洗了一遍数据形式，或者说数据的格式。然后完了之后你存下来了，然后你下次要把它切割，你是不是得还得再再把它这个加载进来再做一次切割。所以对于一些比较常见的场景，可能这个load the split就一次性去做了处理。所以你也可以实现这个load and split方法，那相当于就一次干了两步的活儿。就我们这里看到的，当然也有一些预定实现好的transform的方法，比如说把这个网页变成text等等，所以我们就不再赘述了。包括这个text embedding model，南迁也预定义实现了很多，像这个cohl的，包括OpenAI的embedding model，他都去做了对接。
	业内首个AI原生的不均匀字。再看一个简单的MTB，就采用了标量加向量的混合计量。去帮我们去看一下大的红色的陕西苹果，要先在水果区定位到苹果货架，然后通过大的色的陕西的检索标签找到符合要求的苹果。
	腾讯云向量数据库单索引支持10亿级向量规模，数据接入AI的效率也比传统方案提升十倍。企业可以更快速的将向量数据库与AI模型相结合，实现更高效的在线推理。当这样的向量数据库与大模型相结合，还会产生什么其他的化学反应呢？不如我们将它接入到大语言模型的各个环节来一探究竟。业界的AI大模型采取预训练的模式，使用收集好的数据进行训练，这时候难以做到实时更新。而接入向量数据库后，就像给模型插了个高清的外接硬盘，支持学习互联网等最新信息。企业也可以注入公司信息、产品手册等私域数据以供模型进行推理，比重新训练、模型微调等方式更加便宜高效。
	在线推理时大模型其实是没有记忆功能的，只能通过重新输入之前的问答实现短期记忆，而且输入的内容也是有长度限制的。腾讯云的向量数据库可以充当AI的海马体，让人工智能拥有记忆历史。问答能够当成新的序列饮料进入项目数据库永久储存。甚至当用户提出了重复相似的提问，项目数据库会直接给出缓存答案。这就让AI大模型越用越聪明，越用越迅捷。
	此外面对希望接入AI大模型的企业，腾讯云的向量数据库能够提供一站式的解决方案。除数据存储管理外，还可以帮助企业完成私域知识库中的文本分割向量化两大复杂环节。比起传统的企业自建做法，接入时间可以从30天缩短到三天。当人类想到向量可以让计算机理解复杂的图像、文字、音频等信息时，我们似乎找到了AI理解现实世界的钥匙。不断进化的向量数据库与人工智能这对黄金搭档，还会给人工智能时代带来什么惊喜呢？让我们拭目以待。
	腾讯云的这个视频做的挺好的，倒不是说要用他们家的向量数据库，跟他解答了向量数据库在大语言模型的应用当中的一个定位和价值，其实说的还挺形象的。大家想象一下这个向量数据库就跟我们开始讲的一样，它有两个最常见的场景。一个是作为第二大脑去提供补充的一个知识，第二个就是说提供一个缓存的一个能力。就比如说我之前提过的问题，能不能帮我存下来？因为大语言模型的使用成本挺高的那如果之前你问过相似的问题，并且用户反馈答案还不错，那我就直接使用向量数据库里已经存下来的答案来回答你就好了。就类似于我们的FAQ1样的一个场景。
	向量数据库在南泉里面也有非常多的对接。就比如说chroma，这个post gray的这个就是传统的关系数据库。Post grey口也有这个PG vector，然后python等等这样的一些向量数据库。他们都是包括像redis也有做自己的这个向量数据库，非常多。
	然后除了这些以外，我们最重要的这个RAG其实也是围绕着大模型和向量数据库来进行展开的。就我们这里的这个经典形式，如果整合一下的话就能看得到这个加载，transform embedding。这一步其实就是把各种各样的数据源，从原始数据变成向量数据库里面的数据。我们的实际使用场景就是通过把query变成embedding y的结果，然后在向量数据库里拿到一些相关的一些结果，通过各种各样抽象的所谓的最相似的这种度量方法给它取出来。然后这个相似的这种度量方法在检索器的实现上也有很多种，简单来说就是有的是直接去算这个欧式距离，也有的是算一些别的所谓的相似的方法。但简单来说，你可以理解成，不管怎么样，就是在这个高维空间里面，我们有一些很经典的数学方法去指导我们去比较哪几个向量是靠的比较近的，他们是比较相像的。就类似于我们在传统的这个查询数据的时候的这个检索功能一样。
	只不过传统的这个数据，它存的每一个值都有一个具体的含义。比如说这个地方存的是这个价格等于35，那个地方存的是商品品类，是一个食品。但是在向量里面，因为它存的是一个语义信息，而这个语义信息它本身就只是一个概率，你要解码出来之后才能找到可能最相像的是哪个字，所以去向量数据库里去取数据的过程，也就跟我们之前讲到的在模型推理里面去解码这个信息的过程是很类似的。
	检索器也有很多的抽象，像我们刚刚讲到的这个要去这个数据库里去取数据，既涉及到检索它的效率，像短片里讲的，同时还涉及到了各种各样实现的检索方法。所以这些其实左下角这几页左下角都留了林茜的这个相关的文档，大家有兴趣可以去简单的了解一下，最后再来讲一下这个南茜的数据生态，我们刚刚看到了，其实它能把各种各样的数据源接入到南茜，变成一个统一的document的抽象。通过一个特定的嵌入模型存到一个向量数据库里，最终在被拉起来使用。
	整个南券，其实它的这个数据生态的对接也还在不断的扩张。我们能看到不管是从公域的还是私域的，从结构化的还是非结构化的，然后你是这种散列式的文件，公司数据还是这个data store。就比如说这个云里面的这个对象存储，或者说云里面的其他的云硬盘等等，其他都做了非常丰富的对接。它能做到这个对接最重要的就是我们开始有一页讲到的它的base loader和document这样的一组抽象。然后通过对接这些数据，它实现了一个数据连接，然后再把这个数据通过transformer最终又可以被消化吸收。以一个向量或者说以一个向量数据库把这些数据都存起来，存的是这个嵌入式嵌入嵌入之后的这个高维向量。而这个向量数据库本身它也是一个生态。
	像现在已经对接了几十个不同的向量数据库，那整体的南迁的这个数据生态，我们这样的一个总览看下来，其实会发现，第一，它几乎没有限制你就没有限制你必须要用什么。但是它限制了这个管道里面流通的这个数据的一个，你可以理解这个形态，就像我们用电的时候，你不管你用什么电器，但你是220伏的，它就限制了这个东西。那这个东西限制之后，其实它在它的层面上就可以去统一很多API了。
	但是我们这些不管上面用的什么电器设备，他也都不关心，因为大家是做了这个分层的设计的，这个是南庆的data生态做的很好的一点。同时比如说我们要去做这个RAG了，RAG也是一个很经典的抽象。而在这个南茜的这个实现里面，其实就像我们前面看到很多例子，要去实现一个RAG的机器人，我们真正聚焦关心的其实就三个部分。
	第一个部分就是这个大元模型我们用什么，那在这里我们这门课可能会用。第二就是说它能用到什么样的场景里，这个取决于我们会给它什么样的prompt。就比如说你给他的prompt，告诉他你是一个像我们上节课学的，你是一个销售人员，是卖衣服的，那么卖衣服的这个prompt给到大模型，他就知道我要给出一个什么样的类型的答案了。然后如果前期还有一些比较好的这些参考信息给到他，那就更好了。所以这些参考信息都被放到了一个向量数据库里，就是我们要关注的第三个部分向量数据库。就向量数据库里给了一些比如说产品相关的介绍，然后历史的一些比较好的问答，都放在这儿。然后通过用户这一次提的问题，整合到这儿，形成一个完整的prot。
	锦上添花的就是我们的记忆系统，我们的memory。在这样的一个REG的应用里面，如果我们能够加入一些memory，那它可以更好的去提升我们的这个对话的质量，就比如说咱们的用户之前聊过一些内容，现在通过memory能够被唤起，然后你就不会显得有一点不是很聪明，这个是一个典型的RAG在。能签上面的一个pipeline，也是我们后面会去做的一个pipeline。好，今天其实要跟大家讲的这个快速入门能签的这个技术模块就分享完了。我们再花一点时间跟大家介绍一下对应的这些代码，主要就大家要去实际跑一跑，了解一下。
	我们看到在这个课程项目里面新增加了一个目录叫做南茜。然后我们的requirement里面也增加了能签。
	有点慢，我们就看这个部分，这对应着我们的课程项目。在我们的这个课程项目里面新增加的这个年茜和requirements的目录，这里新增加了年欠，大家拉下来最新的代码的话就能看到了。然后在这里面新增加的能切目录就覆盖了今天讲的几个模块，包括咱们的这个model IO。然后我们可以点开一个关于model的和我们的chance，有我们的sequence chain。
	我们的这个data connection里面有各种各样的loader，然后我们的memory也实现了一些基础的memory。比如说在model里面，大家可以如果没有去拉取最新的代码，在这儿直接pipe in store一下，也能够更新和迭代能签的版本。这里面是给这个应用开发课的同学做了更深入的一些原理的剖析。包括南茜的这个语言抽象的一些定义和参考的API文档和代码。这个咱们如果不想去深入研究的，就可以不用看啊。主要是能够去跑一跑这些代码，理解一下刚刚讲的这些概念其实代码量都很简单，就是我们这里看到的导入一些这个模型，然后运行这个模型，它这里面也会明确去写。但因为我们使用这些代码有一个前置条件是open a的API，所以我不用强制要求大家都把这些代码都跑一遍。有可能有的同学他没有这个OPI的API，但是我们后面就是把这一套就这些notebook大家理解之后，我们要去做的这个RAG，可以用私有化的ChatGLM，那个就可以直接运行了，就不需要咱们有这个OPI的API了，是这么一个意思。
	好，然后这些代码其实都已经上传了，看大家有什么问题，就是实际去跑一跑。然后我们在下节课的上课的时候，可以做一个集中的答疑。因为主要是因为这个不是咱们的主要的考核和这个作业，是一个选修的部分。但是通过这些代码，我相信大家能对南茜有一个相对系统性的一个认识。然后在技术上应该有一个快速的入门。
	因为了解了这些常用的模块是很重要的，包括我们看到这些loader，他这里给了几个像从本地文件加载这个text loader，包括我们要去加载论文，使用这个archive的loader，都可以加载论文。一些非结构化的网页也可以去做对应的加载，然后我们的这个sequence change也有一些对应的实现。好，那那大家来重点来提提问题。今天确实时间也比较晚了，我们珍惜QA的时间。看大家有什么问题。
	有的同学说讲的真好啊，谢谢。串联式调用的欠遇到中间调用失败的欠如何处理？遇到中间调用失败的嵌是指崩溃了吗？还是什么？这个同学可以展开说一下吗？是系统崩了还是指什么？Rng怎么评估效果？这个其实是一个产品问题，不是一个技术问题。
	这个同学就是我我应该这么讲，大家会包括我今天也在讲大语言模型的应用，它最大的特点就是跟以前你做的软件应用不太一样了。以前你会觉得做一个东西出来，它应该有一个确定的产出，你有一个确定的分数可以量化。但实际不是，比如说我们现在就运行一个取名字的，稍等我没有，应该是没有切到对应的。对应的这个环境上来安装一下。
	那就是help。
	给大家举个例子，一个最简单的一个大模型的应用就能看到一个变化。
	这台机器没有放这个key，我换一台服务器。
	就比如说我们的这个root chain，我们举个再简单一点的例子。
	首先我们得理解这个客观性，就是咱们的这个大语言模型的应用，它产生的结果不确定是稳步存在的，就比如说我们最简单的一个例子，取名字对吧？性能卓越的GPU没有变化，这里出现了十个候选的名字，当我们再去执行它的时候，它又变了。其实我们很难得到一个稳定复现的结果，这个是一个客观事实。然后我们在讲理论的部分的时候，确实大约模型不确定性是一个很重要的指标，当然像OpenAI的大模型，它有一个参数叫temperature，如果你把这个设置为零。他可能给你的结果尽可能是保证一致的。这个时候大家能看到我运行了多次，但他给的结果是相对稳定的，是不变化的。这个是OpenAI他做的一些工程化，但如果咱们要做到，其实本身就有一些额外的工作需要去做，这个是第一个点。就是在大模型层面，你可以理解成就是基础设施层面，它的一些天然的特性决定了它的结果的不确定性。
	然后对于我们来说要怎么用这个不确定性，其实是一个很挑战的事情，回到刚刚这个同学问的问题，就是RAG怎么评估效果。为什么说是一个产品问题，不是一个单纯的技术问题？就是你要想象一下，你首先得知道你的RAG是一个用到什么场景里的应用，RAG是一种技术，但是RAG要用到什么地方才能去评估。就像我们就算要做软件工程的测试，那也是我们经过了很久的软件工程研发，才定义出了不同的阶段，有不同的任务，然后抽象出了一些测试集。
	假设我们现在做的是一个销售的场景，这个销售的场景，第一我们技术这一侧能做的事情就是尽可能的去找到一些跟他相关的数据，作为我们模型的预训练，或者说作为我们小模型的这个微调，几十亿的这个微调，或者说把它放到我们的向量数据库里，作为我们的rng里面的参考。这个是我们能做到了极致。然后同时，在这个场景里面，假设我们是一个销售的场景，那我们要尽可能的做到能拿到用户的反馈，就是你做的这个应用要能拿到用户的反馈，而不是拿不到用户的反馈。即使最后用户没有去点这个点赞，或者是给你dislike，就不喜欢你的这个答案，你也能够分析出用户行为。这些其实都不是单纯的技术问题。
	然后如果我们要考虑RIG到一个具体场景的时候，其实是能设计出一些用户反馈的信息，然后去评估目前这个IG不管是模型测，还是我们的这个prompt这个设计，还是我们向量数据库里放的内容，其实都是能够得到一个综合的评分的。但不是像大家想象的那样，我能得到一个比分，然后这个比分能直接指导我去怎么样改进。如果真是这样的话，其实整个大元模型也就没啥好做的，是这样的一个意思。能欠是不是只能做验证或者测试，不适合工程化放在实际生产环境中。我不这么认为，可以讲一下你的实际生产环境，为什么不能用冷谦，我觉得这可能是一个问题。
	然后有的同学说讲的太好，消化不了。就是你只需要理解刚讲的这些概念层面上的东西，能消化多少就消化多少。因为本来南茜这个是咱们另一个开发的实战营的重要的学习内容，也没有说要大家一节课就学好人家也是学了一个多月的这个课程。但是这些概念大家要理解，就这个用自然语言编程的思想大家要埋到脑袋里面。然后agency跟人性的关系大家能够理明白。然后下一节课用到的agent又会把这里的很多概念又给覆盖掉，就包起来了。相当于。两种情况都有崩溃报错得到的结果无法在下一个chain接收后做处理。
	我能理解，比如说你的下一个券需要的结果是，比如说需要这个data time，或者说一些特定的结构化的信息。如果你上一个欠崩溃了，那你就只有从事，也没有别的太多的方法。然后如果你上一个欠没有崩溃，它只是生成的结果，你的poser没有处理好。那你可能需要在puzzle这一层去做一些处理了，不管是正常还是怎么样。
	然后可以参考一下auto GPT它的from temple ate是怎么写的。我们开发课的时候有一节课就是做了auto GPT的源代码解读，重点解读了他的prompt是怎么做的。运行这些代码需要开通OpenAI，需要哪些条件，能简单说一下吗？运行这些代码需要你有一个OpenAI的APIP，就是能够调查API的key就好了。
	国内大模型的温度不能为零，这种做选择题每次做的结果都不一样。对，这个我说了temperature是一个AI工程化的技术，它不是大模型天然带来的。就像我们transformers加载的这些模型，它不是都有这个参数的。
	借助能欠和向量数据库的能力已经可以完成很多事情。那么成本比较高的微调的核心价值是啥？这个我们已经讲了很多回了，这我不再做赘述了。
	前面已经讲过很多回了。对，就是你可以理解成能欠它本身是用来做应用开发的，它不去管大模型，它只是让你把大模型的调用变得好用一些。然后如果你的大模型对于这个销量数据库里存的这个知识，它就是不能理解。尤其是你要理解这个RAG也好，能欠加向量数据库也好，它是可以完成很多事情。但前提是你用的是GPT4，或者你用的是GPT3.5。
	但是实际情况你的应用场景可能你用不到他们，你用的是几十亿的模型。几十亿的模型他做一个RAG是很丑陋的，是也不是很丑陋，是效果没那么好的。但凡你做过你就知道了，然后我们也会做，你就会感受到一个RAG对接GPT4和对接一个开源的几十亿的模型差距有多大。所以这个几十亿的模型你是需要微调的。向量数据库中存储的向量用什么方法获得的evading结果？想把问题进行用什么方法对问题进行背景。
	这个同学问的。这个同学问了一大堆关于in bedding的问题，这么理解首先embedding模型和大元模型它是配套的。就像我们用transformers的时候，token ized和我们的模型是配套的，只不过OpenAI它是服务化的。它的模型不是像我们用transformer加载过来之后，自己用GPU来部署的。所以你是调了他的服务，所以你的took ized可以理解成OpenAI的embedding model。你部署的model，就是你调的GPT，它是一一配套的。你token neither是怎么样去把我们的数据变成vector的，invading model也是类似的。然后你的存下来的这些数据和最终用户提的这个问题，肯定得使用同一个embedding model，这样我们得使用同一个token iser。
	举个最简单的例子，GPT two它的token lizer和bert这个模型的token lizer是不能混用的对吧？那如果你混用了这个模型，数据可能维度都不一样，完全就没法相乘。没搞清楚这门课的微调和ig的关系，微调大语言模型，然后用RAG来应用是可以的。存储层的内容比如向量数据库也可以微调，然后再喂给大语言模型。这个同学你再整理一下你的语言，就是你你到底想问什么？对。
	对，有个同学说的是挺对的。就是咱们写了这么多年的代码，有点思维固化了。认为一个东西运行出来的结果一定得是一样的。但是你真的是做服务，做产品，其实人的期待是不一样的，是变化的。
	我曾经做过一个最简单的例子，就是为什么大语言模型要每次相同的问题生成不一样的结果。你可以把运行这个大语言模型的这个人换成你的领导。然后你的领导问了你一个问题，就比如说举个简单例子，就是咱们在讨论一个技术问题，然后领导说你发言来讲一讲，然后你讲了一遍，说了一些结论。这个时候领导说你再发言讲讲。你想象一下，如果你再把你刚刚讲的内容背一遍，原封不动的说出来是会很奇怪的。你肯定得想一个不同的角度再讲一遍，不然就你想一下这个场面会非常的奇怪，非常的诡异。大语言模型其实也是一样的，他的这个过程跟我刚举的这个场景是非常相像的。
	向量数据库会不会绑定大模型，换个新的大模型，inbreeding到向量数据库中的历史数据是否还能用？这个同学提的问题挺好的，首先向量数据库跟刚刚提到的embedding和大模型之间的关系，你要做客体分离，向量数据库它就只是用来存向量的。你存这个比如说你存这个一千维的向量和存2000维的向量，对于向量数据库是没区别的。关键是你去用向量数据库的这个研发，你得记住你现在存的是一个哪个模型的向量，这个是你要记住的。而这个数据库本身是不关心你存什么的。
	然后其实是向量的维度，就是我们交了交过token either了。大家在再想一想top neither跟这个大模型的关系，除了它的维度要一样，它的他还有很多的信息是要对齐的。你不同的大模型的token ized之间是没法混用的。因为它本来就涉及到分词和映射，然后还有token ID，我不知道大家是不是都忘了这些东西了，我们教过token，neither的有token ID，你不管维维度一样的token ID对不上，那个知识完全是错的。所以这些大家如果都忘了tokenizer那节课了得回去看一看，token ized其实就干了evening这个活。向量数据库的数据是通过训练出来的embedding来存储的。
	这个embedding的过程也可以微调吗？Embedding layer是可以微调的。这个同学问的，就这个embedding的模型是可以微调的，是可以训练的，但它是一个比较很细分的东西，然后它就更难去评估性能的优劣了。通常情况下大家很少会去动它，大部分是在保持embedding model不变的情况下去微调大元模型的权重。
	看看大家还有没有什么问题啊？
	需。
	还有同学问加上validation loss就会OOM，你可以训练完了再评估。我们前面教过就是你可以训练完走模型的评估，这就是这个其实之前学过的，就咱们教的那个技术点是一个一个教的。但不是说我给的代码你不能改，就是我们给的那个代码是突出那节课的技术点。然后首先你的best size可以调小，对吧？第二你得告诉我你是在用什么方法训练哪个语言模型，对吧？然后如果你是用的QLA，然后你的Q罗A有没有可能用更小的，用NF4等等。然后你有没有可能把base size尽可能调小，然后用批量的梯度更新，用我们的这个上节课教的一个参数，就有各种各样的手段。然后实在不行，你就训练几个比如说你训练的一个epoch，然后你去手动跑一下validation，然后去看一下这个validation上面的loss是什么情况。简单来说，就是你如果想要你啥也不用管，特别方便，那就是得给很多的资源。但如果是在有限的资源上，那可能你就得手动去操作一些内容了。
	大家还有什么问题吗？我们最后再等一分钟，没有新的问题，我们今天就到这儿。
	好，那我们今天就先到这儿，大家有什么问题我们这个群里面再题。然后今天这个代码就是不强求，大家有这个open ADAPIK就跑一跑，会对棉签有一个基础的认知的。然后我们接下来要用的这个chat GM加上南茜去做这个RAG，但在这之前可能还会再对chat GM有一些更深入的一些介绍。好，我们今天就先到这儿，感谢大家。时间也比较晚了，十点试试。