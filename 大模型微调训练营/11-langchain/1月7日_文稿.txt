	Hello, 大家好，可以看见吗？声音和画面是否正常？好，都正常。好的，我们今天就开始第十节课的内容，快速入门能片的大模型应用开发框架。因为我们开发课的同学，如果学过开发课的同学，可能都会知道，南茜是这个应用大模型应用开发里面非常重要的一个技术站，也是一个很重要的框架。那么对于我们微调训练营的同学来说，刚好是倒过来的。
	我们的大模型的微调训练就关于这个模型本身非常的重要。但是南茜作为一个开发框架，我们需要知道它的一些基本概念，能够把它安装好用起来就可以了。所以今天我们可能会分成两个部分。
	一个部分是给大家初步的介绍一下能券这个框架，它到底是一个什么样的框架，它有什么样的用处，为什么需要这样的一个框架？我们会主要介绍一下能券的一些核心模块，让大家对能券的这个框架的设计有一个基本的概念。然后我们给了一些主拍ter的notebook，让大家可以实际操作。
	如果你想要深入的去研究它，这样也有一个途径。首先我们就来看看南茜到底是个什么东西，这个名字取的也如果你是第一次听到的话，也会感觉很奇怪。外语言链条为什么会放到一起，连线？首先我们了解它是通过开源社区，可以有一个很好的方式去了解一个项目。就像我们看啊这幅图里面，从人权自身的发展，以及它和上一代深度学习框架，最著名的tene sor flow和py touch去做对比的话，我们看得出来能券的这个电脑太吵了，我去把它关起来。
	不好意思，对，我们能看得到其实从整个深度学习社区的发展，我们看得到其实特色flow目前它的关注度star的数量还是非常高的，超过了20将近20万。然后排套餐，其实也是一直在一个稳步增长的过程。但是我们看到右边这幅图，红颜色的这一根线，能嵌的开源短短现在应该有一年的时间了，一年的时间，飞速拉升的一个斜率，在快速的受到全世界人民的关注。如果说深度学习还是一个AI圈子类的事儿，那么大模型ChatGPT的发布，使得我们很多以前不是做AI，不是做计算机的人都开始关注AI这个社区，或者说AI这个技术相关的一些开源社区的项目了。
	从南线的发展我们能看得到，在2023年的刚刚开年，我们现在是2024年的一月。这一年的时间到去年的Q3的时候，年前已经有非常高的一个增长了。并且这两幅图的对比也能看得出来，一开始是一个个人的开源项目，就跟我们的课程项目一样，后来南茜经过了两轮的融资，在二三年的上半年应该累计有融资两三千万美金，也变成了一个公司型的运作的项目。有越来越多的人开始为南庆这个社区，我说为这个框架本身去做越来越多的贡献。包括像国内的这几家头部的大模型的公司，他们也都有去做自己大模型和南迁的对接。我们从整体的概念上知道了它是一个广泛受到关注的新技术新社区。
	那它具体是一个什么样的开源项目，他自己的一个定位讲的很清楚，就是我们需要用南欠，什么场景用，build application with LLM输through就通过这个可组合性来让我们能够使用大模型快速的去构建应用。说人话就是它是一个大模型应用的开发框架，它怎么能够帮助我们快速的开发大模型应用？它有一堆组件，就像乐高一样有一堆的组件。这些组件核心就有我们右边看到的这几部分，包括今天我们会讲到的model IO，data connection，chain memory agents，其实已经算是能欠它对于这四个基础模块的一个应用，一个最佳实践，就包括我们经常听说过的这个auto GPT，它本身也是属于一种agents，我们叫智能代理AI的应用。我们当然除了这五个以外，所有的大应用层的框架，通常都会去支持call back回调，方便我们去开发复杂应用的时候，能够做一些埋入一些我们想要的触发机制，让我们方便的去debug也好，或者说打印一些中间结果也好，这个是一个常规操作，通过这六个模块其实能够对人性有一个初步的了解。
	尤其是我们接下来会去讲最主要的四个基础模块，它们分别是什么样的一个能力的抽象，为什么要抽象出来？我们做所有的框架都是为了统一一个开发的方法或者模式，一个开发的流程。那这样的话，第一大家不用再重复造轮子。框架这一层有人做，大家就能够更多的人和资源投到做应用这一层。这个框架如果抽象的好，大家才会认可，你才会不断的去使用这个框架来开发。但如果你的框架抽象的不好，那肯定就会受到挑战，就会有新的抽象不断被提出来。年庆本身它是一个相对开放的一个社区，他接收的这个contributor也已经有上千人了，然后他的这些抽象也是紧跟着大模型的能力来的，就是为什么大模型的这个model IO会被单独的提取出来，为什么有个chance？这个我们待会儿会逐步的来了解。
	既然提到它是一个应用的开发框架，这里再多讲一下。就是所有的应用的开发框架，或者说所有的这个开源项目也好，技术也好。其实无非就是你可以把它分成三种类别。一种类别是做基础设施，做基建的。所有的基建类的技术最终都会逐步的被自动化，被框架化，大家都看不见这些技术了。
	从最右边的操作系统开始，我不知道有多少同学是啊在这个渗透S或者这个乌邦图上面用过这个操作系统的指令的，或者说频繁的去使用这种操作系统的指令，然后用过wim或者说e max这样的一些文本编辑器来写代码。在那个时代，其实我们大部分的开发机就是一台单独的机器，很多人还需要去熟背，比如说这个linux操作指南，read head什么各种操作命令。在那个时代，其实我们没有办法，因为所有的底层的基础设施建设没有到一个高度发达的状态。我相信从13年14年开始，从云的这个时代到来之后，越来越多的云时代的这些基础设施被提了出来。就包括我们看到的云原生cub native，包括更好用的这个renter，后来被苏这家公司收购了，他其实做的是一个企业级的b native，包括red hat推出的这个open shift也是一个企业版的corporates，像我之前的财云这家公司也是做了中国版的cuba tis被直接收购了。
	所有的这些公司，其实都在想，我们在云计算的时代，不太可能说整个开发环境一台机器能够搞定。我们需要有很多的机器，那这些云，这些机器需要被管理起来，那我们有一些管理这些机器的工具也好，或者说叫云时代的分布式的操作系统也好。Cubs在云计算这个时代，其实某种程度上是替代了右边操作系统这个角色。我们不太会直接再去操作，一个大的集群下面的每一台机器。就如果大家真的用过云原生的这些CNCF的项目，你会发现其实这个ice这一层的很多的抽象都会被逐步的自动化掉。他可能是通过了carbonates或者docker的一些抽象，你就很难再会去直接编辑他们了。可能在一个一个的dock file背后，这些操作系统的命令都已经被自动化的有一些轮子给替代掉了，人就不会再去操作它了。
	通过云原生的发展，我们成功的让很多的分布式的大批量的集群被我们管理起来，用起来了。但是，单纯的web时代，我们看到有很多的流量，有很多大企业，他们需要部署很多的服务器。尤其是像双十一这样的一些高峰流量的节点，需要非常好的弹性扩容的机制。这些都是云原生这一批项目带来的基础设施能力的一个跃升。让我们现在的这些科技公司能够一次性管理上万台不同的服务器。然后一个服务器的资源利用率还能通过容器化技术被拉伸到原来的好几倍。通过这些技术的迭代，其实我们终于能够批量的去海量的使用资源了。但是我们回到AI TensorFlow py TRCH这样的一些深度学习的框架。其实他们的这个角色使命跟操作系统是非常类似的。
	我们在16年17年的时候，很多人都会去研究TensorFlow的这些NN的API怎么写，torch的这些API怎么写。然后整个框架层也在不断的发展，包括很多的框架也都在卷，包括像咖啡兔等等，然后中间的这个数据结构怎么表示，onex这样的一些抽象也都被提了出来。但是我们再回到今天来看，就像我们提到的transformers，其实已经完成了很多的抽象了，我们的tensor FLOW patch，大家回想一下我们在使用transformer，使用这个PFT去训练，推理、微调，甚至q ora我们的大模型的时候，是不是有直接去操作过底层的API很少可能我们唯一接触的比较多的就是如果我们要去使用模型做推理，或者做一些细致的调整。我们需要让模型跑完一遍这个前馈网络之后，给出一个tensor，就是这个模型输出的结果。然后在这个tensor上，我们可以去选择这个return tensor的结构是一个torch的结构还是一个TensorFlow w的结构。因为这两个不同的框架，他们对于同样的一个高维向量的表达方式是不同的。但也就仅此而已了。
	这个事情就很像咱们现在很少会直接再去操作这个操作系统了，像VS code这样的IDE已经能够很方便的让你在远端的服务器通过一些插件去像使用本地的这个电脑一样的去管理你的远端服务器了。所以整个基础设施的发展一定是一个不断有这个框架，然后技术去抽象，然后让你不用再去关心底层的很多具体实现的这样的一个叠加的过程。这过程就跟我们垒积木，垒乐高是一模一样的。
	然后中间的框架层，其实是随着你可以认为随着技术商业的迭代开始繁荣。就比如说我们看到web的时代，web 1.0、web 2.0出现了大量的互联网公司，当然也有电子商务的公司。那这些公司，因为他们有web开发的需求，甚至到了互联网2000年泡沫的时候，人人都需要有一个dot com这样的一个网站，那就有大量的用来开发网站的应用框架被造了出来。就像我们下面看到这些web的APP。
	那在十年前移动互联网的发展也使得我们有很多的，尤其是以安卓和，我们现在用的最多的两个移动端的系统，安卓和IOS。上面延伸出来的大量的移动APP的开发框架。一直到后来我们看到最近几年的趋势是说，能不能用像flutter或者类似的一些框架写一遍代码，所有的操作系统上都能跑。其实跟在移动APP上面想要出现一个类似于docker这样的平台是类似的。就因为需求开始逐渐的没有像以前那么多了。以前百团大战的时候，可能有1万家公司都在做APP。那现在可能一个新的APP，大家都会很慎重的去考虑，因为已经逐渐趋于一个比较稳定的状态。现在的南券作为大模型应用的开发框架，其实就是一个先行者来探路的。我们可以理解未来一定还会有很多的大模型的应用开发框架，这个是毋庸置疑的。
	而且我相信今天其实大家都已经听到过一些不同的声音了，有微软开源的，有一些其他公司开源的，各种各样的大模型的应用开发框架。但是我们目前可以看得到的是，就现有的技术的积累也好，人气的积累也好，大模型的对接也好。包括这个接口的统一性和设计，能现目前仍然还是一个我们可以认为在做大模型应用开发框架这件事上非常主流，并且受到大家关注的这么一个大模型的应用开发框架。所以我们即使是微调训练课，也跟大家会简单的去讲一下这个框架。并且我们用蓝线来做一个RAG的应用，也是一个非常简单的一个开发，这个难度可能就3 40行的代码就能做出一个RA级。
	只不过对于我们这门课的同学来说，南茜知道他的API是怎么回事，能够把它安装部署起来。同时怎么样能够做出一个相对可用的大模型供南线来调用，其实是咱们的一个学习重点。所以这是两门课的一个区别。但是南茜作为一个当下最热门的技术，还是希望咱们这门课的同学也能够有一个初步的了解，能够去学习，理解它。
	好，那么为什么需要能签？尤其是这个问题在OpenAI的一次又一次的技术迭代过程当中，每次open I的新进展出现之后，都会有人提这样的问题，为什么需要能签？这个问题其实问的很好啊。
	然后这个问题在去年的上半年的时候，23年的上半年的时候，也问了南茜的这个作者发起人，这个社区的。当时就有人问，为什么开发者要用能欠，而不是直接去用OpenAI？比如说用open I的这个GPT的这个APIK，或者说hugin face上，我们已经有这么多的模型了，为什么不直接用这些模型？这一点我相信大家，尤其咱们这门课，因为用这个hugging ing face的这个transformers比较多了。我们知道transformers是有这个pipeline，然后也可以自己去微调模型，然后把它部署起来。但我们始终会感觉这个notch写起来很不是很优雅。就是你需要各种各样的流程，数据的部分、模型的部分，训练好之后还要推理它。然后如果你是各种不同的模型，可能还得调整，就是没有一个一招鲜也好，或者说没有一个统一的接口来做这个事儿。
	杨倩他其实想要解决的问题就是我们刚刚提到这些麻烦的工作，就是对于工程研发来说很麻烦的工作，对于hugg face也好，OpenAI也好，包括其他的一些类似的公司，他们的定位就像我们开始讲到的，更更像未来的云或者说操作系统这一层，他们是基础设施。就我们能够看得到，如果大模型的应用未来真的像mobile APP1样，这样广泛的在大家在生活当中出现的时候，基座模型一定会像提供能源，提供这个云计算资源的这个公司一样稳定。他不太会每天都去改，因为大量的应用是基于它去建设的，他不太可能像现在大家去用GPT或者用一些其他的开源社区的这个模型。它可能一个月就有一个迭代，或者像OPAI，它的ChatGPT使用的这些模型可能两周就会有一个迭代，到未来可能这个迭代的速度会降下来，就像早期的安卓和IOS的版本升级是非常快的。但是到了mobile APP都很稳健的生态的时候，其实他们的更新就不会那么快了。因为该做的工作也做的差不太多，所以一定会有一个分层的设计。首先能劝他绝对不是OpenAI和hugg face的这些模型去做竞争的，它是为这些模型的应用提供便利的。它在中间这一层，上面这一层做应用开发的同学，其实他们也会有一些痛苦的部分，因为他跟咱们这门课的对象又不太一样。
	就比如说咱们假设现在是一个大模型的应用开发者，他其实不懂咱们教的这个课里的很多东西的。他不知道这个transformers这个库是个什么东西，也不知道什么叫PEFT，也不知道模型为什么要微调。在大部分的软件开发领域里面，其实确定性是很重要的一件事情。就是我写好了一份代码，然后我运行了一次和我运行了100次，我希望我的结果一定是一样的。因为如果无法达到这样的一个效果，我就不敢让这个程序或者说这个产品应用上线，它很不稳定。但是我们学完了大模型到今天为止，我们应该知道。因为整个大模型的这个输出结果就是经过了神经网络之后，变成了一个的logic。这个logic然后我们会去通过一种方法，比如说如果是输出一段话的话，我们会用各种各样的评估方法去找它可能最终会被解码成一个什么样的字。然后我们运行十次，可能十次的内容都不一样，这个是一个对于应用开发的同学非常不可接受的一个事儿。
	生这个模型生成的结果如此。然后如果我们再把这个对接，就是我们对接一个模型，对接十个模型，对接100个模型。可能这十个模型100个模型他们的API定义，他们这个返回结构可能还都不一样。那这个事儿对于一个应用开发的同学是非常痛苦的。就相当于今天搞大模型应用开发这帮同学他们就像十年前搞mobile APP开发的同学一样。他们不仅有两个操作系统，他们可能需要对接100个甚至大几百个操作系统，这个事儿是无法接受的。所以南茜在中间这一层其实是一个承上启下的作用。对于上面的应用开发者，他希望通过一个统一的接口，让他们调这1 100个不同的模型的时候，用的是同一套API，这个是南茜为上面的应用开发者提供的便利。
	最下面的模型也是一样，因为下面的模型不管你怎么迭代，你最终一个大模型抽象的能力是可以被描述出来的。就比如说你是一个生成类的模型，像我们之前学过的早期的这个几十亿规模的大模型，都还没有像现在的ChatGPT这么强。所有的任务都集于一身，什么都能做。那么几十亿这个规模的模型，大家还是聚焦一些特定领域的。比如说我是做这个文本生成类的，他是做这个mask相关的工作的，这个是做语音识别的。所以其实对于这些底层的这些大模型来说，他当然他考虑的就是我要把我的这个长板做长，把我的单个的能力做到非常强我是不会考虑上面的应用对接怎么样的。
	大家都各自做各自的，那能确就是说你们就好好做，我把你们的这些工作都能够通过我统一的一套接口给他为上面的人提供便利，他们能享受到你们的工作成果，同时你们也不用管上面的人怎么抱怨，你们就更新你们的就包括OpenAI的这个python的这个API1.0版本的更新，其实是一个不兼容性的更新，但能欠也很快的去做了这个对接。所以其实类似的这些其他的大模型也在不断的迭代。但其实对于能欠的开发者来说，他几乎感觉不到，他都不需要去更新什么代码。这个其实是一个人签的价值，可能都给大家讲了一些背景，毕竟这个分工不同，可能我们这个做大模型微调的同学，他就不一定需要去考虑到开发者这么多问题。然后我们通过一些实际的案例，让大家理解它具体好在哪。
	就比如说有一个这样的开源项目叫open a translator。是几个月前通过chat ChatGPT的这个GPT4的版本，跟他多轮对话，我跟他差不多对话了有六七个小时累计，然后生成了这样的一个开源项目。那这个开源项目里面的代码文档都完全由这个GPT4的，应该是三月份的那个版本生成的。那这样的一份代码它是可以运行的，并且它还同时支持使用这个OpenAI的GPT的API和ChatGLM来进行实现一个翻译，然后这个翻译它的输入是一个PDF的文件，输出是PDF或者markdown这样的一些文件，所以它里面还有一些PDF解析相关的库。那这些代码都是通过你跟GPT的不断交流可以开发出来的，还是比较好用的。就比如说我们看到，比如说老人与海这样一本小说是一个英文的版本，我们想把它输出成中文的版本翻译过来，它可以做翻译。但是这样的一这个代码，它它在这个实现过程当中，首先体现了GPT4是真的很强，它可以生成完整的。
	Github的项目。但前提是你的prom的技术要还不错，你得跟他多轮交流，然后你还得给他做各种各样的回溯，因为他的这个上下文还是有限的，他的记忆力也是有限的，取决于它的这个context的上限。尤其是那会儿应该它就只有不到16K1个长的上下文，所以首先GPT4的能力很强，通过open a translator这个项目我们能看得到。
	但是如果我们想要把这个项目更进一步，不只是做成一个开源项目，而是把它做成一个能挣钱的项目。这个时候我们就会发现你要做好兼容性了，比如说你既要去对接这个GPT的模型，你可能也要对接chat GM的模型，或者对接一些像国内的一些大模型。如果我们要去对接很多模型的时候，那这个项目就会出现一些局限。就是我们刚刚提到的不同的大模型，你要去调用它的方式是不一样的，就是年限它的价值就在于现在这个项目我会去抽象抽象出一个模型的model的这样的一个鸡肋，然后这个鸡肋会有很多的派生的子类。每个子类是一个具体的大模型的类型，然后我自己去实现它的这个调用方法，但是如果你想要把这个调用的模型再去做一个新的增加，比如说今天你现在只是这两种，明天你要去支持百度或者阿里的大模型的时候，你还得再写一份代码，再写一个子类，然后你还得去抽象这个提示词这部分的模板。那这些，都是你，你目前如果要自己去做的话，是一个应用开发者需要做的工作。冷却，其实他做了很多的抽象和好的可以用的类似的工具。我们就可以简单来理解一下这个项目，就比如说对于我们刚刚提到的OpenAI translator这个项目，我们抽象出来输入输出和它的一个内部运行的一个关键的模块。
	输入是一个PDF的文档，输出是一个中文的文档，格式可以多样，那在内部跟大模型相关的，我们讲大模型应用，跟大模型应用不相关的，我们先忘掉它。在内部最重要的就是英文的PDF，我们通过一些手段可以拿到它的正文的内容。然后这个正文的内容因为是大模型，我们需要给它prompt，给它一个合适的prompt，让他把这些原文翻译成中文。然后这个大模型就是我们要去对接的一个服务，可以是OpenAI的，也可以是咱们自己的这个部署的，这样的一个抽象，其实是一个非常典型的大模型的简单应用的一种抽象一种范式。这个抽象如果我们把它展开来看。就可以分成这样几部分。
	我们输入的PDF有一个参数的解析器，就python的一个经典的解析命令行参数的一种做法，或者说文件的配置的一种做法。然后内部我们把它展开看的话，就是有一个用来做PDF解析的，还有一个最终导出PDF的抽象。然后最关键的部分跟大模型相关的其实是右边这一部分，我们需要做的这个应用是一个翻译类的应用。所以我们给的prompt其实是翻译的prompt。然后我们的大模型我们可以想象成先对接GPT，但未来可以对接各种各样的大模型。如果以这样的一个框架去理解，我们就会发现，如果我们要去横向扩展不同的大模型，哪些部分是变的，哪些部分是不变的。
	变的部分其实就在右边这一块，左边这一块其实是完全跟大模型隔离的一个设计，就是你的PDF解析，就是一个文件解析类的一个模块，不管你用什么大模型，这部分内容都是不变的。然后你的导出也是一样，你的导出就是翻译好的内容，你需要把它导出成一个特定的文件。所以我们可以看到整个这个抽象里面，左边和中间的部分是可以固定下来的。而右边其实是取决于或者说我们把它改成隔离成一个做大模型应用需要去考虑的模块。而这部分的价值其实就是能欠它的抽象带来的价值。
	就比如说如果我们要去自己实现一个prompt，temple ate其实也能做。你可以去自己研究用这个python的或者用其他语言去做这个字符串的一些设计管理。我们之前也教过一些简单的这个prompt的一些最佳时间，包括用这个引号，画括号等等。
	大模型也是一样，我们知道调t GPT它有它的API，调我们自己的这个部署的大模型它可以是一个内存之间的直接调用。我们现在已经学会了就把一个大模型加载到GPU里面，然后通过pipeline也好，通过这个model的evaluation，或者说像chat GM它有chat接口等等也可以。这是本地调用，你也可以把它变成一个服务，一个API的一这个server，然后再给你提供调用。但是如果你的大模型这一块，要不断的去做这个新的大模型的增加扩展。
	或者说针对不同的大模型，你有不同的这个提示词模板需要去管理的时候，其实这部分的工作，你每做一个应用你都得开发一遍。我不知道这个大家能不能理解，就今天你做一个翻译的应用，你需要设计一堆的模板，对接一堆的大模型。明天你不做翻译，你可能做的是一个文档总结类的或者说语音识别类的应用。右边这部分你也得抽象出来再做一遍。所以你每一次做不同的大模型应用，抽象的这部分其实就是南茜做的这一部分内容。他把大模型输入的这个提示词抽象成了一个提示词的模板，然后把大模型统一抽象成了一种特定的接口，那这样你不用去管它怎么对接的大模型，因为这部分工作框架帮你干，这个是一个比较好去理解能欠这个大模型应用开发框架它到底做了什么。然后我们大概有一个初步了解，后面会去给大家看它的这些模块到底怎么设计，怎么用的。这是南茜，我们能感受到它的价值了。
	GPT4它还会不断迭代。就我们能想象得到南茜这个公司肯定没有open I这个公司资源多，有钱，然后投入多。那有了GPT4，GPT4会不会干掉能欠干的活，这个其实是另一个维度的问题，就是基础模型会不会往上走，去吃掉这个能欠的这些应用开发框架层做的工作，我们不能说他一定不会做。因为你像apple就苹果这家公司做了IOS的操作系统。但是在IOS操作系统上面有各种各样的应用开发语言和框架。Apple自己也说了自己的开发框架swift，android也做了自己的开发框架coding。然后包括像IDE他们也都有自己的IDE，所以说我们这样的一个思维方式是很正常的，很直接的。
	但是操作系统和大模型对应的这个上面的应用，它们之间的关系其实还不太一样，为什么呢？我们可以这样理解，就是第1GPT4它本身确实能力很强了，但它也还是有一些天然的缺陷，这个是由技术本身决定的，就是我这列下来的一些内容，GPT4它虽然很强，但是它的上下文的这个上限还是一个非常大的限制。这个限制来源于transformer这个结构本身。然后这个结构本身使得你的token增加之后，它或者说你的token每增长一倍，它的算力资源的消耗是它的这个次方，就是N方的一个关系。
	就原来可能你是这个token是十，现在你变成了这个100，那他可能需要的算力就是原来的1000倍这样的一个概念。这个其实是一个很夸张的一个资源上的消耗。然后我们能看得到，整个大模型其实目前有一个非常大的限制，也是来源于这个transformer的结构。所以整个2024年和2025年，我们会一定还会看到有很多做大模型的瘦身，然后压缩。怎么样把大模型一个更小的资源部署起来，这一类技术公司和开源项目应该会出来很多。
	但我们说回来，GPT4它虽然是现在这个星球上最强的模型，但是它也没法做私有化，然后没法去查询外部的数据库和API。同时因为它是一个海外的大模型，所以他的数据隐私在一些比较重要的行业，或者说一些比较在意数据隐私的客户和领域，他也是没法得到广泛的应用的，然后也没法联网。数据目前已经到20，最新版本的GP4已经到了2023年的Q一了，这些其实都是GPT4天然具备的一些缺点。这些缺点首先通过南茜能解决一部分内容，就比如说能去调调用外部的这个API和数据库，能对接一些工具，能够联网等等。这个是南茜可以给GPT4的一些助力。
	GPT4本身或者说OpenAI这家公司，它也有一些它的考量。我们能看得到其实它已经很强了，它是目前远远超过其他公司的这个水平。就比如说我们看到google发布了这个Jimmy在去年底的时候发布了这样的一个自己的大模型。然后GIMNI的这个最强版本，在它的一些测试上有一些局部是赶上了GT4的，但是他还没有真实的发出来。但是我们要知道GPT4其实是22年下旬OpenAI的一个成果，然后GPT4V也是二三年的一些成果。所以即使是google和deep mind，相比于open I的GPT4，也还是有一年多的一个时间差需要去追赶。然后像OpenAI出来的这个s pic这家公司，他们的cloud的模型也在追赶这个GT4，但是也还是有差距，但这个差距在也在不断的缩小，尤其是这些大的公司，包括像这个lama等等。那么OPI它一定会不断的再去迭代它的基础大模型，因为这个是最重要的，包括多模态，各种能力，因为你可以理解他们在卷的是更大的市场和这个生态。
	在底层的就类似于操作系统这一层，他们如果真的做好了，其实上面接的是南茜，还是一个什么别的券，还是一个什么其他的应用开发框架，对于他们来说都无所谓。因为他们更多的去承载了这个资源，而且他们拥有核心的数据。所以如果从这个视角来看，我们可以理解。
	第一能欠其实是补足了很多大模型的能力缺陷。因为他们的维度不一样，技术的维度不一样。第二，这些大模型的公司，他们如果想要活下来，他们现在一定没有这个闲心和这个心思去搞这个框架这一层。前提它是一个大模型的公司，而不只是一个宣称它是大模型的公司。因为现在大模型的这个竞赛其实还没有到下半场，还在一个火热的竞争的阶段。未来可能这个世界上一共存活的也就两三家真正的所谓的大模型的公司，绝大部分的都会消失，因为大模型是一个非常寡头化，赢家通吃的一个状态。
	如果有一个很强的大模型，就比如说你真的熟悉使用了GPT4，你再去使用一个小的模型，或者说，一些其他创业团队的模型，你是会感觉非常不爽的。就像你用到了这个iphone 4之后，你再用回这个诺基亚，你是无法接受的。所以大模型的这帮公司，他们其实天然不会那么在意这个中间这一层。
	但如果有一天这个地球上只剩两三家大模型公司了，然后也稳定了，那个时候他们会不会自己再去做自己的框架，那我们可以到时候再静观其变，但这有一个过程。但如果我们学会了南茜，其实去那个时候再去转型学别的东西也很简单，就像这个swap的提出来之后，其实有很多原来的做IOS用开发同学要去学这个。新的语言也很简单，并且我们现在看到open I它发布的这个assistant CPI也有很多的部分，其实在参考能签的这个设计。
	好，所以一个简单的总结就是大模型的应用肯定不等于一个简单的GPT的一个API的封装。所以GPT4虽然很强，但是他也不会做所有的这个生态当中应该做的故事，他有他自己的使命需要去完成。这个生态要搭建起来，应用框架层是一个非常重要的一个中间的技术站。冷倩承担了这样的一个角色。然后目前我们也看得到有大量的应用，包括大量的大模型也都对接在上面了。未来其实是它已经足够对接了足够多的大模型。那未来其实我们能看到上面的应用会不断的再去做增长。
	好啊，这个是一个auto GPT的示例，我这边我们就不放了。相信大家应该都看过，就是用这个GPT4作为我们的大模型的选择。然后让GPT4像一个大脑一样去思考怎么样去回答我们的用户的一个问题。这个用户的问题其实就是问了一个GPT4本身不知道的那我们能看到这个r GPT自己打开了一个网页，然后使用爬虫的技术在获取这个网页当中的信息，然后再去做总结摘要的总结。
	然后做完总结之后，会把这个总结写到一个文件，就像我们刚刚描述的这个open s translator一样，它也有这样的一个输入输出的过程，到这儿其实就展示完了这个OGPT的这个能力。但我们去抽象一下，它其实核心还是用户输入一个自然语言一个问题。这个问题由大模型去思考，这个问题应该怎么解决。然后一部分的解决可以由大模型自己，就像他回答我们的问题生成内容一样去给你解决好。还有部分内容就需要去调用一些外部的工具，包括这个搜索引擎。它掉了搜索引擎，掉了爬虫，然后最后还掉了这个文件导出等等。这些能力其实都是通过我们现在叫做A键词了，在aut GPT提出来的时候，还没有这么强的一个概念。
	现在我们都知道agents就是大模型加上一堆的外部工具，组成了一种所谓的大模型应用的形态。那么auto GPT其实就是这种形态的一种终极的想象，我们可以理解LGPT是希望做一个就像最近发的这个stanford的这个机器人助手一样，只不过它没有这个手臂，它是一个很强的最强大脑，能够帮你去规划解决你所提出来的任何问题，由大模型来帮你去做问题的拆解和目标的逐步的达成。Auto GPT是一个典型，除了auto GPT以外，比较落地一点的，现在我们能看到的有一类应用叫做RAG，就是应该很多人都听过这个词。RAG它其实是一种检索增强式，一种生成的这种技术。具体来看，其实我们看到它的场景，因为始终我们这一轮都在讲大语言模型，用户通常是通过这个语言，不管是说话还是打字来进行交互的那在交互过程当中，在RAG这个场景里，其实南茜的价值就被进一步放大。我们可以看到在一个我们这幅图里面，南茜处于一个最中心的位置。他在这个最中心的位置，既跟用户去做了交互，用户提问，最终告诉用户答案，同时还连接了三种不同的技术生态。
	最上面的这个embedding model，其实我们已经在使用transformers的过程当中或多或少用过了。就比如说我们用这个transformers里面的token nizing，他干的其实就是embedding model这个活。就是把我们用户输入的一段自然语言原始文本，通过这个embedding model变成了我们的这个token，就是我们的token ID，token ID对应的就是这一堆的高维向量，所以我们能。