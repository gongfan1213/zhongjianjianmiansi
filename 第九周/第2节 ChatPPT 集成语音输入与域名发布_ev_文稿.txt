	Hello, 咱们画面正常吗？我点一下播放。
	咱们PPT现在是播放状态吗？
	对我穿个外套。
	来了，咱们变成八点开始了，今天内容其实有点多的。
	我们还是会把今天的讲完，今天ChatGPT作为第一个多模态的agent，在我们的整个课程里面其实涉及到蛮多模型的。然后我看大家应该都能听见，对吧？现在那咖啡也接好了，大家能画面正常吗？声音正常吗？我把这个切到目录页，看一下我们的目录有没有跳过来。大家能看到吗？OK吗？都可以对吧？好，那我们就正式开始。
	好好，今天我们讲ChatGPT的这个0.4版本集成语音输入和域名发布。这一节其实是内容相对来说比较多的，它主要就体现在在目录。大家应该能看出来。
	首先我们会在这节课教大家使用一个ASR服务的模型。ASR的全称叫自动语音识别，这个技术其实已经诞生很多年了，只不过之前的或者说这个任务这种典型的这种任务SR已经诞生很多年了。但是因为之前的技术不够成熟，所以大家的观感没有那么直接，体感没有那么强。
	那今天我们会用这个一点时间，可能大概四十多分钟，先把自动语音识别相关的基础的，它是一个什么样的任务，有什么样到目前为止很好的模型，或者说sota的模型是什么样的那我们要如何去获取这些模型？有很多的平台。我们讲一个现在应该在国内比较流行的一个词叫版本T0，就是模型的平台有没有版本T0，我觉得hugging face hub肯定算是一个，然后通过这个第一个部分，第一节课我们能够了解什么是SR。然后接着我们会开始去着手把我们的chat PPT的前置部分的工作给做好。也就是自动语音识别应该有一个模型，这个模型最终要变成一个服务才能为我们所用。而它要变成一个服务，我们在过程当中，当然我肯定已经做过了。大家如果没有操作的话，就不太能理解这里为什么会插入一个HTTPS。包括域名、SSL证书，包括反向代理等等。
	简单来说，其实就是当我们去把这个SR这样的服务做到一个浏览器里面的应用的时候，我们有多种方式可以去获取这个语音，也就是这个audio。一种就是我们直接上传一个文件，这个是最简单的，也不涉及到任何权限的问题。那假设我们想要把自己的服务做成一个可以用麦克风输入的一个即时的语音识别，那这个时候就涉及到了权限问题，也就是我们在第二个小节看到的语音输入的权限问题与它的解决。
	主要的解决方案其实就是把我们的radio变成一个HTTPS的服务。在这儿我们就先highlight让大家知道这个HTPS在这儿的一个价值，以及我们的radio如果要变成一个HTTPS的服务，可能有两条技术路线。在我们使用这个radio的时候，其实大家可能没有太关注。我们这最终启动这个radio服务的时候的那行代码里面其实有大量的参数可以去填写。包括我们把它变成一个HTTPS服务需要的证书，其实也有对应的这个参数可以去填写。但是那个不是我们最终用到的方案，因为他把你的这个radio变成了一个服务，然后里面会有一些细，我们在具体讲的时候再去展开。
	方案二就是使用标准的n inx来做反向代理。那么radio服务本身就不需要再做任何改动，即使它是一个HTTP的服务也没有关系，NYX来负责证书的验证。这个只要做过后端的同学应该是一听就能明白，你明白的。然后我们在这个服务发布的过程当中，既然NYX也都配置了，如果各位有自己购买过的域名，那这个时候我们也可以在这儿顺带讲一下。因为你反正都配配了这个反向代理的那我们可以把它配置成一个配置成一个特定的服务。那接下来大家就可以通过这个域名的方式来访问你自己的chat PPT了。这个也是咱们本节课的一个可选的homework。
	第三个小节其实就涉及到了0.4这个版本具体的一些改动了。前面我们是讲了怎么样去集成这个语音别的模型服务，同时0.4版本我们还做了一些迭代，这些就是偏向内容的迭代了，第一个部分就是我们在之前的版本里面，支持了bullet points作为我们整个slides里面的主要内容。但是只支持一个我们可以认为是一级的一个bult points，就是它只有一级。但实际上我们打开模板之后，我们会发现里面会有多到5级的一个内容的分级。那么这个多级的bleed points我们会在0.4里面也去做支持。它需要去扩展我们的slight contents以及对应的解析和PPT生成相关的模块。
	同时我们还是有的同学可能在这个代码里面已经看到了，我们还加了一个极简风格的模板，叫simple template。这个模板也第一它是为我们的多级的bullet points这样的的内容去适配的，他用这样的模板去内容比较多的时候会比较好看。同时在那个模板里面我也留下了一些为下一个版本去做的准备，在下一个版本我们会去用AI生成图片，为我们的内容去塞一些图，这样就是一个更加完整的一个powers points，这个是我们整体这一节课要讲的内容。
	那么接下来我们就具体的来学习这三个部分。好，第一部分就是我们的自动语音识别，SR的概述。这个词我相信很多同学可能听过语音识别，但其实云识别更像是一个偏向商业化或者产品侧的一个词。真正在AI的这个圈子也好，或者说AI的社区也好，大家讨论这类任务的时候，通常会用这个SR。SR的这个A就是自动，不是这个语音，S是这个speech，r就是这个recognition。
	那SR是什么呢？它本质上我们回到最初的时候，它其实是在上个世纪50年代，也就是一九五几年的时候，就已经开始有人在研究这个问题了。只不过那会儿不像现在这么fancy。因为我们看到整个计算机互联网的这个进展，其实也就最近30年的一个迭代。
	最早期的时候，人类是可以把一些语音信号转换成可读文本的。因为即使在在一九五几年的时候，我们也仍然是有通信工具的。而这些通信工具里面就包括电话，这些电话的内容能不能变成一个可读的文本？因为大家想象一下这个过程，就是把把这个大脑先清空掉现在的一些习以为常的东西。
	回到上个世纪50年代，那个时候是新中国刚刚成立。是有电话的。首先这个电话可能不像现在可以全球拨打，没有这么强的运营商。但是这个电话传递的实时的信息，首先也可以被记下，因为我们有这个刻录声音的技术。但这些信息就变成了一个记下来的磁带，或者说更古早的一些记载声音的这个载体。这些载体它就跟我们今天没有语音识别技术之前，在你的手机里放置的各种MP3文件、WAV文件一样。这些文件其实是死的，是非文本的，是无法被语言模型或者其他的NLP技术区进一步加工和使用的。所以这个动机也好，或者说有这样的一个需求，或者有人在研究这个事情，自然而然也就是很早之前就会有的。
	因为大家需要用这些没有结构化和文本化的数据，这个是SR最早的一个起源。随着这几年，尤其是从深度学习开始，我们发现越来越多的新的深度学习的技术或者神经网络的技术被加到了SR这样的一个具体的任务场景里面。现在当然它已经被广泛应用了，从我们习以为常的hi siri到小爱同学，小度等等，我刚把我的电视给唤醒了，所以我们大概了解了这个ASR的它的它是一种计算机的技术，而且它其实在一九五几年就有了。
	它的工作原理通常可以被分成四个阶段，第一个阶段其实就是声音的采集。简单来说SR这个S是它的输入，speech是它的输入，这个过程是叫做recognition进行识别，并且它是自动化的。然后最终的产出是文本，就是我把这个语音信号，我把这个speech变成了一堆text。那么声音的采集就会有很多种方式，最常见的就是麦克风。
	就像我提到的这个SR最早在1959年的时候也只有麦克风，没有数字化的这个音频文件，后来才变成了数字信号。这个跟我们看所有的多媒体的迭代都是一样的。最早我们看的都是什么模拟信号的电视，后来变成数字信号。现在几乎都是数字化的，声音的采集是第一阶段。因为数字化，所以以前只能通过麦克风设备来录制，现在可能可以有各种各样的数字化的文件了。在这个过程当中，从音频文件它其实就是一堆的波形文件，到后续的识别有一个重要的阶段。这也是AI的算法引入进来，把SR从一个计算机的技术变成了一个AI的技术的关键，就是特征提取的。
	特征提取是从简单的原来处理声音信号，变成了开始去提取这个声学特征。其中一个比较关键的词我们不展开，因为这个agent课程是咱们偏应用的课程，大家可以去记住这些关键词，在有兴趣去查，就是叫做梅尔频率的倒谱系数。MFCC这个系数待会儿在我们讲vesper large v3模型的时候还会出现。因为这是一个很关键的特征，然后这个特征可以有效的。帮我们去把原来的原始信号变成一个高度抽象的信号。
	类似的让大家能够通俗的理解，就是我们在做CNN这种模型，去做计算机视觉任务的时候，输入的是像素点。但是通过CN通过卷积核，我们可以把像素点一次次的去抽取它的feature map。大家如果对CNN有所了解的话，就会发现这些feature map可以分层，通过这个激活的方式去看每一层的feature map到底学到了什么。浅层次的可能是学到了一些几何信息。比如说线、这个线条、这个边缘，甚至包括这个前景背景一些主体等等。再往后面就能够识别到的信息越来越多。应该在咱们的应该在咱们的这门课程的，应该有一个预置还是前置的内容里，大家回头可以去翻一翻。我有讲到这个attention这个注意力机制它有哪些优点，其中就有一个可解释性，那个可解释性也能表现出一些CN网络，包括这个呃机器翻译里面，注意力机制它怎么样提取到了一些有效特征。
	所以特征提取是非常重要的那声音信号本身就是一个不可不可视的，看不到的一个东西。就是就在空气当中，或者说肯定是在空气当中。因为只有空气存在才能传播这个声音，那它会有震动，有不同的频率。这些频率人可以听见的就是20到2万。随着年龄的增大，你的这个区间还会渐渐的变小，就是原来的这个区间。人按照正态分布的平均值，大家可以听到的是20到2万，但是这个值是会变的那所以这个是研究声音信号的时候，一个很重要的维度就是频率。然后在不同的频率上还会有不同的声音信号。所以跟我们以往处理的很多输入的这个数据的原始状态不太一样，大家会感觉到陌生的话也是正常的，但不用把它变成一个学习的门槛，这里稍微展开一点，这个特征提取很重要。
	有了特征提取之后，后面就是精简的深度学习的做法。就只要你有特征，那特征最终给到一个深度神经网络，或者给到一个全连接网络。这个全连接网络它可以变成一个分类器，比如说我们在经典的DNN或者CNN网络里面都有啊这个分类器，可以是识别猫和狗，可以是识别或者说分类出你这个是好评还是差评，这个都有可能。
	模型的识别，这是一个比较典型的过程，输入特征输出一些带有标签的结果。最后一步就是后处理，就是我们模型输出的结果再进行一遍后处理。这是因为我们的整个ASR的任务，相比于经典的CNN任务来说，它不是一个一眼可见的，并且是孤立的离散的人物。
	比如说一幅图在这里，像我们现在看到这一页，我们能明显看到有四个步骤。这四个步骤还有一个固定的形态，一个L型的去做划分。然后不同的L之间其实是没啥关联的。
	但SR这种语音识别任务不太一样，是我一直在持续的说话，然后我前后之间是紧密连接的。我这一切没有切对的话，那是会有问题的，它会影响很多，然后因为是一直连续的。你要一直切，你要是所有的刀都切错了，就是我我只咱们把这个声音信号切完变成一个信号，然后这个信号再变成一个文本，切的不对肯定就识别的有很多的问题。这些部分它跟整个获取结果和这个流程是密切相关的。所以通常来说SR自己会有一些后处理的技术。这个技术主要的用途就是进行错误的纠正和优化。
	然后这个过程当中还可以用语言模型来做一遍理解。你可以想象一下，就是我们把这个声音信号最终变成了一段文本，但这个文本里面经常会出现什么呢？就是大家去看视频的时候会有字幕，现在很多的字幕都是用的SR的这个模型来做了。
	但是如果它的后处理没有做好，它就有可能出现有些字他就搞错了。比如说这个鼻音，鼻音原来的声音信号里面可能就发的不是很准。那如果我直接通过这个特征的提取再加识别，它可能就是会错的。你比如说四川人福建人，他们说出来的话他不是普通话。但你如果没有这一步后处理，用语言模型来对语义的连贯性和准确性去做后处理的话，那就会有问题。
	最典型的就是有个脱口秀演员，最近很火，他是这个胖子门窗发蝴蝶，他识别出来就是发福叠。但如果你把它的一整段文本连在一起，你会知道他有的地方是发蝴蝶，有的地方他真的在说花蝴蝶，就这个意思。这个是它的一个整体的工作原理，稍微给大家展开一点，技术演进的话就好理解了。
	整个从20世纪50年代到现在已经过了七十多年了。主要的我们认为里程碑或者重要的发展阶段，其实有四个阶段。初期的阶段也是用统计机器学习的方法，上过这个微调训练营的同学应该看这个就非常有感触。因为整个语言模型的发展其实也是只有四个阶段，从早期的基于规则模板的template base的方法，到中期或者说第二阶开启之前的这些统计机器学习的方法。但是还没有真的进入这个机器学习，只是一些统计学以数学系或者数学学院的老师为主，搞概率论的，搞数理统计的这些老师为主提出的以马可夫模型，马可复联等等一系列的方法，包括像后续的条件随机场。
	到了第二阶段发展阶段，就是HMM的方法成熟了。相对来说同时我们有机器学习的一些算法进来了。90年代的时候，乐坤刚刚发明了CNN网络，叫LE left，开始实习识别手写体数字。同时期还有很多NLP的任务都在百花绽放，所以NLP才能跟CNN分庭抗礼二三十年。但是后来我们发现大模型出现之后，所有的NLP任务都被一揽子解决了。那就是在最近的这个第四阶段近期阶段了。
	但在这个中间，其实是有一个深度学习崛起带来的反思或者说赋能。2012年的时候，这个深度学习首先是通过ALEX net这个里程碑事件，直接把当年的sota以这个机器学习方法的sota干了十几个点下去，这是非常强的。然后16年的时候，其实如果关注NLP这个任务，或者说NLP相关的模型，就会发现这个CTC的模型当时应该是非常火的，包括各种OCR的任务，都在用CTC这样的连接时序分类的模型去解决。
	就NLP这个领域一直到现在，我们看到OPAI不仅做了GPT，他还做了为什么？其实就是因为底层它都有用到transformer的一些其实就注意力机制的延伸，然后注意力机制衍生出来的一种新的网络结构叫transformer。它其实transformer某种意义上是跟CNRN在一个层面上的，但是又不完全一样。因为它比基础的CNRN的结构要更复杂一些。Transformer又可以演变成语言模型，多模态的模型，或者说语音识别的模型。这是一个技术演进的一个大家能看到的四个阶段。那么它的应用场景自然就很多了，语音助手这个就不用说了，ferri. 
	然后客服的这个自动应答，会议记录，科大讯飞也做了很多这方面的产品，这些其实都是属于SR的应用。那么ASR是不是已经没有什么可做的，已经做的很好了？显然不是。他如果关注这个领域的话会发现我们后面也会讲这个vesper。就vesper其实三个版本的迭代时间很快，而且它里面也在不断的解决一些老的问题。
	这里我先抛个砖让大家了解，就是SR这个语音识别的这个任务。我们就算是把它集成到我们的ChatGPT里面，可能还是会发现有的时候不准的原因是什么？最大的两个点其实就是在噪音，就是在这个噪音干扰这里我们列了一个问题。就是实际的应用场景当中，背景的噪音是对SR的性能影响非常显著的。这里大家要想象一下，可能我们如果没有去占到解决问题的这个身份，而只是从用户的视角来想的话，会觉得背景不应该很好的识别出来。就像我们去做CNN的时候，我的前景是猫和狗，背景可能是一个屋子或者说一些路人什么的，这应该很好能识别出来。因为这个图像里面会有一些空间的分割，让你感觉到哪些是前景和背景，以及它的比例。包括现在比较火的深度的信息，depth的信息都是有效的信息。
	但是单纯的只是做SR这一个任务的话，其实那个声音是没有办法辨别出人的。你可以想象就是给了你一堆一个MP3文件，比较fancy的一种编辑状况，就是导进去我甚至能看到不同的音轨。大家如果用过这种音视频的编辑软件的话，包括剪辑你都能看见里面会有很多条不同的轨道。有视频的可以有多条轨道，音频的也有多条轨道。
	这个东西让大家觉得可能识别不同的人或者不同的音轨是简单的。但刚刚我们聊的这些编辑，它都是一个单向的输出，或者说单向的一个操作。如果我有多条音轨，然后我在这儿编辑，编辑完了之后，如果我导出成了一个文件。这个文件它本身是不含有这样的结构性的信息的，就不同的音轨是压缩在一起的。比如说变成一个MP3，你再打开它的时候你也分不出来的。因为这个部分本身就是一个就像举个最简单的例子，就跟大家做菜一样，你做菜的时候，一开始你有很多种不同的调味料，有盐、有味精、有酱油、有醋。但是一旦你把这些东西丢到锅里之后，你再想把他们找出来，这个事情是非常难的。
	而背景噪音对于语音识别的影响，其实就类似于这样的一个情况，只不过能够去做的优化，就是未来可以去做的各种各样的一些优化是什么呢？第一就是可以通过声源分离，就比如说我把这个噪音和我这个主体就有点像，CNN里面的主体和背景了，我给它分离出来。这个分离出来有可能是他们的波段不一样，它们频段不一样，频率不一样，有可能是我做了一些比如说声纹的识别，我知道现在这个声音它是某一个具体的人在跟我聊天，然后我可以通过声纹再去筛掉一些背景。这里有很多不同的方式去降噪，去过滤噪音，大概是这样的一个情况。
	然后未来的发展也多种多样，一个最典型的其实就是多语言的知识。今天一个大语言模型已经可以同时生成和理解很多种不同的语言了。但是一个语音识别的模型也要做到多种语言的话，还是有很多困难的，其中一个巨大的困难就是训练数据没有。我们不同的语种都在上互联网，英文有、中文有、德文有，他们都在互联网上留下了很多信息。但是语音数据没有这么多，大家很少会把语音的这个数据放在互联网上，而且是公开的互联网上。这个就是有一个特定的moza基金会的这个数据项目，后面我们也会简单给大家提一嘴，大家可以去了解，可以去做贡献，也可以从那儿去获取一些非商业化用途的数据集，然后除了多语言以外。
	第三点就是这个实时处理的需求也是非常重要的这也是我们在这个0.4的版本代码里面也要去支持麦克风的一个原因。以便于大家如果未来要把这个项目再去做扩展，至少你知道怎么样在浏览器应用里面去支持麦克风权限的获取以及实时处理的这个操作。包括最近OpenAI的这个real time的API，其实都在对实时处理进行一些响应，这个是SR我们有一个整体性的了解了。
	那vest是一个什么样的模型呢？首先SR的这个任务，自动语音识别的任务，没有一个绝对的线去说谁是第一谁是第二。它只能在不同的数据集上面去比较，并且还要分语种去比较。这个我们后面也会看啊，但是他绝对是在sota这一档的，就是OPI的这个wasp模型。
	那vest是一个什么样的模型呢？它其实是一个22年才发布出来的网络或者说系统，那么这个vest是OpenAI发布的，它是怎么训？首先它的网络结构是长右边这样的，大家不想看可以先看左边的这个文字。我给大家讲一讲。他其实是首先他用了68万个小时的数据来进行训练。训练的时候他不再像以前的这个NLP的任务，或者说以前的用老方法去做语音识别的这个网络做的这种训练方式。他把这68万个小时的数据同时用来进行多任务的学习。也就是我们在这儿看到有一个tokens in multitask training format，就这堆数据可以同时被用来做多种任务的学习。
	典型的两个任务就是transcribe和translate，转译和转录。我不知道这两个词大家有没有见过，转译和转录的区别是什么呢？你可以认为这个translate是包含了转入再加翻译，所以转录加翻译等于转译，我看这个词大家能不能get到，那转录是什么意思呢？
	转录就有点像我们之前刚刚给大家讲的ASR的经典任务要解决的问题。输入一段文本，然后我输入一段语音，然后把它翻转录成一段文本。就像我现在说的话，是这个是中文，然后是正在持续不断的输入一些音频信息。然后这些信息如果大家用一个模型的话，它会把它翻译成中文的文本文字放在这里。但是转印就是指它不仅把这些识别出来了，还顺便把它翻译成了别的语言。那就变成了我在这儿持续不断的说中文的音频的输入的信息。然后通过转译之后，它就实时的得到了这个信息，并且翻译成了其他的语言。这就是他的多任务最典型的两个任务，他在做多任务的一个学习。
	OpenAI也通过transformer这种网络结构去做这个任务的或者说用transformer这种结构来做一个解决ASR这个任务的模型的时候，他也发现了因为transformer本身具有一个能够在大量数据上去进行训练的能力。这个在GPT系列它已经证明了。所以以前其实是没有用68万个小时这么多的数据来训练一个语音识别模型的。但他做了，并且他也通过数据去证明了越来越多样化的海量的数据是可以提升口音，就我们提到的不同的方言。比如说V3又增加了对广东话粤语的一个识别精度，口音背景噪音等等这样的一些问题的鲁棒性，同时因为他在做这个多任务的一个学习，他还可以去做更多的能力，相比于以前的一个模型，做一种一种语种的转译转录来说的话，就要好很多了。
	其实跟他自己研究语言模型的思路很像。以前一个语言段儿一个语言模型，后来是一个语言模型，支持所有的语言段儿，所有常见的语言段。在ASR这边也是以前一个模型识别中文，一个模型识别英文，现在可以一个模型识别多种语言了，所以未来的发展也是能不能可能一个vesta就识别，比如说100种不同的语言。如果可以做的话，然后还能把它做小的话，那这样的一个产品放在你的比如说耳机里或者手机里，其实是非常棒的。
	你想象一下这个language mental，可能它的只要这个体验做到极其好，实时的，然后价格还便宜，那你可能学外语的成本就真的越来越低了，甚至可以不用学了。这是vesper的一个背景情况。具体来看它的这个muti task是怎么去进行训练的，其实下面这幅图讲的比较清楚，这也是他的论文当中的一个截图。在这个68万个小时的训练数据里面，它分成了transformation和这个speech translation的这有两种任务。那么从这里来看，大家应该也能看得到，english transcription就是输入一段音频，并且说的是英语的这个文件，一个audio file。然后输出的是把这个audio file里面的音频转录成文字，并且是没有去切换语种的那ending to english speed translation的意思就是说输入是audio file，并且它可以是任何语种的，是法语、德语、英语、韩语、中文都可以。也可以是english to english，也可以说x to english或者叫any to english。这就是我刚才讲的translation，其实是包含了transformation的能力的。
	这个大家应该能get到，那这个过程当中是怎么做到的呢？他其实就做了一个encoder decoder的transformer，然后整个结构是长成我们下面这个format这个结构的样式，然后这儿我就不展开讲了。其实里面就是大家能看这个的流程图，能看到no speech这个任务就结束了，transcribe和这个translate分别有不同的事情要去做。好，这儿我们就给大家大概理解这个过程就好啊。
	那V二刚看的是V一的这个架构，那V2做了什么事情呢？其实V2相比于这个V一来说的话，它的数据集去做了更多的多样化的一个准备或者说设计，然后它的错误率会更低，然后跨数据集的泛化能力也会更强。简单来说就是我们看左边这个图有一个叫做WER。所有的ASR的任务通常都会把它作为一个黄金指标，这个WER word error rate，就是这个word就是我们识别的一个的单元，识别出了几个字，然后这个几个字的错误率，通常来说这个东西肯定是越低越好的，这个不用想。右边是它在这个不同的数据集上它的一个表现，V二是这样的一个变化。
	那V三刚刚发布的V3又带来了什么样的一个改进呢？这就提到了我们开始提到的一个词叫梅尔频率MFCCV3相比于V2的改进，它的这个频谱图增加了从80个增加到了128个。所以音频的细节就更多，采样的这个就更多，这是它的第一个更新。
	相比于V2，第二个就是它的language tag又增加了粤语，扩展了这个模型的多种语种语言的处理能力，然后扩展了它的训练数据集，使用了100万个小时的若标签音频和400万个小时的伪标签音频，这个是由open I的weber large v2生成的。所以大家看到其实从22年的下半年到24年这两年的时间，whisper模型迭代了三个版本。然后这三个版本的训练数据从最早的68万个小时的音频，到现在的V三已经到500万个小时。并且他还分了不同的标签来进行更稳健的表现的一个训练。这些其实是一个迅速迭代和发展的一个技术。大家如果在关注这个领域的话，vest肯定是不应该错过的一个模型或者说研究的样本。
	然后V3相比于V二还有一个点就是多多种语言上的错误率降低了。就是你想象一下，他为什么要做这件事情。我们看到应该是在年初上半年的候有一个demo，然后最近是开放给所有的用户了。
	大家如果在手机上装过chah BC的话，它的advance的voice mode就是它的。我不知道中文应该怎么翻译，先进语音模式就是ChatGPT的plus用户是可以用这个功能的。而用这个功能的时候，它其实就是有一个人在实时跟你对话。那你想象一下，它不可能搞N个模型，它通常来说这个advanced voice more背后是一个模型，而这个模型最终是面向给所有的ChatGPT plus的用户的。所以全球这么多种语言都会跟这个模型对话。所以它势必必须得提升多语言或者多语种的性能，降低错误率，这个是一个方面。
	第二个就是错误率的降低，还有一个性能提升，除了错误率降低以外就是zero shot，这个鲁棒性和准确性变得更好了。然后还有一个我们在0.4里也会用到的功能，就是这个长音频的转录。就是把长音频可以分块了，那我们也会用到这样的一个参数。就假设直接给了你1个小时的音频，让你去做这个transport，以前的算法可能就会崩掉，或者说不支持这么长。那V3可以支持你给1个小时，甚至你想分块也行。因为1个小时的处理的能力是模型的上限。
	但是你要想象你的上传如果是在一个网络环境里面，比如说你是在HTP里面或者在哪，你如果一次要把所有的文件全部上传上去，有可能会导致比如说这就涉及到很多网络开发或者说后端研发的领域了。就是你在一个网页上传了一个太大的文件，有可能n inx这边是会拒绝的，因为它太大了。或者说你在配置这个网络服务的时候，这里会有一些限制，让它可以去做分块，也是可以去做的。可以去做前端的优化和网络上的优化。这里就是他为进一步去落地做的一些好的迭代，然后还有一些就是方言口音，一些本身训练集里面就很少的一些低资源的语言上面有一些改进，具体来看我们可以看这幅图，这个是OpenAI的whisper github社区里面的一个讨论。然后这个讨论我们能看到这里的一个横的这个值，是WE是它的word err rate，所以值越小肯定是越好的。
	然后我们看到这里还有三个不同的model，分别是turbo v3和V2。在这个图里面我们看到V三显然是性能比V2要好不少的，4.3、5.8、4.75.5。Turbo是这个型号，就是turbo这个模型型号。通常是为了推理，就是你希望它跑得快，然后性能不要降低太太多的话，那么terribles是一个不错的选项，就相当于你可以用更少的时间实时的拿到结果，不用让用户等。但它的代价就是性能会下降，比较典型的是在V3里面的turbo，我们看到泰语和粤语是下降非常多的。大家如果有看到的话，这个泰语它的V三是5.8的WER，然后我们看到它的这个叫turbo的模型是20.8，这个挺挺不可接受的了。粤语也是一样，粤语如果我们直接用标准的V3是10.8，这个用了turbo之后46.1，这个是common voice这个数据集上的一个测试结果。然后它的WER就是错误率最低的前34个语言，它贴了出来，但这个截图我放不完右边是另一个数据集，whisper的这个V3相比于其他的这个或者说应该这么讲，就V三这个模型训练完了，就有点类似于我们在在微调里面才讲过。
	简单来说就是一个模型训练好了，我想要把它用到不同的场景里面。我可以去做量化，可以去做蒸馏，可以去做各种各样的事情，这些事情都可以把这个模型变得更小，计算需求也变得更少。但是整个这些量化蒸馏的技术的核心指标就是在尽量不影响模型性能的情况下干这个事儿。所以你不能为了小把模型性能干的特别糟糕就不好了，但是既然你变小了，你的资源变少了，其实你的速度就会变快。
	所以在这幅图里面，我们能看到X轴叫做SRD，它的测试结果是在A100上面，在一个A100上面的一个实时测试的结果，单位应该是毫秒，然后我们看到这个耗时tiny耗时是最少的。Sorry，说错了。这个提升的速度不是耗时，它是speed，不是毫秒。就相比于原始的这个版本，nh v3的这个版本，它的假设这个地方耗时是一秒。那么比如说turbo它提升了30倍，那它的速度提升了30倍之后，它的耗时就只需要30分之1秒。而tiny可能就只需要35分之1秒，这样去理解，这个加速是对应的咱们刚刚看到的前面那个图。但在前面那个图里面，其实我们只有V2、V3和turbo的WER的数据。但是从这个图我们能看到它的整个模型的这个family，就任何一个比如说V1、V2、V3的版本都会有不同的蒸馏或者量化的版本。
	中尺寸的小尺寸的face，还有这个timing turbo是一个相对来说在不影响这个性能的情况下还能提速的一个版本。所以在这个图里面，理论上应该是右下角是最好的一个区间。因为它既提升了速度，同时没有怎么增加这个错误率。
	然后在这两个数据集上，其实模型的分布都是一样的。那看了这幅图大家就能够有一个直观的理解，就是V3有large v3，还有large v3 turbo，还有V3 more base tiny，我应该用哪个？在这幅图里面大家应该就能理解要用哪个了。理论上来说，只要你的时效性要求没有那么高，然后你又有一些资源的话，就直接用V3，就用这个V3或者V2取决于你自己的一个情况但V3其实肯定会好很多，那用这样的一个tag模型的tag是最好的。
	我们反复提到的common voice这个数据集这个数据集其实是moc基金会下面的一个为学术贡献，或者说大家为了做学术而造的一个公开的数据。企业所有的人都可以为他去做贡献。就像这个首页看到的，你可以直接说话speak，然后你就可以把你的声音，你的voice贡献到这个数据集项目里面。
	然后这个项目大家有兴趣可以看一下这个UIL，这个数据集其实又一直在迭代，大家能看到9月18号才刚刚发了19.0。刚刚我们看到的V3版本是在common voice的这个15.0的版本上，有兴趣可以去刚刚的这个UIL里面写了它的官网去访问，这里还有一个分类，就是我们看到的年龄性别的一个分类。45%，然后百分之这个巴拉巴拉就这里写的有其实有38%是没有信息的。然后数据有一个license叫CC0，这个在咱们应用开发的这个实战引领其实也有讲过。就数据协议。大家对license或者说对这个数据集有质疑，不知道能不能用，用于什么用途的时候，可以去搜一搜，可以好好了解一下。
	到前面的部分，其实我们已经讲了SR的基本信息了。就是SR是什么？Weiser这个模型又是这两年怎么来的，并且它有一些什么样的优势，相比于其他的一些模型，他用了transformer，他用了特别多的训练数据，接着就会逐步到我们要怎么用它的环节。就是它是一个还不错的模型，甚至是一个很好的模型。那我们怎么用它呢？
	这里我们涉及到第一个，或者说我们会去hugin face上面获取的第一个模型了。首先hugger face是一家很厉害的初创公司，他们推出了一个平台，就像github一样。Hugin face也有一个hub就叫HF hub或者hugin face hub。我给他的一个定位就是它应该算是AI时代的开源社区的版本T0，这个词最近很火，版本T0应该就是这个赛道里做的最好的了并且它的这个功能或者说定位其实是超过github的这个范畴的那它这个开源社区AI的开源社区相比于代码的托管平台get hub，它又有什么不同呢？整个hugin face hub其实有几个重要的板块，其中就是搞AI的人最重要的事情就是数据模型算力，数据模型算力在这儿都有了模型。我们看到在这应该是几个月前我去统计的这个数字，现在应该更多了。在hugging face hub上面有超过30万的模型，包括8乘7B的mix，然后我们要用的whisper，包括这个大模型的天机榜等等，在上面都有啊。
	然后中间这一列叫做application，是它的space这个功能模块。在托管的space就是一个hugin face提供的这个具有云资源的一个推理服务的托管平台，你可以把你的模型通过space变成一个服务，然后他还有免费的这个或者还有很多档就跟你去租这个服务器类似。它就像开网吧的那我这儿有不同的型号，然后你又有自己的模型。你想跑在我这儿，让全世界的人都能通过hugin face的一个域名来访问的话，那你就可以这么做。像比较典型的这个stable diffusion就这么火起来的。
	然后open LLM need board，这其实是一个大模型的天地板，这个其实也是持续有人在关注并且很活跃的那data sets像我们刚刚看到的common voice，它在这个packing face hub上面有自己的data set。包括像prompt，我们这个第一个叫awesome chat BT prompt，也有人把它放在这儿。然后VKPD我们看到的各式各样的数据集，超过5万个都在hugg face hub上面有数据，可以去公开访问。所以它其实是提供了一个一站式的AI开发AI算法科学家需要的资源的集合地。并且它有滚雪球的效应，越来越多的模型开发者，数据集提供者都会把他们的成果第一时间先放到hugging and face book上面。
	之前的google因为收购了cargo这个机器学习平台之后，其实一直扭扭捏捏的没有把自己的开源模型均码往hugger face hub上面做第一优先级的支持。但经过了这一年，差不多一年，就金马开源差不多将近一年八个月89个月。现在他们也会第一时间的去往行业top上面放了。而且类似的像欧拉玛，他们也会第一时间的去做这个推送了。
	但huggy face hub比欧拉玛强的地方就在于它支持的更底层，它不是一个基于transformer的大语言模型的托管平台或者说托管工具。而是所有的机器学习。只要能够用marsh learning来描述的，或者说能用text flow和py touch来描述的模型，我都可以托管。他会做的更平台化，也更有未来的前景一些。
	这个是hugin face hub，hugin face hub上面显然也有vest large v3，这个是他在这个high face up上面的UIL，然后我们刚刚提到的V3的相比于V2的一些改进，两个重点。一个就是这个mail franc beans从这个80提到了128。第二个就是new language token，就是粤语增加了这个新的language token，它的模型系列，我们刚刚在那个图里面看过的这些不同的版本，其实它的参数量并不大，有一点几个亿，然后有不同的支持。我们看到Martino它就多语种多语言的支持，只有垃圾系列是支持的。所以如果我们要把它集成的话，就直接无脑选large v3就就够了。
	然后右边其实是在hugin face的主页上面，我们看到V三的衍生品系列。这个就涉及到我们微调训练营里面知道的一些概念。什么叫adapter？就lora里面的这个ALORA的A就adapter。什么叫微调？什么叫量化？其实V三有非常多的衍生品，包括基于它去部署的space上面的服务也有100个。对，这个是在真实的大家体感上可能觉得他好像离我们有点远，但真实的在AI的社区里面，它是非常活跃的一个模型，有很多的人在关注做这个二开，或者说做这个二开之后的模型托管，所以模型怎么获取我们搞定了。好，有了刚刚关于SR的一个整体性的了解之后，我们再来看看我们怎么把它变成一个服务，要写什么样的代码，这服务要怎么发布出来？
	要做到这件事情，我们首先要第一个解决的问题就是思考一下，一个ASR的服务要发布出来，有哪些事儿是最重要的。第一个就是我们已经解决的模型，我们知道在hugger face hub上面可以去获取模型了。第二个是什么工具？就我要用什么样的工具才能够去把它变成一个服务发布出来。第三个是算力，就是你需要有足够的算力去加载这个模型，然后用这个工具把它变成一个服务。对，这是最重要的三件事情，模型我们已经搞定了，算力其实不需要我们搞，你有就是有没有就是没有。那工具是我们接下来要去好好研究的，就用什么工具可以去很轻松的把hugging face hub上面的一个模型变成一个SR的服务呢？这里我们就不提，不得不提这个海滨face为什么厉害，大家看到过去的这两年，22年的六月份，hugin face的开源生态其实还只有hugin face hub在最上面。
	中间这个最重要的就是transformers，我们接下来要讲的工具，然后以transformers为核心衍生出来了很多data x管理数据集的evaluate评估模型的accelerate加速的，如果你有一些先进GPU的架构可以用来加速。然后这个oft mom类似的hugging face hub，就是我们刚刚讲的这个平台，版本T0的AI社区。其中它这有个小的叫space，就是用来托管这些服务的。那么到了二三年的六月份，这一年时间有这些变化，包括我们经常会看到的safe也是一样。最近很火的字节实习生投毒，其实这个投毒不是读人，是读这个训练过程，其实有safe tensor，其实一定程度上也能解决这个问题，就他改了其中的一些tensor的结构什么的，没有去做校验。
	最早的pyto ch的这个病文件其实是没有校验能力的，它本质上是一个序列化的字符串，c tensor其实有一定的校验能力，所以他很早就洞察到了这一点。在hugin face hub上面上传的模型，其实现在很多都会优先支持sip tensor。类似的像这个的users，我们做纹身图的同学应该经常听过，然后transformer agents，甚至在二三年的下半年还出了这个TRL，这些在AI大模型的微调训练讲的，我们这儿只是给大家了解一下。但有了这幅图，大家应该知道，无论怎么变，其实在hugin face的开源生态里，最核心的还是这个transformers。它下面还有三个小logo，TensorFlow python jx，那么transformers显然是核心。那transformers到底是什么？它是一个python库，然后为什么说它比欧拉A做的更底层？是因为它是一个面向machine learning的库，它的名字取得也很好，sota machine learning for jx PyTorch and TensorFlow. 
	令人很唏嘘的是，jack和tensor floor其实都google开源的。然后现在都已经变成把复杂度封装起来的一个看不见的框架。而它transformers就借助transformer这个网络结构的知名度，一跃成为了开源社区。非常或至少说不你不是一是在搞这个AI的，你是半路想加入的，或者你是学生刚刚才开始学的那可能transformers的注意力就被就因为他频繁的出现，你就会被他给吸引到，才会你会去了解他。现在几乎很少会有人再去研究jack，还touch了TensorFlow了。
	它其实本质上一开始，首先它开源的时间很早，他最开始是想要做这个语言模型的一些开发的工具库，后来已经可以扩展到很多的机器学习模型了。所以他也从language model的SDK变成了一个machine learning model的SDK，然后提供了很多预训练的模型，而且可以方便的去跟他的hugg face hub去做联动。然后有了这个图，大家就能感受到它现在可以跟很多生态去做联动。包括微调的去做这个transformers的强化学习的，跟hub，跟数据集，跟评估，跟各种各样的东西。那微调是这个PFT，所以它的价值是比较明确的，这儿我们就不再展开了。
	大家能通过刚才的这个讲述去了解它，我们一个直观的感受就是它的开源社区的关注度。这个它是一个SDK python的库，它也是开源在github上面的，就是这个transformers。我们可以看到这三根线分别是tensor pro pyto h和transformers。然后transformers已经早早就超过了排套去了，然后时间其实很早，我们在在这儿我们看到它其实是在18年D开源的transformers，然后这个flow应该是15年底，py touch应该是16年底。那我们把这个time online之后，我们看到其实它有快接近TensorFlow的同一个时间。这个就是指从开源的第零天啊第一天开始，咱们就拉齐看一下每一天我们增长了多少。它其实是还这个加速度还在往上升，就随着咱们的LLM HIGBT出现之后，很火。
	这样的一个框架是值得大家去研究的，也是我们微调训练营的主力的SDK，就主力讲解的内容。我们今天因为时间原因，只能给大家讲一讲怎么样去把transformers用这个框架把它hub上面的模型变成一个推理服务。这个transformers要快速去掉它的各种各样的线，我们叫下游的任务，就是把模型用到具体的任务上。微调也是这个逻辑。它其实有一个更高层次的抽象，叫最高层次的抽象，叫pipeline管道，这个管道跟轮圈里面的那个那个竖线不太一样，这个管道特指transformers里面的这个API pipeline pipeline。这个pipelines在这里有个表，我们可以看到有四个重要的解读，怎么解读呢？
	Pipeline是一个高层次的API。这个API当我们去用它的时候，就是可以去解决一种特定的任务。一个pipeline对应一种任务，然后这个任务又可以被分类为不同的模态，就是我们看到的第一列modality模态，音频的或者说语音的，computer是计算机视觉的，在云处理的和这个多模态的。这四种不同的模态下面各自有自己的任务。像我们前面提到的SR automatics speech recognition，它属于audio的，audio现在跟NLP在pipeline里面是分开的，那么这个SR它的描述就是为音频，将音频文件当中的语音提取为文本，就是输入是audio，输出是text，这个是它的中文描述。
	Pipeline的API也很直接，就是你可以后面什么都不写，但是你一定要写清楚你要干什么任务。比如说pipeline task等于automatic speech recognition，那么就可以去实例化一个能够支持SR的模型的推理服务了。具体来看，比如说我们用这个notebook，我这有写有写这个UIL。如果之前没有学过微调训练的同学，想要去实践，他能有刚刚那么多种不同的任务要怎么玩的话，可以看这个UIL，这个是微调训练营里面的一个一节课里面的一个代码示例。当然你要去用它的话，记得要新建一个虚拟环境。然后去参考这个微调训练营这个项目的read MI去安装环境。
	就比如说我们刚刚说到的pipeline API，要去实现一个文本分类的任务。假设我们这儿这个task的这个参数是可以不写key的，因为它是第一个参数，那么我们就直接传一个sentimental analysis。那么这个pipeline当它的task等于这个sentimental and analysts的时候，它的model有一个默认值。这里我有写了，只如果你仅指定就是只传一个task的value的时候，他也会用默认模型，但是不推荐这么做。我们正常一点的这个或者说推荐的做法，应该是你至少应该传入task和model啊不要用默认模型，因为它太隐含了，容易出一些奇怪的问题。
	在这儿比如说这个sentimental analysts这个文本分类或者说情感分类，它其实本质上是支持两个输出的。一个就是你的情感，一个就是它的一个判断的分数。比如说积极的、消极的，或者你是开心的还是怎么样的。那么这儿我们说今儿上海可真的，他就判断这个negative是消极的。然后执行度0.89，大概是这样的一个操作流程。所以我们看这样的一个调用方法其实是极其简单的那假设我们要用它来做QVA，那其实也一样。我们去把这个task指定为question authority，然后我们这个QA它需要提供这个context上下文。
	大家要想象一下，不是每个模型都像GPT或者说ChatGPT这么强。其实在ChatGPT出现之前，所有的科学任务基本都是有上下文的，然后你的答案要从上下文里面去找，但它不是直接摘出来一段，不是字符串匹配，而是理解语义之后从里面找。然后后来的InstructGPT推出来了，这个之后又定义了什么叫inside instruction tuning，就是指令微调，才把QA又定义成了两种任务，一种叫做open QA，一种叫close的QAQ。一种open QA就是那个开放式的问没有context，close的QA就是有这个context。
	在这个封闭式的context里面我们去找答案，那在这里显然是close QA，因为它用的模型是很弱的，用CPU在跑，甚至一个很很小的模型，E参数上下的那就问what is the name of the repository，然后the name of the reported the hugging face transformers。它的输出就是下面这样，score 0.9327执行度start end就是指它的答案是来源于context里面的第30个这个字到第54个字符，下面又问willing the capital of china，然后这个context是我从VKPDR里面捞出来的一个1949年10月1号，CCP的主席就中华人民共和国的主主席宣布成立了。然后这个没有直接写capital，大家如果去读这段context的话，但是我们看到这个模型它其实理解那输出answer是北京，因为他是在北京的天安门宣布的国家成立了，所以他知道是北京，是这样的一个逻辑类似的。
	那假设要实现语音识别怎么做？这儿我们用master small这个模型就能做到基础的识别任务我们指定了现在的task是ASR model是OpenAI的这个westers more，然后我们要做的任务是这个，或者说做的任务是SR然后我们把这个pipeline给它取个名字，这样跟我们一开始讲SR它支持什么样不同的任务能关联起来。这里我们就去做转录，这个pipeline输入的就是一个音频文件这个音频文件能够丢给这个pipeline之后获取到一个文本，因为整个SR就是输入一个音频，会给你一个文本，这个音频里面识别出来的内容就是i have a dream one day nation will result。
	经典的一个发言，稍微讲一下这个pipeline的原理是什么。其实整个模型推理的过程，它都可以被分成这三个阶段，就不只是SR，而是所有的transformers上面的这些模型。这三个阶段我们细分一下，其实可以看到有token either model和post processing后处理took either。应该是很多人听过但是没整明白的。简单来说就是这个token im的作用，就是把一个原始文本变成特征空间当中的一个具体的特征的一个过程。就比如说the cause is amazing，那他要干的事情，说大白话就是这是一段这是一句英文。这句英文我们真正模型在处理的时候不是处理这一句话，而是这一句话会被分成很多个单词。像我们人来看的话就是四个单词，一个标点符号。This course is amazing. 
	但是对于这个模型来说的话，它未必是这么分的，它甚至可能会把amazing翻成了amaze ING，表示我现在正在amazing。因为我们人其实去提取语义的时候，也有这样的一个过程。只不过我们在学这个单词的时候，我们知道mazing一个单词，但它有别的含义。那么类似的，假设有一些别的单词，它可能也会根据词根去做拆分，但我刚刚说的这个amazing n这个例子不是说正这个正在的意思，因为它不是个动词。想描述这个逻辑就是分词再加这个embedding的过程。所以通常你也能看到一个句子它的单词的个数和token的个数是不一样的，这个也很正常，取决于它自己的这个分词和embedding是怎么去实现的。
	而有了这样一个过程之后，真正的模型输入其实是token的序列。这个token就是跟模型空间里面的那一个一个的他之前训练出来的这些我们叫向量也好，叫这叫vector也好，这样的一个对齐的一个值，然后这个值才会丢到模型里面去做这个网络的前向计算。前项计算之后输出的就是这个logit，把这个logit最终通过后处理才拿到了一个特定的结果，因为后处理可能会是有不同的处理方法，有的是分类的模型，有的是我们叫生成的模型。分类的模型最简单，直接可以换算一下就得到了，因为他这边可能不是归一化的。
	但是像生成的模型，在我们的这个训练营的微调，训练营的课里面，大家就会看到有一个fine tune。其实它是全量微调了一个google的bert模型来做QA来做生成。那在过那个里面，他虽然也是输出了一对概率，但那对概率其实就跟我们前面看到QA任务一样。他要去我们的context里面去找结果的时候，其实还涉及到一些后处理的方法，才能知道我到底要怎么样去匹配这个起点和终点。有兴趣也可以去LLM quick star，或者说这个微调训练营里面去进行学习。刚刚我们说的token lizer分成两个步骤，其实这两个步骤就是分词再加上embedding n。一个最典型的大家如果想要去延展了解的就是word web是这件事情的开始。
	有了word web之后，后来大家就发现把单词从一个字典里面变成一个词向量空间是一个很靠谱的事情。并且因为我变成了一个词向量空间invading的过程又提出了一些语义。那么不同的这个词，它们之间其实是有语义的信息的。就像man、woman、king和queen一样，他们man减去women和king减去queen之间的这个差值是最靠近的。因为他们都剥离了这个属性了，或者说类似的含义内涵语义级别的一些信息。并且更神奇的事情是，我们如果把这几个单词换成了中文或者说法语，那么这个信息的保持的特点依然还存在，所以embedding n就成功的。成为了所有的我们叫语言模型的第一件工程或者说工作。把一个一个的词，我们叫原始数据变成一个特定的向量。这个过程我们再回到今天这节课的开头，我们讲SR其实也是一样的，就去提取这个频谱的特征，其实都是把原始的row data的特征找到，然后在这个特征空间上面再去做进一步的训练和学习。
	这个是所有的深度学习技术，包括transformer也是深度学深度神经网络的标准流程啊啊啊那么右边这个watch back的这个图，在整个pipeline里面，其实我们看到，大家如果去深入的研究，我们再稍微多说一句，就是transformer的这个网络其实就只包含in bedding和layer。而在transformer这里大家注意一字之差一个单这个叫什么？一个字母之差。Transformer network特指我们学过的就是google发布的这篇论文。所有的基于transformer的网络，它其实只包含in bing和layer。大家可以去回看我们预制的这个课程，就前置篇里面有讲过attention transformer的网络就长这样的。但是在transformer这个SDK这个python库里面，它为了让代码简洁，接口简单，它的model会比transformer network还多一些，多了一个隐藏层。
	Ahead这个head就是面向最终的下游任务，也就是我们说的pipeline API那些任务，12345有各种各样的任务，那这些任务到底是怎么样跟我们的这个model去进行连接？其实都是通过这个head来进行表达的，因为它就是最终的输出了。而transformer network更像我们在在前面的最早的章节里讲的，它就是一个经典的transformer的网络。而很多下游任务的训练对齐，包括这个网络结构的定义，都是在这个python里面的top player，或者说更上的这个层里面去定义的。所以有这样的一个区别。大家如果去读相关的文档或者说代码的时候，要捋明白。Transformer和transformer是两个东西，一个是一种网络结构，一个是hugin face开源的一个python库。好，那么到这儿应该我们能很清楚的知道，hyper API是一个我们可以用的高层次的API是一个抓手。
	下一个小点我们就来讲怎么样用radio和hugging face的transformers来托管这个SR的服务。重点就是这个服务化的过程当中，不仅有API，还涉及到了一些权限等等的问题，包括环境的管理。我看一下时间。我们到这儿可以先提两个问题，看大家有什么问题我们先快速解答一下，然后再往后去推进咱们的内容。
	大家有什么问题吗？
	还有人在线吗？
	没啥问题。这个同学问的那些什么环境什么的，那是下面要讲的内容。前面讲的内容大家有什么问题吗？没有的话，我们就往后推进了。
	大家还有什么问题吗？好，先往后讲。好，那么我们接下来讲怎么用我们已经学过的刚刚讲的这个transformers和我们之前已经学过的radio来托管一个SR的服务。首先我们其实在上节课就见过whisper large v3的1个最佳实践。在radio 5的这个最佳实践里面，radio的一个研发负责人重点提到了。然后这个示例我们在这里能看得到，它有一个UIL，大家有兴趣可以去访问。那我们要怎么样去做一个这样的服务，并且不托管在space上面。这里其实是把这套服务用hugin face的space托管起来的。但我们现在希望它在我们本地运行，然后在我们的服务器上运行。
	那怎么做？首先我们需要做一个环开发环境的搭建。这个开发环境有一些特别，所以我单独提一下，就它不再是用一个type install r杠requirements就能解决的。因为我们涉及到了语音识别，还需要引入一个非常有名的多媒体处理的库，叫FM pack，就是我们左上角看到的操作系统级别的依赖。然后我们还是以这个links系统为例，大家可以用APT install FM pack。如果你是非linux系统，可以去它的官网去查看一下，怎么在你的操作系统上去装这个FMP。装好之后requirements也更新了，可以重新instore下。这个就是它的官方文档，这个最下面是它的链接，大家可以去官方文档上去查一查怎么安装。
	Apple pack在对应的操作系统装好之后，它的代码其实简单的。因为我们前面看过了，就是一个pipeline的应用。那左下角我们看到我们从transformers里面import一个pipeline。这个pipeline有重要的两个参数，前面已经讲过了，一个叫task，task我们设定为automatic speech recognition，这个没得说。
	这个任务model我们设置为OpenAI的vesper large v3，这样它就能直接去hugin face hub上面去获取这个UIL，就是我们看到的open I vester large v3。只要我们model传这个，那么他就会去去到这里，这个路径就跟我们用get up上面看到有一个开源项目，我们想要get come on get pull一样。这个URL就长OpenAI west or large v3，就是我们看到的这个UIL。从hugg ging face到CEO之后，这个路径就是我们的model name。所以我们在pipeline里面传的model就可以传这个，完全一样的，就是这个可以分段。我们看到V三支持分段，长音频分段，那这个时候我们可以把每个分段的长度，每个音频片段的长度做一个设置。
	60秒，然后指定设备，这里我用的是一台服务器，它上面有GPU，我默认就用GPU来跑会快一些。但如果你没有GPU，这是好消息，这个vesper也支持在CPU上跑。所以你看这里的这行代码写的库打0，touch gooda is available，这样就会去有一个true or host的返回。如果你本地的酷大相关的环境都装好了没问题，那么这里自然就可以用你的扩大0，也就是零号GPU，不然的话就CPU。那device等于device，这是最基础的一个pipeline管道。
	然后接着我们要进行语音识别的时候，定义了一个函数，因为我们下一节课要用这里的这个OpenAI的whisper PY文件，然后会复用这些代码，定义了一个叫ASR的函数，那这个函数的输入是一个音频文件，audio file task是transcribe转入。然后我们看到这里就有一个细节了，就是动手能力强的同学应该都能或多或少的找到我们的应该是这一页，我们都能看到这里有一个最佳实现，并且它在hunting face上面有一个链接。但其实这些space它托管之后，它的文件也是可以选择公开出来的。事实上它也公开出来了，但它公开出来的文件里面就少了这里我处理的一个流程。大家如果想要自己去捞回来的话，会发现有些问题，就是少了一个这样的流程，这个流程有可能是space帮他干的，就是转换音频文件为WAV的格式。说人话就是如果大家对音频，对多媒体了解的深的话，知道这个门道有多深的话，你就会发现咱们的同样是MP3结尾的这个文件，或者同样是FLCMP4结尾的这些文件，它内部还有很多细的格式，就像PDF这个点PDF1样。那么convert to WAV这个函数是我们自定义的，它会把先用FM pad，先把这个音频文件加载，加载之后转换成一个标准的格式，然后再把这个文件作为我们的SR的pipeline的输入。
	所以我们看到最终这个pipe去执行的时候，传入的是这个转换后的WMV格式的文件。然后有个best size，有一个task，有一个result time steps，这些大家就记下来就好。这三个都没啥好说的。然后有一个识别结果，就是我们用VSV3的一个文本输出的结果，最后我还会把这个临时文件删掉，所以也不用担心这个临时文件带来的问题。
	这个是它的pipeline的API，有了这个我们至少知道输入的一个音频文件可以变成文本那radio怎么办呢？Radio可以看到这个是关于，所以我们要做一个west life的前端的相关代码。首先它要支持音频上传，所以我们有个transfer的方法是支持音频上传的。
	这里我们也要去判断一下，就是他到底有没有提供音频文件。因为大家会发现用radio的时候，有很多button，有很多按钮可以按。但如果你没有提供输入，你就按了它其实会报错。但如果你没有去检查，它甚至都不会给你任何信息，你就卡在那儿了。或者他他error报出来之后，就只有一个error在在他的组件你也不知道发生了什么。所以我们对于一些情况会去做判断，比如说没有提供的，格式不对的等等，那么这儿通过radio点error，它能谈一个error的消息框，然后右边这一部分的代码，其实就是gradual的UI部分的代码了。
	比如说我们用一个blocks，然后这个blocks里面有一个叫做tab的interface，就是两个tab。我们在language mental里面也用过，这上面就是具体定义了这两个tab，分别是用麦克风输入和用这个文件上传，音频文件上传。那它就长这样，一个是音频转录，长这样的一个样式，一个是麦克风。所以就刚刚那个就是它的主体代码，有这个radio的部分的代码，应该是大家最熟的transfers的代码。如果之前没有用过的可以去回看一下。最近这两部分的这两个小节讲了pipeline API，并且把pipeline API和radio做了一个集成。
	然后我们在这个麦克风这儿是要稍微花点时间给大家讲一下。就是假设你未来要去做实时的这个语音输入的话，麦克风这个组件和麦克风的权限在这儿。我们看到，我是用浏览器，用我的book的浏览器打开了刚刚写好的这个代码的服务，然后它也显示了我有一个macbook pro的microphone，mack phone这个是可以选的，你有多种，然后点record它就可以开始用我的macbook的这个麦克风开始录了。但是如果我们什么都不干，就比如说大家把代码捞下来，然后你直接去运行它的话，有可能会有问题，问题就是你这儿找不到输入的麦克风。为什么会这样？其实主要的原因就出现在了这个语音输入，这个语音输入特指麦克风的输入，它的这个输入是有权限的要求的，尤其是你用的chrome浏览器。这个权限的问题的解决方案，其实主要就通过HTTPS这个方式来解决，HTPS来启动你的服务。
	那我们接下来看看怎么样去解决这个输入权限的问题。这个权限问题，我把刚刚那个界面再往上多截一点就会看到。首先我们现在的这个界面已经配置了域名了，这部分可以先不管。然后前面是用的HTTPS，我们关注过radio的服务的话，就会发现之前启动的所有的服务都是HTTP的，而不是HTPS的。
	那么右边，其实我们点一下这个麦克风，这个icon，这个图标，也能看到prom给出了很多的信息，其中这个锁就是告诉你connection is secure，这个连接是安全的。SSL证书是通过了验证，然后麦克风的权限，现在麦克风正在使用，需不需要重置一个权限？那cookie在data about this page等等。这个是配置好了的状态，大家可以自己去捞一下自己的这个，或者说自己去启动一下新版本的代码，然后看看你的麦克风能不能使用。这个启动了是启动open I vester这个服务，待会儿我们也会去演示。
	这儿稍微讲一点，我知道有些同学不一定了解这个事儿，HTPS和HTTP是两种东西，那啥是HTPS呢？其实这就先说什么是HTTP了，HTP是指hypertext transfer protocol。如果我们学过计算机网络就知道计算机网络有经典的七层架构，那么HTP通常来说是在应用层的最最顶层的这个架构了。然后HTTP就是这前面的超文本传输协议，那S就是。Secure加了安全的超文本传输协议，是HTTP的安全版本，怎么实现的呢？在原有的HTTP协议上面添加了SSL和或者说TLS层来实现这个数据加密。对信息安全技术比较熟的同学就知道了，TLS其实也是SSL的早期的这个名字，后来基本大家都叫SSL了。
	TOS是更早的时候，计算机网络的早期的时候大家描述它的一个方式，当然有很多代码，包括软件仍然在沿用的那重要性不言而喻。因为你要知道，像中间人工机，像各种各样的计算机网络安全的事故，其实都是因为这个服务提供方他没有使用HTPS，这儿我就不去列举了。然后HTPS本身它的数据传输也是加密了的，不太会被拦截之后做很多事情，对这个是重要原因。
	而我们刚刚聊到的这个事儿，为什么会有HTPS的要求？就是你的个人麦克风，语音输入的你的声音本身在互联网上或者说在绝大部分国家都是属于敏感信息。所以chrome浏览器当然会有这个要求，我觉得也是非常合理的。所以简单来说，要想用麦克风，要想在chrome上面用麦克风，需要你的服务本身是提供的HTPS的这个协议来进行运行的。
	那他们之间的区别到底是啥？就是刚刚的描述其实是说更安全的版本更安全的版本。那实现上面是用了一个加密层，就是我们看到在HTP基础上增加了一个加密层。然后同时可信度上面有浏览器来提供这个可信度，就比如说我们在这里看到的这个个小锁，如果我们这会儿没有在用麦克风，这个iphone就会变成一个锁，这chrome大家经常用的就知道了，那就说明这个网站是安全的，相对来说，这是信任度的问题。
	第三个就是ICU的影响。因为现在大家除都有想去做这个独立站，但其实独立站的流量问题是非常严重的。所以又都会去做SEO，做搜索引擎优化。然后做SEO像google现在几乎不会给这个HTP的这个网站或者说服务去导流，或者说做搜索引擎的优化，这也是一个点。
	最后就是性能，HTP two或者叫HTPS的这些协议和服务本身性能也要会好一些。这个就是计算机网络相关的技术，我们就不展开了。落到我们要的推进，我们部署的这个问题上，我们缺什么？缺的其实就是第一条，这个HTTP基础上我们要做一个证书。这个证书有了，我们就至少解决了安全性的问题。那个锁那个小的锁就有了。
	这个证书SSL证书其实本质上是由CA经常听到的，叫认证机构颁发的一种数字证书。它是一种证明，这就是经典的。只要在这个大学本科研究生稍微涉猎过信息安全相关的课程，应该都会讲到，就是这个证书加密是怎么一回事儿。其实就是你需要提供你的服务方，需要你作为服务方，你需要提供证书。然后在数据传输过程当中，可以通过证书来进行加密。那传输的就不再是明文数据，而是加密之后的数据。简单来说就是这样的，这里我也简单提到了，SSL代表的是安全套接制程。然后虽然很多的网站还是使用TLS叫传输层安全协议。
	因为SSL本质上是TLS在某一个时间点之后，大家其实技术上都改叫SSL了，这样的一个历史，那么怎么去装它，这儿我们就会涉及到一个具体的操作了。第一，我们为了因为学习的用途，我们不太会让大家去花钱再去找一个商业的CA机构去买这个证书或者说租这个证书。因为它它也需要去续费的，它有点像云服务启示，按年限付费的，我们会用一个开源的软件叫做nets in cracked，来进行自签发，如果我们不把它用到广泛的商业化，其实是够用的，然后也是认的，然后是这样的一个流程。
	那如果我们要去申请？这个时候就有很多的CA机构，大家可以去去网上搜索。然后在这个过程当中，你的服务器环境不一样。比如说你用的是engines或者你用的是阿帕奇，它的这个证书的安装和配置的步骤也会有一些不同。那么在这儿我们确定咱们要演示的，或者说咱们课程当中要教大家的这个流程，其实是流程是通用的。然后如果你自己实际最后要用的服务器环境证书类型不一样，它其实步骤不会有变化。只是有一些细节上，你可能根据文档或者跟GPT的沟通可以去做适配。
	所以最后我们要做的发布服务的准备工作就是三件事儿并且我们这里认定了，或者说我们假设你是有域名的，那更好。如果你没有域名，没有关系，你最后就把它签到一个签发。然后这个代理以IP port，就是IP加端口的方式去访问，也是OK的。如果你有域名那更好，其实就是把那个IP加端口去做了解析，做了域名解析，然后那你就可以通过域名去访问了。那这儿我们假设有域名，那么最终就是有域名，有证书，有反向代理，那么后面的版本我们也都会有域名了。
	因为最终我们这个agent希望它是一个生产级的，所以我们在第三个agent项目里面会教大家怎么把网络这部分也给解决好啊，在后面的版本我们就不单讲这个事情了，域名的配置怎么配，首先你得去你的域名的服务商那儿，如果你有域名你就去登录，没有域名，你就觉得这事儿好玩。你可以去找一个服务商去买个一个月或者一年的玉米，应该至少就得买一年，然后这个是我用的服务商叫axel net，UI非常不友好，并不推荐大家如果有自己知道的这个域名服务商去去关注去用就好了。然后最关键的是要配置一个A记录，A记录是什么？我就不展开了。然后我的服务器IP写好，然后有一个A记录，然后这儿配一个at，就是指我们的这个应该什么。比如说我们有一个特定的域名，在我这儿我就是把公司的域名暂时用了，用宅基data点com然后配艾特，就是一个所有的，只要我的二级域名是载机data，然后我的这个顶级域名是dot com，所以都可以被解析过来，这个是一个配置。那配置好之后，理论上只要我配置好了，生效了，你去NS book去去看这个域名都是能够被正确解析的，只是有没有服务的问题。
	这个第一个步骤完成之后，那第二个步骤其实就是这个SSL证书的安装和备份了。我们要把这个SSL给证书给它装好，这儿我们就推荐用一个。当然你有商业的，你本身就有你公司或者你个人有这些东西，那你就用非这套流程也是一样的。但如果你只是学习用途，你都没接触过这些，那么你参考这个事例是OK的用next in cracked去生成一个免费的SL证书，怎么做呢？首先第一步就是设置DNS的解析，这个我们在第一步已经做好了，就确保你设置了A记录，然后去装这个third boat，因为我们是在linux的系统上面，然后我们就看到support可以生成管理这个证书，怎么装呢？Update一下，然后install这个support，以及support的n inx。因为你的证书签发之后，它会给你的NYX的配置去做一些更新，后面这个就会有对应的用途，然后生成这个证书，就比如说sr fort杠杠n inx，然后杠D，后面这个your domain就是你的域名。然后比如说刚刚我们那儿写的是at，他会去适配所有的以这个二级域名。
	就比如说举个简单例子，假设你最终的这个服务想要部署成3W点UR domain点com或者说ChatGPT点UR domain点com那么理论上你这儿应该签一个就是这个生成证书的时候，在前面应该再加一个点心，就是证书和解析是两回事儿。想要把这个事儿说清楚，这个证书只会为一个特定的，就相当于他担保一样的。我说的通俗一点，就你这个地方传进去的这个certain bot杠engines生成证书的时候，传进去的这个东西，最终在浏览器那儿它是必须得完全匹配的。
	然后如果你想要最终未来用的时候，前缀就是三级域名，这儿要各种换，你这儿就签一个新点your domain com，但是不去不推荐，因为他签的证书范围太大了。像我的话，其实我最后的签的这个证书是ChatGPT到载机com data点com，就是一个三级域名。给这个三级域名签了一个证书，它就会生成一个证书。生成证书之后，你就可以去这个单，你就可以直接去用它了。然后如果你要自动续期的话，可以用下面这个V6转让，它会自动设置这个续期任务。
	然后最好你的证书如果你真正要商业化了，是需要备份的。然后这个一旦出了问题，比如说线上出了故障，或者服务器挂了证书在的话，你能够立即考进去，这个服务就继续上线。不然的话你还得折腾这个证书的事情。然后国内的话我印象那么好还去备案，所以这个流程大家要去稍微了解一下。
	然后备份有多种方式，第一步肯定是确认你要备份什么，这里是有四个文件，主证书、文件私钥、证书链和主证书和证书链的组合，然后压缩变成一个压缩包，你可以把它备份到你的服务器上，或者你自己的某一台机器上，也可以用脚本来进行备份。这个就是把前面的这个自动化，而且是定期执行的。以上这个就是第二步，我们把证书给装好啊。第三步就是去做NX的配置，很多同学可能没配过，你就直接使用这一套这个配置就好了。
	然后我们可以看到它有几个主要的部分。第一个，当然这些里面的这个比如说server name，你可能得改一改，然后证书你得改成你自己的，然后剩下的应该就没啥要改的。如果你是用的radio的默认端口，那也是你的local host 7860，你只需要改server name，然后改这个证书路径就可以了。
	好好，这个是我们主要要去发布服务要做好的几个步骤三个步骤。接下来我们来实操一下，看看怎么玩儿。在0.4的这个分支，当然也是命的分支，我们为了明确切到0.4的分支，在0.4的分支里面有一个新的文件，叫做OpenAI whisper这个PY文件。那么这个PI文件里面就定义了我们刚刚说的这些内容，model name，然后device I plan，convert to wave SR, 然后transport，包括我们这两个interface和最后的这个blocks启动。
	这里我建议大家，因为如果你配了域名，那就是公网可访问的了。这儿一定要设置为有密码的状态。然后这个share等于false是什么意思呢？之前没有专门讲过，其实就是指这个地方share设置为true的时候，你其实是可以把它托管到hugging face，或者说通过hugin face的一个临时的链接是可以访问的。然后如果你设置为boss，那就只有本地可以访问。但没有关系，因为你又设置了index反向代理，所以大家可以通过域名访问到你的服务器，服务器通过enix的配置又会转发到你的本地的7860端口，所以设置为boss是对的，这里专门做一个说明。
	好，那么这个文件我们先启动一下，看看是什么。什么概念？这里我们就。Python稍微放大一点，去启动这个source下面的OpenAIr vester，因为我们这儿把它做程序运行的时候，是可以启动这个radio的。
	就只是running on local URL。
	这已经启动好了，我才想起来他没有更多的日志输出了。然后我们可以访问这个ChatGPT，这儿我再额外给看一下这个links的配置。这个就是这个engines的配置，然后记得它需要通过速度来进行访问，因为它是root的权限。然后左边这儿其实我们就能看到这有一个麦克风，有打开和关闭的这个权限。如果我们关闭，其实你这儿是没有办法录制的，你就能想reload，你看他这边有写。放大行，这个也不会放大，就带有行字，就大家能看。Reload this page to apply your updated sitting on this site. 
	Reload之后，你大家看到这个好像不能放大了。This page没有麦克风，no麦克风放这个地方，没有麦克风也无法点击，就是没有给他权限。那我们如果把它打开，这个是HHTTPS才可以的。如果你是HTP这儿是没有这个选项的，这就可以去选，用的是这个macbook，再放大一点，用的是macbook还是teams，还是we meet，还是我的摄像头，摄像头也有macbook的功能，那这我们可以就用macbook然后试一下。
	我们可以在这儿看到这个实时的日志输出，假设说测试中文english。输入识别，stop，我们播放。这里会卡一下，这个卡一下，目前我没有找到特别优雅的解决方案。这卡一下的原因是刚刚我们是麦克风录入的，然后他录入完了之后，我点stop这个radio的组件，会把我刚刚一直在讲的这些内容，把它C保存成一个音频文件。这样我再点提交的时候他能提交，但这个过程会导致前端界面卡一下欧巴性的测试中文english。输入识别。好，那这个播放没问题，我们提交一下试试。右边就获取到了一些信息，上传的音频文件，识别结果测试中文、英文输入识别当然我们也会传入一个音频，这是我传入一个提前准备好的上传uploading。
	播放一下，老彭，你给我讲讲什么是多模态大模型然后提交。这个也可以识别OK所以这个whisper跟radio的集成其实是简单的，代码册因为已经有了。但是这个部分难的点是在于大家要把那三个step给搞一搞。应该对于咱们去了解网络等等的配置是很有帮助的。这个radio的代码是在OpenAIr whisper，专门取了这样的一个名字，方便大家去寻找。然后里面有对应的比较详细的中文注释。好，那么这个是我们成功的把OpenAI的wasp模型和radio做了一个集成。
	然后我们下一个小节再来讲，就签PPT0.4的功能。并且我们再看，如果我们有了这样的一个签约PPT0.4的代码，又怎么样能够绑定到这个域名呢？需要做什么额外的工作？如果这个大家理解了，应该就把我们的域名绑定，NX的反向代理和SSL证书的逻辑关系就给真的屡清楚了。好，我先把这个服务停掉。行。好，那么我们接着再来讲最后一部分，也是有了前面的铺垫。
	各种讲解之后，包括实践给大家看完之后，我觉得ChatGPT0.4的语音识别功能的集成，城市化内容的这个布局，这个是我们这个小节要单独讲的。因为前面没有这个附件，以及我们怎么样把它跟一个特定的域名去做绑定，并且去配置这个SSL的证书。然后它就可以支持这个语音输入的这个麦克风的权限，并且也更加的安全，这些是在0.4我们要去做的工作。
	第一个就是这个多级内容的布局，这个多级内容什么概念？我们先看右边，右边是我把测试输入的文件又给做的更样式多样了一些，这个多样就体现在我用黑色就选中的这一行，第十二行，能看到首先玻璃的points有多种级别了，有一级、两级、三级，maybe它能支持更多级。然后这个还有用两个星号包裹的加粗的，就在mark down里是加粗的这样的一些内容。这个其实是新样式，也是所谓的多级内容和层次化，并且支持加粗这样的一些设计。要支持他，这个是为了给大家打个样，希望大家也能去做对应的slight content的扩展，我们看到在这个结构上数据结构上有一些变化，在slack content里面，之前我之前我们这里的bullet points就是一个list，现在是一个list。但是这个list里面放的是字典，是一个dipt。
	然后在解析上面我们也看到了，之前我们去解析这个内容的时候，是首先会把前面的空格都处理掉。就是所有的每一行按行来处理的，每一行前面就做叫什么前缀的空格都给移除掉的。但现在不能再这么处理了，因为移除掉的话，那所有的前缀就没有缩进关系。没有缩进关系也就不知道你是几级的这个标题了。
	所以之前有一个line strip的这个代码，我们就把它给挪到了解析bullet point level这里面去，然后有这样的一个改变。然后在解析这个玻璃points的主流程里面，我们看到它同样会去match这个line，这是一个正的表达式。然后接着我们会在这儿去获取它的缩进的空格和这个项目符号的内容，into the space和bullet。然后我们可以去做一个简单的迭代和计算。好，这个是我们去支持这个多级内容在解析册做的一些改动。
	然后在PPT generator里面，我们也同样的去做了一些迭代。在这儿我们看到，首先之前的bullet points，它就是一级的一级的内容。所以在之前的这个PPT generator里面，我们的要点内容就是取一个bullet point，加一个add paragraph，然后设置一个level等于这个0，然后内容等于内容就输出了。现在我们有不同的level，不同的level就从当前的这个玻璃boy里面取出来。
	取出来之后，它本身就有这个level这个字段，我们在input这个power的时候就做了对应的设置，然后就可以直接去去做对应的这个要点不同级别的设置。然后同时我们还有一个加粗，就两个星号包裹的这个加粗，我们通过format text来进行对应的处理。把中间的这个加粗的这个文本设置成这个属性，对应的设置为true，就是这有个border run，count boat设置为出，那显然大家应该就能get。如果我有这个一对信号在markdown里面就是斜线，甚至我还可以设置一些属性，我加粗了，甚至我要用红色的字体等等。这些都是我们的文本个性化或者说字体个性化可以去做的一些扩展。大家都可以参考0.4的关于这个多级和这个加粗等等这样两个口子来做对应的一些设计。
	同时我们还加了一个极简风的模板，也是我做课件最经常用到的一种模板。然后我设置了两个title，这是两个头图，然后里面还有不同的样式，不同样式的组合。就比如说我们问这个多模态大模型的介绍，这个就是它生成的。我们看到他首先支持了二级标题，并且它的一级标题的玻璃points是加粗了的。他在这儿都能够很好的去做展示了。
	聪明的同学可能就会想，这个模板其实非常清爽，比之前更清爽。而且它的字体行距我都有专门去做调整。那你其实就要去想象一下，代码没有做太大的改动了，那你对于模板的理解如果能更深一点，是能做出更好的这个内容来，就是最终的这个PPT的内容来。同时这种极简风格其实对于你的人工的二次去做改造也很友好。而且我们还为他留下了大量的空间，就是为了再加一个版本，我们往里去用AI生成插图，也留下了一些空隙。
	好，那么最后这个0.4版本迭代的一个内容就是要把这个west模型集成到我们的video的这个流程里面来。在0.3的video里面，我们就已经有一个多模态的chatbot了。这个大家一个影响。在这个radio的这个chatbot里面，理论上其实它有一个上传文件的一个按钮，只不过我们没有去处理那个文件。所以本质上，我们就是把那个上传文件按钮传上来的音频，我们去做一些文件处理，然后这个文件由谁来处理，由wasp er模型来进行处理。但是它如果传的不是wasp er的模型怎么办？所以我们看到这里也要对后缀进行一些检查。
	但这里有一些机灵鬼可能就会想了，那我直接给你传个MP4，但是我把这文件名给你改成B3，那这就绕过去了，还真有可能。所以这儿其实理论上文件格式的检查还可以做得更深一点，但这属于这个connect case了。我们先不断不要去把这个，尤其是产品研发过程当中，corner case肯定要考虑。但是主功能的开发和功能case的优先级，其实是取决于一个通盘的一个考量的结果。
	好，有了这样的一个集成之后，我们接着再来看看0.4又是怎么样去运行的，首先要有第一个问题需要处理的，就是现在这个chat PPT的域名是绑了这个OpenAI的whisper large v3。那如果我们要去启动chat PPT的这个界面，而不是这个wester的界面，它是什么域名？这是一个很有意思的话题，我们可以看到从代码的角度来看radio server它最终启动是这一行代码，demo q这个是可以支持排队，因为考虑到这个过程当中可能有多个生成任务和多个用户，然后我们的OpenAI的waster刚刚启动的把它。这样好，上面是radio server，下面是open I的whisper。那显然他们俩的启动方式是完全一样的。这两个启动方式完全一样，在我们配置了n inx之后，理论上就是我启动哪个，其实域名就会解析到哪个，那个转就会转发到哪个，我们先启动起来给大家看，左边还是刚刚我们看到的这个域名的服务。这其实就是懂计算机网络和经常搞这些服务的，做网站应用的，应该一眼就能，你就一下就能理解我说的意思。他可能有些没有这方面经验的同学，我再给大家演示一下。
	好，这里显然我们启动了一个radio server，它长得不一样了，然后the radio server，我们现在就刷新一下这个页面。它变成了chat PPT的样式。那主要的这个变化其实在哪儿呢？
	我们能看得到，我们用NS look up这个是查看我们的域名解析的，对不对？我们去look up这个ChatGPT载体data com的时候，我们看到chat PPT到载体到com他是解析到了我的这台服务器。然后我们的links的配置使得所有到这台服务器上的HTTP的请求首先会转到HTTPS，那转到HTTPS之后就看下面这个HTTPS的这这套配置了。所以大家如果去理解，就是这个上面的配置是为了把HTTP的请求转到HTPS，而HTTPS的配置是在这儿，HTTP走80端口，HTPS走43端口，这个大家应该都熟。那么四十三端口？同样server name都是一样的。然后443端口配置了SSL的证书和它对应的key，然后大家看到这有一个注释叫managed by soft board。这个就是我重新签发了这个三级域名的SSL证书之后，由sort ort自己去改的engines还蛮好用的。
	SSL的protocol的协议也有不同的版本，这儿我就填写了支持1.2和1.3的版本，也使用了兼容TS1.3的这个套件，还有一些其他的SSL的配置就不说了。那有一个这里重点可能有的同学会改的，就是客户端最大上这个上传文件的大小限制，最大是支持50兆。那接着就是为什么我启动两个不同的，一个是radio server，一个是open I whisper之后这个域名打开的都不一样了，就在这个地方。
	Location, 这个是links的转发相关的配置了。我们看到pro proxy pass就是指我们启动之后，要把前面不管是80还是43来的这些请求转发到哪儿呢？转发到本地127.0.1的7860。而这个本地其实就是我们这儿上面这两条的demo启动起来之后，它这个服务radio的服务其实就在local host的7860。Local host就是127.0点，0.1就是一个回环地址。这个为什么是他呢？也可以在这里看得到。
	简单讲一点这方面的计算机网络方面的基础，我们在ETC host里面我们看到127.0.1的这个回环地址被映射到了local host。所以我们在这个地方写127.0.1是没问题的，其实就是local host。退出。
	然后我们看这个地方，设置了127.0.1，然后proxy HTTP的version这些都不用再改了。然后大家可以看一看自己的这个endings的这个设置，回头因为这部分不太方便，我回头考虑一下，看看看他是不是要放到咱们的。我把里面的一些关键的部分设置成模板值，然后也传到项目里面，传到chap PT的github地址里面，那大家应该就更好去操作。
	还有一个time out的设置，刚刚忘讲了。就我们的就我们的这个session绘画的timeout，现在设置的是十分钟。然后如果你上传的文件太大了，十分钟都没有这个解析完，都没有传输完，也会报错的。然后这个报错也能在这里看到。哎。我们通过，但这些怎么怎么去生成的这就是标准的engins错误级别的日志的位置，然后这里会写之前我就因为没有设置，我记得有一个报错，应该是文件太大了。这有一个large玻璃，有NIX默认配置不会搞到50兆这么大的一个client body的max size。然后包括下面还有一条应该是handshake的错误，我找一找有没有。
	找不到了。之前应该还有一个这个handset的错误，就是1.3的这个协议的这个问题，那也都被改掉了。现在的配置是都没有这个问题的，所以大家到时候可以用这个模板去进行操作。好，那就我们这儿再看一下ChatGPT集成了这个时候会是一个什么样的场景。你想象一下，首先radio肯定不是，最终大家如果真的要把这部分agent的代码完全商业化的话，这个前端其实是最好换的。
	我们现在就假设一个场景是什么？就是我们这个公司里面有同事或者有领导会来跟你请教问题或者安排任务。我们先假设是一个同事，一个女生，大家都很难拒绝女生的要求，对吧？这个女生很想让你给他讲一个东西，或者或者说他是你的徒弟，你是他的门头，然后她现在不懂让你讲什么是这个多模态的大模型，对吧？这就我们现在正在教的这个东西，这个部分其实就是一个测试文件，这个我就不上传了，大家可以去，我也待会教大家待会儿怎么去造这个测试文件，那我们先给他付一个月。
	深入讲一讲上传。上传之后，因为这是一个多模态的chat port，所以它本身就支持。那上传的这个玩意儿上传的这个玩意儿，其实就是刚刚鹏你给我讲讲什么是多模态大模型。这个其实就是刚刚我们试过的这个test文件，就是老鹏给我讲讲多模态的大模型，我们再深入讲讲，这就有一些对应的结果。然后这个结果，首先我们先看一下新的模板，我们点一键生成，它会有一个多模态大模型简介下载下来。
	点开。我把这个退出了。这个是刚生成的这个多模态的大模型简介的PPT，跟这边是对应起来的。首先它有个标题，多模态大模型简介，什么是多模态大模型，这个是新增加的功能，我们看到有多级标题，甚至你有更多集，他也能去做适配对应的这些内容，这是第一版。那么现在显然他不够详实，结构性也差点意思。经常听我的课的同学应该就知道，我们去讲一个东西的时候，有费曼学习法，是一种很好的学习知识的工具。那么去分析问题的时候，有这个WYH，那么使用这个5WYH的分析方法来深入讲讲。再发给他。
	这边是它的一个日志，我们看到它这有啊what，why、who，when, where, how. 因为我们的prompt考虑到未来是有图要插进来的，所以我们不会在一页里面写特别多的内容。但现在这个版本没有这么多的，或者说现在这个版本没有图，所以我们希望内容多一点。这个时候大家不要忘了，它也是一个GPT级别的模型，那背后调的就GPT CO。那么这里可以这样去做，就比如说我们刚才要求一不用在标题再放大一点和内容也体现WYH，而是这是第一点，第二点就是每一页扩展。内容。
	至200字以上。我们还可以强行要求大家新增中国和美国在多模态。领域的研究进展。对比的研究进展对比或者优劣对比，我们发给他，这边也收到了这个需求，需求如下。
	这个他把格式给忘掉了。对。使用要求的格式不改变内容。
	好了，保持了这个版本，那我们再生成一下，看看多模态大模型的深入分析。
	点错了。山路37。这个结果就看着会好一些了。
	所以大家这里想要展示给大家的就是这个内容的引导，其实也是多方面的，然后要跟他多轮的对话。毕竟你要想象一下这个过程，其实它解决的是什么企业办公的效率问题？理论上如果你没有其他PPT，你要去准备这样的一个PPT材料，你其实也是需要要么去用搜索引擎，要么去用GPT或者类似的一些大模型跟他交流。先获取一波原材料数据，也就是这里的这些数据，然后再把这些数据塞到我们的PPT里，所以是这样的一个流程。然后现在的好处是说，我们可以通过多轮对话去迭代的一些信息，同时还能一键快速的去生成这些power point。然后如果你未来有多种模板，你还可以针对不同的领域去做对应的模板。这些模板可以帮助我们更好的去提升我们的效率，然后这个是咱们这个0.4版本的一个流程，包括它的域名的绑定。
	这儿我们再额外讲一点，就是在代码当中注释掉了的一段radio的组件。我们看到在代码里面还有一个叫做定义语音转文本的接口。这个部分其实是把麦克风那部分的代码给简化放过来，做了一个新的组件，我们先启动一下。它的用途是什么？我启动起来给大家讲一讲。好，刷新一下。
	好，那么这个就相比于我们刚刚正常的这个多了一个组件，这个麦克风输入它的用途就是假设咱们要去做测试，要去造各种各样的要上传的音频文件，可以用它来造这个文件，就比如说现在我们想了解一下什么是OpenAI whisper模型。这里会稍微卡一下。做现在我们想了解一下什么是OPI wasp模型。好，然后我们提交之后，它一样会进行对应的处理，但他这个wasp这处理的不太好。为了我们试一下这个GPT4欧米尼能不能做好。首先我们看到上传的音频文件，在我的服务器的这个路径下把它拷出来，我们CP这个等于test 0点WAV，那么在我的服务器上就会有一个test 0WLV把它下载到本地。
	当loading下载好了，然后我就可以直接在这儿teston。深入讲解，深入分析一下。
	我看一下。
	这是第二档trace back。我刚刚应该点成一键生成了，sorry, 你这刷新一下。
	穿一个。
	还没穿上去。这么久。
	会这么久。
	好了，然后我们深入展开讲讲。这个就正常了，刚刚没有传，直接点了这个。然后这儿我们看到什么是OpenAI的whisper模型，它直接用了我们这个音频文件，但他因为识别的有点问题，这个weber没有识别准，那么我们可以再考问他。这个是他的我们没有用GT4M再去调这个搜索引擎，但是我们可以试试看能不能纠正。
	他这个。目前没有没有这个模型它能判断，但生成的结果不太对，这个case确实就比较挑战了。但这个其实是未来我们要去在这个market agent上面要去做的一个事情，理论上其实就是去改造一下我们的，去更新迭代一下我们的这个内容。我们现在的prompt相对来说还比较简单，就是这个对话的chatbot，它它拿到了用户的一个需求，然后他基于这个需求去回答他的问题，并且把内容以一个像我们的input power能够解析的markdown种结构来返回。但实际上确实会有更挑战的事情，我们会放在最后一节课去给大家讲，就怎么样用market agent来优化这种情况。
	就是你的输入里面就是有一些错误信息。这个错误信息首先不应该出现。但是market agent能做的一个好处就是找一个agent他先来反思或者思考输入的信息有没有可能有问题。以及原来的chatbot可能就直接生成了一版结果。而第二个agent可能它不是做生成结果的，但它可以像我们的前面介绍的反思的这种机制一样，它可以根据你的特定的场景。比如说你是要介绍一个概念，介绍open and whisper，介绍多模态的说，那你生成的内容就应该是足够的多样的内容。那如果你是介绍一个什么风景旅游点，介绍一个项目，那你可能图就要多一点。那这个时候就要生成图的模型多干点活。
	我们现在只有一个生成文本的模型，下一节课会讲一个生成图的模型。然后最后一节课我们就会去讲这个第三个，或者说一个agent他来要求我们到底应该是基于什么样的情况，做图多一点，还是文字多一点，还是怎么样去迭代。好，这个就是我们ChatGPT0.4这个版本的主要的内容。然后希望大家能够在这个动手方面再多做一做，然后去不管是我们的模板的迭代，还是我们的prom的迭代，还是新的语音识别的集成，都能够去动手实践，多学一些。好，我们看一看大家有没有什么问题。
	还有什么问题吗？现在提的这个问题，刚才都已经讲过了。就这个什么环境什么的。
	语音中文能转英文吗？可以，可以，对，但那个不在我们chat PPT的功能里，你可以去看一看OpenAI whisper，里面有一个task的字段，你可以设置一下对。就这些同学问的问题挺好的，在open a whisper里面有一个task的字段这儿妹妹。它只能通过放大这儿看到没有？是一个radio的组件，你选了translate，那么在OAI vest的这个gradual里面它就会转过去。但因为我们这个gradual server，我们不强行去做translation，因为有可能它本身有一些场景，它就是要多种语言的，它不能强行转，比如说有些单词他转了可能就比较尴尬了。
	有的同学问sota的评测一般都看一下平台，我觉得这个跟平台没关系，应该是看着他测的是什么数据集合，这个benched mark。对，然后这种平台很多，我不知道这个一般是什么概念，就看你关心什么任务。对，就你关心什么任务，它就有对应的数据集。你再去看那些平台有没有对应的数据集的比分，因为它最终是一个比分，就不要落在那个顺序，12345谁第一谁第二上面。如果你学了我的课，还是这样的话就有问题了。你应该落在你关心的是哪个benchmark，是MMLU还是GSM pro，还是什么human evil的这个code相关的测试对。大家看还有什么问题吗？好，这节课这个网络部分大家都没有提问，是不是都很擅长做网络？那nex的配置就不传到项目里了，因为本来这个也很个性化的。
	大家还有什么别的问题吗？
	能你写代码就可以。这个同学问的就是我们这儿是在教大家学技术，不是在接受大家提需求。你们问的这些都能并且并不难。对。然后我一直在说让大家去动手实践去弄一弄，就是这个意思。对。
	好，大家要是没什么问题的话，我们就。安全证书配置怎么可能传？那这个同学你开玩笑，东西不可能传的。对你去生成一下，你稍微搜索引擎查一查安全证书能不能传，然后什么而且传了你也没用，那是给我的域名签的。对你要是没有听懂前面那些小节的内容，你就再上网搜一搜吧。因为那个要展开完全是取决于你对于计算机网络有多少了解了。对。好，我们就到这里，感谢大家。