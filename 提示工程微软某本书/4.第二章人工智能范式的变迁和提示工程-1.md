# Chapter 2 第2章
人工智能范式的变迁与
提示工程
# 2.1 人工智能范式的变迁
# 2.1.1 人工智能模型及其训练
## 1. 人工智能模型

在现代人工智能技术中，模型是最核心的概念之一。但是对于一些初学者来说，他们可能还不太了解模型是什么，以及它在人工智能中扮演的角色。

我们可以把人工智能模型想象成一种基于机器学习算法或深度神经网络的函数。这个函数的样式是既定的，但是它里面的参数是可以训练出来的。这些参数会根据不同的数据集和应用场景来调整，从而使模型能够更好地完成任务。

例如，可以把最简单的一元线性函数y = ax + b作为一个模型。在这个模型中，a和b是参数，它们可以被训练出来。当人们把一个x值输入到这个模型中，它就会输出对应的y值。这样一个模型，总共有2个参数。如此简单的模型，自然能起到的作用也非常有限。

但是在现代人工智能技术中，人们需要的模型远比这个复杂。现在非常流行的ChatGPT，本质上就是一种语言模型。

简单来说，语言模型的功能就是接受一段输入的话，然后输出另一段。从概率论的角度来看，语言模型实际上是一个概率函数，它将自然语言转化为概率函数，通过自动学习来生成符合语言规律和上下文的语句。这个函数有很多参数，这些参数可以看成一个非常巨大的向量。

例如，ChatGPT使用了一个非常大的参数向量θ，这个向量的维度高达1750亿。当人们输入一个问题x给ChatGPT时，它会将这个问题转化为一个概率函数，并基于θ中的参数计算出一个对应的输出y。

例如，当输入一个问题“世界上最高的山是多少？”时，ChatGPT会通过θ中的参数和之前学习到的互联网数据，预测出正确答案是“珠穆朗玛峰，高度为8848.86米”。这个过程实际上就是ChatGPT的一个基本逻辑。在这个过程中，θ中的参数扮演了非常重要的角色。这些参数是在训练过程中通过学习海量数据得到的，它们可以使ChatGPT自动学习语言的规律和上下文信息，并生成符合语法、语义和逻辑的语句。

除了语言模型之外，还有许多其他类型的模型，如图像识别模型、语音识别模型、自动驾驶模型等。

## 2. 人工智能模型的训练

所有的人工智能模型都是基于不同的数据集和算法构建的，但它们的核心都是一个函数，它们的参数也都是可以被训练出来的。

现阶段的机器智能，就是指训练一个人工智能模型，让它能够对所有输入的x都能够得到期望的输出y。这个模型是通过学习大量数据中的规律和模式来训练的，其中的参数θ也是通过自动学习得到的。

例如，训练一个语言模型时，人们会给它大量的文本数据，让它自动学习语言的规律和上下文信息。在训练过程中，模型会不断调整θ中的参数，以使它能够更好地预测下一个词或生成符合语法、语义和逻辑的语句。最终，这个训练好的模型可以接收任何输入的x，并生成一个对应的输出y。

## 2.1.2 人工智能范式的变迁详解
人们怎么样才能得到这样一个承载了智能的模型呢？具体的方法在不同情况下是不同的。
### 1. 从零开始训练

在最早的时候，人们训练模型时通常会从零开始。

这意味着人们会对模型的参数进行随机初始化，然后将训练数据输入模型进行迭代。这个迭代过程通常包括前向传播和反向传播，以不断地调整模型参数，使其能够更好地适应数据。

最终，经过多次迭代，模型的参数会被训练成一组最优值。模型能够基于这些参数在给定数据的条件下达到最优化目标，如图2-1所示。

![image](https://github.com/user-attachments/assets/5db3e339-f197-4054-b461-613c87733ea4)


图2-1 从零开始训练的人工智能范式

例如，如果我们训练一个全连接神经网络，首先需要架构这个网络，比如第一层128个神经元，第二层256个，第三层又128个；其次会随机初始化每个神经元的参数，然后将训练数据输入网络，反向传播算法迭代训练，直到模型的表现达到了一定的标准；最终，模型的参数会被训练成一组最优值。这个模型就能够对新的数据进行预测，并且表现良好。

这种方法称为“从零开始训练（Training From Scratch）”。

从零开始训练模型的方式，虽然简单易行，但也存在一些问题。由于参数是随机初始化的，训练过程可能会陷入局部最优解，从而影响模型的性能。从零开始训练需要大量的计算资源和时间，而且需要对模型的超参数进行手动调整，或 / 和在训练过程中进行dropout等操作来增加模型的稳健性，对于复杂的模型来说非常困难。

最早的模型可能是线性回归、支持向量机、隐马尔可夫模型等。相对都还比较简单。然而，随着深度学习的发展，模型越来越大。要训练这样的大模型是非常困难的。

模型规模的增加带来了许多问题。例如，大模型需要更多的计算资源和存储空间，训练时间更长，容易出现过拟合等问题。当然，除了技术，成本问题更加严峻。

### 2. 模型训练的技术挑战与成本


训练人工智能模型是需要投入资源的。一般来说，模型训练涉及的资源包括硬件、人工和数据。这些成本不仅仅是时间和金钱，还包括对计算机资源和人才的需求。

硬件包括GPU和存储设备，需要构建一个计算机集群。软件包括训练框架等，需要专业的工程师进行开发和维护。

数据的获取和整理也需要耗费大量时间和人力。当模型越复杂、规模越大时，需要的数据量也越大，因此需要更多的计算能力和存储空间。

设计和实现算法的研究人员、工程人员的薪酬也是成本之一。所有这些因素都会增加训练大模型的成本。

训练一个大模型需要巨大的投入，OpenAI的GPT-3就是一个1750亿参数的巨型语言模型，它需要的计算资源和数据量都是非常庞大的。计算机和数据的成本是相当高昂的，很少有个人或公司可以承担这样的成本。

另外一个例子是，如果你想要训练一个大型的图像分类模型，你需要有大量的图像数据和GPU计算资源。为了训练一个高质量的模型，你需要处理数百万张图像，这些图像需要进行标注和分类，这就需要雇佣一些数据科学家和工程师来完成这项工作。而且，你还需要购买GPU集群来加速模型的训练，这同样需要花费很大的资金。

近年来，模型的规模变得越来越大，参数量也随之增加。如图2-2所示，BERT Large模型的参数量是3.4亿，GPT-2模型是15亿，而GPT-3模型已经达到了1750亿。

![image](https://github.com/user-attachments/assets/5a0572b3-8cfa-452b-8043-2c2fd67dacb0)


图2-2 人工智能模型规模的变迁

巨大的成本倒逼了人工智能范式的转变。越来越多的人开始关注如何利用已经训练好的模型来解决自己的问题。
### 3. 预训练模型与微调

模型规模对训练成本的影响使得人工智能范式发生了转变，人们开始探索一些不需要从头开始训练的方法，来让大模型获得更多的领域知识或者满足更多定制化需求。

其中一个方法叫作迁移学习（Transfer Learning），它的思想是基于之前已经被训练得差不多的一个模型，再去做一些小规模的改进，以达到获得更好性能的目的。

迁移学习的好处是它能够显著减少训练大模型所需的成本，因为它不需要从头开始训练一个全新的模型。相反，它使用已经存在的模型进行微调和改进，以适应新的任务或者领域。这种方法的应用非常广泛，例如在图像分类、自然语言处理等领域都有很多成功的案例。

“迁移学习”这个术语现在已经很少被提起了，但相关的很多概念仍然是今天的热词。

预训练（pretrain）是指在大规模的数据集上，先进行一次训练，然后把得到的模型参数保存下来。这样的模型就叫作预训练模型（Pretrained Model）。在使用预训练模型解决某个具体任务时，人们会针对目标任务进行微调，使得模型更好地适应当前的任务。这种方法的好处在于，预训练模型已经获得了大量的数据和背景知识，可以更好地解决当前的任务。

优化已经训练好的模型，从而使其适应新的任务或数据，一般使用微调（Fine Tuning）技术。微调是指在现有模型的网络架构上稍微调整一下网络结构，例如添加一层或多层网络结构，目的是让模型适应不同的下游任务。因为其他人已经训练了这样的模型很长时间，所以可以利用这些结果。这个过程相对于从头开始训练，需要的改变比较小，所以称为微调。

这种技术可以提高模型的性能，使其更适合新任务或数据，而不需要花费大量时间和资源进行全新的训练。现在，微调技术已经成为深度学习领域中常用的一种技术，广泛应用于自然语言处理、计算机视觉等任务中。

现在来看看微调的具体过程。

首先，我们需要进行一个预训练的过程，这个过程就像从零开始训练一样，需要使用神经网络架构、参数初始化和大量数据进行训练。预训练完成后，我们可以将预训练模型公开发布，包括它的训练代码。

在这方面最有名的模型应该是BERT。BERT是2018年Google发布的一款很著名的自然语言处理模型，它先通过一个大型的语料库进行了预训练。预训练完成后，BERT的训练代码和预训练模型的参数都被公开发布，其他机构或公司可以轻松地下载并使用它们，然后进行微调。在过去的几年中，在做自然语言处理（NLP）任务时，通常使用的方法是使用BERT预训练模型，然后对其进行微调。

同一个预训练模型，经过微调可以用于各种下游任务，如文本分类、标记，或者实体抽取。只需要更改额外添加的网络结构，人们就可以以不同的方式利用它。预训练模型在微调之后，可以适应很多不同的下游任务，这也是预训练模型非常流行的原因之一。

微调有两个重点。首先，模型的网络结构会改变，毕竟增加了神经网络的层数。其次，原始的整体参数都会改变。这并不是说，仅仅新加入的网络部分的参数会改变，通常情况下，除非你屏蔽其中的某些层，否则所有的参数都会发生改变。

图2-3展示了预训练模型和微调这一范式运行的过程。


![image](https://github.com/user-attachments/assets/7d3eec3d-457a-439a-92bb-6a54adb1bd07)


图2-3 预训练模型与微调训练结合的人工智能范式

下面来看一个BERT的微调的例子。如图2-4和图2-5所示，是基于BERT预训练模型微调成目标任务：一个是文本生成，另一个是文本分类，第一个任务有点类似于GPT。

![image](https://github.com/user-attachments/assets/19657cd0-f891-4799-ac65-215514133ba6)


图2-4 BERT模型执行文本生成任务

![image](https://github.com/user-attachments/assets/a3396d8e-1dd7-4e2f-a55d-1c0d0957384c)


图2-5 BERT模型执行文本分类任务

它们都是在原始的预训练模型上再加上一层，然后再进行训练。这就是微调的过程。
### 4. 大语言模型与提示学习

BERT的参数为3.4亿，对它进行微调，已经需要投入不少的资源。到了GPT-3之后这些千亿参数级别的模型，即使仅仅是对其进行微调，所需要耗费的成本也不是一般的机构能够负担的了。

因此，又产生了一种新的范式：大语言模型（Large Language Model，LLM）与提示学习。

这种范式的重点在于，大模型本身不会有任何变化，无论结构还是参数，都不会改变。用户通过输入不同的提示来激发大模型在不同方面的潜能。图2-6展示了这一范式的运行方式。

![image](https://github.com/user-attachments/assets/7f88d849-684f-4ba4-9735-db096465f8cf)


图2-6 大语言模型与提示学习结合的新型人工智能范式

也就是说，面对大语言模型，人们已经彻底放弃训练了，而仅是通过对其进行不同的提示而获得不同的输出结果来完成相应的各类任务。
## 2.2 提示工程的兴起
作为提示学习的重要组成部分提示工程，将是本书后续部分所围绕的核心内容。因此先来认识一下提示学习。
### 2.2.1 提示学习
### 1. 什么是提示学习

什么是提示学习？提示学习的英文对应词是prompt learning，但如果在Google上搜索，它更多的情况下被称为prompt - based learning，这两个词基本上意思相同。

提示学习是一种在不修改预先训练的模型结构和参数的情况下，通过向模型输入提示信息来调整模型行为的方法。这种方法可以通过让模型从提示中学习目标行为，从而改变模型的输出，来适应特定的任务。

以大语言模型为例，语言模型是一种处理自然语言的模型，它预测语言的概率分布。在语言模型中，有一个参数θ和输入的语言x，通过这两者的条件概率，得出预测的语言y。如果预测的结果与实际不符，就需要通过调整模型的参数θ来得到更符合实际的结果。如果想要改变模型的参数，可以从头开始训练一个新模型或者对现有模型进行微调，但所需要的投入不容小觑，可能大到很多机构承担不起。

幸运的是，我们可以使用提示学习的方法来处理这种情况。这种方法的目标是利用输入数据x本身的结构和分布来训练模型，而不是依赖于标签y。通常使用自回归模型和自编码器模型来学习输入数据的表示，让模型学习到输入数据的概率分布，并在推理时预测输出结果。

与传统的有监督学习不同，提示学习可以从大量未标记的数据中学习模型，从而不需要人工标记大量的数据。因此，提示学习可以用来激发非常大的语言模型的潜能，而无须进行昂贵的重新训练或微调。

为什么仅仅通过提示就可以让大语言模型产生所需的输出？

这是因为，大语言模型（如OpenAI的GPT-3/GPT-4等）在训练过程中学习了大量的文本数据。这些大语言模型通过分析和理解数百亿个单词、短语和句子的上下文关系，学会了预测和生成自然语言文本。它们具有很强的语言理解和生成能力。当用户向这些大 
