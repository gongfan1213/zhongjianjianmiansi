型时的巧思，为读者的实际使用抛砖引玉；其二，当我们由于某些
原因不能使用最新版大模型时，依然有办法为初级一些的模型构建
有效的提示。
# 4.2.1 上下文信息和指令
为了更好地向大模型描述我们的需求，我们会先在提示中用适
当篇幅给出问题的上下文信息，然后用指令对大模型提出具体的问
题，根据问题的复杂程度，指令中可能还会包含解决问题的所需步
骤。在构造提示时，应该尽量给出丰富的上下文信息和清晰的指
令，并且视大模型的理解和完成情况，需要时将复杂的指令拆分为
多个更简单的简单指令，确保任务可执行。
## 1. 给出丰富的上下文信息，以及清晰的指令
下面的例子由GPT3.5模型生成。

假设模型的使用者是一名业余中长跑爱好者，希望提升自己5
公里跑步的个人最好成绩。以此为目标，如何来对指令和上下文进
行逐步完善，以最终引导GPT3.5给出详细、参考性较高的回答。
首先，直接输入“如何提高我的5公里跑个人最佳成绩？”这
是最基础和简短的提示方式，直接提问，既没有上下文（如训练者
的身体情况、训练水平、饮食习惯等），也没有明确的指令（“制定
一个计划”）。

GPT3.5对此的回应是：“1. 制定一个目标；2. 增加训练里程；
3. 间歇式训练；4. 采取多种锻炼方式；5. 保持体格强健；6. 科学饮
食；7. 保证足够的休息；8. 放松和拉伸。”为避免困惑，GPT3.5还
简单地解释了每条建议的概念和定义。


可见，这种过于简单的提问得到的回答较为宽泛、笼统，对业
余跑步爱好者恐怕没有足够的参考价值。


在此基础上，改进提示，为模型提供部分上下文信息，和明确
的指令：“我想在2个月内将5公里跑步个人最佳成绩提高1分钟。
你能帮我制定一个计划吗？”这个提示让模型知道了，用户想实现
两个月内5公里提速1分钟的目标（上下文），以及需要制定一个计
划（明确的指令）。


GPT3.5的回复是：“好的，为您制定的计划如下：1. 提高跑步
频率，争取每周跑步4～5次，依您的身体状况而定；2. 在每周的
固定训练中增加1～2次间歇性训练，选择400米或800米这样较
短的距离，配合组间休息；3. 控制跑步的节奏，尝试以更快的步频
跑更远地距离——比如，尝试以5公里地配速坚持跑30～45分钟；
4. 采取混合训练模式，在每周的固定训练中，加入游泳/单车或力
量训练；5. 让身体恢复：每周争取休息1天，并且保持每晚充足的
睡眠。”


很有进步！但是与职业健身教练能为学员量身定制的训练计划
还有些差距。继续尝试在保持简明的指令的基础上，向大模型提供
更丰富的上下文信息：“我是一名28岁的女性，体重60公斤，我
的5公里最好成绩是23分15秒。我每周做3～4次简单的有氧训
练，基本不做力量训练，饮食也比较随意，经常吃垃圾食品。我的
目标是在2个月内将我的5公里跑步时间提高到22分钟。请帮我
制定一个适合我情况的详细培训计划。”


这一次，用户的提示为大模型提供了丰富的上下文信息，它了
解到这位业余跑步爱好者的身体状况、目前的训练水平和训练习
惯、饮食习惯，以及其明确的目的：得到一个符合个人情况的详细
训练计划。这一次，GPT3.5的回复更加专业了：

“您的两个月的训练计划如下：

第一周：

□周一：45分钟有氧训练；

□周二：力量训练——动态拉伸15分钟，20分钟自重力量训

练（推举，平板支撑，自重深蹲，等等），以及10分钟慢跑；

□周三：45分钟变速跑训练；

□周四：力量训练，在周二的基础上小幅度增加负重；

□周五：休息；

□周六：以期望的PB配速的80%完成一次5公里训练；

□周日：休息。

第二周：

……

第三周：

……

……”

不错，对于一名业余跑步爱好者，这看上去是一个值得参考，

而且可行性较高的计划！
## 2. 拆分复杂指令

有时候，人类能理解的指令对大模型而言过于复杂，只能执行
完整复杂的指令中的一部分。此时就需要将复杂的指令拆分成简单
的逐步指令，保证模型能够理解并执行任务。下面举两个例子，帮
助读者理解。

第一个例子，用GPT3.5来解决一个“最短时间”问题。当我
们的复杂提示不拆分成多个简单步骤时，模型可能显出较弱的推理
能力，例如下面这个例子。


小李是一个小学生。每天早上，他要完成4件事：洗漱5分钟，
加热早餐5分钟，吃饭10分钟，听广播新闻8分钟。小李怎样安
排事情的先后顺序，花费时间最短？最少要花几分钟？


GPT3.5的回答是：
小李的最佳安排顺序是：洗漱→听广播新闻→加热早餐→吃饭。
洗漱花费5分钟，听广播新闻花费8分钟，加热早餐花费5分
钟，吃饭花费10分钟。最少需要花费的时间为5分钟 + 8分钟 + 5
分钟 + 10分钟 = 28分钟。


很明显，这不是花费时间最短的解法，因为有些事情可以同时
进行，如洗漱和加热早餐，听广播和吃早饭。改进一下指令，将任
务分解成更简单的3个步骤，改写后的指令如下。



小李是一个小学生。每天早上，他要完成4件事：洗漱5分钟，
加热早餐5分钟，吃饭10分钟，听广播新闻8分钟。小李怎样安
排事情的先后顺序，花费时间最短？最少要花几分钟？

将问题拆分成以下步骤：

（1）首先，判断这4件事中，哪些可以同时完成？

（2）将可以同时完成的事件归为同一类，然后取较长的时长为

完成这一类事件的所需时间；

（3）最后，将所有的所需时间相加，得到最少需要花费的时间。

GPT3.5的回答这次是对的：

答案：

1. 同时可以完成的事件：洗漱 + 加热早餐、吃饭 + 听广播。

2. 将可以同时完成的事件归为同一类：洗漱 + 加热早餐为一类
（5分钟），吃饭 + 听广播为另一类（10分钟）。

3. 所需时间最短：洗漱 + 加热早餐（5分钟）+ 吃饭（10分钟）=

15分钟。

这是最佳答案。这表明当人们把指令拆分成具体的小步骤，有
助于模型作出正确的推理。

第二个例子使用text - davinci - 003，目的是生成一份采访微软总
裁萨提亚·纳德拉的德语采访提纲。首先，试着用一句话的复杂指
令描述任务：

写一份采访微软现任总裁萨提亚·纳德拉的简短德语提纲，重
点提问公司将来对人工智能方向的投资意向，以及公司准备如何在
可持续发展等环保议题上做出贡献。不要超过4个问题。

很遗憾，text - davinci - 003忽略了“德语”这个关键字，返回了



一份中文的采访提纲：

1. 总裁先生，微软计划在人工智能领域投入多少资源？

2. 您在今后的发展目标中，关注环境保护问题吗？

3. 如何改善微软的可持续发展战略？

4. 微软计划如何通过技术发展为当地社区做出贡献？

为了达到目的，下面将复杂的指令拆分为“生成采访提纲”和
“翻译成德语”两个步骤，重新构造提示如下：

首先，写一份采访微软现任总裁萨提亚·纳德拉的简短提纲，
重点提问公司将来对人工智能方向的投资意向，以及公司准备如何
在可持续发展等环保议题上做出贡献。不要超过4个问题。然后，
把提纲翻译成德语。


这一次，text - davinci - 003充分理解了我们的分步骤提示，并且
返回了符合要求的回答：


1. 萨提亚·纳德拉先生，您能谈谈微软未来在人工智能方面的
投资策略？

2. 您能介绍一下公司在可持续发展方面的愿景和采取的步骤吗？

3. 微软有哪些改进或发展策略可以推动环境保护的实施？

4. 微软在现有的技术发展中能采取哪些措施以确保社会责任可
持续？

1. Herr Satya Nadella, können Sie uns etwas über Microsofts
Investitionsstrategie in Bezug auf künstliche Intelligenz in der Zukunft
erzählen?

2. Können Sie uns beschreiben, wie das Unternehmen seine
Vision für nachhaltige Entwicklung umsetzen und welche Schritte es
unternimmt?

3. Welche Verbesserungen oder Entwicklungen sieht Microsoft in
Bezug auf Umweltschutz vor?

4. Welche Maßnahmen plant Microsoft, um die Nachhaltigkeit
seiner sozialen Verantwortung in Bezug auf die aktuelle
Technologieentwicklung sicherzustellen?

可见，当模型不能充分理解复杂的提示时，用户要根据任务将
指令进行拆解，引导模型正确地执行。
## 4.2.2 角色扮演


顾名思义，角色扮演是根据要完成任务的特定背景，赋予大模
型对应的身份角色，再进一步与大模型进行交流。这种提示最简
单的套用格式就是“你是一位 [角色/身份]，+ [角色相关的专业
问题]”。


以GPT3.5为例，假设用户要向一名三年级小学生解释勾股定
理，首先，直接提问：“什么是勾股定理？”


GPT3.5的回答是：


“勾股定理又称毕达哥拉斯定理，是指在直角三角形中，直角
边的平方之和等于斜边的平方（即a² + b² = c²）。它由古希腊数学家毕
达哥拉斯提出，是数学中最重要的定理之一。它可以用来计算直角
三角形的边长，也可以应用在其他数学和物理问题中。”


回答正确，但是对三年级小学生来说，抽象的公式可能不太容
易理解。于是，我们尝试让GPT3.5扮演一位小学数学老师：“你是
一名很有耐心、经验丰富的小学数学教师，善于举例说明复杂的问
题。请向一位三年级的小学生解释，什么是勾股定理？”


GPT3.5的回答是：


“嗨，小朋友，我是你们的数学老师。今天我要给大家讲解一
条非常有用的数学定理，它就是勾股定理。


勾股定理是希腊数学家毕达哥拉斯发现的，它是一个三角形的
重要性质。简单来说，它可以帮助我们计算出任何一个直角三角形
的边长。


那么什么是直角三角形呢？直角三角形是指有一个角是90°的
三角形，你可以想象一下一条竖着的线和一条横着的线相交，就会
形成一个直角，这样的三角形就是直角三角形。



勾股定理的公式是这样的：c² = a² + b²，其中c代表直角三角
形的斜边，a和b分别代表其他两条边的长度。就像这个公式一样，
它们的平方相加等于斜边的平方。



现在我们来用一个例子来说明。假设一条直角三角形的一条
直角边长是3，另一条直角边长是4，我们可以用勾股定理来计算
出斜边的长度。根据公式，c² = 3² + 4²，也就是c² = 9 + 16，所以
斜边的平方就是25，那么斜边就是5。所以当两条边分别是3和4
时，斜边的长度就是5。是不是很神奇呢？



勾股定理有很多应用，比如测量建筑物的高度或者海拔高度等。



通过勾股定理，我们可以更方便地计算出直角三角形的边长，
避免烦琐的计算。所以，要记住勾股定理哦，它会在你以后的数学
学习中经常用到。希望我今天的解释对你有帮助，有什么问题可以
随时来问我。”


第二次回答中，不仅包含了勾股定理的定义，还举出一个具体
的例子，方便小学生更直观地理解和记忆。我们还注意到，第二次
回答的语气也可以模仿了小学教师，循循善诱，非常耐心。



为什么“角色扮演”能帮助大模型更好地回答问题呢？通过分
配角色，用户实际上为GPT提供了一些背景信息和隐式的上下文，
这有助于大模型更充分地理解问题，从正确的知识空间中生成答
案，最终给出更好的回答。
# 4.2.3 从零样本到少样本
## 1. 零样本、单样本和少样本


零样本提示（zero - shot prompt）就是不向大模型举例，要求大
模型直接输出答案。以四则运算为例，“Multiply 9 * 108:”就是零
样本提示的典型。

单样本提示（one - shot prompt）即为大模型举出一个例子，再
提出问题，例如：


“Multiply 42 * 11: 462

Multiply 34 * 27:”

少样本提示（few - shot prompt）顾名思义，就是为模型多举出
几个例子，例如：



“Add 16 + 78: 94

1197 divided by 3: 399

Multiply 52 * 61: 3172

7892 minus 1531: 6361

Multiply 987 * 23:”

通常举的例子越多，大模型预测的效果越好 。单样本或少样本


但是例子数目也受到模型输入长度和推断速度的限制。如果我们拥有大量
的样本，那么可以使用一定的参数调优的方法，而非将例子加入提示里面，
如部分的模型微调，或者下文进阶技巧章节里提到的提示调优方法等。



提示在文献中也被称为上下文学习（in - context learning），即模型完
全是通过理解提示中的上下文信息来习得应该如何给出结果，而没
有经过任何基于梯度下降（gradient - descent）的参数更新。


截至目前，我们在本章中讨论的提示技巧都属于零样本提示。
下面，我们将通过具体的例子说明少样本提示的优点以及撰写时需
要注意之处。
## 2. 少样本提示


少样本提示的大体格式为：“例子1 [输入1 - 输出1] + 例子2
[输入2 - 输出2] + 例子3 [输入3 - 输出3] + … + 需要GPT回答
的真实输入”。以情感二分类任务为例，少样本提示可以写成：


Touching story: positive


Boring and incredibly verbose: negative


Highly recommended: positive


It’s not to my taste:


在这个少样本提示中，前三条是为大模型举出的例子，最后一
条是情绪二分类任务的输入。我们期望的输出是negative，读者可
以自己试一试，大模型是否不需要我们给出任何任务描述，直接从
先前的3个例子中领会了我们的意思。


少样本提示不仅适用于二分类/yes or no等简单问题，当用户
对大模型的输出有格式要求时，它也是一种有效的规范输出格式的
手段。


以某餐饮点评软件上对餐厅多维度评价的用户留言为例，如果
我们希望GPT根据文字评价给出“环境”“服务”“食物品质”“性价
比”等多个方面的打分时，可以构造如下的提示：



Given user’s comment for a restaurant, rate the restaurant’s
service, food quality, dining environment and cost - efficiency on a
scale of 1 to 5. The better the user’s comments, the higher the score.



“Food served very slowly, and side dishes are of extremely
limited kinds. Besides these, the main courses are too mediocre to cost
so much money.” - [service: 1, food quality: 1, dining environment: 2,
cost - efficiency: 2]



“The 2 main courses, grilled eel over rice and thousand - layer beef
bibimbap are so tasty that they go far beyond our expectation. The
dining environment is overall clean and quiet, with friendly waiters.
You can also use coupons to get large discounts.” - [service: 3, food
quality: 4, dining environment: 3, cost - efficiency: 3]


“With very few streets in front of the restaurant, we had
to walk 15 minutes before settling down for dinner. However, the
authentic Thai hotpot really hit our spot. They served dishes quickly,
also provided free shrimp crackers as appetizers. It’s a little noisy
and hot inside, but it worth it.” - [service: 3, food quality: 4, dining
environment: 2, cost - efficiency: 3]



“We’re so hungry and spent $25 on a single beef burrito near the
gas station. Guess what? There’s barely 10g beef and a tiny slice of
lettuce inside. The advertising picture is only to mislead customers,
and staff turned a deaf ear to our complaints.”

给出3个示例后，真实的评论（“我们太饿了，在加油站附

近花了25刀买了一个牛肉卷饼，猜猜怎么样？里面连10克牛肉都
没有，生菜也只有一小片。菜单上的照片是用来误导消费者的，而
且商家对我们的抱怨充耳不闻”）作为第4条输入。GPT的返回是：
[service: 2, food quality: 1, dining environment: 2, cost -
efficiency:1]


这个回答比较合理，尤其是food quality和cost - efficiency两个
维度，与真实用户一致性较高。另外值得注意的是，少样本提示使
得输出的格式与示例中保持了一致，遵循了[service: , food quality:,
dining environment:, cost - efficiency:]的特定格式，而如前文提到
的，输出格式往往难以用自然语言指令向大模型描述。


总而言之，在向模型提供少样本例子时，我们应该以“输入 -
输出”的格式成对地给出示例；少样本在输入和输出的分布，最好
能较为真实地反映实际数据的输入和输出分布，比如正例和负例应
该较为均衡；而且，我们应该精心构建少样本例子中的输出格式，
这样能更好地确保模型的最终输出格式符合我们的预期。
# 4.3 提示工程进阶技巧
本节介绍提示工程的一些进阶技巧。这一领域在快速进展，最
新的文章也层出不穷，因此难免挂一漏万，且任何文献的综述都极
易过时。细心的读者将发现，很多方法在根本思路上有相通之处。
本节选取若干方法进行介绍的目的主要是希望读者领会其中的思
想，从而为自己撰写提示打开思路，而不必过多拘泥方法的细节。
## 4.3.1 思维链
前文提到的大语言模型的“幻觉”，表现在推理任务上就是模
型生成的内容缺少逻辑，与人们在日常生活中的思考和推理相差甚
远。思维链（Chain - of - Thought，CoT）是一种鼓励大语言模型解释
其推理过程的方法，它可以帮助我们理解和评估模型的能力和局限
性。比起传统完形填空式的解题，拥有思维链的大语言模型能像人
一样进行学习推理。思维链有多种类型，如手动思维链、零样本思
维链和多模态思维链等。



（1）手动思维链（manual chain - of - thought） 这种方法需要预
先设计一些特定的问答作为示例，就像单样本或少样本提示那样。
然而不同于一般的提示，除了给出结果外，这些回答还需要展示
出推理的过程。我们可以在回答中加入一些逻辑连接词，如“因
为”“所以”“如果”等。这样有利于模型从问答中学到一条完整的
思维链。如图4 - 3所示，一个手动思维链的例子是：计算快递小哥
完成配送一共所需要的时间。在提示中，给出了一个关于计算播
种机播种亩数的问答作为示例。不同于一般提示，这里除了给出
计算结果（“5台播种机6天播种300亩地。”），还描述了推理的过
程（“4台播种机4天播种160亩地，故一台播种机一天播种10亩
地”）。模型可以通过学到的推理链条，更好地去解决其他问题。在 
