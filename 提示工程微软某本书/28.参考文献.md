### 参考文献

[1] LIU P, YUAN W, FU J, et al. Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing[J]. ACM computing surveys, 2023, 55(9): 1-35. 

[2] BROWN T B, MANN B, RYDER N, et al. Language models are few-shot learners[J/OL]. 2020.DOI:10.48550/ arXiv.2005.14165. 

[3] WEI J, WANG X Z, SCHUURMANS D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. arXiv preprint arXiv:2201.11903, 2022. 

[4] ARORA S N. Ask me anything: a simple strategy for prompting language models[J]. arXiv preprint arXiv:2210.02441, 2022. 

[5] DIAO S W. Active prompting with chain-of-thought for large language models[J]. arXiv preprint arXiv:2302.12246, 2023. 

[6] KOJIMA T, GU S, REID M, et al. Large language models are zero-shot reasoners[J]. Advances in neural information processing systems, 2022, 35: 22199-22213. 


[7] LESTER B, AL-RFOU R, CONSTANT N. The power of scale for parameter-efficient prompt tuning[J]. arXiv preprint arXiv:2104.08691, 2021. 

[8] LI Y, LIN Z, ZHANG S, et al. On the advance of making language models better reasoners[J]. arXiv preprint arXiv:2206.02336, 2022. 

[9] LI Z, PENG B, HE P, et al. Guiding large language models via directional stimulus prompting[J]. arXiv preprint arXiv:2302.11520, 2023. 

[10] LIU J, LIU A, LU X, et al. Generated knowledge prompting for commonsense reasoning[J]. arXiv preprint arXiv:2110.08387, 2021. 

[11] MIN S, LYU X, HOLTZMAN A, et al. Rethinking the role of demonstrations: what makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022. 

[12] SUN Z, WANG X, TAY Y, et al. Recitation-augmented language models[J]. arXiv preprint arXiv:2210.01296, 2022. 

[13] WANG X, WEI J, SCHUURMANS D, et al. Self-consistency improves chain of thought reasoning in language models[J]. arXiv preprint arXiv:2203.11171, 2022. 

[14] ZHANG Z, ZHANG A, LI M, et al. Multimodal chain-of- thought reasoning in language models[J]. arXiv preprint arXiv:2302.00923, 2023. 

[15] ZHOU D, SCHARLI N, HOU L. Least-to-most prompting enables complex reasoning in large language models[J]. arXiv preprint arXiv:2205.10625, 2022. 

[16] ZHOU Y M, MURESANU A, HAN Z W. Large language models are human-level prompt engineers[J]. arXiv preprint arXiv:2211.01910, 2022. 

[17] LAZARIDOU A, GRIBOVSKAYA E, STOKOWIEC W, et al. Internet-augmented language models through few-shot prompting for open-domain question answering[J]. arXiv preprint arXiv:2203.05115, 2022. 

[18] PENG B, GALLEY M, HE P, et al. Check your facts and try again: improving large language models with external knowledge and automated feedback[J]. arXiv preprint arXiv:2302.12813, 2023. 

[19] SUN W, YAN L, MA X, et al. Is ChatGPT good at search? Investigating large language models as re-ranking agent[J]. arXiv preprint arXiv:2304.09542, 2023. 

[20] GILARDI F, ALIZADEH M, KUBLI M. ChatGPT outperforms crowd-workers for text-annotation tasks[J]. arXiv preprint arXiv: 2303.15056, 2023. 

[21] YU F, QUARTEY L, SCHILDER F. Legal prompting: Teaching a language model to think like a lawyer[J]. arXiv preprint arXiv: 2212.01326. 2022. 

[22] LI Y, LI Z, ZHANG K, et al. ChatDoctor: a medical chat model fine-tuned on a large language model Meta-AI (LLaMA) using medical domain knowledge[J]. arXiv preprint arXiv: 2303.14070, 2023. 

[23] WANG S, ZHAO Z, OUYANG X, et al. ChatCAD: interactive computer-aided diagnosis on medical image using large language models[J]. arXiv preprint arXiv: 2302.07257, 2023. 


