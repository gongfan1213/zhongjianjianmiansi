	我们就直接开始，今天其实是一个比较轻松的一节课，相对来说比较轻松。我们这节课讲两个内容，一个是大模型的开源生态，就以以hugg face这么一个公司和平台为主。还有一个就是讲这个显卡的这个选型推荐叫指南，主要也是探讨为主。因为本身这个选货就属于帮大家选东西，这还是要根据需求，根据你的特定需求和场景来的。
	那么今天就分成这上下两节课，一个桨开源生态，一个讲硬件的部分。有点卡吗？是比较明显的卡，还是只有一些部分同学觉得卡？我就不太清楚这个酒店的网络稳不稳定，应该是没有这个批量出现卡的情况。OK行行行，好，那我们就正式开始今天的环节，就是我们的上课的环节。行。
	大模型的开源生态是一个很有意思的话题。我们都知道课程的进展当中，我们一直有一个项目叫OpenAI的quick star。这个项目是托管在github这么一个代码托管平台上面的那为什么我们今天叫大模型时代的github b这么一个标题呢？其实或者说我们把它这个概念再扩展一点，现在有一种新的说法叫AI2.0，就之前的AI都是老的AI，现在大模型来了，我们叫AI2.0，也有这样的一些做法。但这不重要，重要的是说在新的这个技术发展的过程当中，原来的github确实没有办法满足所有的场景了。
	所以我们需要有一个平台来类似服务于大模型时代的开发者和用户。他跟face抓住了这个机遇，我相信很多人可能在获得多少都了解过这个平台。但hugin face到底是一个什么东西？然后在我们的课程的过程当中，我记得当时有些同学还用过这个hugin face的transformers，加上s transformers这么一个python的库还有的同学在说这个transformer是什么东西，跟我们的理论篇讲的attention is all you need，这篇文章发表的这个transformer的网络架构是什么关系，hugg face它本身又运营了一个很大的一个开源社区。那这个社区里面有很多很有意思的有价值的数据也好，模型也好，包括应用文档都是值得我们去挖掘和学习的。这个我们今天多一些互动可以去了解，大家有问题可以提问。
	然后大模型有这么多，到底要怎么样去比要怎么样去比较不同的大模型？其实比较就比较这件事情来说，最核心的是抓到比较的维度，就是我到底在对比什么，有一个对比的维度，这个其实是一些也很前沿的东西，包括我们上节课讲大模型的可解释性，其实也是一个很前沿的研究方向。主要还是因为大模型这个技术本身太新了，有很多的工作，其实我们现在已经跟进了最前沿的研究进展了。还有一些同学在说这个课程里面有一些咱们准备好的代码，包括embedding ing，包括model。
	可能换了一些弱一点的模型，效果就没有这么好了，怎么办？这个就是要转变思维的一个很明显的点就大模型它不是一个基于规则来写的代码或者说模型。我们以前写的代码确定性很高，你运行100次它的结果都是一样的。当我们在学大模型基础篇的时候，就告诉过大家有一个参数叫temperature。这个temperature是大模型在推理过程当中的一个很重要的入参。这个入参决定了同样的输入、同样的prompt、同样的messages，它给的答案是不一样的，它有多样性。这些多样性本身就是大语言模型的优势和它的魅力所在。所以大家转变思维，不要在认为同样的一份代码在执行的结果一定是确定性的。
	其实大语言模型最核心的就是我们讲的不管是agent还是具体的react这种形式，都能体现出它本身的这个推理能力带来的价值。我们找到了一些可用的维度，包括我们看到大语言模型的这个社区也在蓬勃的发展，上节课的stanford的这个羊驼，其实就是一个社区发展的产物。有没有一个类似的像我们打dota也好，打游戏也好，有天梯榜对吧？那有没有这样的一个天梯榜，大家来跑个分？有啊，我们待会儿也会讲到。然后下下半节课我们讲的这个显卡的选型指南，其实希望其实不是我一直在想怎么讲这个话题，让大家能深入的通过这1个小时的时间再能自己有兴趣可以再深挖。
	所以我找了一些更底层技术的部分跟大家来分享一下，到底这个显卡是个什么东西，跟GPU有什么关系，我们所说的这个哭打tensor这种卖显卡的时候，就跟卖手机的时候，你会说我是几纳米的工艺，我用的是高通的什么芯片，麒麟的什么芯片。对于卖显卡来说，有很重要的一个参数，就是我的CUDA和tensor的核的数量。因为显存是一个死的东西，对吧，它决定了你的能够存储的这个数据的总量。但是这个CUDA跟这个合数又变成了一个很重要的一个考量。那它到底是个什么玩意儿，会不会是一个没有那么重要的参数，或者说这个参数其实是一个决定我们选择显卡的关键的，这个也很有意思。然后同样的显卡其实也有人在做一些类似于天梯榜之类的东西，跟大语言模型可以对比着来看。
	好，我们就进正式的进入到这个课程内容了。首先我们讲讲这个hugg face，大模型时代的github。我相信上个月下旬的时候，8月20多号的时候，国内传来了很多中文社区，中国互联网开始传hugin face的融资的D轮融资的信息了，我们能看到最上面这条，就是hugin face其实在去，上个月底8月28号，然后8月20多号那一周，有大量的文章，大家都能看到，哈根face又融资了，融了多少钱呢？融了2点三五亿美金，D轮融资融了2点三五亿美金。
	很多人就会很好奇，尤其是可能没有之前关注过这家公司的同学会在想，这公司不是刚刚出现没多久，怎么都到D轮融资了，然后怎么已经都到45亿美金的估值，45亿美金非常高，大家可以感兴趣的去看一看，45亿美金就相当于，我看472是吧，三百多亿人民币的一个估值是了。在中国的AI公司里面，你可以找一找，有几家公司达到了这样的一个估值，或者市值非常少。那么having face这么一家公司到底是怎么来的对吧？怎么突然一下就到D轮融资45亿美金了，这已经是四个独角兽了，这个规模那怎么来的？一轮融资过来，它是怎么发展起来的？我们简单能给大家追根溯源，让大家了解一下这个公司平台具体怎么回事儿，还是用数据说话。
	我们看一下这个hugin face本身它在融资，它是一家AI的公司。这个公司其实明星企业，就这一轮的明星企业有很多，像非常头部的OpenAI我们就不说了，那是马斯克和sam奥特曼去主导的公司。那么hugin face可以算是第二梯队，或者说第一梯队里面的偏后。我在这个一些群里也发了最近福布斯还有创投圈统计的一些现在硅谷和欧盟欧盟一些地区比较先进的或者说欧美地区，拿到AI的这个赛道里面的一些排行，就拿了一些比较好融资的一些公司。这两家公司我相信或多或少大家都听过，hanging face就是我们今天的一个主角。
	那么stability AI这家公司，其实他们一直跟一个很热的词分不开，就这个stable diffusion，就stable diffusion这个纹身图其实跟stability AI这家公司有千丝万缕的联系，包括之前他们公司也爆出过一些负面的文章。那stability AI这家公司，其实大家能看到它的搜索热度没有哈根face高。我们我们主要的这个参数，一个是这个范围在全世界范围内，worldwide全世界范围内过去90天，过去三个月的一个热度，我们能看得到是hugin face一直是处于一个应该是四倍左右的一个热度。就stability I这公司其实之前有很多的融资的声音，所以非常其实如果大家没有用过它它其实是一个非常在海外有非常多用户基础和开发者基础的一个平台，那么区别在哪儿？
	我们继续看这个统计数据在全世界范围内，非常有意思的事情是，我们都知道谷歌搜索引擎因为各种原因，中国地区应该是用不了的。但我们能看到这两个公司在全世界的范围内，第一次就，大家可以去试一试别的关键词，别的关键词中国没有突出过。因为本来各种原因中国的这个指数就会下降，他他搜索人数总的用户数比较少。但就这两个关键词，我觉得中国人民真的是想了各种各样的办法去访问他们，所以非常的突出这个颜色。中国这个颜色的含义就是在在规划的这个概念大家已经懂了，在全世界范围内，中国是直接打满的100分。他如果有一个搜索指数的话，那像旁边一些东南亚的国家可能都不到10，这是很有意思的一个现象，这一轮有大量的大陆地区的用户和开发者参与到了这一轮的AI的建设当中。
	然后我们从搜索关键词来看也很有意思。就搜索这个stability AI这家公司的话，大部分的人是在问stable diffusion，或者是你看其实搜的都是stability diffusion。其实大家没把这个公司和stable diffusion这俩整明白过，像有一些相关联的搜索是这个stable division，stability还还算还算OK但。但是它本身没跳出stable diffusion的影响，甚至你去对比这个SD的搜索的热度是超过了这家公司本身的。
	但hanging face就有一些不同了，我们看hanging face其实它没有铆定在某一个特定的火热的技术上。跟hugin face最相关的指数居然是AI这是一个非常大的词，然后hugin face AI也是一个非常大的关键词。然后hugin face model hugin face models其实我们从这些关联搜索能看得出来，不管是中国还是全世界其他范围内它的累积热度top 5的这个关联搜索词，其实它in face他们他的自己的一个生态定位，它不是锚定在说我有一个纹身图的一个工具，或者说纹身图的一个算法。而是我一来我的姿态，就是我想要做这个这一轮大语言模型也好，2.0也好，相关的所有的事情，这个是它的一个很重要的一个现状，定位很有意思。大家可以回头自己去看看这些google trains的这些数据，有一些不同的洞见。
	那么hugging face到底是什么呢？其实hugin face是一家很早以前成立的公司了，甚至他跟OpenAI的时间是差不多的，所以他才能会现在融到轮融资，也融了应该有小10亿美金了，估值45亿美金。这家公司其实16年成立的，总部是在纽约的一家AI的初创公司，他们最早其实没有想干现在这个事儿。这也是很多大家如果参与过创业会发现你最终要做成的这个事儿，跟你当时成立公司的初衷，几乎没什么相关性。
	他跟face最开始他想做的是一个聊天机器人的企业。国内最像的就是小冰，微软小冰，他最开始是希望说做一个聊天机器人的企业。那这个hugging就拥抱你就是给陪伴你，给年轻人解解闷什么的。创意很好，但做不大。国内的小兵也没有做的特别大，所有的这种直接做聊天机器人的企业都没有运转的很好啊。但是它作为一个大企业的这个入口还是可以的。比如说siri，比如说我们的这个小度，比如说各种各样的手机厂商的这个云助手，那他还是有机会的。
	但是单纯聊天很难，因为16年的时候大家都知道技术还没突破。我们回忆一下理论课的知识，16年transformer都还没有发布那会儿才刚刚发布了attention，然后我们整个GPT bert都没有没有大语言模型，大家都还在做很细碎的一些NLP的任务，但它有一个很成功的转型是什么？就是在于transformers这个库其实也是来自于这一次转型它有一个很准很好的转型的契机是当时我们看到在2018年的时候，GPT1发布了，然后过了四个月bert也发布了。这俩模型其实开启了我们说范式的改变，让NLP的研究不再是一个子任务一个模型，然后搞一堆的子任务的benchmark。而是我们开始把研究的重点转向怎样去提升语言理解能力，我们improving language understanding，开始去做这个事情。
	一旦开始去做语义理解这个事儿之后，其实大量的学术圈的资源就开始疯狂的投入到这个领域里面。但是大家都知道搞算法的人，他写代码不是特别强，这个是一个全世界范围内都存在的现象。就bert发布之后，大家都想基于bert去做研究，做各种各样的研究。但bert是一个论文。他提出了这个by directional的transformer，吧？双向的一个transformer。但这个双双向的transformer也需要写代码，这个代码由谁来写？然后如果有一个代码，它它的开源协议友不友好？大家回忆一下上节课我们讲过开源协议，bert的这个代码本身，这个网络架构的源代码，它的协议友不友好？大家能不能在上面去进一步去做工作，甚至训练出来的这个模型能不能给大家去反应作用，继续去使用，这个是非常关键的一个事儿。
	Hugin face他们团队就做对了一个事儿，就是他们在bert发布之后没多久，19年前后，就基于PyTorch这么一个机器学习的框架，去实现了一个birt的预训练模型，这项目就叫排toch的pretrail bert，这是一个非常狠狠抓抓住了这个风口。我们只能说为什么是bert不是GPT对吧？回忆一下这张图，我们在第一周的第二节课，就讲这个GPT的一路风云变幻的时候，大家也想想为什么bird在中心？为什么是but在这里？就是因为开源真的是推动了所有的技术的发展，只有开源是最直接的。因为说句简单一点，就是大部分开发者都没有什么大的资源，大家都是野生玩家，都是个体户，都是野生玩家。那么我要怎么样能够去参与其中？最好的方式就是在开源社区，有一有那么一个技术能让我去推进。这也是为什么lama出现之后这么多人很兴奋的一个原因。
	因为今天的lama 2就像是19年的bert，呸这个排套期实验这个bert，所以当时也有很多的这个人基于这个bert去做各种各样的研究。当然king face也就成功的切入到了机器学习这个领域。我就不再是一个做应用的聊天机器人的企业了。
	而是我其实也在做这个开源，并且我有自己的开源的项目，有成果，就是这个实现了一个birt。后面更进一步，他们做了什么事情，他们把这些类似的机器学习的这些工具，做了一个专门的库。这个库还很鸡贼的起了一个名字，也是给无数的接触大语言模型的新手造成了一些歧义和困扰。他们取了一个叫transformers这么一个名字，这个名字是我们也不讲他有没有碰瓷之类的，但这个名字确实很好记。
	然后它的定位是什么呢？就在这里写得很清楚，叫sota，state of the art。Sota就是最好的当前最好的machine learning for jack pyto ch and tensor floor jack是谷歌开发的一个机器学习框架，是在tenn sor flow之后开发的一个框架。大家能看到这三个框架几乎是现在最热门的机器学习框架。然后他自己的这个定位就哈根face这家公司其实是一个很会运作的公司，通过各种各样的，大家可以去体会一下，包括哈根face是一个有小红书账号的公司，作为一个外企的初创公司，那他们把这个transformers库的定位就非常好。
	就这是一个，集模型和工具于一体的一个开源的库。这个库，现在也有11万的一个star了，然后fork数量也有两万多。这个库，在github上面去维护的，就是在github上面有那个开源项目，我们在右下角能看到它的链接。这么一个项目在get up上面开源之后，提供了丰富的一些机器学习的工具和模型，你可以理解它比南券更早一步让这些你可以作为一个时间点。
	大家这么理解，在ChatGPT出现之前，很多同学是不关注大语言模型的。但是也一样的有一些机器学习的算法专家或者说工程师需要去做这个相关的研究。在那个时候transformers是非常抓住了这波用户。不过南茜抓住了这波增量的用户，就是咱们在座的各位，咱们肯定绝大部分应该是在ChatGPT出现之前，是不看机器学习深度学习这个赛道和相关的文章工具和开源库的。那么在那个时候，transformers就是大家的主要的生产产力工具。当然现在这帮有生产力的同学仍然或者说这帮有生产力的算法工程师和科学家们，他们依然会使用hugin face相关的一些平台和工具。因为一路走来已经习惯了有这么一个逻辑在里面。这也是为什么大家会发现男券和hugging face他们好像都是服务于开发者的，但是定位有一些不同。
	然后南茜的热度比他高，但是transformers的这个粘性好像更好，并且它上面的高质量的内容很多，因为它搭了一个平台，它不只是一个开源的项目，或者说开源的仓库。但这一路走来我们会看得到，哈根face也不是一步就跨成一个开源平台的。他也是先做了一个bert的开源实现，然后又开源了一个为机器学习服务的库。我们这还没看到它变成一个开源社区，那怎么变成开源社区的呢？其实更进一步，就他把transformers做好之后，这家公司的战略其实是非常好啊。为什么要分享这家公司？也是对就大家如果对AI的这个创业，你哪怕是去当这个科普的段子，或者说创投圈的这个段子去理解的话，其实跟face这家公司都是值得关注的。他们转型之后的每一步节奏和这个动作都很到位，就我做了，你可以想象一下，现在能券有这么多的用户了。但这帮用户一旦把代码弄下来，好像就跟他脱钩了，就没有什么关系了。
	Hugin face解决了这个问题，就你克隆了我的transformers，但他现在还开源了更多的这个库，开源了各种各样的项目。在github上面我们有开发者关系这样的一些传统的对接。但是我没法深度的去找到我的这些用户，就开发者们。那我需要搭一个社区，就跳出github，我搭了一个社区。区这个社区可以下面管辖各种各样的github的项目。
	这个是阿根face做的非常成功的一步。从这一刻开始，他们变成了一个平台。所以你看他自己的定位，它是一个AI的开发者社区，并且是面向未来去构建的这样的一个开发者社区。然后在这个社区上面我们能找到各种各样机器学习相关的。它定位不太一样。因为github的说法是我是一个面向developer服务的一个社区，他跟face说我是面向machine learning的服务的一个社区。然后我后缀可以不加任何东西。你可以是march learning的这个developer、user scientist, 各种各样的人，包括投资人都可以。然后他在这上面结合了模型数据和应用。
	所以hungry face当他自己平台化做开源社区的时候，他抽象了四个非常好的模块和概念。包括他自己也有日积月累的一个积累。首先他在这个平台上，开宗明义的去讲，我就是为或者说我们叫价值点比较高的算法数据科学机器学习服务的一个社区。然后他收集了大量的模型数据和这个APP，就我们最终可以使直接使用的这个APP。包括我们看到的大模型的天梯榜，其实也是一个APP，包括我们前面用的这个前面几节实战课用的这个radio，也是哈根face他们开源的一个项目。所以他自己做了大量的这样的便捷的工具，直接解决了很多痛点解决了很多开发者的痛点。
	这四个重要的模块是大家如果想要在这门课学完之后，还要再再更深入的去做项目的开发也好，去做这个实战也好。是值得我们要在这个节点，相当于最终完整我们要再提升的时候去关注的一个平台。在这个平台上，它这四个部分models data set是比较好理解的。就是我们这些预训练的模型，以及我们可以用来做训练，甚至是做这个instruction tuning的数据集。
	还有一块叫space，这个space其实也是它商业化盈利的一部分，这个space其实就是换了一个名字，你可以理解，他很很会做营销。也体现在这里普通人说我把你的这个模型部署起来，我叫这个serving，比如说TensorFlow w叫TNSERLOW serving，或者叫TensorFlow light，hugging ing face，直接就取名叫space。因为这个space其实确实不只是包含这个代码部署的一个形态。我们在使用大语言模型的时候都知道，大语言模型要部署起来会非常消耗计算资源，尤其是这种GPU的资源。Space其实就是包含这个GPU的资源。
	简单来说就是你把你的代码，你把你的模型放在hugin face上。现在你想把它变成一个服务，那这个服务你也可以托管在hugin face上，然后甚至我还给你提供了非常多的便捷的工具。比如说这个inference，就做推理的这么一个API，甚至帮你去做负载均衡。那你快速的就可以把你训练好的内容就集成到你的实际应用当中。但前提是需要科学上网，因为它在国内时不时的也会被屏蔽掉。同样的他还去为了更友好的去对接一些开源的新用户，或者不了解他技术的用户，还做了大量的一个dox的模块，这个模块里面就是有各种各样的文档，尤其是他自己开源的各种各样库的文档也都在里面。那我们接下来可以看一些示例，让我们可以实际访问一下。
	第一个就是他模仿的github去做了这个趋势榜。大家了解github，github也有一个趋势榜，然后去看热门的新的开源项目是什么，这个他直接像素级的模仿过来，并且做了分类。在模型这一侧有什么新项目，应用这一侧有什么新项目。就比如说我们看到这也是我今天的截图，我们能看得到我记得是今天还是昨天，meta就facebook这家公司开源了一个新的，我说开放了一个新的space，一个demo，就叫这个samedis m4T。
	下面我们看到这儿有一个像打电话一样的，这个项目是他们最近刚刚开源的，开源这个项目的场景其实跟我们课程里面的OpenAI translator非常像，就是一个翻译场景。只不过这个翻译场景它做了进一步的扩展，它把输入从PDF甚至可以变成语音的输入。这里我们都知道语音到文字的识别已经相对来说很成熟了。大家如果有兴趣也可以去把我们的OpenAI translator去对接。然后一个对接一个语音识别PI。也可以把我们的open a translator去做扩展。并且我们的open a translator也有radio图形化界面，你会发现哈根face上面的APP space基本都是用radio来做的图形化界面。这是他聪明的一点，就他统一了很多的东西，不好意思点错了。
	还有一部分就这个data sets，我们在使用这个data x上节课其实是销售机器人。那节课其实教大家用GPT4可以去生成各种各样的给向量数据库里面去存的一些内容。但我们也能看得到整个这个课程一直在强化一个概念，就是AI agent概念。AI agent是一个很重要的一个未来的发展方向。我们这儿看到趋势榜第一的最近这个awesome x GPT proms，就是一个给agent用的一个很有用的proms。待会我们也可以看看这个data sets，那它的models就会有各种各样的models在这个模块里。这些是为了方便可能有些没听讲的同学，我发现经常会有各种问，我们把这个也放到课件里，方便你去找特定的像这个拉拉妈兔7D我们上节课问过。
	那么是不是为开源？很有意思的是facebook meta他们是不允许你直接去在higg face上面去去使用这个inference API来把它变成一个服务的。具体原因我没有去查看，大家有兴趣可以去看一看。但是他把他的限制协议这里写的非常清楚，就我们上节课有好好分析。当你的应用月度超过应该是7亿用户的时候，然后这里没写，就写他的那个参数，当当你的这个用户超过这个阅读超过7亿用户的时候，是直接就所有的他的开源协议就都你都可以忘掉了，需要重新向他提供这个申请，这个是two要实际去商用的时候要注意的一个点。
	然后data sets也是一样，像我们刚刚看到的这个awesome ChatGPT的这个prompt。并且它左边的这个筛选框维持的非常好。就不管你是搜这个模型数据集还是这个space，都有比较好的筛选功能，方便我们在当前这个时代去快速的找到到一些好用的信息。比如说这个circle create context也是一样的。而且它能通过从这儿拿到circle create contest这个数据集，然后方便到我们自己的模型上面去做训练，甚至它的平台也支持训练的功能。
	你看这儿有一个train in auto train，就我们刚刚提到的它收费的这些功能，包括它的space，这就是这个sailless m fort。我们看到其实这个界面和咱们做的这个open I translator很像，因为整个radio就这么一个配色。然后我们在这儿可以去选这个任务，包括目标语言，包括你是如果你要用音就上传的是一个这个音频文件的话，是用文件来上传，还是你直接用麦克风去念，去直接去说。因为radio这个界面大家会发现radio是一个没有把屏幕占满的这么一个配布局其实也是进一步为了更好的让你在手机上能用这个radio的界面，那它就很方便了。因为你如果在手机上你直接按住这个语音就能直接输入，当然你也可以直接上传一些文件，它能翻译出来，这个是他的文档的部分。
	就他自己开源的这个transformers，包括这个deep users，这些都是非常有名的一些框架，我就跳过这个transformers库本身在它的hugin face的平台上，也有对应的这些介绍。好，那我们实际来看一下，就这个是hugin face的界面。我们需要注册，注册之后，他就会有一个像这个dashboard一样的平台。然后里面你甚至可以关联你的github的账号，然后你可以写写你是什么组织感兴趣的这个领域。然后他的自己的官网其实也做的挺简洁的，你可以跳到你自己有一个个人主页，这个人主页跳进来之后会有你自己的一些感兴趣信息，包括这个是跟别人因为我们都知道github是这个程序员的社交平台，其他跟face也在往这个方向去发展。所以它会有一些跟平台用户去互动的部分，他也在不断的去扩展。
	那我们回到刚刚看到的这些重点内容，就比如说我们看模型，大家如果要在汉根face上面去找一个开源的模型，要怎么去找呢？首先你点到models，这里面出现的就全都是模型了，那我们要找模型通常会有需求的去找。就是真正你入门上道了之后，你就会发现其实你提问题一定是越具体越能精准的找到问题。
	跟人是这样交流的，跟大元模型其实也是这样交流的。我们能看得到，它跟github的风格很像，就所有的这些models，包括其他的。都是一个组织名称，再加上具体的项目，那我们能看到stability AI，刚刚这家公司对吧？包括meta的这个lama这个小组，这上面也有facebook这个小组，在在facebook或者meta内部这是两个组，他们关注的这个重点也不太一样。包括像runway ML这家公司其实是真正在stable diffusion这件事情上有技术侧比较大贡献的公司，包括今one金two这两个商用的纹身图纹身视频的这个产品。当然也会有一些其他的公司，包括像我们自己的这个waster，他也在上面。清华的这个chat GM two 6B也都托管在上面。所以你在这上面找模型，一定会比在github上面找方便。
	这一第一个是因为现在大家默认你做了一个大模不行，你不往这个上面放，你可能就不会被看到。因为这里流量很大，这么一个逻辑。但是是很多对吧？我们知道这model已经32万了，怎么找呢？
	其实这里最简单的就是你已经知道是什么了，你可以通过名字去筛。但更多的其实通过右边的筛选器，我们能看到他这个积累的过程其实做的非常好。比如说他这儿有一些大的筛选，从任务的角度有做多模态的，有做计算机视觉的。我不知道这个是不是太小了才反应过来。
	这个要再放大一下吗？这个字。再放大一点，它这个布局就看不完了，大家也没说太小了。好，然后有计算机视觉的，包括我们的自然语言处理的，音频的这些你能看到一些具体任务。我们的text classification，包括基于这个表格的QA和我们直接做QA，zero shot这classification translation，这都是一些很经典的任务。
	那么我们在选大模型，咱们都在问，就GPT我又用不了，对吧？那要商用，那我就想找一个我能用的，我能私有化部署的那怎么办呢？首先上一节课已经教过大家要怎么样去识别出什么样的协议，你是可以用来干什么事情的那下一步就是你先要找一个模型，具体要干什么样的任务。如果从任务视角去找，其实这里能提供非常多的选项，包括像我们说的text to speech，然后文本tabinet classification，对表格的处理，甚至有强化学习和机器人学的，从任务的视角有很多的方式去做筛选，包括像从库的角度。因为我们都知道，你在真正去做集成的时候，比如说你公司里面一直在使用橙色flow，然后你不太想再用这个pad touch的实现去做集成，因为你肯定要在源代码级别去做集成，而不是这个接口层面上去做集成，那你这是可以去做一些筛选的。然后像open reno之前我也提过的TF light，可能大家没用过，但这其实在在业内也是有一定声量的，包括cross和unix，data sets，这里有各种各样的data set，就是它跟就跟这个模型它用了什么样的data sets，我们这就不再具体展开了。包括语言，支持了很多种不同的语言，还可以通过license来筛选，这个是不是就很友好。
	现在大家经过了上一节课的洗礼，再看这对license是不是脑子里面能能给他做个分类了，对吧？首先我们要理解license其实它不是一个它不是有楚河汉界的，为什么它这儿没有再去像task一样去做分类，因为license就是一堆权益的排列组合。那么针对这个权益的排列组合，最早大家对源代码OSI，开源的这个促进会，我不知道中文翻译成什么比较好。OSI他们其实是有去提这个开源的一些定义和精神的。在这个精神的响应这个精神的号召下，有各种各样的比较友好的协议，像阿帕奇的这个协议，MIT的协议，后面我们会发现也有一些协议相对来说没有那么友好，或者说叫所谓的病毒是传播的这个GPL相关的协议。还有一些协议是CC的协议。就creative common，它可能更多的是对数据本身的一个保护。未来我们还会看到一定有各种各样的，就大家逐步规范了。
	逐步规范之后，针对model，针对模型，一定也会有一个组织跳出来说，我们一起去定义一下模型应该怎么样用吧。这个事儿我相信一定会发生的，可能最快说不定明年就会有这样的一个组织出现了，OK当然还有一些其他一些标签，比如说他自己的想要去筛的一些内容，我这儿我们就不再展开了。假设我们现在看到了一个模型，我们想要去玩一玩，比如说这个meta这个模型我们想要去使用的话，首先它它整个hugging face是做的蛮交互式的一个，就简单来说，它它的核心还是服务于机器学习的开发者和同学。所以他会尽可能去做出差异化，但他也会吸收githa的优点。
	那怎么样要去看的？因为你刚刚访问这样的一个平台，如果是一些新新的同学，可能不是很清楚应该怎么去看他的一些关键信息。我简单跟大家说一下，首先我们刚刚看到的各种标签在这上面都能筛出来，比如说他主要语言是什么，然后用什么框架实现的，用python实现的，其实跟facebook meta都有关系，对吧？他meta发布的facebook跟它有强相关，它本身是lama two，那lama跟他也是有相关的，包括它对应的论文在阿卡姆上面的这个链接，它也放在了这里。然后整个lama two是一个text generation的一个模型，用来做文本生成的。然后对应的这份代码，其实他这可能。得要同意之后才可以去看到。
	好，还看不了。对，这个是妈妈兔，没有这个特别，我们换一个，这个也确实比较僵硬。对。你找一个。
	比如说whisper。比如说像open I的这个whisper，他会把它的代码都放在这里。然后整个这一部分的页面，其实都跟get up非常的相像，甚至它内部的实现其实也是用的git所以大家能看到这边也有它的不同的代码分支。然后我们不是还说他跟开发他的平台的用户，本身是想要去做用户关系的。所以你能看到他这里也有一些讨论，就针对这个model，有一些对应的讨论也可以在这边去发生。当然你也可以在这去提一些pull request，因为本身它可以托管代码。然后这边有一些相关的论文，包括他的这个协议，像这你看whisper他支持的语言就不再只是英语，而是九十九种语言，就nama two还是以英语为主的一个模型。但vesper large v2这个就已经支持99种语言了，它支持的框架也很多，这个其实是看模型，我们从这样的一个方式去看，比如说我们要看数据。
	比如说我们要看数据，数据集其实有六万多了，又又增加了。那么我们刚刚看到的，我想提到这个数据集是很有意思的一个数据集，也希望大家能玩一玩。
	第一是因为它他的标签打了7GPT，这是一个很有意思的一个标签。大家看可以去搜索这个标签下面有什么数据，他核心想要做什么事，就我们在南券的实战的这个部分，其实跟大家说过，react这个核心是说我们用大语言模型来做推理，然后得到了一个结果，或者说生成的一个结果。生成的这个结果，因为我们自己做了各种各样的tools，然后这些tools，其实我们有一些描述，然后这些描述是交给大语言模型进一步去判断我应该用哪个to。然后接着我如果判断我要用某一个特定的to do之后，我还会生成一个类似于自然语言的函数一样的内容也prompt，那这里其实就是一个很有意思的。比如说我这边有写一些特定的，在这个数据集里有写一些特定的内容。比如说这个linux的命令行，然后javascript t的这个命令行，excel的这个表格，那这边都会有一些预制的一些prompt，那类似其他的这些ChatGPT标签相关的数据集都有人一直在做常识。像这个是非常火的点赞的数量就有点类似于star的数量，是非常高的那也推荐大家能下来玩一玩。然后他的CC的这个CC0这个license也是比较方便大家去做进一步的扩展的。
	CC零是什么？这个数据协议大家回头可以再看一看，它有几列比较重要的关于它的拷贝再复用等等。这个是从看数据的角度，我们能看到有各种各样的数据。大家如果在发愁，因为我没有数据怎么去做冷启动，那这上面能提供非常多的数据。
	还有一块就是叫space。Space其实简单一点就是一个机器学习的APP，这个概念是一个新概念，就是一个机器学习的。最近比较火的这个529，我不知道刚刚点进来的时候大家有没有看到，包括这里有一个标签，就整个space，它跟一个静态的源代码最大的区别就是它它是运行态的，它是运行时的running起来的那怎么running的呢？我把它放大一点，能看到这里有一个running on a100，它跑在这个A100上面的。然后整个这个项目这个MOT这个项目其实就是一个翻译，你可以简单理解其实是一个翻译。这个翻译具体是做什么翻译呢？
	它有支持多种翻译。
	比如说语音对语音的翻译。大家想象一个场景，而且翻译为什么我们选了OpenAI translator这个项目贯穿了整个课程，是因为翻译真的是一个刚需，我们也做过市场调研了，知道翻译的这个需求，甚至价格，甚至机翻和人翻的价格还不一样。那么就翻译这个场景来说，一定是就如果我们做一个足够好用，然后又便宜速度还快的应用出来，一定会有市场，这个是毋庸置疑的。包括像meta这个项目火起来，火起来之后，如果他真的稳定好用，一定会有很多人去对接他的。就像早期移动互联网的时候，各推这家公司，也是一个学长做的，个推其实就做这个很底层的一个API，但是很多的APP都会去调用它。我相信机器学习的API如果未来做的足够好，足够稳定，也是有大量的应用会去调用它。
	那OK那他具体做什么？大家能看到，从形态上就是从这个文本内容的形态上会有，前面4种就是speech就是语音有语音到语音的语音到文本的文本到语音的，文本到文本的。其实我们现在做的就是文本到文本的，对吧？那你看这个界面是不是我们做的就特别像了，只不过我们不是下拉框。我们在进阶篇的作业里有说让大家改成下拉框。
	当时m forty可是没没发布出来的，但是你看这个，其实所有人对于这种应用的UI的需求是差不多的那咱们如果做出来了之后，其实也能把代码放到hugging face上，然后也可以让大家去点赞，也可以去跟大家交流。这是一个非常好的为大语言模型和机器学习的一个新的平台。OK这其实也能输入一些对应的内容，比如说我爱学习，对吧？我们看看会不会这个。
	难道没有中文这么悲剧的吗？还真没有。没有。
	然后这里大家能看到，它是一个实时的demo，你会去排队，在队列里面，然后它会有一个预计的完成时间39秒。刚刚右上角有一个小的提醒，所以整个实时的前面有一个完成了，在这个完成就该到我们了。那么整个应用，首先我们通过哈根face能看到各种各样的一些，这已经是全球最新的信息了。就是我们看到MFT刚刚在硅谷，在湾区，大家把它这个应用抛出来，放在hugin face上。然后咱们即使是在国内，也能够去访问他们。甚至因为现在大语言模型很热，这个计算资源都不需要我们来出，我们就能体验到它。因为我猜是make facebook在为这部分的计算资源付费。
	大家能看到翻译出来的文字是这样的，他给了一些example。所以其实就我们这节课我们这门课程里面学到的radio的一些基础知识，要做出这样一个UI是不难的。然后要能够支持不同的语言，对，我们也是都学会了，甚至我们去对接一些不同的API，比如说语音识别的API也是能做出这个效果的那最后最大的核心的差异点在哪？就是你会发现在当下大语言模型APP的这个是时刻最大的两个部分的工作。一个就是基础模型，为什么要去做m for t？我猜背后肯定用的不是GPT，用的是自己的语言模型。
	那做这种应用，其实某种层面上也能帮他们收取数据，他们也需要数据去帮助他们自己的大语言模型去做迭代。而我们作为一个本身不去搞大语言模型的训练微调的开发者来说，最好的策略就是在你的你的允许使用的范围内用最好的模型。其次因为大元模型的质量越高，它他的一些问题就会被基础模型层给消化掉。我们也对比过GPT3、GPT3.5、GPT4这三个大大的跨带的模型，针对同样的prompt的输出的质量的好坏，吧？包括我们处理什么大运会的联网，当时用google search加上我们的大语言模型的那个例子。GPT4没有做任何的改动，它就是能把那个语义理解出来，但GPT3.5它输出的结果就是不稳定，所以它后续去调另一个two的时候就会出错。
	我们能用更好的基础模型，一定要用更好基础模型，这个是毋庸置疑的，生产力不一样。第二个就是说作为开发者要学会这些好用的工具，radio就是一个好用的工具。我们做销售机器人也就40行的代码就能跑起来，把向量数据库对接起来了，然后把图形化界面也做好了，把大语言模型也对接上了。这些是很很香的一个状态，就对我们现在大语言模型这个应用开发来说，这个是models。好，我们再回到文档，这个是它的文档的这个模块，下面有同学提醒我有这个中文matter in chinese，我没注意。
	好，在这个文档里面其实有一些很重要的库是咱们可以去关注的。比如说有这个前端开发的同学，transformers这个python库，它也有GS的版本，包括像哈根face也有这个GS的版本。然后radio这个库就是having face开源的。所以我们在这上面看到各种各样的space都是hugg face，influence API和这个influence end points，就是他们hugin face的一个平台，可以用来在平台上很方便的去提供API的一套对应的库，包括巷口coking ized，还记得我们好早以前去学这个，我们去学的课程的时候有讲过token nier。当时我们提到OpenAI开源了一个做token ized的这么一个库，叫tik token。然后他当时还发了一个benchmark的对比，就是跟他face的这个to nix去做的对比，当时是说他这个token的效率更高，吞吐量更大，这里面还有一些别的像阿曼这个云平台的CG maker等等，auto train他自己提供的API和UI，大家有兴趣的话可以去深入再了解关于这个having face。
	好，我们剩下十分钟提问，看看大家有什么关于前面这一部分开源生态的问题。然后喝点水。如何在hugin face上找到质量高的模型？好问题，这是一个买货的逻辑，就是我觉得好好研究一下它的参数是很有意义的。就是咱们能看到去他为什么要做趋势榜，对吧？包括你看我们现在这儿，也可能我这画面有延迟，有排序，对吧？
	首先找的动作就是从32万个里面找到一个符合你要求的最好或者说最符合你要求的一个模型。那怎么样去找到符合你要求的呢？从需求侧来说，你要筛一筛，你本来要找的是这个image to text的模型，你就别去点这个text to image的，因为有可能是他们并不互通，当然有也有互通的，或者说你找了一个image to text的，就不要找这种什么image classification的这肯定不一样，它是不兼容的那比如说你就要找一个我们还找一个我们课程相关的，这个文本生成的，在在text generation对吧？比如说我们要找文本生成的那这有training，有趋势？
	有你可以认为这个是点赞最多的，也有下载最多的，这个是最近更新的，大家其实你看一下一个好的产品，它就是他就设计的好。我们我其实在整个课程里面一直在跟大家讲数据触发数据的观点。为什么还会有一个recently updated的？其实好多平台不会有啊，这说明这是活跃度，我们通过这个能看到活跃度。就是有的很多已经功成名就的开源项目或者说模型，他不怎么更新了。那那你通过这个其实能找到另一个维度。
	Most下载最多的，比如说这个GPT two对吧？为什么下载最多？很有意思，这个GP two我印象中是hugging自己实现的。是的，这个是hugin face的团队实现的。哈根face这个公司真的挺挺厉害的，他们把bert实现了，然后把GPT two也实现了。
	我们还记得那幅图吗？就是那个那个bird在中心的图，我们回到这幅图，他真的是抢占了很多好的位置，所以他在这一轮谁也没得罪，然后打开门做生意，大家都是好兄弟，对吧？我也不收你们钱，我作为一个平台，大家还能在我这蹭点免费的计算资源，特别有意思。还有问那个solution是啥的，还有这是他自己提供的一些工具，这个同学回头一定要自己去访问包括它硬件，价格，什么报价之类的，这些涉及到钱的东西我就没有讲了，就是跟他跟face商业化相关的一些东西。然后大家还有什么问题吗？还有什么问题吗？
	其实这里也有一些很有名的模型，包括bloom这个模型。我记得bloom本身很早以前也是哈根face一直在推动的一个模型，并且当时应该是训练还一直在更新训练进度训练的进度，然后bloom这个模型最终训练出来的那个模型参数规模应该是跟GPT3差不多的。
	托管是什么意思？就是你不用自己租云服务云云这个什么你不用自己租这个公有云上面的服务器了，happy face他自己你可以认为他买了，或者他跟云服务商合作了，你就把你这个代码直接在hugin face上部署成了一个服务。而不是说你自己去，某一个比如说华为，阿里这样的，公共云厂商。那租了一个服务器要搭环境，然后要部署就不用了，在他这就可以完成部署。
	这些模型都是训练好的吗？请点开每一个你关注的模型的页面里面，看看别人的read me。这个同学问的是不是都是训练好的，然后到今天倒数第二节课了，不应该这样问问题了，不存在训练好的模型。
	模型是什么？对，我们在聊模型的时候，我们在聊什么？模型就是一个文件，这个文件里面存了一个神经网络和他们的权重，然后群众没有好不好的说法。介绍一下最热的几个模型。这个我们在很早的理论篇的时候去对比过模型，我们待会儿在天气绑的时候会简单的说两句。但这个模型的介绍，大家可以在网上搜到各种各样免费的文章，我们这儿就不再赘述了。
	Macbook运行这些模型要注意什么？Macbook运行不了，运行不起来这些模型。同学GBG two你在macbook里面怎么运行？除非你说有一些量化之后的模型，然后在macbook上也许能运行。比如说intel 4量化的小模型也许可以。对，但是这种百亿、几十亿、千亿级别的macbook上面跑是比较难跑的。
	可能很多同学确实问出这个模型的问题，应该是之前可能没有太多机器学习的相关经验。我这儿再简单说一下什么是模型，模型是一个文件对吧？那么那这个模型本身它要跑起来，那它到底怎么跑起来？其实就是要用资源。那他用什么资源呢？他其实就是要把那些模型参数加载到你的GPU显存里去，对吧？
	你想象一下，你可以算个账，这个几千亿需要多少显存？需要几百G甚至上千G的显存。你说你的笔记本怎么可能有这么多的显存？对吧？
	然后为什么要用GPU？待会儿我们在硬件的部分跟大家尝试做一个刨根问底的一个解释，让大家能理解笔记本上跑大模型是不靠谱的。那些说在手机上跑大模型的，我认为也是不靠谱的。然后手机上为什么要跑大模型？我斗胆预测就是高通的芯片卖不出去了，现在最新的芯片没地儿用了，那手机上的最好，这就是画质最好的游戏。我用去年前年的手机我也能跑得动了。那我造新兴造这些更新的芯片，我用来干嘛呢？那总得给他找应用场景，端侧大模型对吧？不是挺好的吗？我是这么理解的。
	大家尽量提跟我们讲的这个课程内容相关的问题，就是什么这个同学，还有个同学问用妈妈兔做QA的机器人embedding推荐用什么模型？用它配套的那个embedding的模型。我们给你找一个。
	我现在还看不了这个two这个项目的文件，然后这个其实跟我们看get hub的read me一样的。我之前没有讲，但这应该不用交的，这个大家来进来看看吧，包括怎么样他的licence是怎么回事，他对应的论文啊啊啊，包括它这个硬件需要使用什么样的硬件，它这边都有写。在什么样的数据集上面去做的训练。训练完的模型，如果我们非要说什么叫训练好了，只有对训练的一个阶段的模型去进行评估的结果这么一种说法，然后这里会有一些评估的结果。这个是对应的在一些基准测试上面的一些结果，这个是妈妈兔的聊天模型的版本。
	模型参数有什么工具可以看吗？这个同学问模型参数，你要用工具看什么呢？这个同学你再详细讲讲你要问什么。
	跑CV的视觉模型，macbook的meta是可以的，make macbook的这个M系列芯片是可以的。本来M1这个芯片做的就比它的英特尔系列芯片要好，因为计算机视觉的模型没那么大。但是咱们如果要动辄搞个百亿级别的模型，要在macbook上跑，这个是很很很容易让自己高血压的，这个咱们要注意一下。对，看参数量具体有多少，你就打开read me，他会告诉你的。但是你看到了一个项目，在哈丁face上，他连他模型有多少参数他都没说，那你可以直接关掉了，这个项目肯定是有问题的。然后还是那个观点，就是专业的人干专业的事儿，专业的工具干专业的事儿。为什么推荐了higg face？就是因为它能提供很多我们不可能穷穷尽的信息和知识，是啊授人以渔的一个方式，去让大家去关注这个平台，会得到一些有效的信息的，哪怕我们课程里面挑出来的那几个。Circle的awesome的这个prompt，这俩数据集你先用起来试试，咱们别贪多别觉得脑子里好像过一遍这个东西我就学会了。先手上得过过一遍，你才真的能学会很多东西。
	所有计算机视觉的这一波模型，都是TensorFlow那一波完剩下的，就我们这儿看到这些，他当然能在笔记本跑起来了。你要知道这个open wino那都是直接用的NPUUSB1样的算力。这个计算机视觉就是视觉，去看彭老师的tensor floor的课，录播课很简单，给你过一遍，你就大概知道计算机视觉现在已经卷到极限了。对，计算机视觉我当时用这个，我看在哪个里面有写用OPPO reno是非常便宜的，硬件上就能跑的一个像USB1样的算力卡两三百块钱就能跑起来的。
	还有问多模态开源都有哪些？同学自己到这个部分来看一看。VCOQA55个，这种让控制老师手指点有哪些东西的这个问题，这个还是希望同学自己多研究研究，因为你再看一下，你并不能更进一步去研究了，多多理解这个平台上一定是有好东西的，但前提是咱们得花心思去去学去看啊。如果我们现在教的这些内容，比如说南县的基础概念，这三个实战项目还没整明白，赶紧去先把它整明白。我理解大家都焦虑，但是焦虑的前提是你真的在努力啊不要只焦虑对。
	好，那我们到GPU这一趴，然后如果还有时间，待会儿可以再回答关于high face的，我们来聊聊GPU硬件。还有同学问这个VIT色flow那一套，这个同学再补补补一补。一个是一个是网络框架，一个是开发网络框架的框架，TensorFlow跟VIT不是一个层面的概念。
	我们再看看横向的对比，这幅图我相信很多人看过两三回了。我们看到这儿有个bloom这玩意儿，然后我们在刚刚的哈根face上看过这模型，然后清华的GLM，包括OPT，这个也是facebook主导的。然后大家如果关注硅谷这个八卦的话，lama的这个研发团队大模型团队最近又又包了一些八卦。对他们内部这个OPT的团队和拉马是两个团队，有很多的算力上面的一些事情。
	然后像ether etho pic这些模型，其实一路过来我们看到大模型真的非常多了。我们刚刚在平台上看到甚至都有三十多万个模型了。那么模型到底用哪个？对吧？除了刚刚说的我能塞，但前提是我知道我要怎么塞，还是刚刚那幅图里的一个很有意思的文章，就为什么hugin face最终在那个网站上呈现了那些数据。其实你往深了想，就是这个底层逻辑也好，真正搞大语言模型的人，我们说搞大语言模型的人不是用大语言模型的人，这是两拨人对吧？真正搞大语言模型的人，他们其实是整明白这个事儿的。
	然后他们整明白之后，又有一个很懂大模型产品设计的人做了一个产品叫hugin face。我不知道这么说我表达清楚没有？所以hugin face是真正你不只是学完这门课，你是学完这门课你有技术了，你还想再深入研究的时候，我觉得可能去把大把hugin face和这些硬件有一些了解是好的，不然你就只是变成谈资了。就是你知道什么跟你手上有活是两件事。如果是只聊谈资的话，那就看自媒体文章就好了。但如果你是真的想深入去学一学，我觉得一定得动手多动手。
	然后我们回到这个横向对比，其实你会发现这篇综述的文章其实应该在理论课的时候，第一周就发给大家了。这篇综述文章应该就叫大元模型综述，是中国的团队，我忘了是复旦还是人大的一个教授做的了。在这篇文章里面，刚刚那幅图，包括这个表格，其实他总结的非常好。大家去细看一下它在大语言模型的横向对比的时候，我们能看到这个表，应该是能看到的模型的名字？
	发布的时间尺寸，10亿级别的参数作为这个单位，我们能看到这个模型的尺寸。其实现在再大，也就可能像华为的盘古，搞过这个1万亿的参数，很很夸张。现在GPT4也传言是2万亿的参数，因为是八个几千亿的参数，in simple起来的。
	其实到目前为止，其实人类资源有限，就是我们的GPU算力有限，黄教主不可能给一家公司供那么多货，大家的这个总数都还在几万张卡这个尺度上。再大的公司可能也就玩万亿的这个参数了。往小了玩，其实也就是百亿的参数，当然有些更小的像6B但它其实也接近百亿了，60亿也接近百亿了。其实就是从模型参数尺寸的角度来说，就这么一个值。但对我们的使用者来说，不要去被这个值给迷惑掉了。就是你想象一下你在淘宝、天猫、拼多多上买东西，他给你出各种参数好的时候，你一开始不懂的时候你肯定会被割。
	你后面用多了之后，你就知道还是要看实际的体验，实际的参数。那个玩意儿跟他说的这个产品参数产品规格又不一样。所以看这些参数的时候，抱着一个不要认为参数越大就越好这样的心态去理解它。因为参数只是单纯表明了我的就是这么讲，你很大并不代表你很强，对吧？这个逻辑大家应该很懂。好，这个是模型的参数，然后这个模型的参数越大，虽然它不一定很强，但它一定很贵。因为你运行它需要更多的资源，这个是跟成本直接相关的。
	还有一个叫base model的概念，就是我们知道有很多模型，它是在一些模型的基础上做出来的。就比如说我们在讲react，讲这个react这篇论文的时候，当时有提过一个叫web GPT，鼠标这里有贴，这其实是open I很早期的时候发的一个，你说叫文章也好，应用也好，当时就把GPT3去做了一个初有点类似于ChatGPT的原型。然后你能在web界面上跟他交流，叫web GPT。大家去回看我们南券的，应该是下基础概念下那个部分。讲react的时候应该有那么一张截图。然后就讲这个GPT3的类似的一些应用迭代。2021年的是这个年底的时候发的，类似于这样的一些继承关系，对于我们去理解这个模型是有帮助的。
	简单来说就是InstructGPT也好，web GPT也好，code x也好，它核心的base model还是GPT3，那它跟GPT3.5就是有很大的区别，因为它训练数据不一样。当然大家如果这儿这个课正儿八经去消化好了的，你会知道它的不一样的点有很多。GPT3和GPT3.5训练数据不一样，in bedding模型也不一样。刚才还有同学问这个numa two用什么embedding模型，这些都是最基础的东西。就是你你课本没看，你老问老师课本上面写了什么这种问题就很伤心？花这么多时间给你准备GPT3和GPT3.5的invading模型不一样，encode的这个方式也不一样。然后我们在讲invading的时候也提了invading这个模型跟大语言模型，像DNA双螺旋一样的大模型也越好。一般来说它对应如果升级了那个embedding的模型，那个embedding模型其实也会效果更好的，这也是为什么让大家无脑的就去用这个open I的in bedding的ADAV2，就这个逻辑，然后又很便宜，那这个ADIV2又同时服务了GPT3.5和GPT4，这个是base model能看出来的很多一些差异性。
	然后除了这个以外，包括像它的pretrail的这个数据的一个规模，这里有一个相当于我训练集的一个大小，然后还有更新的一些时间，使用的这个硬件，包括他训练的时间，这个是还有他有没有做这个评估的时候，有没有去做这个chain of thought，大家如果以前看见一张表肯定是很猛的对吧？但是学完这个课，你应该是能看懂这张表的。我我我的理解，咱们应该是能看懂这个表的chaf south包括在这里有一个adaption，有没有使用基于人类反馈的一个强化学习来做adaption，为让他更好的能回答的像一个人的问题。
	这个其实是从学术圈，从论文的角度去看，就是咱们的语言模型通常会关注哪些？是横向对比的话会关注哪些维度我们再总结一下，首先这个模型有一个一个尺度，有一个大小。这个大小我哪怕只训练一个token，它也是这么大的，我们这个是第一个事儿。
	第二就是说模型不是石头缝里蹦出来的。我们都能看到理论篇里面讲过GPT3到GPT3.51路是怎么魔改的，它有base model，那base model也决定了它的下限和上上线我们不好说，至少决定了下限。如果他的训练水平不太差的话，别搞得特别离谱，还不如走了。然后他有训练集，然后这个训练集在训练过程当中有一些adaption的手段，然后它这个模型本身需要消耗硬件的资源，训练的话还有训练的时长，才能把它收敛，才能让他逊什么叫Martin learning这个基基础课？我们这个课没有讲这个基础课，大家感兴趣可以去说一说什么叫训练。我们在讲理论课的时候有简单覆盖掉一点点，就深度神经网络那部分的内容，通过梯度下降，最终能收敛，那么就要有一个时间。然后最后完了之后训练所谓的训练完了之后，我还要评估一下他好不好，一些不同的评估手段，然后有公开的，也有源代码没有开放的那我们能看到首当其冲的这个GP3，就是没有开源的，开源的也有很多，这个是从学术界的角度我们能看到的。
	当然妈妈的开源，虽然他商业没有那么友好，商用没有那么友好。但是如果你只是去做一些学术研究，它还是因为它代码是给到你的，然后还是有一些好的点的。比如说我们看到通过这个合成数据，人造的数据，lama有一个孪生兄弟叫alpaca，是stanford的一个羊驼。应该是羊驼，我没有翻译错的话。这个羊驼的模型其实还衍生出了各种各样的一些小的分支，包括用mora来进行的这个版本。
	然后这里是大家想象一下，这个就是我们更新了之后，之前那个bert在中间对吧？Transformers开源了，那现在穿出花蘋face。最聪明的地方在哪儿？你去评价一家创业公司，他很聪明的点在于我不用再去把所有的新的东西用我的员工去实现一遍了。我搭了一个台子，facebook OpenAI，你们来唱戏，你们来实现你们开源，把你们的成果放在我这个台子上。然后所有的人都会在这个台子上继续去深挖，有你你你做的好，你有base model，大家会基于你去做演变延伸。这个其实是一个非常聪明的策略。
	我们也能看得到，其实不管是na ma也好，还是未来也许还有一些别的大语言模型，它会也会开源出来。然后肯定也还会。如果它足够好用，就应该是这样有人为他做延展的。这就是一个我们不便于去量化讲的一个维度。
	就是你说这个大语言模型好不好，你就看看有没有人在上面做生态，对吧？我们不去点名哪个模型好，哪个模型坏，你就看看这些模型是有没有人做生态，你就知道。因为也这些生态的这些模型，都是投入了巨量的时间资源做出来的。大家花心思做出来这些衍生模型，那他为什么要做？然后为什么做得出来？那一定是他看中的一些点，这我们就不展开讲了。
	那么大模型能不能像手机一样跑个分呢？像雷总吧？我们大模型跑个分谁强？首先跑分这个事儿怎么说呢？跑分我一直在想，通常大家会问哪个大模型强这件事儿。大模型强其实是一个不太好的表述方式。就跟就是我我记得我在开播之前，就咱们这一期开播之前，我就有跟大家分享过，包括线下一些分享，也会有一些同学在一些谷歌的会上问，到底哪个大模型强？
	其实这个事儿我想了一个很简单的方式，就是大家去想一想你跟你同事谁强，尤其是同岗位的同事，你和他谁厉害，你和你的leader比谁厉害？为什么？就这个问题如果你说不明白的话，那自然大语言模型的对比也不太好说明白。然后你会发现最终会落到benchmark上，就是会变成他比我强。
	因为我们同时去做这个项目的开发，他只要两天，我要五天，或者说我们都是我们都只写了两天。但是他写出来没bug，我写出来可能产生了20个bug，所以一定是找到了一个benching mark，那这个benching mark用来去评估它的一种能力或多种能力，是这样的一个模式去聊谁比谁在某件事儿或者某个能力的表现上更好，所以用这样的视角去看问题，其实你更能抓到本质。然后其实整个课也在讲这样的一个逻辑，就像你说哪个开源协议友好，没有哪个开源协议友好。因为开源协议的核心是去看那些权益，你现在需求是什么，你整明白了，那你才会去找哪些协议满足你的需求，对吧？所以这个是一个很重要的思考逻辑。
	我们看看大语言模型所谓的跑分，其实也在hugg face上托管的这么一个项目。然后这个项目首先它不是唯一一个用来跑分的，既然是跑分，居然是找他的一些能力指标来做对比，那肯定不是同一个。我们也能经常看到国内有一些所谓的大模型，又超过了GPT3.5了，又超过了什么什么。
	大家理智一点，至少学完我的课，大家要有能力去区分谁在说我不能说谎，谁在巧妙的说话，谁在用一些，比如说他跑了一些很特别的benchmark对吧？然后他还搞了一个平均值，大家能看到这里是平均值，这里有平均值，这个平均值是平权的。就是你简单想象一下，你简单想象你的高考每一科都是一样的分数，你觉得吊诡吗？很吊诡，对吧？但是我们现在对大元模型这个新生事物来说，我们不知道给他怎么学科打分，没有权重，所以我们都是平权的。这个是现在所有做大语言模型的评估的人来说，他没建立这样的一个能力模型。他就算能建立一个雷达图，但这个雷达图的权重是平权的。
	所以要站在这个视角去理解大语言模型的跑分和天梯榜，你才能看到问题的本质，你也才能知道什么样的模型是适合你的，你要找什么样的维度。就这个leader board d他找了四个基准测试四个维度。然后我们能看到，最早理论课的时候我们也讲过GPT替他们包括这个prompt的learning那个课的时候我们也讲过不同的基准测试。
	首先基准测试就大语言模型未来会养活很多人的。因为大元模型迭代太快了，所以分析mark不够用了，评估的工具不够用了，就光做大语言模型评估就能养活好多开团队和公司的，你去想想，今年79年重新开始高考了，你说你现在开始做一门这个生意，是去给大家写写这个高考刷题的。355年高考三年模拟，你就干这个事儿，你说你你你是不是有前景吗？肯定有啊，因为都还不知道怎么玩。
	那就这个刚刚看到的天气板来说，它其实是用了一个开源的语言模型的评估工具，然后做了四个关键测试，一个是这个AI two，也是一家印象中这是以色列的一家公司，然后年轻里面也有他们的模型，我没记错的话，然后他做的是一个推理的挑战。都在小学这个级别去问问题。大家还记得我们看prompt learning chaf source的时候，包括self consistency的时候，有这个小学好像8000到人工出的题目对吧？那个也是一些可以用来做对比的，然后还有一些常识推理，然后多任务准确性，57种任务，包括或数学、历史、计算机科学、法律，还有一个QA，choose for QA就是让它相当于也是一个常识性的问答。我们到这儿你会发现为什么要讲理论课。
	你现在再回过头把第一周的每个月看一遍，你都会有不同的体会的。理论课里面我们讲最终讲prompt learning那一部分的时候，为什么prompt的迭代会促生benchmark的迭代？是因为我即使今天大语言的模型不迭代了，我们人类还没把大语言模型用好，所以我的prompt得先迭代，我发现我prompt的技术变强了，我又找到了一些这个就相当于你这个领导艺术变强了，你下面的员工没变化，但你把人家的这个正向激励做的非常好，他他干起来有劲，他知道你想要什么了。那那这个时候他就会更出色，然后你就能给他交一些更复杂的工作任务去给到他去做。那这个过程就跟现在我们看到的prom的工程学的进展和benchmark的进展一模一样。
	你说大语言模型这么多大语言模型它的训练技巧这个思路有本质的变化吗？没有，还都是基于为什么我们要讲attention mechanical transformer transformer到后面的这一波，都还在这个框架下玩，玩的都是一些工程学上的技巧。然后同样的这个open LM lead board也能看到源代码，很友好。除了lama需要申请以外，大家都能看到。然后hugin face做的这个lead board，然后它也很大，这个running能看到它也是一个space，就是我们还跟face上面托管的一个应用。所以其实我打开给大家体验一下，其实我们能在这儿直接去去去筛选，有点慢，因为它实时的。
	好，我们把它稍微缩小一点。这就是我们刚刚看到的课件里的内容。首先我们看这个m benchmark，这个bot有关于radio这个面板的一些解释，最重要的就是这四个玩意儿，不同的四个基准测试，这个分数就是直接平均。算了一下这个面板我相信大家应该已经比较熟了。
	然后我们能看得到，这里有一些什么可以选。比如说我不看平均分，我不或者我去掉一些特定的基准测试，这是可以的。就我们刚刚说的，根据你的具体问题，你现在这个问题是对常识推理，就不需要常识推理。这个常识的问题可以忽略它，那就把它勾掉，就能刷出一个新的分数来。
	比如说我把这个QQA干掉。他他这个是实时计算的，这个地方有一个数字，他正在重新去刷这个。因为模型太多了，三十几万个模型，他每一次勾选会实时的去算，这个重新给你刷出来一个列表。所以大家如果觉得点了之后它没反应，它不是卡了，他只是单纯的在在运算而已。然后他大家能看到这个average的分数有变化，因为他重新算，重新拎出来这三个数。
	然后类似的模型的类型有很多种，我们在理论课学过的什么叫预训练的，什么叫微调的，什么叫指令微调的，什么叫继续强化学习微调的。不记得的回去看ChatGPT，就GPT1到GPT3到3.5到ChatGPT那一节课。模型的尺寸我们刚刚看到了，在论文里面也有提到，这个是最重要的。为什么会放这些呢？是因为模型尺寸跟你部署的成本息息相关，这个很重要。这边是一个能力的判断，也很重要。接着就是你你想要用哪些类型的模型。它只有free train，那你可以自己来再做find。如果他已经find tune，你只想做instruction tune的那那你可以就把前两个勾掉。这个我们就不再赘述了。
	这个是关于大语言模型这个时代，我们看到的开源生态最好的一个入口。当然我们不排除这个世界上肯定还有人在做类似的事情。但是就跟我们不会讲所有的类似男性的框架一样。我们讲最头部的讲天花板。那你看看天花板现在做到什么程度了，然后去靠近这个天花板。然后过程当中你会发现天花板也没有完美，所有事情都没有完美。那么就你的需求来说，如果现在这个最好的框架平台生态都不能满足你，你还需要你的需求哪一部分没有被满足？然后再通过各种各样的方式搜索，各种方式去找你的问题的答案。
	好，接着我们讲讲硬件硬件这一部分会过得快一点，因为我怕大家听睡着了，我尽量讲简单一点，因为我本身我也不算是一个硬件的资深的专家。硬件的这个选型，显卡选型，其实它本质逻辑和大元模型一样。就是你想你选显卡，到底你想选出一个什么样的卡，对吧？就是落到核心以终为始。你想比如说你核心还是你预算多少，然后你选出来这个卡，它要每秒钟生成多少个token，你是满意的，从这两个视角一下就锁定了关键的指标了。就跟之前我记得讲哪节课有人就在问这个问题。我说就跟买房子一样的，你不能上来就说老师我要买房子，那个销售我要买房子，然后我我我就这么一句话肯定不行的那更多的是说我要几房一厅对吧？然后预算多少，要不要临街，我为什么要做这个销售房产销售。
	机器人也是有一些很重要的一些刚需参数在里面的那好，第一个问题就是在讲硬件之前，我相信如果有些不是计算机科学专业同学可能不一定能整明白这个CPU和GPU有什么本质区别。这有不明白的吗？如果有不明白的，有不明白的按个一，好像大家也不会很好意思的。
	那就简单说一下，就是CPU和GPUCPU和GPU的区别我觉得比较有意思。就是GPU它是根据需求后面才产生的，计算机的历史可以追溯到上个世纪四五十年代，对吧？但GPU的历史很短暂，GPU也就最近十几二十年的事儿。显然CPU更早，CPU是一个中央处理器的单元。那我们跳跳过了说这个CPU是一个中央处理器的单元。
	在没有GPU的时候，CPU干了很多的事情。我们而且你想象一下有很多笔记本电脑，它也没有独立的显卡，对吧？所以CPU其实做了计算存储各种各样的工作。这台电脑的计算的部分，存储的部分都通过它来做中转，它就像一个大脑一样，但GPU不是干这个事儿的，那GPU是干什么事的？
	我们来看看GPU，首先我们知道CPU和GPU是两个东西了，但GPU和显卡是一个东西吗？我大家觉得是一个东西吗？我GPU和显卡会经常被被误以为是一个东西，并且你在各种各样的搜索引擎去输入GPU，然后如果你收的是图片的话，基本谈的也都是显卡。然后我们通常有时候也傻傻的搞不清清楚，简单讲一下就是CPU也不等于CPU也不等于电脑，对吧？那GPU肯定跟显卡还是有一些区别的，GPU是图形处理单元，然后我们听过很多的一个词叫哭的。
	那么酷达的前身大家去了解酷达的前身，在我开始接触这个计算机的时候，还没有酷达。酷达的前身英伟达的命名在它的内部叫GPGPU，GPGPU，通用的这个图形处理单元，后来改名叫酷达，酷很酷对吧？酷的就很酷了，这个名字GPGPU听了多绕口然后那那我们回过头来，待会儿再聊哭的。GPU是图形处理单元，它其实它首先它肯定是个硬件，计算机内部的一部分。为了直观的理解，我右边放了两个图，对4090这个同学说的，专门放了4090，有很多同学高度关注这个，其实这个GPU是这下面这个玩意儿，你买的这个玩意儿叫显卡，上这显卡里面有很多的计算单元，那GPU是当中的这个处理单元，但是它现在GPU有很多不同类型的单元了，这个是GPU叫图形处理单元。
	然后他为什么跟CPU不一样，是因为计算机的发展，以前计算机是干嘛呢？我们讲这里又可以跟这个之前联系起来，我们讲evading这门课的，讲这节课的时候讲过计算机的数据表示这个学科也好，这个方向也好，就是我们讲波普尔的三个世界，对吧？我们计算机在解决世界三的问题，就客观知识组成的世界，以前我们都写在这个纸上，现在我们写在硬盘里了。这个计算机要来把这些数据都存下来，这是最早计算机要干的事儿。纯数据，然后简单的展示一些表格文字。但到了后面我们发现算力变强了，算力变强了之后，计算机承担了起很多娱乐功能。这个跟很多娱乐功能现在也被手机承担一样，电视就越来越不重要了，电视的很多功能被剥夺了，娱乐从这个电视从这个下象棋变成在电脑上下象棋、下围棋了。
	那那既然我们能在电脑上面去做这个下象棋，那是不是能做更多有意思的事情？就是你想象一下，最早我们都跟小伙伴一起在院子里玩，大家一起玩游戏。后来发现有这个GPU之后，有游戏之后，大家都在这个电子游戏里面去深度去玩耍了。那是因为当然它有各种原因，这个我们先不讲，所以有这样的需求了，不管是商家触动的还什么原因，但这些电子游戏，包括这种CG剧情的动画，它需要用数用数据表示。
	这节课我们教大家的，它需要被表示出来，那它要表示出来它的计算量是很大的。就是你想象一下一个大家可以回头去算一下，一个8K的一张图片，它它有多少个它的位图的分辨率是多大？那这个分辨率里面每一个都要显示对应的这个数字，我们讲过这个图彩色图像是怎么显示的，然后如果它还有这个透明度的话，还要再增加一个维度。所以它要存的数据和他要计算的数据都变大了。而这么大的计算量又是服务于一些特定的任务。比如说这个图形渲染，游戏动画，包括深度学习，甚至包括挖矿，对吧？都是需要大量的算力的，简单的就是需要有人干活，需要有人去计算。
	那那为什么说英伟达这家公司是一家特别好的公司？就在于他们特别会创造需求。就从早期的游戏到虚幻引擎，这样的一些做cg动画和电影的需求，都需要显卡，都需要GPU，然后一些通用计算。比如说深度学习，涉及到了大量的高维矩阵运算。对高维矩阵运算天然就是一个可以去并行化的可以去并行化运算的。
	我们讲我们讲理论课的时候也讲过，transformer这个架构好在哪儿？抛弃了IN，IN是一个没法并行计算的一个网络架构，前一个算完了才能算后一个，那我就用不出来GPU的。好，那么transformer可以穿梭的那个架构大家还记得吗？六层的in code，六层的decode，然后每一个decode都直接跟encode连接上了。六个抵扣的可以直接并行计算，速度就可以提升六倍。在抵扣的这个层面上，这都是一些非常好的需求，并且是一个大市场的需求。所以英伟达做的非常好，然后也在快速的迭代，所以包括一些高性能计算，只要能够并行处理的那GPU都可以去进行并行处理。因为GPU天然它内部的一些单元，使它能够去做并行处理。
	我们后面还会有一两张图，GPU是一个图形处理的小单元，甚至是里面这个以芯片技别的东西。就我们看右边这芯片级别的东西。然后显卡其实是包含一个或者多个GPU芯片的这种硬件设备，当然他们是同样的任务。然后显卡其实就是你最终买回家的那个玩意儿，它有很多不同的公司，英伟达MD包括国内也有些公司在造。
	然后不同的厂商，大家卡友们还会亲切的称之为英伟达的为N卡对吧？MD的为A卡。然后它能够方便我们把GPU集成到计算机当中，因为GPU是一个计算单元。但是大家了解一点计算机的这个背景就知道，我这儿是相当于我有两个大佬？一个大脑在CPU里面算东西，一个在GPU里算东西。然后我的大脑里面还要把这些东西存下来。我的CPU用了内存，GPU用的显存，那数据怎么互通呢？
	显卡在做一些这方面的工作，比如说以他就要去做接口，PCIE第三代、第四代等等接到主板上，主板就跟大家一起坐在同一个桌子上，那才能谈判聊天。同样的它还需要去提供一些图形输出的接口。就我算完之后，我最终要显示渲染出来，让我们的显示器能够显示它。所以他还会去做一些这样的转换的一些接口，让它变成可以在一特定的显示设备上显示的这个数据形式。
	当然显卡还要解决一些问题，就是我要解决它的散热的问题。所以你看至少显卡本身就得有风扇。当然你如果你是一个一直在高频计算的场景，这风扇还不够，还得在外部有一些散热的方案，水冷、风能各种，那这个是显卡，所以显卡是一个更大的尺度，心芯片级别的东西是这个GPU，所以GPU是显卡的一个核心组件。GPU的不同决定了它的计算性能不同。那么显卡是它外围的这些东西，让它更好的能接到现有的计算设备上，计算机或者别的一些设备上。显卡需要考虑它跟别的设备去对接的时候，数据怎么交互，怎么散热，怎么跟电源一起去做这个协作。因为这个GPU运作是需要供电的，并且是一个耗电还挺大的，一个GPU几百瓦的，两块GPU就跟一个空调差不多了。
	好，我们刚刚讲到GPU，也又提到了这个显卡。我们再深度看一下，就是GPU这个玩意儿，其实最有名的两家厂家其实就是英伟达和MD了。那那GPU这个核心，它叫GPU的这个芯片也好，叫显卡的芯片也好，叫GPU的core也好。那MD是怎么样去定义，或者它内部是怎么去规划的呢？这个涉及到后面我们怎么样给英伟达的显卡选型，所以就稍微啰嗦一点，你可以简单理解成英伟达跟MD这两家公司在造显卡的时候，或者再造GPU这种通用处理单元的时候，他们的思路是略有不同的。在在英伟达这边他自己其实是造了，就在很早期的时候他造了一个概念，就是很早以前的GPU的时候，还没有扩大的时候造了一个概念。就是那会儿其实你可以理解成一个显卡里面可能就只有一个显卡的核心。
	然后那个显卡的核心，你可以想，我举个简单例子让大家理解，就是你把显卡想象成一个餐桌，然后早期的英伟达造这个餐桌就是在餐桌上给请客吃饭的时候，就一个餐桌上就一道主菜，那主菜就是那个显卡芯片，GPU call。然后也有一些配菜，这个配菜比如说配了一些凉菜，什么乱七八糟的，他们这餐桌打包成一个团购的套餐，叫显卡，这是早期的状态。但后来我们知道，大家食量越来越好了，食欲越来越好了。一个硬菜不够，我有好几个硬菜，就大概这么一个区别。
	所以早期的GPU call基本上是和这个显卡差不多一一对应的。但后来不是了，后来这个GPU变得越来越多了，甚至因为核心还是需求导向的，我要计算的类型也不一样了。我单纯去做并行计算不够了，我还要去针对特定任务去做设计。那你想象一下那个芯片设计，它就需要再做的细分一点。我的主菜，我的这个波士顿龙虾是跟麻婆豆腐一起做一个菜，还是我去清蒸呢？那那他是这样的一个配套关系，但MD不一样，MD其实它它不是说我要做模块化，就是英伟达的核心是我能模块化，我能做各种拼接。MD其实倾向于整个模型能更简化，我就给你套餐，我也没得选。
	然后我有一个比较大的计算单元叫compute unit。就这个计算单元，它是一个执行运算的一个集群，然后里面有一些小的计算单元，但是都包在这个CEO下面，然后直接用CEO来讨论这个显卡的核心数量就好了。但是因为英伟达整了很多小模块，所以你会发现英伟达去讲卡的时候，他会说各种各样的CUDA的核心数，tensor的核心数。
	我用的是什么架构？这个是因为它极度模块化了。所以你去理解英伟达的显卡的时候，你要学的这个油耗很多，你得理解这个架构跟那个架构差了多少。大家知道比如说安培架构跟它的特斯拉的这个架构，它有什么区别吗？
	你整不明白的，你得把它的架构的实现里面的芯片是怎么来的整明白，你才能看清楚它的架构的代差到底差异在哪儿。但是也可以理解，因为他做一个架构其实要留一次片，留一次片就是几千万。他可能这个流片成功了，他就用这个架构了，他不太会再换了。这都是GPU这个产业当中一些事。但MD不太一样的点就在于它没有做这么多的模块化。然后这里有一个叫英伟达的这个SM，这个其实是一个流式多流式并行处理器的一个概念。待会儿我会跟大家讲，你可以简单理解，就是MD没有做这么复杂的模块化，它的这个计算单元更像是一个专门用来做并行计算的一个单元。
	然后MD的这个A卡的这个CEO和英伟达的N卡的酷达，这两个是不能哭大的core是不能比的那什么是酷达的core？我们待会可以再讲一讲。因为我们还是在讲比货的逻辑，就MD的卡到底能不能用，MD的参数选什么？这部分我们这节课不深讲。我们给大家讲讲GPU在英伟达这边，在英伟达的显卡这边有一些什么样的，我们也不叫坑，什么样的一些特点参数你是需要去了解的。但是大家只要记住一个，就A卡的CEO和N卡的CUDA core是不能直接比的，这俩概念是不一样的。
	A卡的这个CO更像是一个并行处理单元，刚刚提到了这个词，你扩大的core就扩大core是你买GPU的时候一定会被标出来的一个重要参数。在N卡上面的这个扩大call很有意思。就是首先我们刚刚都提过了库达这个概念是十年前还是十几年前提出来的。就更早之前他这个英伟达其实在做这个显卡驱动的时候，他自己内部有一个驱动这一层的软件叫GPGPU。后来才叫改名叫哭。苦打出来之后，后面基于苦大那会儿这个咖啡的作者贾扬青，包括torch都是基于苦大去做的深度学习的框架。后来扩大越来越好用，深度学习的框架也就越来越好写。但科大本身这个概念是一个你可以认为本身酷大就是一个软件，那CUDA core其实是一个硬生生造出来的概念。
	那这个概念是什么呢？就是英伟达其实一直在想，我造了一个硬件，我总要给这个硬件找一个算力的单元也好，炼钢。那一开始他就把这个扩大core造成的作为他自己的最小最小的运算单元的一个这个相当于表示我这个卡有一个量纲了。你就跟我们在吃饭，我有一个终于有一个标准的碗了，然后他能吃一碗饭，你能吃两碗饭，这张卡能吃4碗，这个碗就是这个最小的运算单元，这个碗是标准的这是他一开始的初心。
	然后这个最小的运算单元其实那会儿叫CUDA，然后一个库大块就是一个执行基础运算的一个处理文件，在最早的时候的一个状态，如果是这样的话，其实在在刚刚提出这个概念的时候，在英伟达提出一个概念说那会儿的一个库达core是真值钱。就跟你CPU就你买了一个笔记本，你它是四核的还是八核的？那会儿的酷达跟那个是一个概念。就是我们买了一台macbook是八核的，是英特尔十六核的那那会儿酷的core就是八核十六核的。但现在大家都知道英伟达的卡都几千个库大块了，对吧？很会营销。我只能说就现在的扩大core已经不可能直接等价于CPU当中的核数级别了。因为现在他自己把core做了很多差异化的设计。
	所以简单理解成就是最早的计算单元处理原件就是扩大core，就跟CPU的核一样，CPU的核跟操作系统，计算机科学操作系统的这些基础课，大家不懂的可以有兴趣去学，就是一个基础单元也是。到后来我们都知道GPU要处理的任务越来越复杂了，他可能不是做基础运算了，就比如说甚至他为了这个矩阵运算专门做了tensor call，对吧？就另一种call了，就是重新设计。
	所以到后来之后，它的这个基础单元就不再是一个酷大块了。它要完成它的一个基础操作，你想为什么叫库达？是因为写了酷达这个驱动，酷达定义了一种操作，这个操作最终需要硬件的芯片来执行。但是库达的操作概念越来越抽象，越来越难了。之后那一个库达或者就执行不了了，有很多个酷才能凑出一个运算单元。
	我不知道这个大家说明白了吗？就是我要的，我现在在软件层，就是老板的需求越来越高了，我现在不吃米饭了，我现在不吃这个米饭，我要吃这个炒饭，对吧？我要吃蛋炒饭。那同样是这个碗，然后我同样是这个饭，但是我就要几个call一起服务了，里面就要有蛋，要有饭，然后几个裤裆core才能凑出这一碗饭。那这一碗饭的这个那就不再存在一个标准的处理原件了。所以他想要去噪的这个亮钢是造不出来的，很难造。
	所以到今天为止，这个CUDA core已经不可能再简单的去对比成一个计算单元了。但是它一定可以有最下面的计算单元。但那个计算机单元的概念意义也就不大了，因为不会再去写那么低级别的指令或者说处理原件了。所以扩大本身它的定义是有一个演化过程的那到今天为止它的定义越来越复杂。
	然后他数量只能用来只能用来做一个什么定义呢？就是说库达core就在英伟达内部体系里面来比较的话，那库达core越多肯定比库达core越少。好啊，那那这个概念我理解大家可以get到对吧？就是在它内部的话，同样的这个架构上面，这个扩大扩越多肯定是越好的。因为它的扩大扩的能力，它的设计跟它的架构是有直接关系的。那么在最新的这个线应该也不是最新的，现在还有更新的。
	我看在安培这个架构上，我们所谓的A100，就安培这个架构上面，他所说的这个扩大扩的数量其实是对应着一个浮点数，32位浮点数的一个计算单元。你可以想象最早的GPU显卡，它是不会做32位浮点数计算的，可能它可能做更小级别的那现在一个库大core就能对应一个FP32的计算单元，FP32就是我们刚刚讲蛋炒饭，那他除了蛋炒饭也可以吃米饭，对吧？这个事儿，我觉得这个例子讲的挺好的，大家去品一品。
	就是为什么量化之后，可以小的这个计算资源就能跑了？就是因为原本这个模型每一碗都是蛋炒饭，对吧？但现在我把它量化之后，我每一碗都变成米饭了，甚至是隔夜饭了。但它也能吃，对吧？它的量没变，它也能吃，就是质感变差了。
	那我们再看看NA100，这个其实就A100的1个架构图。然后内部的这个架构，我们刚刚说的UMD的那个计算单元，computer unit，这个就有点类似于英伟达内部的安培架构里面的这个streaming mut processor，流式多处理器流式多处理器在哪呢？就这儿大家能看到这有L22级缓存，然后这有一个很小的字叫SM，这个又是多处理器。
	那那这个流式多处理器内部展开我们能看到很多的细节了。就是二级缓存、一级缓存，然后指定缓存，然后里面有这个调度器。然后调度器下面我们重点看刚刚有讲到啥，讲到一个32位的浮点数对吧？这个其实是就我们现在在看GPU core这个概念就没有意义了。因为因为我们其实已经把这个，还有同学问库大阔是啥关系？
	扩大扩是一个营销概念。我知道你你get到我们刚才说了半天了，你这蛋炒饭是什么概念？半头对吧？就是CUDA call在安培这个下架构下面等于这里的FP32。我不知道这么说你能不能理解，就是你能看到这有个FP32吗？然后整个这个是一个流式多处理器，一个流式多处理器下面有很多的不用于不同计算的单元，这些就是为了计算高GPU这些芯片，它都是写死了一些芯片用来做不同的处理的。
	我们用来做32位的浮点数计算的时候，它就是会走这个FP32。但是为了加速，它还可以在软件层面上去做一些处理。就比如说我现在要算64位的浮点数运算的时候，我可以把2个MP32拼起来。那我其实可以相当于软加速。虽然我的硬件上面没有做这么多的P64，但我可以在驱动层去做软加速。这些计算机的概念我们我们不再深究，简单来说在A100这个特定的显卡下面，一个库达core等于FP32的这个处理单元，可以这么理解。那么你能看到这个FP32这个面积几乎就等于它的算量，你可以这么算，几乎等于它的算量。
	这旁边有一个巨大的tensor call，我不知道大家有没有注意，一个巨大的tensor call那是什么？我们再把这幅图打开，tensor call是什么呢？其实tensor call是应该是在在五六年前的时候，面试的时候是五六年前，就是发布出来是五六年前。但也许内部更早，就V100这个架构我不非常确定。大家如果有同学深度感兴趣，可以再去深度调研一下。我印象中在在V100这个架构，甚至T4这个芯片里面有没有特色call我不记得了。但探测扣这个概念提出来，并且做这个设计的点，就是为什么要提探测扣，还是应用驱动的。
	你可以想象成TPU，就谷歌的TPU我不知道大家了解吗？包括像所谓的NPU，寒武纪，英特尔做的这个NPU。其实这些芯片你叫它这个神经处理单元也好，叫它向量处理单元也好。我为什么要讲这个这一节课，就是让大家理解，不要被营销遮住双眼了。你如果真的是一个研发或者技术驱动的人，你能看到背后本质的东西。
	你说今天为什么还有人讲TPU呢？TPU之于GPU和GPU之于CPU是不一回事，对吧？就是因为有一堆新的海量的需求出现了AI，我需要做矩阵运算。然后矩阵运算不是一个简单的浮点数运算。矩阵运算是什么？大家回想一下线性代数，高等代数矩阵运算的那个计算跟浮点数0.3加上0.67那是不一样的，那它本身天然就得有矩阵，然后这个矩阵这个高维向量里面去做计算很方便。
	我那为什么会有单独的TPU？就是因为我就把这个硬件拿来干矩阵运算，它就能提效。它中间没有这些不必要的数据的转换，然后也不用浪费这么多芯片的位置用来做32位整数运算或者32位浮点数运算。所以大家从这个视角来理解，TPU也好，GPU也好，CPU也好，它就是一个就跟我们选大模型一样，它就是用了一些特定的需求，用来满足一些特定的需求OK。那么tensor call其实从这个视角来理解，从volta这个架构之后，就V100这1代开始探测call出现了。
	所以后来谷歌发现，我靠英伟达你你你这个卖游戏的，你去给人家卖卡挖矿的，你去给人家卖卡搞AI你现在还卖卡，你这有点过分了，你这什么生意都要抢，你明明就是一个搞通用计算的那举证的你也来来站来掺和一下。所以谷歌做了TPU，并且你看facebook现在很被动，因为它内部没有英伟达的这个卡的话，它内部资源很难云腾挪开。但谷歌牛逼就牛逼在他自己造了TPU，所以他算力没差那么多的。就现在我们搞transformer不也还是举预算，所以他内部TPU很够的，他他自己就能解决这个问题。
	所以他在几个硅谷的巨头里面相对来说压力是最小的，就算力紧缺的这个压力。所以在硬件级别上面，我们能看到这些巨头都在搞驱动的迭代。然后从这个视角大家再回想一下最近的这个新闻，什么7纳米、5纳米，这又是一个什么样的参数，它决定了这个硬件什么样的性能。我们看问题越到本质，你越能看明白谁在说一些营销册的内容，谁在说一些本质的内容，但我没有说营销策不好，因为作为一家公司，所以这是tensor call跟CUDA call不一样，对吧？CUDA call是跟着它的不同架构可以去演变的那架构的有一个变化是怎么样的一个变迁呢？我们能看到其实从最早的特斯拉到kepler到maxwell pyle，v100图灵架构、安培架构open我。这个H100、H800这个架构的发布时间，你能看到其实它基本上都是两年左右的一个迭代。这个跟硬件、芯片、显卡、GPU这些行业的生意模式、流片，整个工艺制程是有一定关系的。
	然后我们能看到什么呢？这里有一些重要的参数，就像看大模型一样，我们看看有哪些参数值得重点关注。这个SM叫流式的多处理器对吧？Streaming multiprocessor. 
	这个course其实有点像什么呢？有点像我们提到的这个CPU的几何，或者说类似于MD的这个计算单元。从这个视角来看，是不是SM的数量最重要，一个SM下面套了四个，大家看一看，这么大一幅图叫一个流式多处理器。然后我们抛开一个流失度处理器当中的4分之1，看到了这么一个内容，然后这个合数是最关键的，因为这个直接决定了你那个大盘子有多大，对吧？就相当于你团购套餐有几个，对吧？你最后那个饭是什么？那再说，然后总的SM数在这儿。
	对，然后CUDA core，包括它的几级缓存，这个CUDA core这个数量增长是非常夸张的。你要是今天看到一这个H系列的芯片18000，你认为他他有这个并行度直接干到18000，那那这个肯定是不可能的，你想象一下，这个逻辑上说不通，因为他要很多个库大core才能干一个矩阵运算的。甚至它的矩阵运算直接放在了tensor core里面，tensor core出来之后，矩阵运算的中间还会有一些细碎的一些浮点数运算，它可以放在扩大框里面，所以是这样去理解它的。
	那好，那我们最后再看看这个，对，有个同学说打的护城河挺深的。是的，苦大护城河相当深，它软硬一体的解决了很多问题。所以你看他这个就是你今天这两家公司都很有特点的。Hanging face和invidia是值得大家去学习的。他们的技术很很好，然后取名字也很好，然后客户心智的植入也很强。
	GPU的显卡的这个性能天梯榜，有这么一个网站，也是我搜刮到的，我觉得蛮有意思。大家可以在这儿看到一些对比和信息，我们可以直接跳到这个网站上去，叫GPU rank，它里面有什么呢？有一些份额。首先它不是一个单纯的去比较性能的，他有各种各样的信息。比如说两家一个对比，然后直接放在这儿了。然后你也可以只看英维迪亚他们家的。你选到不同的卡，它会有很多关键参数。现在再看这个参数是不是有一些不同的体会了，我们应该能看到一些不一样的这个值了，不一样的数据和观点了。
	比如说咱们在这儿也能看到显卡性能和这个发布时间的一个对比。它这个对比在同系列里面是比较能说明问题的那当然你也可以在这儿具体去选，然后有这么一个网站值得大家去可以去关注。我这儿最后再花一点点时间去跟大家分享，就是说专业级的显卡和消费级显卡的区别。我们能看到英伟达有刚刚说的这个安培架构，各种架构。同时还有一个消费级的显卡叫g force，这我们能看到的g force。
	那消费级的显卡和企业级的显卡最大的差异在哪呢？首先我们看性能都会知道消费级显卡香便宜，然后同样的配置便宜好多。为什么这里都写了价格的，但这个价格作为一个参考，其实最大的原因还是因为服务不一样。就是如果你去买一个企业级显卡的时候，英伟达这家公司是要承诺很多售后服务的。这些售后服务在在消费级显卡里几乎是没有的。就比如说如果你是正常使用的情况下，它的GPU显卡出现了故障，那么他是会在一定时间内给你换货的。简单来说就跟你在买大电器的时候，他有保修保换之类的这样的一些服务。这个是企业级显卡一定会有的。
	而且企业级显卡的服务通常是比如说几十万上百万的采购，甚至上千万。我记得我最早116年在华为的时候，那会儿我们华为云刚刚成立，华为云成立之前，2012实验室都还会有很多的CPU的卡，那会儿我们都用的这个企业级的卡，甚至还要我们自己去这个服务器上面去去看去操作。企业级的卡稳定性、质量、售后都会很高。甚至他的销售团队或者说他的这个渠道商还会去给你出一些方案。但消费级的卡就跟你点了外卖一样。你想你点外卖，要是这个外卖不好吃，或者外卖坏了，你的维权成本和你在饭店里面，在这个大的餐饮店里面吃出来的问题，是不一样的。那那他这个体感我觉得大家应该能能体会到。
	然后还有一个不同在于，其实整个g force这个系列，其实它是一个挺快消品的。就是为什么说大家说矿卡不好用？就是因为它的当然这个也跟稳定性直接相关，它没法那么高频，24小时一直高速的运转很长时间。因为g force它是消费者游戏级的卡，就是你想象它的使用场景，就是我今天打游戏，我打我哪怕打了一天一夜，打了一个通宵，我也就打24小时。你不会见到一个人他不睡觉，他就打游戏，他他打了一个月不太现实对吧？所以它by design就是这个g force这个系列，它by design就是设计它的设计上，它就没有考虑说你要一直24小7乘24小时不断的运转非常长的时间。
	这个是从使用场景上，如果咱们是自己做开发，你不是说我要去搞一个线上的服务，服务于全全世界的人或者你公司的人。并且他是一个QPS还挺高的一个服务。大家一直在在用这个服务的话，那你就用g POS的卡是OK的。我不知道这个区别有没有描述清楚。
	好，其实显卡也就这么多东西，关键是大家整明白我刚刚讲的那些概念，别被纯忽悠了。对，那我们看看大家还有什么问题，关于这个显卡。谷歌TPU是可以用的。像像我们作为谷歌的这个developer expert，每年还是有一两千美金的这个额度可以用它TPU。然后collab我记得可以免费用一个TPU。
	他已经开始交流自己买的价格了。
	小公司想要私有化部署一个商用大模型，有什么高性价比的方案吗？自己买企业级的显卡，还是云服务商hacking face托管同学就是你的需求的重点不在小公司，也不在商用，关键在于你的高性价比。那既然是性价比，那你得有预算，你得有你这个预算花出去想要挣回多少钱，得聊这个东西。对，而且什么叫小公司呢？老师不懂华为手机啊，也不懂5G芯片，这个实在打不了。
	有的显卡支持32位浮点数，却不支持16位浮点数，为什么不支持呢？为什么不支持16位浮点数呢？一定是需求侧的原因。同学就是我一直在这个课上，希望让大家从很多可能之前不一定关注到的维度去理解问题，一定是需求侧的原因。
	你想一下你会你现在写代码会用fat 16位的flow，大家都是用的double，就是java就double，或者你大家都是用的双精度。因为单精度有很多问题，这个是第一点，就是写代码的人大部分现在都用双精度了。第二，我能做双精度，对吧？那么单精度我自然能够通过去弄成模拟出来，只是多一步转化，慢一点，但这个慢一点无所谓，因为16位单精度用的太少了。我把那个就是一个时间换空间的逻辑，就我把那芯片照在那儿不用，那还不如换成双精度的那他偶尔用一次单精度，我就消耗一次转换的时间而已。但是我大部分时候我双精度就可以把原来用来做单精度的时间空间给换回来。
	今天这几个参数很干货的，大家消化好了，你可以成为半个硬件专家的。尤其是这些图里面的这些核心要素，就不要被被这些概念吓住了，没有那么难，就有一些底层逻辑。对怎么看达芬奇芯片，这是华为内部的这个芯片，这个不发表评价，对，这个不是我的擅长。
	还有同学问怎么变成谷歌的experts？你可以把你你要觉得你有这个你有兴趣，并且你也觉得你差不多是这个水也比较差不多是这个水平。对你有兴趣，你可以把你的简历信息发给我，我可以给谷歌推荐。但是我可能自己也会稍微评估一下，因为我会是第一轮的面试官。
	对，就是有个同学说CPUGPU，GPU是不是可以这么理解？CPU是通用的GPT，不GPU不是GPTGPU处理特殊数据，TPU处理矩阵数据。是的，还是从需求出发，为什么要造这个芯片？一定是有需求。那这个需求是什么？对吧？然后这个需求如果足够大我就可以把它拎出来单独做一个事情了。就像大家觉得我的要打游戏，我要做这个CG动画，所以才出现了GPU。
	但CPU的地位是无法被取代的，因为它解决了最通用的那一部分的事情，就是我各种调度。你想一下CPU在干什么活？我们现在看的这个直播，我们用的各种APP都在CPU里跑。对他还是很重要的，他他是我们冯诺一般架构的最最重要的一环。
	TPU挺香的对TPU对TPU挺好的，TPU的第二代更好用。我们再回答，我们到10点05分结束，看大家还有什么问题。
	简单复盘一下那个同学好算力怎么理解？怎么评价特斯拉的GPU？特斯拉在造GPU吗？我不知道，对不起，我真不知道中文的L1M有什么推荐的lead board吗？
	问了好多问题，我捋一下。Hugin face上的数据和模型符合暂行条例里面的数据和基础模型吗？同学自己去看啊，每一个都不一样，对。酷达去做模型的应用不是巧合。同学你看我留的这一页，不是巧合，同学们不是巧合。在那我简单复盘一下，我看大家还有一些不是很清楚的概念，就是今天我们讲的这两个开源的生态和硬件，本质上的逻辑都是需求被挖掘出来之后，然后做了一些很好的产品，然后使得这两家公司都在这个时代拿到了很好的结果。
	不管是软的这种大模型的服务平台，还是我们这个硬的为大模型提供算力的硬件，那么它的他的对需求的挖掘好在哪呢？第一就GPU来说，就英伟达的这个N卡来说，他很好的去捕捉到了我们从这个CPU时代，这种通用计算的时代，越来越显示器进化了。然后我们的芯片工艺进化了，那我们就可以包括软件层面上也进化了化了。那么我们可以做出更好玩的游戏。更好玩的游戏当然要越来越像人真实的体验，我想要我不想my craft的这个方块，我想要真实这个CG。这些算力需求被创造出来之后，他就单独做了一个产品叫GPU。
	然后他又捕捉到了AI这一波的热潮，然后有矩阵运算的需求，单独在GPU的这这个内部造了tensor call去服务于矩阵运算，然后使得它的GPU的计算效率非常高。然后每一代的架构都有在为不同的AI的，你可以认为是算法或者模型，或者主流技术的迭代做适配。所以它紧跟着最前沿AI圈子的需求在走，它非常好用。我就举个最简单的例子，现在我们真正在跑这些计算视觉，比如说人脸检测、分类、识别、分割之类的应用。用的还是什么T4或者V100的卡，就因为那些卡其实就够用了。但是你要搞大模型，就发现必须要搞A100的卡。第一是因为显存的原因，就我们没聊的一个维度就写存那显存它有这个显存的需求，那这幅图你没有放这个显存，那显存很大才能把这个模型存下来。这个天然它就把后面的架构做大了，那也是一个它捕捉到了需求，所以去做的一个产品设计上的一个变化。
	同样的行业face也是一样的。他从做一个聊天机器人的应用的公司，到发现大语言模型的这个可以去all in可以去赌这个大语言模型说不定突破可以带来一波新的浪潮，他都赌对了。他从去实现这个bert的python版本，到实现GPT2，到后来做这个transformers的这个库。服务于各种各样机器学习的用户。一直到做了一个平台，把这帮用户圈到一个平台上，大家一起玩，持续的去积累这些模型数据集。包括提供了托管的服务，这个space自己去租算力，给大家提供算力服务，包括提供这个auto train这样方便做训练的服务，把机器学习的门槛降低，把各种各样的人圈进来。这个是hugin face做的非常成功的一点。
	好，那就这个是一个简单的复盘，看看大家应该没有什么。好，我们要不今天就到这儿，然后看看大家关于我国内的这些大模型评价谁好谁坏。这种很tRicky很难回答的问题，确实不太好在直播里回答。大家如果有机会可以群里或者线下交流。
	明对，明天最后一节课我们会讲亲爱的GLM的这个私有化的部署，然后包括它的部署，然后基于一个样例数据集，我们会去做训练，做微调。但那个微调不是在课上去做微调，不然大家就我们直播就等着那个GPU这微调也不太现实。我会提前去做好微调，对，然后会跟大家再讲一讲，最后会花1个小时的时间整体去做一个这门课程的一个盘，帮大家去梳理。大家有什么问题，我们留下来半个小时。