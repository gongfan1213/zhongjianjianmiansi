	就正式开始最后一节课，刚好确实像同学们说的，在9月10号这个教师节，正好给咱们这个大模型应用开发训练营画上一个正式授课的结尾。但是课程上课的内容结束了，但是咱们整个学习大模型这个事儿其实也就刚刚开始。整个这个课程一共八周的时间。因为中间我身体这个复阳之后，其实我们整个花了九周的时间跟大家在讲AI的大语言模型最新的一些进展。从理论到我们的OpenAI的API的应用，以及使用open ADAPI开发的OpenAI translator这个项目到我们学习能欠能嵌花了三四周的时间。基础的模块三个实战项目，相信大家应该是把这个demo都跑起来了。然后一些动动手能力强的同学也有一些优秀的作业产出，把我们的实战作业的一些要求，包括自己还做了一些扩展都非常好。
	我们最后的生态片其实就只有这一周的两节课，这两节课其实定位成生态片也是希望给大家在学习这些基础知识、理论技巧以及工具框架之外，包括实战作业以外，有一些额外的或者说能开拓视野的一些篇章，所以这一周我们主要是在干这个事儿。像星期三，其实我们讲了两个部分。一个是开源生态的平台，我们介绍了hugin face。第二个就是关于硬件。其实很多同学对于硬件这块比较陌生，然后也一直找不到一个抓手。我们通过给大家分析，其实英伟达的这个芯片，包括它的显卡以及它的扩大，如何一步一步拆解去理解。其实这些显卡也没有这么复杂，它的核心的技术指标也就只有那么几个。比如说CUDA的这个核心数，我们的tensor的call，以及我们的流式并行处理器的这个规模大小。
	最后一节课，其实我们也是希望回应很多同学的这个需求，有很多同学都在问GLM的这个模型然后怎么用？然后GLM哪个好用？GLM巴拉巴拉有很多的问题。那今天我会先花一部分的时间，可能一个多小时，尽可能的跟大家展现一下我了解的这个GRM这个大模型家族和他们提供的开源的6B这个级别的模型相关的一些技术和文档。然后同时也是最后这节课也留了1个小时的时间，可以充分的再跟大家交流。有什么问题关于整个这门课程的问题都可以大家再来提问。
	好，那我们就开始今天的内容。GOM这个大模型其实也是一个家族系列，我们能看得到其实在get up上面，THU就是清华大学有一个DMTHUDM这么一个github上面的组织。然后它的详细介绍其实是knowledge engineer，就是你可以理解成这个知识工程的小组。就KEG和我们的这个数据挖掘，这两个小组合在一起叫清华的DM这个组，他们的这个组里面有不少的开源项目，大家如果没有关注他的话，还可以关注一下。也有4600人的follower，我们今天要介绍的这些大模型家族，其实都在这个里面。包括待会儿我们也可以看一眼这个关于AI agent，就我们一直在讲年券，讲各种各样的这种未来的基于大语言模型的应用应该是什么样的。Agent是我们认为一个非常好的模式，那agent有没有benchmark？就关于agent本身有没有benchmark，其实THODM他们也做了这么一个开源项目，之前在国内的媒体里面也比较火，然后像我们待会儿会介绍的GLM的130B，就是1300亿参数的这个大语言模型，以及我们每天都会在各种各样的地方见到的这个ChatGLM6B，我们待会儿可以一起来看看。
	首先我们讲讲G2M130BG2M130B其实应该是在去年的十月份左右，就在TSBT发布前一个月，在r cap上面最后更新了他的文章。然后整个GLM130B这篇文章其实也是2023年SLR这个顶会的一个接收的文章。SLR这个会议比较有名。这个LR其实在我们讲embedding这节课的时候，就应该有不少同学有印象的话。我们提过有一个很重要的概念叫表示学习，表示学习LR其实就是学表示学习的意思，就learning repenting，就怎么样去这这所以这个会叫到国际的学习表示的这么一个会议。IC整个表示学习是一个非常重要的方向。我们在embedding的课程里面也讲那一节课程里面也讲过，表示学习的核心就是能够自动的从一大堆的数据里面学到一些特征，这个特征能为我接下来的下游任务去服务。GLM130并能够发在这样的顶级会议上，也说明了其实它非常符合表示学习这个范畴。
	我们具体来看，首先GM130B是一个双语模型，我们现在都用了GPT，用了现在GPT会感觉全世界的技术都已经到达ChatGPT这个状态了。但其实不是我们一直在说非常先进的大语言模型，其实离GPT4都还有很远的距离。就跟我们看到这里，其实GLM130B也是在跟GPT3，包括palm和OPT在做比较。当然今年发布的这些新模型，那会论文肯定还没法去作为这个参考。但无论如何，其实GPT3.5是一个目前很多大元模型都无法达到的一个高度，这个是毋庸置疑的。包括在我们待会儿会看到清华的这个DM这个group自己发布的agent benchmark里面也能看得到，GPT3.5现在仍然是天花板的一个存在。那么说回来G2M130B首先这是一个开源的双语的大语言模型，所以最重要的点是它是开源的。第二它是支持英文和中文的，因为它的训练集里面有大量的中英文，但是其他的语言它就不太能支持了，所以相比于我们看到在这个理论课部分，包括这个基础篇的时候跟大家讲GPT3.5和GPT4在多语言上面有优势。
	大家可能没有直观感受。但现在你们来想一想，其实像GM130B也只是支持了双语，然后包括像前两天这个反抗发布出来，大家很多人都在炒作，它在各种语言上都很强。但实际体感我们还是得看观众或者说这个用户用脚来投票，看他他的实际这个留存率用户到底多不多。所以其实要把多语言做好本身是很难的。因为我们再回到理论篇的这个第一节课，注意力机制的时候，其实有题，不同语言之间要统一对齐，统一来做表示。学习本身就是一个比较困难的一个训练的技巧和工程方面的工作，我们看到这儿在GM130B里面，其实它的性能分成了两种不同的阐述。英文和中文。在英文方面其实它比GPT3150175B。
	我们现在看这个模型参数应该大家就很熟了。GPT3175B其实就类似于我们用到的这个达芬奇003 text。达芬奇003这样的模型当然是一个模型系列，包括这个Linda和OPT175B。
	OPT175B是一个对标GPT系列的开源模型，由facebook meta这家公司在主导，包括这个bloom 176B这些模型其实在G1M130B这篇论文里面都有明显的一个对比。在标准的benchmark上面，英文GM130B是表现的更好的。但是我们知道GPT3我们这个课程用下来感觉很傻，对吧？就感觉没有那么智能。尤其是官方现在都在把这个GPT3系列的模型，在明年1月份就要下架了。所以整个大语言模型的进展是非常猛烈的一个状态。
	大家想象一下，这篇是2023年SAR的顶会论文，去年底发布出来的。然后他对标的GPT3在明年1月份，也就是一个季度后就下架了。所以这个迭代速度是非常夸张的。
	然后我们看到它在中文上面，因为中文其实是国内现在有一至少有100家公司都在宣称自己做大语言模型，然后有各种各样的中文的benchmark数据集，像GM130B又要在国际会议上发表这些文章，选取的还算相对比较客观的这个CLUE这样的一个数据集上面去做零样本。然后这个ERINIE应该是百度的，这个我没记错的话，应该是百度的这个大约模型。然后分别在zero shot和这个few shot的数据集上面去跟百度的做了比较。因为本身在国内做中文大语言模型的公司在去年底并不多，我们看到其实当时清华的这个质朴，这两家一起做的这个GM130B还是有非常大的技术优势的。在去年年底的时候，同时它还有一些除了单纯讲这个性能以外，它的性价比也好和它的推理成本也好，也有一些优势。
	他当时提出了这个快速推理的概念，就是使用单个A100的服务器，单个A100这里跟我们上节课有一些关联。我们上节课其实有讲一个显卡是什么，但我们没有讲服务器，一个A100的服务器里面其实放了八张显卡，应该是8乘40GB的这么一个显存。并且他自己在这个推理的部分也提了一些新的概念。比如说这个faster transformer，能快速更快速的做推理，我们我们的理论时间有限，这里简单讲一下为什么大家在做这个推理这一侧的时候觉得特别慢要等这个API等很长的时间，核心还是因为transformer这个技术架构。大家再回想一下我们理论课的时候讲transformer的技术架构，它是一个encode decode的一个架构。Encode部分是没有办法去做并行的，但是decode部分可以去做并行。但它就算要并行，它的这个序列就我们输进去，我们再算token，再算这个上下文的这个序列长度跟它的计算量是一个平方的关系，是一个ON方的关系。
	假设我的序列的长度是N那么我的计算量是ON方，这个是非常夸张的。尤其是我们考虑到这个序列的长度，其实未来是想往上万甚至10万这个级别去做扩展的话，这是一个非常海量的计算量。所以现在有很多的人都在研究怎么样去把transformer的计算复杂度再降低一些，能让我们算的更快一些，这也是一个很重要的研究方向。所以快速推理也是目前很多大语言模型在讲的一个重点。
	GM130B自己有提到可以使用单个100服务器来进行推理。同时它也非常好复现，就有很多大语言模型和论文成果是很难复现的。那么GM130B这项任务首先它开源了代码和模型，这个模型检查点其实就相当于模型文件，check point文件，很方便能够去复现它在各种各样的任务上面的一个结果。然后除了因为本身是中国的公司和高校做的模型，所以我们除了支持英伟达以外，整个GM130B对于中国的的硬件生态也在做支持。比如说海光，比如说华为的这个升腾910之类的硬件，都在进行对应的跨平台的支持。
	这个是G1M130B但通常其实大家不太会用这个模型，因为它的推理成本比较高。对于个人开发者来说，可能更多的还是去使用更小规模的模型。这种千亿规模的大模型，它的部署成本通常是在几百万这个量级。我们能看得到，我们在刚刚的这个对比里面，包括上一节课我们有讲大模型跑个分，大模型的横向对比应该比什么？
	这里我们再重点提一下GGLM的一些不一样的点就首先我们都知道我们在这节课里面学到这这个课程里面学到的OpenAI的一系列模型。其实这个backbone这这个backbone的概念你可以理解成就是我的这个核心的网络结构，都是用的GPT为主。不管是我们讲的open I的GPT，还是这个OPT还是pm，包括bloom他们都还是用的这个GPT，这个GPT是什么概念？大家现在应该都记得，我们没有这个插班生对吧？就是通用的预训练的transformer，还是使用这样的一个结构。但是质朴和清华的这个团队一直在高度的高调的去去讲。就是他们自己的这个网络结构其实没有用GP，而是用的这个GLM他们自己定义的一个回归的这么一个结构。
	然后这个是一个很重要的重点。大家如果要再深入去看GLM系列的这个延伸的话，我们可能得看一看GM这个模型本身是什么样的。然后除了这个以外，它的这个训练目标也是一个多目标的一个回归任务，不只是一个跟GPT1样的单目标的任务。然后他但他有一些弱点是什么呢？比如说这个bloom可能是支持了这个多语言对的，那么GLM只支持了这个单语言对的。然后在去年年底的时候，GPT3可能只支持这个主要语言是英语。
	但随着现在GPT3.5GPT4，也就是说在GPT的上线之后，open I的GPT4其实应该可以严格意义上来说是一个多语言对的这么一个很强的模型了。但它主力语言肯定还是英语，这跟训练语料有很大的关系。我们中国的大模型公司为什么有机会？就在于是中国人产生了大这样的中文的语料，而这些中文语料其实并没有被海外的公司充分用起来。所以这里确实是有一个差异化的可以切入的点。
	好，这个是一个咱们可以去关注的。一个是它的网络结构没有用GPT。第二个它的训练目标是动目标的。第三它有中文的差异化的优势，然后在训练的这个效果上面，其实GM还为了提升速度做了一个int 4的1个量化。Int 4的量化是什么概念？
	我们再回想一下上节课讲GPU的时候，给大家埋了一些伏笔。我们讲GPU的时候，内部结构里面有一个流式多处理器。多处理器这个展开之后，里面有各种各样实际用来做计算的，在硬件层面上去做支持的一些芯片。包括16位的浮点数，32位的浮点数和64位的浮点数，还有一个大大的一个tensor call。这些浮点数其实就是可以在硬件层面上原生支持这样的浮点运算。但是我们为了去节约显存，因为我们知道一个GOM的130B是一个1300亿的大模型。
	这个1300亿的模型光要加载到这个显存里面，就需要消耗非常多的显存，几百GB的显存。那这几百GB的显存普通人家肯定没有，那怎么办呢？比较简单的一个方式就是这1300亿其实都是那些模型权重。
	我们这个1300亿的这个量纲单位是指的这个模型的参数。这个模型参数可以用32位的浮点数，也可以用64位的浮点数，甚至可以用int四这样的一些量化的值。简单来说就是我把精度给降低了。本来我可以小数点后这个十几位小数，但现在我就把它变成int 4这样的整形变量，可以可以带来的直观好处就是我不需要那么多的不需要那么多的显存，还存在一个的模型参数了。那是不是直接就把整个我需要的显存总量降低了一个数量级甚至更多。
	那么带来的缺点是什么呢？就是它可能会变得有一些精度上的损失，因为那个小数点没了。但它实际上做模型量化是一个很重要的学术研究方向，他没有那么傻，他不是说我就直接把小数点给砍了，而是说它会有一个量化的过程，大家可以简单这么理解这个背后的逻辑。所以他可以省多少呢？让他自己的论文里面的这个说法还可以省75%的显存。那他使用int 4的时候去做这个推理，只需要我印象当中只需要4。
	当时有看他的这个报道，8个V100还是八个，他这有写八个。2080钛就能够去做推理，或者四张3090就能去做推理，这个是一个很重要的点。然后同时除了量化以外，就我把这个要存在显存里的这个东西弄小以外。我同时在推理的时候，我们刚刚有讲这个faster transformer是他自己改造的用来做加速的一个结构。这个结构对于他来说也是有非常大的优势的。当然在论文里面的结果很好啊，在论文里面有写到有7到8倍的这么一个提速，相比于直接用拍套起的实现，这个是综合来看，包括我们刚刚提到的硬件上面的一些兼容性，是G1M130B和在去年年底的时候，这些主流的千亿规模的大模型的一个对比。
	那我们相信其实随着今年的这个大元模型的快速发展，比如说谷歌的palm也出了第二代了。然后他自己也对palm的这个第二代模型上面搭建了bar的这么一个类似于ChatGPT的应用，GPT本身当然也迭代了非常多，GPT4都已经发布了有半年了，GPT5的商标也注册了。并且你看这个sam奥特曼就是这个open I的CEO自己也有提到GPT5很有可能。首先GPT4的翻译二就我们用GPT4的模型微调是年内会发布出来给到大家的那GPT5也很有大概率可能就是在年底或者明年年初的时候会发布出来。面试。所以整个单元模型确实是一个快速迭代的过程。
	我们课程也是希望很多的时候不是讲一个死的知识，而是给大家提供一些能去开拓视野的这么一个窗口。这个是具体的GM130B的一个硬件资源需求。量化之后，比如说int 4的量化，这里只需要八张这个2080钛，就可以部署起来。具体的这个值大家有兴趣可以看一看，我们这儿就不再赘述了。刚才在上面也有去讲到，如果我们标准的这个部署不做量化的话，是使用320GB的一个显存。
	好，刚刚讲的是GOM130B这个1300亿的参数。1300亿的参数我们普通人用不起，这个66 10亿的参数好像我们普通人用得起，就这个chat GM杠6B但其实一直有一个ChatGLM夹在中间，被大家傻傻分不清楚。然后刚好我们在上上节课讲合规的时候，我们看到最近第二批人工智能的这个备案通过了。其中就有质朴公司的这个chat GM的应用。所以他们最近在很大力的在推这个质谱清言这么一个生成式人工智能的应用程序。待会儿我们也可以打开这个界面让大家感受一下。是一个类似于ChatGPT的界面，但是预制了很多prompt，就叫这个chat GM logo是一个大象，它其实核心就是我们刚刚讲的GLM130B作为它的基座模型。然后做了这个代码的预训练，做了有监督的微调，然后实现了人类意图的对齐，整个这个思路其实就是沿着咱们理论课讲，从GPT3到ChatGPT这么一条思路在走，在代码上训练指令微调，基于人类的这个反馈的强化学习，实现这个更是人的表达。
	这个是它的一个产品界面，我们看到左上角，就是像ChatGPT1样，会有各种各样的聊天对话记录，你可以创建新对话。然后中间是你的当前的激活的聊天对话框，右边其实是做了各种各样基于不同场景角色的prompt的一些参考示例，甚至有人点赞。对这个界面其实我们可以直接访问的。普通大家待会儿也可以自己去试一试，就质谱的这个质朴AI这个稍微放大一点，智谱AI的这个网站，大家如果没有访问用过的话，可以来稍微看一看。
	这个应该算是中国目前国内做，或者说对标OpenAI做的比较靠前的公司了，或者说最靠前的公司。然后他们其实一直在做这个投入质朴，也是这三年来四年来一直在长线的去投入这个大语言模型。所以他们自己有自己的这个backbone g rm，我们能看到整个GTM的定位现在还是一个阿尔法的版本，就是一个预发布的版本，还不是这个贝塔或者说GA的这种正式版本。那么在GM如果我们点进去点这个体验，其实是直接就跳转到了这个质朴青年。因为我已经登录了，所以就直接加载进来了。然后在GM现在也是提供手机版本，就是APP的版本，这是它备案的好处。
	如果我们普通的人想要去做这样的应用，我们在这个之前的课程里有讲过需要去做备案。但一旦备案通过，其实我们就可以去做这样的应用开发了。整个政府目前还是非常鼓励我们大家去做生成式人工智能的。只要你按照规则来去做备案，所以你能看到下面有两个备案号，一个是所有的网站都需要去做的原来的这个网站备案，还有一个就是网信办给的这个算法备案号，这个是非常有必要的两条，包括对应的用户协议和隐私政策。所以如果你正在用一个生成式人工智能的应用，然后你发现他没有去做网信这个网信办的算法备案，那你就需要小心了。因为有可能你的数据并不会得到保护。
	好，这个质谱清言的整个页面，其实我们创建一个新对话，这个是一个新对话，这个是我们之前提的是测试了一下，那么创建一个新对话，右边其实是各种各样的prompt，包括我们看到下拉框，有各种各样的这个包括像什么编程之类的。你可以试一下circle建表正则表达式，假设我们要做一个生成代码，它其实是跟HIGPT最大的一个差异化优势。就是使用中文来对我们的大语言模型进行交流，这个是它目前最大的一个优势，应该有很多的用户，尤其是不是编程的用户，可能对中文是比较友好的。
	这个是一个我们的致富青年chat GM的这么一个界面，这儿我就不再过多的展示了，大家可以自己来体验。这个是九月份刚刚开始正式对外的，我估计还会有一些比较多的扩大，或者说能让大家免费去试用的这么一个期限。大家早早去体验，会有更多的一些新的看法和观点出来，我们也可以在群里去积极的讨论，这就不再赘述这一部分了。它的域名也很简单，就是chat GM点CN这个域名。好。接着我们看一下，我们能够部署的chat GM版本，就6B这个版本。
	就在应该是在今年的一季度和二季度的时候，主流的初代的这个开源模型还是像GM6B，它其实就是一个简单来说大家可以这么去类比，这个大语言模型其实最近太新了，之后就有很多人发各种各样的东西出来，让大家很迷茫。但其实你掌握了这个底层逻辑，我们一直在讲meta learning这个概念，就有很多烟雾弹也好，这个没必要的核心概念也好，就能绕过他们看到本质。其实整个GM这个架构就跟我们最开始去学这个GPT3的时候一样。GPT3其实不是一个模型。大家还有印象的话，我们讲OpenAI的models这一部分的时候，GPT3其实是一个模型的集合。他使用了GPT3的训练集，它的in bedding模型以及它的训练方法。但最终可以用不同的尺寸，比如说有这个ADA，有ABCD。大家有印象的话，从ADA到达芬奇。然后ADA其实是一个非常小的模型，也甚至没有60亿的规模。我印象当中也就是一个数十亿规模的一个模型。
	那GLM的类似，就我们同样可以用同样的GLM这个结构，然后用更小的模型参数的量，然后去做一个小尺寸的模型，就是我们的叉GM6B，或者说首先有一个GLM6B但他提的不多，因为直接用一个GM的6B用处不大。但是在这个GM6B上面去做一个结合了各种就是我们为了做一个聊天模型需要用的这些常规套路。现在是常规套路，因为open a已经把这个路趟过去了。去做这样的一些常规套路，是能做出一个M6B这样的模型的。当然说起来很简单，里面还有很多AI工程方面的一些细节。这个也是现在所有的大模型公司最核心的技术和竞争力。
	我们能看得到，其实整个逻辑过来就很顺了，TIGM6B在今年上半年应该是最主流的国内的的聊天模型，可以私有化部署的开源的。然后他遵从的协议，我们在讲数据隐私，那就可以讲过了。只要你不去做这个商用，不去做这个违反国家的公序良俗和这个法律的都没有问题。你只要自己去学习，都可以去用的。并且它也因为它是用的底座，是用的GR模型，所以我们刚刚讲到的GLM模型的那些优化技术也都可以使用。
	比如说模型的量化，最低就只需要6个GB的显存，就能把chat GM6B这个模型加载到GPU里了。但是我们都知道加载到GPU里面只是一个静态的模型。真正他要去做文本生成，或者说跟你聊天的时候，你跟他聊天的这些token，就我们把这个自然语言变成了token。那这一个一个的token最终也是要丢到显存里面去做运算的。所以如果你给的是一个非常长的文本，那这个长的文本本身它也会占用一定的显存。
	所以实际上大家在使用CIGM6B的时候，假设你这个是一个16GB的GPU，然后你把整个模型加载进去，你没有用这个量化，它可能占了这个12 13GB。然后你又丢了一个非常长的文本之后，有可能就会触发他的这个OOM，就我们的显存的溢出，这个是有可能的，但这个没有那么危险。大家可以理解，就算他这一次请求生成的时候超过了这个显存的上限，但也不会影响什么，也就只是这一次的生成失败了。你可以把它切短一点，再给它丢进去重新去做生成，没有超过它的显存上限，救人还是可以使用的。
	好，那么亲爱的先我们刚刚讲他他现在也出了第二代的版本了。他官方的这个说法上，其实对这个chat GM2，或者说第二代的开源聊天模型，最大的几个新的特性，其实就是在下面我们列的这里面他自己提到的第一个就是性能更强，这还是跑分，就跑分的逻辑在原有的这些基准测试上，MMLU，然后中文的这个模型评估JSM8K这个大家应该很熟了。我们在讲理论篇的时候提过这个小学数学应用题，对吧？然后BBH上面都有一些大幅度的提升，都是20个点往上走的一个提升。然后GSM8K大家看这个提升了571%。
	其实主要还是因为第一代的时候这个差距太大了，因为它本身是一个小模型，之前我们看到GSM8K是在什么时候讲的呢？是在我们讲思维链的时候讲的。思维链是要在足够大的模型规模上面，才能够体现出逐步推理的这个深度逻辑。那么GSN8K刚好就比较符合思维链的这样的一个效果测试，但是60亿参数它很难有思维链的效果。所以早期第一代的TIGM在这个基准测试上的性能很差。
	第二代模型参数的规模没有变，我消耗的资源没有变的情况下，提升了五倍将近六倍，这个确实是一个非常大的提升。说明他的推理能力变强了。然后第二个就是他的上下文变强了。我们知道大家现在都调过API了也用过这个能券了。这个to token的这个长度，我们普遍还是在2K到4K这个长度是比较常用的。那么CGM26B也有推出了一个长版本的一个上下文的模型，所以当然把它放到最后一节课。
	我们前面的这些课程内容大家都学过之后会发现，其实质谱的这个路子就非常聪明，就是OKI你做什么我就做什么，反正我就对标着做。然后大家也都被市场教育过了，因为做的最好的那个闭源的商业版本大家都用过。那现在杠32K对吧？我们用过GPT4-3 12K的话就会理解了，其实就是一个长上下文的版本。甚至现在有人把这个上下文扩展到10万甚至更多。
	但是无线的扩展并不是一个好的事情，就因为扩展本身虽然很难，但也是有一些一些技巧是可以把它弄得很长的。就比如说我们看到微软的那个non net，就是把我们的上下文长度弄到了10万。但是弄得越来越长之后，其实注意力也好，或者说我们的算力的消耗也好，都有比较明显的一个增大。这个增大带来的收益到底高不高，其实是一个值得大家去，或者说值得整个学术圈都在深入去探讨的一个问题。
	32K目前看来还算是一个比较比较居中的，比较综合的trade off的这么一个值。然后更高效的推理是因为用了一个meti Carry attention的技术，使得他在做这个推理的过程当中的速度更快。并且他一直在强调他的inter 4的这个量化。然后在协议上面，其实我们上次有读过，对学术研究是完全开放的，就是它的model license。如果你要去进行商业化，需要向质朴去提交这个申请。就跟我们看到这个number two一样，你要对他去提交申请。并且他还不是说要777亿的这个月活，而是只要你要商业化，就必须要提交申请。而如果你的这个商业化特征比较明显的话，应该是需要付费使用的据我了解，但是咱们自己做研究无所谓。
	那我们直接对比来看一下这两台模型有什么样的差异。一个是在数理逻辑上，但是我这首先要直接说明一点是这个chat GM26B是跟GPT3还有差距的，跟GPT3.5也有差距的。所以咱们理论课的时候讲过很多比较难的这对于大模型来说比较难的考题，大家都可以去问问chat GM26B试试它能不能搞定。包括这个咖啡，这个黏腻等等的问题，都是一些很对于大语言模型来说其实是很难的一些问题。
	那么在这儿其实我们看到这里问了一个简单的一个数学题，其实就类似于GSM8K里面的基准测试里面的一些题。就有一堆煤，原来每次运八吨需要14次才能运完，现在每次运七吨需要多少次能够用完？这个正常，应该小学三年级左右就能回答出来的一道题，就是算一个煤的总量，然后再除一下这个更新的单次的量，就能算完8乘14除以7。我们看到第二代的这个chat GM two是把整个计算过程类似于一个思维链的展示，并且是中文的版本的方式去输出出来了。所以这里达到了一个正确的结果，就8乘上14 112112除以76，所以答案是16次。
	那么之前的这个版本TIGM我们我们看到他他应该是在做这个to n就是做指令微调的时候，转了一些奇奇怪怪的东西进去，所以他居然把它做成了。他应该也是学了很多应用题，然后他还做了我们以前做这个小学数学应用题的时候，都会做去设对吧？设各种SD然后做了一大堆，然后把自己绕进去了。需要72次才能运完煤堆。
	其实这个很说明问题，就我们现在最先进的公司，这个质朴也花了北京市政府好多个亿，做出来模型。这就是说这种自回归的，不管你是单目标的、多目标的，还是你去累加这个transformer，去理解语义，然后再来做处理，就是一个非常难的一个坑。是非常能够去区分度有区分度的去去考察这个单元模型的数据，推数理逻辑或者说推理能力的强弱的。大家如果要去选择大语言模型的时候，自己也可以去攒一些类似的这种这种题。当然你也可以让GPT4来帮你出这些题，这个都是没有问题的，很类似的。比如说这个知识推理，我们看到这个其实是一个多选题，咱们在做理论课的时候，我们也做过类似的题，就人来做这些题。
	这里这个题目的题干是说商周时期手工繁荣，具体表现青铜铸造，陶瓷也铁，玉器制造。这里其实GLM2第二代的这个聊天模型，它把冶铁也给排除掉了，他这边有详细的一个说明，主要表现在这个青铜、陶瓷、玉器没有得到充分发展。就这个链接。
	然后在chat GM里面，第一代里面其实他去引经据典查了一些是相当于训练集里面有的一些信息。从模型参数里包括什么商周时期的时间，然后再尝试去内部做一些推理。但其实他他没有整明白，然后直接最后选了一个A就123，没有把这个预期制造业选出来。
	其实也不知道为什么这个例子，大家如果有印象的话，我们在讲这个思维链的时候，当时这个Jason way那篇文章chaf sorts有一个定位或者说说法就是其实我们这里看到这个题目很像叫做standard的IO prompt，就标准的输入输出的prompt，没有做任何思维链的提示。然后那篇论文里面当时有讲，他是一个什么样的例子呢？就是我有这个网球，然后每个网球有单个的网球和有一个一罐里面装了几个网球，然后算出来我打了几个网球巴拉巴拉。最后突然话锋一转，问这个池塘里面的苹果，有这么一个例子，大家如果印象的话，然后如果用的标准的这个输入输出的prom的话，大语言模型会答错，即使是GPT3可能都会答错。但是我们如果把这个中间思考过程，这个网球到底是怎么算的给它展示出来，就是这个思维链的引导，其实它会答对。
	在这种小语言模型上，其实这个思维链的优势是不大的。在像source里面有专门有提到。所以对于校园模型来说，要去做思维的推导，或者说思维链的这种prom的设计，是一个现在所有人都在想办法的一个话题。然后像p two我的理解这种指令微调的手段在小规模的模型上，就是相当于我没有办法很精巧的去设计这种复杂的思维引导。
	因为你模型参数规模不够，那我就直接通过指令微调的方式，有一大批数据提前针对性的对把模型参数锁住，然后针对性的对这个语小规模的语言模型去做调整，是可以达到效果的。所以话说回来，就是GLM two在这样的一个问题上，我们都知道它内部应该是有一些逻辑判断的。但其实很难去直接展现出来他这个判断逻辑是怎么来的。只能说通过这个例子他确实答对了，但背后的这个逻辑它无法像大眼模型那样很清晰的展示出来，但我相信肯定未来应该会有很多手段能做这个可视化。
	包括对于长文本的一个总结，就文本摘要其实大元模型最重要的一个基础能力。这里其实是讲两个不同的。首先输入是一样的，他们对这个结论的一个总结。右边其实是说了一大堆，但是没找到重点。左边其实是抽象出来的一些题干。好，这个其实是chat GM。
	接着我们会看到在刚刚在这个事例里面很热门的一个项目，web GM。就整个质谱这个公司和清华的这个DM小组，其实是做了大量的论文成果和对应的产品或者说模型出来的。除了我们刚刚看到的ICLR2023那个GM130B以外，还有一个在KDD，KD也是一个数据挖掘的顶会。在KDD2023其实也发了一篇文章叫web GLM。到今天如果大家前面都学过正儿正儿八经的去动过手的，看这个架构就会会心一笑，为什么呢？就我们在做包括做销售机器人的时候，做这个agent的训练的时候，我们讲过AI agent对吧？
	一个最基础的就是一个谷歌的surp API加上一个大语言模型。那你再看这个其实是非常像的，就web GLM其实核心是什么呢？第一它换了基座模型，我们现在已经涉及到两种基座模型了，一个是130B1个是6B那web GM其实是用了一个10B100亿参数的一个GIM。在首先就算是用的GLM的backbone，它也没法联网。这个单元模型的特点就是我把训练语料给你一顿学学完之后变成一个静态的模型文件。这个模型文件里面就各种各样的模型参数有值，然后就是这个语言模型本身，但他没法联网，所以他想说能不能联网？这个思路其实也是现在所有搞这个AI agent的人都要去解决的问题。因为大语言模型天然就不具备这个外部知识，联网是一个最简单高效的方法。
	外部GM就是尝试去给100亿参数的GM去做这个网络增强的问答系统。说人话就是跟我们之前做的一样QA然后这个问答是它的这个形式，就我问你答就跟聊天一样。然后这个问答里面，我是可以去通过大元模型的推理，同时也能够去联网。然后最终我还可以把这个做成一个所谓的问答系统。
	因为你能联网，那你的结果可能就有多个，甚至在多个我还可以用大模型帮你去梳理一下，然后这样像搜索引擎一样，只不过这个搜索引擎的背后实现逻辑是大语言模型加上一个搜索引擎，或者说加上一个问答系统。所以整个结构我们能看到，他还跟b GPT去做了类比，我们当时看react那篇论文，就reasoning action那篇论文也提到了web GPT，因为web GPT就是很早期的这个形态，就两年前的这个形态。Web GGM其实也是类似的，它通过这样的一幅图跟大家解释了它三个重要的组件。Retrieval这个大家很懂的，我们就不再赘述了。这个retribution就是大语言模型去找一些知识，然后拿过来给到我们前面跟问题相匹配的一个结果。就跟我们问这个售房产的机器人一样。
	中间这个地方是generator，它是一个引导式的生成器，它不仅是像就相当于简单来说就是他把大语言模型的优势用到了这里，能够按照我们课程的用过的术语润色。他把we travel里面的结果润色了一下，提供更精炼的答案，然后这个润色本身也是可以去做训练的那最后还做了一个什么呢？人类偏好感知评分器这个corner像什么呢？有点像我们最开始给大家讲这个ChatGPT。
	最终的这个训练有三段，前面有个PPO，然后最终有一个给或者说中间有一个很重要的步骤，就是让人来对AI给出来的结果，生成的结果去打分。这个打分其实大概率上打到最后面是所有的结果都是对的。但是有一个说法我就是更喜欢听，他他这个都是一样的答案，都是正确的答案。但我就想觉得这个人听起来更舒服，其实就这3333大块，这三大块共同组成了这个以web GL为主的这么一个问答系统。我们可以看一看这个是发布在这个ACM，然后KDD的youtube上面的一个视频blue。就是一个搜索引擎的这么一个样式，但它的背后实现跟搜索引擎不太一样。
	大家可以看下面有多个reference。他在问give up the copilot能做什么事情？我怎么能够获取到这个github的copilot，就我们刚刚看这篇论文里面的截图。
	than. C. In the transformer. 还是外国的，我们who is the president of you，他选了一些比较好的这个结果。
	这个web GLM也是开源，大家可以有兴趣的话可以去访问它，也用到了serve API。就是整个不管是蓝券也好，还是我们上节课有跟大家介绍的hugging face的transformers这个库也好。其实这个大语言模型的生态还是有一些玩家做的很好的。就比如说这个KPI其实是很多人都在用的一个直接去连接搜索引擎赋能的一个工具。另一个版本就是他在hugging face上面去托管的这么一个项目，所以现在你会看得到大元模型的项目都越来越多的，是两头都有啊。包括我们待会儿还有一张PPT1页PPT去讲怎么样把我们的这些成果放到哈根face上。其实也很简单，它也提供了API。
	好，刚刚其实我我们过了好几个GM的这个大模型家族了。我们再来看一看这个video的这个GM6B就大多模态其实是非常诱人的一个前就是有未来的一个大元模型的一个终局。在GPT4的发布会之后，其实我们一直在讲质朴，在疯狂的跟进这个OPI。在GPT4的发布会讲了多模态之后，我当时有关注到这个项目，VGGM没过多久，我记得是一个月之内就发布出来了，然后宣称实现了这个双语的。然后图像的多模态，大家注意这个定语很重要，开源就不说了，因为它本身就开源出来。第二就是说它是一个多模态氮，这个多模态的多模态也没那么多，图像我可以做识别，但是视频不行，音频我印象中这个项目也不行。但是你是要去做对接，那肯定是能去做对接的。然后整个这个项目V9GM6B其实也是托管在THO的DDM这个组下面。
	然后他是一个怎么样的干做的一个事情呢？其实就是用chat GM6B还是第一代的，因为这个项目开源的比chat GM26B要早，然后一个62亿的这么一个聊天模型，然后图像部分用的这个believe two q former，在做什么呢？在做所谓的视觉模型和语言模型的桥梁，整体模型是78亿的一个参数，那这我们就会发现就是多模态的核心在干什么事情，其实跟当时做这个attention的时候一样，attention是说我把对齐和翻译两个任务放在一个attention。Mechanical也好，你叫alignment function也好，放在里面去一起学。现在语言模型学到头了，因为训练数据就这么多，然后算力也就这么多。我不可能把GPU搞这个100万张给你。那么学到头之后你会发现单纯学语言也还是有问题，就跟你这个听觉、嗅觉、视觉差了几个信息渠道一样。你希望说你是一个完整的人，完整的能能触碰到无感的各种各样数据的这么一个人。
	这个时候图像很重要，视频也很重要，包括音频。Facebook其实在几个月前也宣称他们做了一个多模态的模型。很多公司都在宣称自己做多模态的模型，但实际情况就是这个事儿特别难。坦白来说就是没有哪家公司现在真的敢说我做的特别好。因为他真的做的好就真的发布出来给大家说有点用了。
	那么video GM6B也是一个大家可以去尝试用的一个模型，它自己也给了一些示例，就比如说因为它首先基座是一个TIGM6B，所以它是一个聊天的形式。但是这个聊天的形式里面，去接了一个视觉模型，用来识别图像这种单纯的图像位图。然后那您能看一下它这写诗描述一下这个场景，给了一张图，那就能识别出来这个里面是有泰坦尼克号，有rose，有jack。但是背后怎么实现的？甚至这个demo是不是挑的特别好，你随便丢一个图他是不是就认不来了。这个大家自己去试，我们不做评价包括这部电影的导演是谁，詹姆卡梅隆这个回答的很好多模态的能力，包括像左边这个例子是GPT4，当时发布会也有给出来的，就是有一些很滑稽的或者说不合常理的图。他要能够通过大语言模型的视觉加语言能力能够识别出来。
	左边这个就是一个典型，因为这里是这样，他解释的一样，图片很奇怪，一个男子站在一辆黄色出租车的车顶上，熨斗放在车身上，表明正在使用出租车作为移动熨斗店，在城市中不寻常。不是，这个是他奇怪的点，因为他它不是一个正常生活中看到的一个现象。然后像右边这个描述图片，它也能够去描绘。比如说虽然这个核心的你说人脸识别也好，这个生物识别也好，是一张狗的这个犬的脸脸，但是抛开这个部分，他还能看出这个是蒙娜丽莎的微笑的这么一个图做的地图。这些其实都是未来我们希望视觉模型和语言模型。如果能够对企业甚至视频这种识别能够对齐，那那应该会做非常多的工作。
	但是我们能看得到，这前路漫漫，还是有很多的阻碍在这儿的。就单说一张图，为什么要把视频拎出来？单说一张图，这张图的语义信息就挺难去对齐的了。然后同时如果你考虑到视频，那视频和图像之间的难度就在于视频是有时序信息的。就是你可能这种一秒钟就是几十张图，那这几十张图之间这个时序变化，这个时序变化怎么再跟语言结合起来，这中间有两个坎，如果这两个坎都能迈过，那真的就非常强了。就是你想象一下会有一个大模型，这个大模型看完一个视频就能直接告诉你这个视频里面讲了什么，有什么重点，甚至你问的问题不一样。大家想象一下刚刚那个web GLM，他对接的可能不是搜索引擎，对接的是一个更强的多模态的大语言模型。让他直接去看那个视频，给你一些结论，这个是非常香的。
	对，有同学问visual GM也是质朴的吗？是的，是的，还有一个生成代码的叫code jx two，也是发在KDD23的，跟web GOM1样，发在这个KDD这个数据挖掘的顶会上。它其实跟这个ChatGLM26B类似的，它也是在第一代上面去做了一些升级，具体做了什么升级呢？比如说在原来的基础上加入了这个代码的预训练，然后它的一些直接收益其实是来源于chat GM2，就这个搞法大家就是搞搞到头。其实你就会发现，就是我开始讲的，包括刚刚这个第一周讲理论课的时候，就想跟大家尝试去讲到这个时间为止，我们有这么多的课件动过手操作了这么多次之后，你就会发现其实没有那么多神秘的色彩。
	对，就是因为chat GM two本身迭代了，所以code GX2自然有很多的优势的迭代。因为基座模型更重要，那么video GM那你能想到他他要是有心思去做，也能出一个visual的这个GM two，对吧？但是他没去做，为什么？这背后也有一些各种各样的原因。那么QGX two它的核心其实就是有点你可以理解，有点类似于对标这个github的copilot。
	因为生成代码本身就是一个非常香的事儿，如果我们可以去把这个代码生成做的质量高一点，其实有很多的初级程序员，甚至说各种CRUD，或者写circle，写脚本的工作，包括运维的工作是有可能被替代掉的，所以很多的公司都在做这个事情。Cog x two，它它的这个基准测试上面能看得到。首先它的代码的数据训练的量比之前有提升。然后在6种不同的编程语言上，也有这个编程语言的基准测试了。Python、c加加、java、javascript、t go name、rust, 这些六种编程语言上都有一些提升。其中在python上面的一次通过率甚至超过了一个150亿的参数。
	就相当于在大家可以理解这个结论，就是在python这个基准测试集上面，它超过了一个两倍规模的，甚至2.5倍规模的这么一个大语言模型，然后它的模型特性也是比较好的，因为它支持中英文输入，所以我们能够想得到在刚刚去看这个质朴轻言这个1130B的交互式的，有点类似于HIGPT体验的这么一个应用的时候。里面的代码生成肯定也是用的代码预训练的，然后它的长度也变长了，支持8K的这个上下文，速度也提升了一量化之后也只要6GB的显存。大家有没有发现，其实整个这一套就有很多都是GLM的优势，这个基础模型的优势。但是它上面的应用层的模型可以反复的去讲，因为这个确实能够被继承下来，然后支持的插件也很多，因为最终这个玩意儿要变成一个插这样get up copilot能够装在VS code里面，我不清楚他好像也能装在patch AM里面了，这个大家可以如果说错了，大家可以去纠正我。
	对，然后支持的这个编程语言也比较多，不，协议也比较友好，跟GM的协议是一样的。那它的形式就是一个VS code的插件，大家能看到比如说这里它生成了这个，他这里还不是，他这里是解释了一下这个函数，然后其实就把这段代码抄过来了，就相当于他这个截图比较少。其实整个这个体验就跟我们在课程里面曾经演示过的一样。
	我遇到一个特定问题不知道怎么做。就比如说上节课你在销售机器人那个课的时候，我们去生成了可以用来做向量数据库QA对存储的用户提问和销售回答。这个过程当中需要处理一些数据。当时我们把这个正则表达式作为能券里面切分这个文本块的这么一个很关键的正则表达式。那个其实就是使用GPT来生成的，那这个代码你也可以写，你是要生成python里面的一个正则表达式，那它就会给你对应的这个python的这个，你可以认为是它的代码。那这个部分其实它的使用场景是比较灵活的，有更多还是咱们用户得想清楚，现在我这儿插一段什么样的代码。就现阶段来说还很难有一个大语言模型。你直接给他布置一个任务，他就把整个代码库都写给你了不太行，那你还是需要自己去做一些设计。
	好，我们可以看一看刚刚提到这几个大语言模型具体首先刚刚都放了对应的这个链接，大家可以看一下课件里是有链接的。然后同时在清华质谱的在清华的数据挖掘团队group里面，其实有这些大元模型，比如说刚刚提到的130B，我们提到的GLM、web GLM、video GM和code GX，再看一个项目，这个项目因为不是大语言模型，而是一个benchmark，有没有放在课件里，但它很有参考性，代表有这个代表性。三天前更新的还是一个非常活跃的项目。然后我印象中应该是一个月前开源的，然后这个项目其实我们可以看一下做了一个什么事儿呢？像不像奥特gbt当时那个界面？一个游戏的小AI对。
	各种各样的事例，其实核心就是想想告诉大家，AI的agent可以做很多事情了，给你做游戏的NPC，然后帮你去查资料等等。所以我们现在都知道大语言模型有很多的应用形态，ChatGPT只是其中一种，就做一个问答机器人、聊天机器人。但未来我们想要真正把大语言模型用起来，要用react这套思想，就reasoning加上action。Raining是大语言模型，action就是各种各样的外部API原有的这个软件世界，我们这么多人写了这么多年代码，构建出来这些能力。这些能力需要由大语言模型驱动来调用，就是我要用什么这个能力由他来决定。然后他们一起组成的这个东西叫做AI的智能代理。
	这个智能代理当然也有强弱了，那它的强弱由谁体现呢？肯定不是由那个tool来体现，我们在南线里面它叫tools或者tool kit，它肯定不是由那个工具本身来体现，因为核心是大语言模型的推理能力生成的文本，然后文本要规范化，变成可以由这个to o调用的输入，以及判断什么时候该调用哪个兔子的时机。这些其实都是大元模型。
	那么这个时候你会发现欠的这个概念很巧妙了。我们知道大语言模型本身，如果你要把它用的好，prompt得设计的非常好。然后prompt加上大语言模型是什么？是一个最简单的LLM欠。
	大家还记得我们进阶篇里讲连欠的基础概念，所以一个大语言模型不能称之为一个应用，它用不起来，一定要有prompt。那prompt的加上或者说prompt模板、提示词模板、prom template加上你在应用它的时候输入的这些动态的参数，加上这个大语言模型本身组成了一个最基本的能欠的调度单元。LM键IM chain加上tools组成了agent对吧？那这个这个图应该在课件里也出现过无数回了。说到头这些agent的比较就是在大语言模型以及你的prom的写的好不好？如果我们把a gt当成一个应用的话，好，那么这个agent bench里面它就有写真实世界。我们的挑战不再只是一个大语言模型了。要把大语言模型当成一个智能代理，然后要跟环境去做交互。
	大家会发现最近大元模型的迭代是很慢的。大元模型迭代通常是以年为单位的。但是应用层的迭代，大家每天都想有新花样。所以很多人开始对agent开始有各种新的命名，新的改造了。大家一定要擦亮眼睛去理解它背后的本质，其实就是大语言模型，就是react的这个机制reaction的机制，这里面一些真实问题的挑战就是比如说在这个命令行程序，然后API的调用mysql，包括这个图形化界面等等，这一堆的问题需要由agent来解决。它还有不同的环境里面获取信息的方式不一样一样。
	有的是本身就已经结构化了，比如说circle，比如说这个命令行，还有一些是需要你通过现有的这个识别的API也好，或者说其他的API也好，去去处理一遍这个数据的。比如说你要跟机器人去做对接，或者说你要去跟这个视觉系统去做对接，或者你要从游戏原有的系统里面去卡牌游戏，你要从原有的游戏系统里面去获取数据等等。这个是一个典型的agent的一个结构。在这里面他有去对比出我们的不同的这里这幅图，有去对比不同的agent的一个水平，简单来说就还是跑个分对吧？我们已经看过不同的跑分了，大模型跑过分，很多都跑过分。那么现在agent能不能跑个分？但其实agent跑跑分的核心还是回到大元模型。
	我们看这幅图里面中间有一道天谴，这道天谴是首先区分了这个模型是不是开源的，下面的这些单元模型都是开源的，然后上面的这些单元模型是闭源的，叫API base。我们学这门课程的时候，一开始也是先学的上面的，大家有印象的话OpenAI的API。后面我们学的nature是可以服务于API base和开源的模型，这两种都可以服务。那么在这个API base里面，我们看到GPT4其实是一骑绝尘。为什么讲我们看这个最核心的OA这个值，就OA这个值GPT4是4.41，然后class然后他用的是0613这个版本。现在大家再去看论文就没有那么难看，没有那么难看懂了GT40613这个版本其实也就是我们在学基础篇的时候会调用的版本。包括我们蓝线里面，我们演示这个agent模块的时候也用过GPT40613，那OA值表示的是这个overall agent bench score diary from a waited average说一大堆，简单来说就是上面有8种不同的场景。然后这八个场景里面agent都要去试一试，看他的得分表现怎么样，这是八个场景里面我就加群，我是平均的算术平均的，我没有谁的权重高，谁的权重低，然后最后得了一个平均分，然后平均分GPT4是碾压级的。
	首先我们看到这七个环境任务上，它都是最高分。除了这个WS这个环境里，其他的这七个上面大家就跟奥林匹克比赛一样，对吧？有个选手就跟博尔特，他除了可能马拉松没有拿冠军以外，100米、200米、400米、1500米全是冠军。那最后平均值肯定是最高的。所以大家能看到GPT4是4.41，第二名是2.77，然后到开源的这个部分就只有1.15了。就开源最好的这个open chat，130亿参数的这个1.15。再往下是连一分都没有了，就零点几分了。
	所以很多同学说南京的这个prompt，我换了一个模型怎么不好用了？那是不好用了，因为那些模型就是不行。简单来说就是那些模型就是不行，他他水平有限，他真的不行。然后你想通过你的prom的去去救他，你是救不活的，因为他差太远了。
	对，但我们能看得到的是，再过一两年，我相信应该大部分的模型都会赶上来，因为OpenAI它走的比较坚定。我们有讲他的首席科学家cofounder，当时也是谷歌出来的，然后也是hinton的学生，跟anna t any这个小哥一起发了any net这篇文章，在深度学习时代还参与了TensorFlow的建设。所以OpenAI在这件事儿上，不管是transformer GPT这个事儿，还是prom的设计这件事都走在前面了。但后面的这个cloud，包括google的palm facebook这些模型，肯定都还会追。
	那么慢慢其实就卷起来之后，我们会看到OA的这个值，会慢慢提起来的。到那个时候agent才成立。但现在agent至少我们看得到GPT4作为agent这个大语言模型的这个发动机也好，这个大脑也好，还是有戏的。所以大家如果要去做agent类似的应用，简单一点就是你你用GPT4就是比GP3.5要好一倍。但如果你要换成GPT3，那可能就更差了，因为GPT3结果没有那么稳定。但即使如此，你能看到text达芬奇003还是第四名的对，这个位置还是很高的。所以cloud也是OpenAI这家公司的田员工出去创办的。就除了ChatGLM two以外，上面就全是OpenAI系的公司做的，下面会有一堆的公司在在卷，这个是比较考验真实水平的。
	我们说现在有各种各样的benchmark在那刷题，然后做中文的，然后也让简单来说是什么意思呢？就是为什么我把这个举例为这个奥林匹克，是比较有直观的一个感受。那为什么现在说大语言模型，尤其是国内这么多家大语言模型，今天又说我超越GPT t4了，明天又说我这个跟GPT4差不多了，就是因为他搞了一堆的基准测试。那他搞基准测试有点吊诡的是说，这些基准测试都是平权的。然后完了之后可能国际上认的基准测试，它只保留了一两个，然后自己选了一些有差异化优势的搞了特别多。那这样肯定就把分数拉起来了，对吧？因为我我拿我的这个长长项在在比，然后本身大家都都需要去看的那些考试项目都取消了。那那这个结论是怎么样的就不说了。
	好，然后包括这个雷达图大家也能看得到GPT4是真的，这个可以说是六边形战士一样的存在。在各种各样的这个八就是八个不同的agent场景，是遥遥领先。它是七个全项第一，七个单项第一加这个总分第一，所以你看这个雷达图就能感受到它为什么这么强了。当然我相信这个agent bench，或者说这个智能代理的benchmark也还会演进的，所以现在也不是终局，但是至少就目前这个GPT4的一骑绝尘的这个领先优势来看，它就是特别强。它比其他的模型是强了不不只是一个身位的，所以这一点是毋庸置疑的。所以如果有人在跟你说GPT4，我就跟他做的差不多，你你就你就大概理解，其实这个人他要么在骗你，要么他不懂。
	对，好，我们到这儿为止看看大家有没有什么问题。就是对于GLM模型，大家这个大模型家族有各种各样的模型，然后也有benchmark。我们留五分钟时间看大家有什么问题，接着我们就去看看chat GM26B。看看大家有什么评论区的问题，看到大家问题，我来回答一下。
	有个同学在刚开播的时候就在问，用消费级的显卡跑一个模型的推理，显存要大于24GB是不是只有3090双卡交火这一个方案？你也可以4090教我，对吧？就是消费级的显卡，然后你单卡不够，那么就只能拼双卡呗。如果你能接受一点精度损失的话，那你也可以做量化，是把模型对于显存的需求降下来，然后看还有什么问题。
	我们如果想要chat GM two加图片识别得搭上，同学一定要把模就是聊模型得给后面那个数字。你的这个chat GM two是指6B吗？还是指质朴轻言130B然后visual的这个GM本身就是基于chat GM来做的，所以不需要再加上chat GM了。它包含了这个能力。
	能对比这两个模型吗？这个可能上节课讲过了，同学这个rock同学问有两个用来生成代码的模型，在hugger face上面有这么一个space，就是去对比不同的模型有不同的benchmark。可以看一看，关注一下，就是我们的hugger face的LLM的leader board。看看还有什么，还有同学问文心一言和这个千问对这个我就不知道了，可能清华他们没有把他俩拉进来。对，这里好像确实没有中国的这两家有百川，我感觉这里出现的应该大部分至少是在hugger face上面有这个托管的，所以大家比较好去做测试，这也是为什么现在哈弗跟face很香的一个原因。就是大家都要做研究，然后你藏着捂着，那肯定不好玩。然后有没有什么方式让大家都能够把你的研究成果快速拿来做测试，做复现呢？还有in face现在推的这个space的功能区就干这个事儿了，我能一键把我这个大元模型给部署起来。
	还有一个同学问chat GM light pro standard这是啥？我都不知道，是不是？是这个质朴团队出的吗？我没听说过。这个light pro和standard的是什么新鲜的命名吗？这行业取名的人太会取了。对，太多新名字了。
	对，雷达图是怎么绘制的？你去ChatGPT的plug in上面去找找。对我印象中这两天有个插件叫canvas，很火，好像很多人在推，就用它来画图。然后这种雷达图都有专业软件的，他就把那个值给填进去就完事儿。对。
	6B和130B的性能差距大吗？这个差距非常大。大家可以看看论文里面的这一些题的数据，6B你可以理解成6B就做做特定任务就好了，它都不可以称之为通用的对它它离通用有距离。你看这个agent的肯定应该就有概念了。对，你看那些12B的是多少分，然后你再看看GLM two是什么分数。
	有权威的测评大模型的系统可以使用吗？咱们要测什么呢？哪怕是这个agent的benchmark也是挑了八个a gent的环境，八个任务。咱们测大模型测什么的，对吧？这个很关键。现在部署私有化模型没有价值，这有没有价值是看你对这个有没有价值是看咱们自己。
	还有个同学说下载了亲爱的GM6B好像核心代码只有2000行左右。因为它的模型文件是单独下载的，是不是？Chat GM6B本身模型结构不复杂，获取足够的训练数据和好的训练方法上比较难。
	这个同学问的问题挺好的，需要回看一下第一周理论课的部分了。简单来说就是现在大家都知道一些大的词汇量。我知道fine too，我知道instruction tuning，我知道这个要做prompt，也就知道这个级别。但真正落地为什么你干的比我好？这就是在真正落地的时候，大模型训练包括info包括做分布式，有各种各样的细节在里面了。这些细节就不是三言两语能说清楚的，甚至针对不同的训练集，训练数据都还会有不同。对，没有13B有个同学问做垂直应用的模型，用13B和6B的区别在哪？没有13B。
	所有让评价哪个模型强，哪个模型不强，谁超过谁，这个我都不做评价。大家可以看看，我提供了这么多的benchmark和包括agent的benchmark，大家自己去去看啊，不做这种引战式的评价。然后那个模型文件不在那里，面对小公司想要私有化部署支持多模态，劝退一手，小公司就不要干这个事儿了不适合你们，你能调API就调API。
	对，性能本身就是一个舶来词。这个同学提的很好，就performance的前提是找准你这个performance针对的是什么事儿。就比如说这个agent benchmark这个performance针对的是这八个不同的environment里面的表现的一个分数。比如说网购，然后网上的这个网上冲浪吧？用搜索引擎的能力，操作系统各种需要脚本的使用，包括卡牌游戏。
	那行，那我们接着往后走，还有整个课的一些问题我。那行，那我们接着往后走，还有整个课的一些问题，我们就收一收。好，我们看看后续的问题，后续的这个内容。然后大家最后我还留1个小时时间，大家可以再交流7GM two 6B这个大模型的应用开发其实没有那么难。大家说应该这么讲，就是要做到我们进阶课的那三个那三个实战作业的效果没有没有想象中那么难。但是它的效果可能跟你用GPT3.5和GPT4的差距就比较大了。
	我们在刚刚看agent benchmark的时候，我相信大家有有感受。看过那个评分，一百多亿的模型参数的表现是0.0几，0.1都没有。GPT3.5是二点几，差了两百多倍，然后GPT44点几差了四百多倍，甚至八百多倍。那么这个大几百倍的差距，两个数量级甚至接近三个数量级的差距，靠咱们个人开发者去追上他是不太现实的对所以简单来说，chat GM26B这类的模型就是在垂直领域里面，然后基于你的自有知识库，就有点类似于咱们那个销售问答机器人。或者说这个OpenAI translator这样的翻译项目里面，不涉及这种常大量的常识。然后多步骤推理，多步骤的推理，以及这个agent里面需要的react这样的场景，那么用6B这一类百亿级别或者大几十亿级别的模型是还能用的。
	怎么还有人在问凹凸GPT是不是一种模型，给我气吐血了。有同学自己有其他同学知道的回答一下呗。Chat GM26B这个私有化附属巨简单，所以我没有做太多的事儿，就简单截了一下这个read me的图。首先从github上面把这个代码库clone下来，然后进去之后，把requirements就是他需要的这个包安装一下。完了就这两步就把这代码下来了。但是实际运行的时候，它还会去加载这个模型文件然后这里其实我们会引申到上节课讲到的transformers这个库。就是为什么我说hugin face很聪明，然后作为一家创业公司，他们做的非常成功。在于transformers抓住了很多学习，机器学习到深度学习，现在转过来做大语言模型的开发者的需求。我们看到chat GM2同样的也使用了transformers这个库。在这个库里面有几个比较重要的模块，我们稍微讲一讲不展开。
	第一个就是从transformers里面导入的这俩，一个叫auto technic er，一个叫auto model token。Ized这个词大家应该不陌生了，model也是一样。Token nier其实你可以简单理解。我们在之前的课程里面学习使用过OpenAI的类似于token ized的一个库叫tiktok。Tiktok当时是用来帮你去把一个输入的自然语言或者输出的自然语言，帮你按照一个特定的编码方式去分割成token。然后你便于你去算成本也好，去算有没有超过这个上下文上限也好。然后在transformers里面这个token会干的事儿要更多一些。他其实简单来说就是把这个模型输入到这个单元模型之前，有一堆的前处理的工作，它都包在这个tokenizer里面了。所以它会比open I的tik token的能力更强，这是一个最简单的理解。
	第二在这个model这一块，我们都知道这个大元模型最终其实就是一个模型。我们在使用年前的时候，有LLM的model，也有chat model，这些都是model。这个model有一个很重要的方法叫做以evaluate就评估的方法。
	在大家如果没有机器学习和深度学习的背景，可以推荐去看看之前我讲过的TensorFlow的那个课程。里面有差不多几节的，应该是四五节课去跟大家讲机器学习的基础。然后那个课现在应该打折也很便宜了，还是应该大家买过可能有折扣，应该几十块钱。你们有讲什么呢？最最关键的核心就是所有的这些learning，继续学习、深度学习。
	不管这个词怎么样，只要是基于这个反向传播这样的一个方式来进行学习的话，其实整个大模型就两个阶段的事儿。一个叫训练，一个叫推理。训练的时候训练的计算量更大，因为他既要做正向的推理的工作，把你输入的这个值算出一个最终的结果。
	然后这个结果如果我是一个有监督学习的模型，它就会有标签值，就是真实值，这个行话叫grown truth。这个真实的值会跟模型现在的这个值去做一个对比，做一个度量。然后这个差异如果比较大，那就说明这个模型还远远没学好。因为整个学习表示学习，这都属于表示学习的范畴，在embedding那一节课的时候，大家回头看看我画的那个维恩图，都属于表示学习的范畴。那么这个表示学习的核心就是给你一堆数据。我要把这个说说的最简单的大白话就是这个模型就是一个很难形式化定义的一个非线性的函数，那这个函数就是用来拟合这堆数据的，就只要这个数据丢进来我学的足够好，这个数据的分布我是都学会了。然后只要它的分布没有变化，那么你符合这个输入数据的分布，再丢一个别的数据进来，我还是能给你预测对的，或者说给你一个你要的这个值是符合它的分布的。
	然后在刚刚提的这个场景里，比如说我正向的这个训练的结果，得出一个值之后，跟它真实值有差距。这种有监督的学习过程，那就要不断的让这个模型的参数去做调整，调整到我模型输出的这个值跟标签的这个值真实值几乎就没什么差异了，那他就认为学习可以停下来了，结束了。那这个时候我要怎么调整这个模型参数，就是我们说的这个6B1300亿和1750亿这个参数，这个就是我们要调的那个值。那个模型参数也是我们最终到存到文件上的那个check points文件OK那这个怎么调呢？就要通过一个反向传播，就是我们学过数学高等数学也好，我数学分析就这些课都会讲，就是我我可以对它不断的求导对吧？链式法则然后一起往回倒，这个过程就可以去调整这个模型参数。
	但也因为这个过程，所以层数太深或者RN的这个序列太长会导致什么呢？导致梯度消失对吧？这个梯度回不去了。因为就算是64位的浮点数，它能表达的精度也就可能几十位小数，就是有效位数。大家回想一下这些小学数学概念，有效位数有限。然后有效位数有限的前提下，那你这个层数就得有控制了。所以需要用raz NET这种短链接或者LSTM这样的一些记忆门来实现这样的事情。现在说了这么多，回到这儿，这个模型如果训练好了，这个模型参数是不会变的，变成了一个静态的文件。你可以从网上下下来，下来之后加载到显存里。
	当我们人在去使用的时候，就我不再做训练了。我在使用的时候我是不需要那个反向传播的过程的。我不需要对它进行求导，做梯度下降，然后再做反向传播。没有这个过程，只需要做正向的推理。这个正向的推理通常会有这样的一个说辞，叫做evaluate EEVAL。然后它这里其实就通过这样的一个调用，把这个模型加载到系统里，通过这个auto model from free train加载到这个内存里或加载到显存力。再把它变成这个evaluate，只做正向的这么一个模式，那就是就这么几行代码就干这么几个事儿，我们能看到进入这个模式之后，剩下的部分就跟我们用棉签也好，用之前的OKI的API也好很像了。
	传入一个tokenizer，首先他要用这个model的chat这样的一个接口，一个函数，然后传入三个，一个是tokenizer，这个token lizer是自动去进行一些前处理的。根据我的模型一开始在这个训练的时候一些设计，然后中间这个是我这一次交流的，就我们维护过message的话就知道这个是我这一次user要提过去的。然后有history，这history就约等于我们维护的这个messages，在像GM里面叫history。Token iza是一个单独的，你可以认为也是一个单独的文件，专门用来做预处理的，然后得到的就是这个assistant的回复。我们假设现在用的是这个OpenAI的，或者说南茜的这个chat model的话，就是最终的那个assistant的这个回复。然后当然你也可以把history也打印出来，就是他会把你的这个历史聊天记录也都存下来，待会儿我们可以实际操作一下。好，我们可以实际看看这个怎么玩。
	放大一点。这个字号能看见吗？大家。现在这个字号能看见吗？还需要再放大吗？OK那就不用了。好，这是一个VS code的一个远端的服务器。然后我这边有一个窗口一直在看GPU的情况，现在是没有使用的这是一台T4的服务器这是一个小的命令行的一个指令就可以实现了。Watch杠N是指循环的去执行，或者说周期性的去执行后面的指令。杠N是一个watch的命令行的参数，在后面接的这这个是周期的时间，就每一秒查看一下这么一个值。
	所以他就会看到现在这个T4的这个GPU里面是没有使用的。然后我们回到这个命令行界面，这个是我们从get up上面抠下来的代码。然后还有很早之前可用的第一代和这个visual GM。我们今天着重看一下这个chat GM，其他都是一样的。
	好，那现在假设我们我们从这个我们直接从这个方式来启动，在命令行启动一下，简单一点，然后我们需要干嘛呢？需要从transformer里面input这两个库，然后把这俩模型加载起来。大家如果第一次执行的时候会稍微等待一点时间。因为我大家能看到这里有一个进度条，这个进度条是在加载这个check point的分片。因为他把一个完整的大模型分成了七个文件，它在逐个文件加载，把每个文件它定义为一个文件。这个模型文件的分配，然后咱们第一次执行的时候会去下载这七个文件，会稍微等一点时间。
	如果你的科学上网的手段，或者这个站点，或者what's ever各种东西不是很顺畅的话，网上也有各种解决它的方式，包括chat GM自己就有这个解决方案。我印象中在FAQ里有提。这里包括这里有一个小技巧，就是如果咱们之前在已经拉下来这个模型文件了，那就另说。但如果咱们是第一次执行，然后下载这个模型的时候，出现了问题，不管是你去下载这个量化后的，这个是去下载它英特斯的量化后的模型，如果你不加这个后缀，就是直接下载这个6B模型本身。除了这两个以外，它还有int 8之类的各种各样的，它列表里面都有写。然后如果大家下载到一半失败了，要清一下它的缓存，就在hugging face下面默认有一个路径把缓存清掉。如果大家下载的过程当中发现特别慢，最好就换一下这个连接的你接入的这个站点。
	好，那么把这个加载进来之后，其实这个模型就是我们看到的chat GM26B这么一个模型了，然后我们把它进入到这个推理的模式，可以实际去跑一下看看这里。稍等我看能不能这样。好像不行。还可以很好。你这样操作一下。然后把这收起来。
	我们看到这里，它其实有这个怎么样？大家好看一眼这个数据。
	好，这样是能看的比较清楚的。然后我们可以看到这边是它已经加载到GPU里面了，然后这个进程其实就是这个python的进程，然后python 3加载了模型，模型占12GB的显存。然后这里然后我们再继续向他发问，看看还有这边会有一些显存的进展，在使用它简单一点直接复制一下。这个也是我的这个老问题，我问一问，看他怎么说。大家有注意这里有一个收权证，这里有这个使用率，包括这里有有数字的变化的。然后现在生成完了就结束了，这里结束了，然后这就降下来了，我不知道大家有没有注意到这个变化，这就是非常典型的在远端服务器上面的一台T4的GPU。然后在运行的时候，能看到GPU在运转，然后也在消耗我们对应的显存，也在实时的在用这个GPU的算力核心。大家能看到他这边有输出一个结果，睡不着很多原因巴拉巴拉这个是他标准的一个答案，跟它的样例一点差异都没有，没有任何变化。那我们接着再看看，稍等这边好编辑一点。
	好，这里的history其实我们是把前文的它的这个输出是带进来了的。因为你能看到这个history是它的一个输出结果。你可以简单理解成这个推理模式下的模型，自己在维护这个history，我们可以直接输出出来看一看。
	这个是第一轮的对话。这个是第二轮的对话，然后它整体维护了一个列表。然后这个列表里面是一个二元组。这个二元组的第一个部分是用户提问，第二个部分是AI的回答。从这个history的结构大家就能发现，其实还不支持更多角色的设定的prompt。我们在很早的课程里面就讲过，GPT3.5，GPT3.5支不支持，GPT3.5支持，对，GPT3不支持，GPT3.5和GPT4都支持。
	这个chat completions API，当时大家可能还没有体会，就我说能用这个chat方式的API很少。比如说ChatGLM26B其实它就不支持。比如说我们最典型的system user和这个assistant这三类不同角色的prom的设定，它很难搞。第二个就是function call，其实function call就是一种特定的agent的实现了，对吧？那也没有。好，这个是他存下来的这个history。好，那我们再再调用一下看看，大家注意观察一下右边的这个数据，也想跟大家有一个了解，因为很多同学都对这个GPU非常的关注。
	大家能看到这边的数字的变化，我这边有高亮。
	这个是它的耗电，这个power是下面这个是耗电，就它在高速运转的时候，它的功率会消耗更大，这个逻辑大家应该明白对吧？就是你你想象一下你在用找一个所有人都用过的电器，这个电吹风，你应该用过电吹风，你开这个小风中风、超大风的时候，功率是不一样的，这个是一样的逻辑。我们看看它最终这个回答怎么样，这个是超出他的这个事例的一个连续发问。给了，我们问具体如何放松，深呼吸、冥想、放松肌肉、做运动、读书或听轻音乐、泡热水澡、与朋友或家人聊天。好，那我们今天试一试这个能不能改善睡眠。好，然后我们再试一试，让它生成一个长内容，然后不给他history，看看会怎么样。看会不会显存爆炸，我们把history设置为空。
	看这样的指令他现在能不能听懂。
	我们把history带上，这样他应该会消耗更多的现存，看他能不能画风一转，这个别过来。
	大家能看到这里的变化，终于有变化了，12389变成了12395，403。这个就是我们刚刚提到的，在大模型推理过程当中，它会把这些token在推理过程当中加载到显存里，然后这里会有一些额外的开销。并且它的history，我不清楚这个hugger face里面实现的这个model有没有把history也存进去，你可以看看。
	这边有生成一个内容，只有1000个字吗？没有。我们可以再延展一下问题，看这个显存会不会再增加。
	大家明显看到这个5000字的就从413增长了一百多兆的显存开销。
	把API端口开放出来，这个同学这个是不能开放的。因为这个GPO的开放，我们在隐私那节课就讲过了，这个要备案才行，我这个开放有法律风险的，可能明天老师就去被网信办约谈了。好，大家看到这继续增加了两百多兆的显存开销，并且还在持续运行。
	就简单来说，我们刚刚还有提到一个点也想跟大家证实的。不管是transformer还是GLM用的faster transformer，其实整个你的上下文的这个token的长度都会第一会增加显存的开销，就你会越来越多的消耗这个显存，你剩余的就会更少。第二个就是说它的上下文长度跟算力是N方的一个关系。所以你的token越长，就比如说你举个不恰当的例子，但是表达意思比如说生成100字的时候我们等了一秒。那生成1000字的时候，理论上它不是变成10秒，它有可能会变成大几十秒，就是这么一个逻辑。因为算量变，这个算力变了，你看这个还不错，五千字的这个它还有章节标题，大家可以看看。教师节那天，全校师生聚集在操场，我们把它拎出来看一眼。
	这个是它生成的内容。教师节那天，教师节活动掠影、主题班会、黑板报、教师节短信、礼物、活动感悟、结语。可以，大家回头自己去跟这个TIGM玩耍一下，它的启动就是这么简单。
	然后接下来我们再跳到后半部分，就是讲咱们的token lizer和model token ized。我们稍微再啰嗦一句，就是transformer显卡会烧掉吗？不会的，同学显卡会报错，就是硬件和软件是分开的，这个是咱们计算机最好的一点。就是我我记得初中第一次用计算机，这个老师跟我讲的一个最大的好处是这个计算机做实验，没事儿，对你使劲搞也不会怎么样，不像隔壁化学物理做竞赛的对吧？他们的实验课就麻烦了对。
	然后简单来说，我们看一下transformer的，不这个transformer这个库就hugin face的transformers cool，这个took night。在transformers库里面的这个定位非常简单，一句话来说就是它是用来给这个模型的输入做预处理的，或者叫preparing，就做一些一堆准备工作的这么一个模块。然后这个模块有各种各样模型的具体的containers，它是一个模块，它又是一个库，这个库里面有各种各样的模型，然后你甚至还可以把你的模型需要的token ized上传上去，然后他还会有这个批量的版本和这个快速的版本，就这就这么个东西models也是一样，然后这个models的我们讲能券这个基础概念模块的时候，跟大家提过很核心。就是你要整明白这个框架的类的继承关系是什么样的，你就能捋清楚了。因为你只要把这个类的继承关系捋清楚了，一条你明白了，其他的实现就触类旁通了。比如说在人圈里面这个模型有base model，然后最后回到根儿上就是一个基础模型的抽象。然后这个基础模型抽象的输入，有你可以是聊天的这种message的方式，也可以是一个prom的方式，输出的都是这个模型生成的结果，对于prompt的最底层的抽象也是一样的，包括这个two之类的那在transformer这个python库里面，基础的这个class模型的基础的这个类基类有三种，那么为什么有三种呢？
	第一，我们能看到有就这里写的这三个，一个叫pre trained model，一个叫TF的retrain model。这个最好理解的就TensorFlow的，还有一个叫flag pretrail model，就这三类。然后这三类实现了基本上实现了加载保存一个模型的大量的方法。包括你从哪加载是一个本地文件，本地的一个目录，或者是一个远端的一个文件。比如说这个hugin face上面，所以我们能看到刚刚的那个代码那么简单，是因为它使用了from这个pre train model，它会自动的就去把清华的这个DM团这个团队在hugin face上面托管的那个模型文件拉下来，就from这个pretrail，就我们看到这儿这个tokenizer也是类似的。它实现了from free trade，然后它就会去下面这个就有点像github上面我们的这个记录一样，就比如说我们的课程项目，这就叫疆苟鹏的OPI的quick star，这个就是属于在harding face下面有些HODM这个group这个项目。好，这个是这几类模型。
	然后我们还能看到这个free train的和TF free trade还实现了一些别的方法。比如说把音把这个输入的，你可以叫文本也好，数据也好，给它resize，就把它的输入的这个中文叫什么呢？规格或者说尺寸shape重新调整。然后还可以去做这个token embedding，然后把它加到一个词汇表里面，然后也可以去做一些attention。这就是我们学这个理论篇的时候讲过attention，也讲过transformer的这个muti head的attention。可以去做一些减值相关的一些操作，总之这两个是最常用的模块，咱们整明白它的几个基础方法就好了。
	然后我们自己如果做了一些，比如说我们基于ChatGLM的6B做了一些微调做了一些指令微调。然后想要把它像get hub的这个仓库一样去使用having face怎么用呢？它也提供了这个类似的方法。就我们在github的使用上，其实是在这个get up上面，你需要注册一个用户。然后这个用户你有一个这个key，然后这个key使得你可以去拉取一些这个项目和推一些代码。类似的，其实这个transformers也是一样它不过它的区别在于它没有用。它不是说可以在这个命令行里操作，它可以通过python的代码，就transformers这个库本身也能操作推送。
	就比如说这个push to hub，就会把模型和对应的我们的这个token either都传上去，包括这个文件，这里就可以替换成你自己的这个项目的路径。你这个token lizer和咱们的这个model都是一样的，使用push to hub的这个方式。Chat m2刚刚我们看到了加载，就是用transformers的这两个模块能加载一个token ier。然后一个model加载起来之后，在命令行这边就可以去运行，那同样的它的图形化界面用的radio。这个应该大家也再熟悉不过了，对吧？就是我们前面好几个实战项目都介绍过这个玩意儿了，也是hugin face这家公司开源的一个项目，然后可以通过这个对话框的方式来实现聊天，它还额外开放了三个单元模型相关的参数，方便大家去做调校。跟playground很像。所以其实某种程度上来说，咱们也可以把这个界面做成跟playground一样，去设置各种各样的参数都开放出来，这样是不复杂的。好，我们回到这个。目录里面来，我们可以再看一眼，就这个ChatGLM。
	对，然后这里其实有提到，我看很多同学在问，macbook还想跑各种，有人在MC macbook的这个笔记本上提供了一些方案，就这个chat GM的CPP。然后我看朋友圈有些朋友也把它跑成功了，跑起来过，但我自己没有实际去跑过，如果有这个实际想测的可以去试试。但可以预见到的是它会比较卡，它可能会相对来说比较卡一点。它需要使用应该是M系列的那个芯片，不是英特尔的那个芯片这个是一个。然后也有GP那个GPU上面的这个方案，我们在上节课有讲过。TPU我们使用collab的时候，它会开放给咱们去使用这个TPU。然后如果咱们能用这个TPU，那么也是OK的，可以用这个方式。还有一些花活，就是这个fast LLM，这个我没有用过，然后它是在手机上跑的，可以去试试。然后还有一些跟质朴合作的平台，就是点开的这个叫model wear，应该是要合金。我记得他中文名叫，然后他们最早做深度学习的这种类似于点lab的这个环境，现在也提供了大语言模型的这个环境，是大家可以去尝试的。
	如果咱们手边没有GPU的话，然后这个评测结果其实有对比一下答案。我看有同学在问，咱们在刚刚这个课件里面有提到chat GM26B比第一代强，然后比第一代强。然后具体强的有不同的测试。MMU和c evo分别是对应的语言，英文和中文。然后GSM8K我们介绍过很多回了，这个数学推理那个应用题，BBH，能看到这里的这个chat GM2的6B还是挺有优势的。然后它还有一个12B的版本，然后相对来说分数更高一些包括GSM的这个8K、BBH，推理性能这边也有一个参考值给大家，就是每秒钟能够生成的字符，然后具体的这个字符数配置，你看这个很快，一秒钟生成了44个字符，但它的测试硬件是A100，所以大家不要想当然的认为你在macbook上有这样的一个速度，还是要根据你实际的GPU的硬件来实测一下。
	这边是有给对应的需要的显存资源。我们说要把这个模型加载起来，先不管它快慢能加载起来，最重要的是这个显存大小够不够，这个是一个硬指标。然后我们也在那个transforms的transformers的这个库里面有看到，就我们这个方法在这儿，其实也能看到，你可以再加一个后缀杠int 4去获取量化后的模型。
	那么它的量化有这么几个不同的等级，比如说16位浮点数的，八位的这个int的，四位int的，然后这个是最小规模的模型尺寸，你可以理解成这个应该是极限了。如果连int 4你的硬件都跑不起来，那你可能得想办法换一个硬件来跑这个CGM two 6B的模型了。然后能看到这个准确率，就是ACC，它的精度有一定的上降低，但还在一个可接受的范围内，至少从这个绝对值上来看是这样。然后接着我们看到这个从本地加载模型，就是指我们第一次运行之后，这里会从hugin face上面去就hugin face上面有这么一个路径，我们可以看一眼亮models。
	这个是hugin face上面THODM托管的chat m256B，然后我们from free train model就会从这儿去下载它的文件，下载的是什么呢？就是这里的七个分片的文件，这里就会有同学发现怎么是py touch对吧？对，因为它默认用牌套实现的这个节省文件是给这七个分片加了一些原数据，包括它的token nier也在这里面。然后我们还看到有int 4对吧？是因为int 4也是一个独立的，跟我们这个rapper很像，跟github的rap很像。这里是它int 4量化的对应的页面，这些都不变，主要变化是在这里面。大家能看到明显模型都不用分片了，这个直观感受大家能感受出来吗？就是之前那个模型挺大的，为了防止大家下载过程当中中断，或者乱七八糟的分成七个，这个就一个就够了。
	好，这个是我们能看到的下载模型，包括transformers这一部分，还有一个就是我们在怎么样去运行网页版的，这个我就不演示了，因为之前都已经演示过很多回radio了，我们就节约点时间。还有一个web demo，跟我们跑之前实战的radio一样，python demo这边就有了。什么是量化？这节课刚刚讲过的这个同学是中途进来的吗？什么是量化？回到1个小时前有跟大家专门讲了机器学习的，花了五分钟的时间给大家讲量化是什么。
	然后命令行的demo也有一个，但这个命令行demo不是特别好用。因为命令行本身这个标准输入，如果你有一些想要回车干掉什么的，会引起一些报错。它的这个命令行的demo字符串处理还没有那么的健壮，推荐还是使用这个radio，或者是使用这种流式处理的。还有这个stream net，这个stream net需要额外下载一个包，把这个stress装上就好了。我印象中在在他的requirements里面应该也有这个包，我们可以搂一眼。
	对，就这个包，大家应该装上的话，就会把它带上了。好，然后我们再看一下它的p two。就这个P2我们有提到思维链，这些东西都是属于提示词的技术，然后需要在大规模的模型上才能体现效果。那么HIGRM这个团队也发了一篇NLP领域的顶会，就ACL。ACL是NLP这个领域的国际顶会了，应该是非常顶的会议了。然后在2022他们发了一个prom tuning的文章，叫P20V2。他们之前还有一篇文章叫P2。
	所以第二代P20v two首先要定位一下，就是我们知道在看理论在学习理论片的时候，这个GPT3当时就提出来我们用in context learning方式不去做five tuning，不去做模型的微调，模型参数固定。然后我们只是通过在上下文中学习，或者通过提示词来改变模型的输出结果，相当于我的沟通技巧变强了。我这个例子我说了很多回了，就大模型很强，但他听不懂人话。然后我的沟通技巧变强了，我能让大模型听懂我的话，那大模型没有变，他没有再去是读四年大学了，他没有学新的知识了，但是他的结果就变好了。所有的prom tuning其实本质上都在干这个事儿，就大语言模型的模型参数本身没有做什么变化。
	然后P20的优点是什么呢？就是说相比于fine tuning来说，p tuning的成本很低。因为它的模型参数是固定的，所以它不需要去做反向传播。我们刚刚有讲过，它不需要去修改这些模型参数，那么它的成本低。同时因为成本低，它就能够去prompt通灵更大的模型，然后这个是一个比较优势。
	第二点就是说如果我能不改模型参数的前提下，还把你的效果做的跟模型微调一样，那就牛逼了对吧？因为模型微调是很贵的。大家知道一个GPT3.5要去做模型微调可能是几百万，几百万去微调一次模型，跟咱们这会儿心心里面在想去租一块GPU这个是完全不是一回事了，对吧？
	那么P20至少在他的论文里面有提到，在一些中小模型上，包括一些序列标记上，它的效果已经跟翻译成差不多了。然后为什么是要强调中小模型？也是因为这个P20本身它的聚焦点就是在这个上面。如果我们是非常大的模型去做这个prom 20，结果未必非常好。这个是一个疑问，是一个需要再去研究的点就如果我是一个GPT4这种万亿参数的模型，我去做p tuning它不一定非常好。对，可能更倾向于去做prompt template或者说做agent的方式，就不要把这个图灵做那么深。对，这是这么一个逻辑。
	那么具体怎么干呢？就是说到这个P图你具体怎么干呢？其实在仓库里面有这么一个脚本，叫train点SH。然后train点SH里面的这些参数就是跑过脚本，大家应该都跑过了对吧？只是之前跑的python的脚本，这里面我们能看到有三个是命令行的参数和给到这个train SH，就是实际执行里面用python去执行的这个python要给的参数。
	上面三个参数稍微解释一下第一个就是这个soft的prom的长度和学习率，这俩就是经典的所有的learning可能包括这个prompt或者说instruction to里面都会提到的。这儿这个值应该比较简单，大家可以去稍微看一看LR就learning rate。对，然后number GPU是指如果你的服务器上面或者说你家的电脑上面有多张GPU卡的时候，可以根据能够使用的GPU卡的数量去做设置。这个也是PITTCC本身支持的，就python框架支持的。
	然后刚刚有提到这个P20本质是冻结这个模型参数的，它不会去做fine two，这个模型参数是不会变的。然后这里我们刚刚其实在讲这个之前讲过模型也有多种，有完整的chat GM two没有量化过的，也有32K的长上下文版本的，还有int 4量化的。所以这里有一个参数叫quantization ation beat，就是原始模型的这个量化等级。如果你不弄的话，默认就是FP16这个精度，就不加这个选项的话。然后我们还看到，就刚刚提到的，假设我们这个quantization bit设置为4，它就会用int 4对吧？默认配置还有这个per device trim bead size，这些都是机器学习深度学习的基础概念。如果大家对这个是一头雾水，我相信只要认真学过课程的人，对大模型的很多概念肯定不是一头雾水，应该很清楚了。
	但是这个技能数你没点过，那你可能就会有一些迷茫。比如说这个batch side是什么东西？其实就是一批训练的数据的量，就是数据太大了，不可能一次性丢到显存里去去训练。这个分批就跟我们那个南线要分块一样，这个by side是这么个概念，然后包括我们做反这个前后向传播的时候，这个部署就这么一个事儿。
	所以整个参数其实都摆在新的SH里面，大家当像这些model name or path这些名字很清楚的，其实就是直接从transformer的从transformers这个库然后去找hugin face上面的那个项目路径，包括输出的这个目录，巴拉巴拉这些东西这些。Evaluate EVAL这个刚才也讲过了，是对应着正向的这个推理。然后train的话就是推理加上反向传播，包括训练的步数，然后多少步一次来做日志，然后多少步一次一次来保存模型的这个文件，中间结果等等，都是一些传统手艺，就是深度学习的传统手艺。好，当然要跑这个训练，肯定要用GPU。因为他的第一，你的生命很可贵，对吧？别把你有限的生命拿去等待训练了，你又不是神人，一次就训练好了，吧？你越少的时间花在训练上，你越能够快速的去吸取一些经验。好，我们说要留1个小时时间来做全课程的这个QA整个生态片其实是一个开拓眼界的一个篇章。
	然后这个生态片其实讲了几个部分。第一个就是hanging face这个库非常有价值。大家在之后的工作和这个编程过程当中，跟大语言模型打交道，在上面能找到有效的models，dogs，datasets, spaces, 还有各种各样其他的好玩的一些新玩意儿。也许他要去做发展，然后在实际去做一些私有化模型的时候，比如说chat GM26B，你会发现要考虑硬件的事儿。硬件没有那么玄乎，你把它拆开来看，其实就是显存最核心的。因为模型参数是需要被加载到显存里的，显存不够跑不起来，第二就是算力，就是我们每次说这个4090、3090到底是增加了什么？好像大家都是16GB显存，那个T4是16GB的显存，那个消费级的卡也是16GB的显存。他俩除了企业级和消费级的名字上的差距以外，有什么区别？
	我们可以看到内部的真正的大块的处理器的单元是SM，就是流式的并行多处理器。然后流式的并行多处理器里面我们可以看到里面有各种各样的框，用来跟我们的量化相关的。实际执行的。有我们的16位的浮点数的，32位的、64位的，以及我们看到的tensor call，用来做高维的矩阵运算的。
	然后我们还看到最后这一部分，就是我们以一个具体的也是被广泛的传播的这个GLM这个大模型家族。它其实除了我们听到的chat GM以外，最核心的是GRM本身是一个backbone，是一个网络结构。它是一个多目标的自回归的网络结构，然后也是一个双语的共同训练的方式。然后这个GM作为基座模型有很多不同的尺寸，有130B的，有6B的，也有10B的。这些不同的尺寸都完全取决于这个团队对于训练数据的处理。所以这个数字怎么来的其实并不重要，是一个经验值。
	然后基于这个基座模型可以去做各种各样的应用级的模型。比如说ChatGLM，比如说visual GM，web GLM或者是其他的GLM，包括code GX这样的一些代码生成的模型。然后到最后我们会发现ChatGLM它要跑起来，一定是需要GPU是比较实用的。然后也有一些在交互层面上的一些工具，比如说radio前面我们已经教过了，然后命令行的工具不是特别好，适用于聊天模型。然后有一个stream net，这个也是一个图形化界面的一个工具，大家有兴趣可以去把它玩一玩。但这些都不是本质的点，核心只是说提供了一个。
	然后在这个生态篇我们讲完之后，我们有一个简单的小作业。是希望大家能够把之前在用能权实现的open s translator这个模型的基础上，这个代码项目的基础上，把我们的GPT改成ChatGLM6B但是因为这个本身需要咱们的一些，自己需要去找一个设备来部署。这边不做这个强制性的要求，如果咱们的性能效果没有达到特别高的要求的话。好，这个其实就是今天的内容。我们再来花一点时间回顾一下整门课程整门课程的这个内容。其实整门课程我们是九周的时间给大家做了一个从基础的理论到开发的一些必要的一些coding也好，或者是咱们了解的这个技术背景也好。直到我们又学习了OpenAI的API，做了一个实战项目到南券，然后再到最后的这个生态片有一个全面性的了解。在回顾整个之前，看看大家对于HIGM的后面这部分的内容有没有什么问题，我们五分钟的时间先回答一下。
	微调没听懂，跑一个脚脚本就OK了。微调没听懂，来来来。
	第一就微调，其实就两件事儿给大家同学们讲一讲。第一它有提供一个自己的数据集，这个数据集咱们可以下载下来，然后按照它的格式输入。这个格式其实就是一个根据你要做的目标来的。就比如说你现在的模式是要让他做这种广告词的生成，那这个输入就是这个content，输出是咱们最终要用的时候希望模型输出的一个结果，然后这是一种方式，那这种方式可以广泛的应用于什么？用于各种社交媒体的场景。比如说小红书，或者什么乱七八糟的，或者是电商的平台都可以。因为这样的场景里面很多输入都是标签。然后还有一些场景，比如说我们的这个销售机器人，也能往这个场景里面去套，那它的输入可能就是一个问题，然后回复是一个答案。
	但是这里需要告诉大家一个点是微调本身这个指令微调本身它不是一个确定性结果，所以需要大家去多试，这个不是靠给大家演示一下大家就能学会的。甚至是说咱们拿同样的硬件，同样的参数训练出来，结果是不一样的。所以为什么让大家用稍微好一点的硬件去租一个GPU，是因为本身通过这样的方式，大家要去理解微调这个过程，跟咱们的然后有的同学问微调的数据是一问一答的吗？不是，这个是根据你的使用场景来的，好吧。
	强化学习和fine tune是什么区别？这是完全不同的两个概念。Find two是针对模型参数去进行微调，然后强化学习是一种你可以理解它跟深度学习是一个层次的概念，它的区别在哪儿？是因为它的我学习过程，因为核心是表示学习。我们讲媒体的时候讲过就是一堆数据，然后学出一个它的分布出来。这个是深度学习也好，单元模型也好，比较叫本质的东西。
	但强化学习不一定，强化学习可以是学习一个策略。比如说阿尔法狗我学的是下棋，那我学下棋的话学的不是一个分部，我不是一个判定性的一个条件。比如说我有这个呃给你一个一张图片，你告诉我里面是不是蒙娜丽莎的这个图，蒙娜丽莎的这个画。我给你一个图，你告诉我是不是泰坦尼克号里面的截图，它不是这样的一个事儿，更多的是给你一个环境，这个环境你是你在训练一个策略，然后这个策略你要学习，就跟我们讲ChatGPT的训练过程一样，有一个策略。这个策略是用来给大语言模型生成的结果去打分的，然后这个策略是你学习的重点。
	那要把策略学出来，核心有两个，一个是你要构建一个环境，这个环境是跟你的真实的使用它的环境是一致的那为什么阿尔法够好？就是因为围棋的环境条件定的非常清楚，也很客观，这个环境是好定义的。但是环境好定义其中包含的另一个概念很难定义，就是这个real word他这个奖励机制要怎么设计，这个就是阿尔法狗优秀的地方，就是围棋的规则什么的好定义。但是我的策略学习的本质是我要做出一个操作，我要把一个棋子下在哪儿。下在哪儿之后，这个reward的这个机制是告诉你下过去应该有个打分，它不是说像深度学习一样，是有一个标准答案跟标准答案差多少。
	而是我有一个奖励机制，这个奖励机制给我的一个操作去打分。如果这个奖励机制设置的足够好，我给他的打分足够好，那他就能学习出来一套非常好的策略。但是这个奖励机制本身也是在学习的，我不知道描述清楚没有。环境需要你去设计好，这强化学习是非常重要一个点。奖励机制需要你去设计好啊，奖励机制设计好，你就能学习出一个策略。这个策略可以是下棋，可以是机械手臂，可以是自动驾驶。
	微调不在这个不是一节课可以讲明白的。微调的核心是大家要去多试，这个是第一个点。第二就是说微调本身是一个独立的技术手段，大家如果感兴趣的话，可以先拿我们现在已有的课程里面教过东西去试一试啊不要想着这个听就能听明白。对你先多试试，然后怎么样去做微调。刚刚已经讲过了，就那个train点SH的脚本大家去跑一跑，至少你先把这些样例的数据和我们课程里面提供的这个数据去跑一跑吧，好吧。
	还有同学问P图微调之后会灾难性遗忘，这是普遍现象吗？我不知道这个上下文是什么，是指在G2M6B上面是不是普遍现象，还是指所有的大语言模型都会这样，我印象当中不会是所有大语言模型都会这样，6B会不会这样？我是看到有的同学反馈会会遗忘，甚至会有幻觉。
	然后微调和lara的微调，lora是一个低质的一个微调手段。对，然后还是抓到本质。大家微调就是去调整模型参数，然后指令微调不会去动模型参数，就是改变的是沟通技巧。我不把这个说明白没有，然后他会去对它的前置部分做处理。我们还讲过这个大语言模型，它它是一个DNA双螺旋，它前处理prefix这个encoder这部分就是前面这一部分的这个encode的部分是可以去做调整的，这个调整不会动后面那个大语言模型。然后P20调的是前面那部分，我不知道这个事情表达的清不清楚，然后他调的前面这部分，相当于模型的推理知识都没变化。但是因为你做了P20，所以你接下来跟他的聊天会更顺畅，因为你们越来越能听懂对方在表达什么了，就像我说了这么多遍微调指令，微调和这个P20的区别，我的我的认知没有变化。对，但是你们可能在逐渐的就能理解我说的话，也许你们的这个认知是到位的，只是你们可能有些词什么没理解到位。
	不错的性能需要多少数量级的语料进行微调，这些都是玄学问题。大家去多看看这个几个基准测试，先了解你的场景，对是偏语言的、偏推理的，还是偏什么数学应用，还是什么的，这个比较关键。然后多试一试。我对于所有想搞叉叉learning，但是想要问出一个金标准的同学的一个回复，就是你去多试一试，多训练，没有金标准。
	对，有个同学问请求API只能排队吗？对，负载均衡的传统手艺，web server的传统手艺。
	还在问微调的同学回去看一下理论篇的部分，对讲GPT的。好，那那我看问题也比较熟练了，我们再回来讲一讲。我们再回到整个课程来回顾一下，就是在问微调问了这么多，对，首先说句实在话，绝大部分同学用不到微调的能力的，还不需要咱们去修改模型本身。先把现有的模型用好吧，把prompt学好，把怎么样去构建一个大语言模型的应用和agent的机制给用好啊不要想着随便去改模型。对，就是你回想一下读书的时候，你有没有去改这个中考和高考题？你没有，你改的是你自己，对吧？就是别轻易去动这个量纲和这个机制本身，先把自己力所能及的先玩一玩。这个是一个我自己一路走过来的一个经验，也分享给大家。
	有特别聪明的同学这些都已经走过了，要去玩微调，那我相信他可能问的问题也都不一样了，那么我们回到刚刚微调这个话题上来，其实本质还是这个理论基础，大家可能还是需要再回去都看一看，这个理论基础还挺重要的。如果这个理论基础没整明白，你后面会花很多冤枉的时间。比如说我们在理论篇讲了什么？我们在理论篇讲了整个大语言模型是怎么从14年的注意力机制发展到2023年。就这99到10年的时间到底做了什么事情，有什么本质的变化，有什么没有变？
	那回到14年的时候，注意力机制刚刚提出来，alignment和translation这两个任务在一个attention mechanism或者说在一个神经网络里面进行了学学习和训练。这个是注意力机制最核心的，也是今天其实这个机制，这种思想，这种方法论到现在为止也是一个可以一直去沿用的方法论。我们的多模态也是需要解决这个问题。就我们刚刚提到的，怎么样把语言概念和一个语言文字的in bedding结果和我们的一个图像识别出来的embedding结果对齐，这是很难的，但是这个对其实是需要突破的。
	接着我们学了transformer，transformer是一个在注意力机制之后我们讲的非常重要的概念，也是很多大元模型的基础。因为GPT的T就是transformer，transformer相比于注意力机制做了什么事情，他们把我们的注意力机制从一个简单的对齐和机器翻译，变成了一个抛弃INN这种纯串行化训练的新的网络架构。这个网络架构从17年那个节点来看，有性能上的优势，因为它的抵扣的部分是可以去并行计算。这样我的decode只要层数越深，并行度也越高，这个能带来算力上的一个天然的优势。因为本身那会GPU多，现在GPU不够了，因为transworld太大了，这个GPT搞的。
	然后性能提升的同时，transformer还把alignment function，就大家看attention的这个论文里面，aligned function其实是一个比较简单的那transformer就把它变成了一个dot product点击，并且还做了多个做了多层的多product，做成mart head、marti head. 就像我当时给大家讲的一个概念，就是击鼓传花。A告诉BB告诉C以前就一条路。只不过这条路上从最早的RNN a告诉BB告诉C变成了有一个attention机制，A告诉BB告诉C同时A也直接告诉C然后CT这两个人的表述之后，得出了一个结论，这个是attention的机制。那么transformer的机制是说我都先不管三个人的情况，我就是A要告诉B，我可以有多种方式去告诉你，一个是写纸条，一个是录音，一个是录像，对吧？那这多个方式就是marty head的最大的价值。
	这多种方式的信息渠道的表达必然各自有优缺点。那么muti head就能把这些信息统一起来，然后在decode的部分它增加了一个mask的muti head attention，使得整个decode部分更偏向于一个生成式的网络。因为它前面做了这个掩码，然后后面是直接去生成。然后这个过程使得transformer这个事儿逐渐开始把这个语言内部的关联关系给学出来了。我们看到transformer里面its包括这个law application这些一句话里面内部的关联关系指代关系学出来了。所以OpenAI就发现自然语言处理可以从以前的各种细分任务变成一个语义理解，就把一个大模型搞出来之后，我真正把文字内部的这个语义关系学出来的一个新的研究方向。所以GPT和bert都是在朝着这个方向去努力。
	只不过GPT依然延续的是以decode的为主的这么一个生成式的路径。而bert是做了这个by directional的transformer，双向的增强了它的语义理解能力，从右到左我也可以去学而不是严格意义上去做这个从左到右的mask。那么GPT它自己从第一代到第四代的过程当中，也经历了一个迷茫期。然后直到后面以力破巧，通过足够大的transformer网络和足够多的数据学出来的这个语义理解。并且在GPT3到GPT3.5的过程当中，有开创性的去做了很多的tricks，包括使用代码来进行训练，使用指令为好啊，使用各种各样的黑科技在ChatGPT里面，比如说刚刚说的PPO，就是我们的web GLM也在沿用的这个策略，让人来给GPT生成的结果打分，然后评价。最后一路过来有这个ChatGPT，然后有线上的数据，包括现在为止ChatGPT也有这个线上数据的反馈。
	然后GPT4是开启了多模态的一个很重要的一个新的研究方向，也是现在所有的大语言模型都想要去攻克的一个未来的研究方向。然后我们就讲到了提示学习。在提示学习讲提示学习之前，其实就有GPT3这篇重要论文提到的in context learning，在上下文学习的这么一个概念。这就要提到刚刚很多人都在问的模型微调。为什么现在模型微调不再被这么多人去强行去提？这就是因为太大了，大语言模型太大了，你微调不起。简单来说就是没几家有这么多GPU能够把这个大语言模型加载进去，然后再去调整它的参数。这就不是普通人家玩的事儿，所以大家执着于这个微调的意义不大。
	核心是大家好好想一想，前面七周的时间是怎么样把我们从一个大元模型的小白变成可以做这么多实战作业的成果。然后怎么在这些实战作业的成果上，进一步去叠加你的场景和prompt的优势。因为因为这些理论的部分，框架的部分，然后这些基础概念的部分，包括实战的部分都可以教。但是这也就是一个定下来的东西，真正大家学会之后可以去掌握的核心是基于他举一反三的去把我们的prompt去做设计。基于你自己的场景，就像我们的实战销售的机器人，基于你自己的场景去造数据，可以做各种各样的演进应用。那么prompt learning就是去回应大模型太大微调不起，或者说微调不一定微调的更好。就是OpenAI的科学家他微调一次的结果都可能是更差。
	那咱们现在想问怎么样能够微调的很好？这个问题你自己去现在想一想，是不是一个很难回答的问题，因为微调本身就是一个概率上的事儿，然后模型越大这个微调越难。那么为什么现在GPT3、GPT3.5、GPT4还在用2021年的数据？一方面是因为成本的问题，一方面也是因为要在训练一个更牛逼的机，这种模型是很难的。这个是从另一个角度去跟大家讲微调。然后既然微调这么难搞怎么办？大语言模型我们还是想要把它这个潜力挖掘出来。
	所以这个时候提示词学习或者说提示学习from learning这个概念出现了，就我不去动你的这个大元模型的参数做了，我在外围去做工作。不管我是把我的输入的提示词设计的更巧妙，还是说我多次的去调用大语言模型，然后最后三个臭皮匠顶一个诸葛亮一样的去做这个学习。对，然后这个是一个prom learning的一个核心思想。
	然后有了这个思想之后，我们可以开始去做一些简单的开发了。比如说我们开始了解embedding，通过学习embedding我们了解了这个表示学习，或者说这个计算机的表示学习和in bedding之间的一个关系。然后我们知道了其实深度学习也是一种表示学习。然后我们的计算机的数据表示经历了一个从记录简单的文字数字到能够开始统计文字数字。然后使用深度神经网络之类的表示学习的方法来学习这些大量的数据和文本，然后通过不断的累加，直到我们产生了大语言模型。这一类既有深度学习网络，又有这个语言模型的优势。同时还利用了in bedding的优势，把我们的这个语言模型不同的表达统1到1个向量空间。这个向量空间里面又去学习我们的这个大语言模型的神经网络，这些部分如果大家有遗忘或者还不清楚的，一定要多看一看。
	这些基础的理论可以指导我们走得更远，那么通过embedding的学习，我们知道embedding ing本身这个embedding模型就有一定的浅层次的语义推理能力。我们能去做美食的评论，相关联的一些搜索，甚至做一些聚类的搜索等等。然后通过这些embedding，我们了解到了浅层次的语义能力。In bedding能学习，但深层次的语义理解能力还是需要靠大语言模型。
	OpenAI提供的大语言模型，包括通过今天的这个benchmark我们也能看得出来GPT4和GPT3T3.5还是毫无疑问的，比其他的大语言模型要拉出很多个身位的那这个时候如果条件允许，我的建议就是直接去使用GPT的模型。就不要想东想西的对，就是你现在要请一个人来帮你携带。你是想请一个一天能帮你解决问题的人，还是100天都解决不了问题的人，然后你还要去给他做教育辅导。这就是一个很简单的底层逻辑。
	然后当然大元模型本身开源社区也在迭代，我们也能看得到他们的一些不断的变化。但实际情况就是这些还没有追上。GPT3.5的开源模型本身也是花了很多的成本训练出来的。不管是nama 2还是GLM这个开源模型都对商用有很多的限制。所以那些纠结于私有化部署的同学，我在良衷心的劝一句，就是你如果能用GPT就直接用，然后你用这些有化部署的模型，你要折腾一大堆研发团队硬件最后效果还不行完事儿。你真要商业化的时候，人家又不给你用了怎么办？对吧？这些问题更本质。
	然后我们了解了OPI的API之后会发现，不管是去调它的API还是用能券，都是可以很方便的去跟open I的大语言模型交互的一种手段。然后我们也讲了，如果我们要去做合规，它提供了moderation这么一个模型，就这里moderation的这个模型可以帮我们把大元模型生成的结果去做一些判定，有一些即兴的判定。比如说这个社恐，然后这个道德方面的一些问题。然后我们也介绍了OpenAI自己的take token，用来方便的去统计不同模型的这个token数量。然后除了这些以外，我们还初步的开始使用OpenAI的这个API。跟大家简单介绍了一下怎么样用open I的API去生成代码生成这个文本，讲讲笑话等等，包括聊天机器人，我们开始跟这个OPI去用上下文管理来做这个ChatGPT类似的应用。然后在应用实践这一节，我们跟大家讲了用OpenAI的大语言模型，核心要把它的提供的这些聊天模型的多角色赋能给用好，这个也是一种prompt的学习的技巧，多角色system应该分配什么样的prompt？User和这个assistant应该怎么样去保留这个聊天记录。
	然后在生成的过程当中，有没有一些比较好用的描述方式，其实那个时候我们就开始引入类似于prompt temp late，就提示词模板的东西，当时在分步骤执行这个复杂任务，包括评估模型的输出质量，代码调试助手等等，我们都加了各种各样的花括号，然后大括号等等。这些括号就是用来作为一个占位符，在真正输入这个提示词的时候去替换的那这种方式就是一个典型的prom template的一个方式，动态构造提示词，接着多角色是一个GPT3.5和GPT4首先提出来的概念。然后接着他们又提出了这个function calling的概念。就我们也那function calling目前也是大部分大元模型都不具备的能力，我们使用open I的function calling的能力去构造了两个案例，一个是天气查询，一个moke的天气查询。一个是直接用circle light去生成这个circle的代码。这些例子都是可以方便大家去再扩展的。当然如果真的工作中想要把它扩展，这些代码都是很实用的，可以去进一步扩展的代码，核心是咱们是不是真的有这样的一个恒星和需求。
	接着我们去做了两个实战项目，作为我们基础篇的结尾。一个是贯穿整个训练营的OpenAI的translator，我们做了市场分析，这个是一个非常旺盛的市场。包括上周facebook还开源了类似的一个翻译助手。只不过它支持多种形态的输入，音频输入和文件输入，所以这个需求很大。然后我们做了详细的模块设计和分析，然后也讲解了一个PDF解析的开源库。然后基于这些，我们做出来了一个open a translator的一个项目。
	然后给大家一个选修实战作业，就是我们的ChatGPT的plug in，我们把样例项目做了实现，同时把之前的function calling里面的天气预报第一次让大家接触了AI agent的这个实操。但是当时为了这个不给大家造太多的概念，没有去提出来。ChatGPT的plug in或者说function calling都是一种典型的agent的实现手段。
	在ChatGPT的plug in实战这一节教大家用了第三方平台的对接。在这个ChatGPT的plug in这种场景里面，我们第一次实现了agent把第三方的天气预报的API和ChatGPT的plug in，以及ChatGPT大语言模型本身去做了一个对接。从这个视角来看，在bug in里面，我们聊天对话的这个窗口里的ChatGPT模型就是那个LLM。而天气预报的这个第三方平台其实是这个tool，是我们action要实际操作的这个内容。
	然后我们的ChatGPT承担了生成内容和调用我们天气预报服务工具的这么一个角色，对比了这两种形式的agent，一个是function calling。以咱们的应用为主，我们的这个应用在前面。那这种形式我们需要备案，因为我们如果我们在国内提供面向国内的用户的话，需要去做这个备案。我们是一个主要入口，而这个入口是具有生成式的能力，然后第二个，就是我们的这个ChatGPT pluggin这种形式，那就是ChatGPT作为入口，它会有各种各样的pluggin作为它的工具，由他来判定应该调用哪个工具。所以光从这个视角来看你就知道open I的野心非常之大。因为一个plug in其实就要对应着一堆的工具。通过对接各种各样的plug in，其实chat BT的大语言模型的能力会有更进一步的提升。他就知道什么场景应该调用什么样的two，它的prompt的记忆水平就会更加的越来越高。
	然后从这个基础篇的回望，我们可以看到理论的演进。我们从最开始的attention到prompt，度过了这十年的核心技术的一个发展路径。然后开发基础上面把embedding和大元模型这个DNA双螺旋模型两个模型都做了详细的拆解。让大家既能够了解理论，同时也能够去接触open I的API，还会有这个jpy tern notebook的demo，让大家去做手的实践。最后为了进一步提升，如果大家对于这个大元模型的开发，真的有长期的想要学习的动力。那么多角色的赋能和function calling是最重要的技术，这两个技术如果能够咱们能够举一反三的再去学，不断的去练去用，那么这个会直接有生产力上面的一个比较大的提升。然后最后两个实战项目的基础篇，作为这种项目级的代码仓库，让大家能够完整的去体验怎么样用open I的API去做一个智能翻译。
	接着我们来到了这个进阶篇。南茜其实是目前最火的大模型应用的开发框架。在这个里面我们讲了各种各样的大语言模型的模块，然后包括我们的这个data connection model chain agent，我们的这个memory。然后我们在这个里面核心有三个实战项目，分别是我们的智能翻译助手的实战项目，使用我们的大语言模型和这个prom的合体的这个LM券，来接管之前的智能翻译助手的自定义的那一套实现。然后同时也做了这个图形化界面radio，然后第二个alt GPT通过拆解年前的源代码的方式，让大家理解实现一个AGI的agent。
	通用人工智能其实也并不需要那么多的复杂的实现。最核心的部分还是在prompt的设计和跟现有的单元模型生态的对接。就比如说我们深度研究了auto GPT的核心是他自己把这个LGBT的tools有做对接。比如说对接了我们的这个serb API能联网，对接了这个读写文件的tl能够做文件交互。然后同时他自己还会有这个prompt template，方便我们去做多目标的拆解。并且多目标的拆解，最后模板的这个原模板里面还会有这个停止的prompt finish。
	然后把我们的所有的生成结果都会用output poser转换成一个符合tools的输入的模板形式。把这个language，把自然语言当成了一个编程语言来实现的一个最佳形式。现在就是alt GPT这种agent的形式。
	这节课也是希望大家能够反复的去看啊，去理解南茜的alt GPT，实现这样的一个套路和最佳实践的方案。然后基于知识库的这个销售顾问，我们用GPT4来生成了数据。然后用我们的这个年轻去实现的这么一个，既有大语言模型作为聊天，又有GPT4生成的数据作为向量数据库的知识库，做这个外部知识的一个接入，这么一个实战项目，它的功能很丰富，并且还有图形化的界面。但是使用南茜，使用radio，使用我们的face，这些向量数据库其实也就是四十多行的一个代码。我们就实现了这样的一个项目。
	那这样的一个项目其实是现在大语言模型时代，真正的比较落地的快速实践的这个项目形态。不需要特别多的代码，但需要你把这些概念整明白。整明白之后，其实你要去做一个类似的项目，就是一天的时间就能够做出来，甚至半天的时间。
	然后你要核心的理解那个造数据的prompt应该怎么设计。你的场景里面应该是什么样的问答形式或者交流形式，还是这种摘要汇总的形式。然后这个整明白之后，接下来在向量数据库的处理上，怎么样能够把这个向量数据搜索这件事儿弄好。我们搜索我们讲了多种形式。直接去找相似度，或者是给一个top 5 top k去回答，或者说给你固定的这个答案数量，也有这个阈值，对吧？那这些东西是非常关键的一些操作，这些操作如果你真的理解了，那么去构建一个这样的应用是非常快的。
	接着我们最后在进阶篇的最后一节课，我们讲的这个应用落地和数据隐私的内容。就是我们怎么样能够把我们的这个大语言模型做到合规。然后既是在国内合规也是在欧盟合规，这两个最重要的场景。然后在开源生态，就是我们这周讲的内容，讲了hugin face，讲了我们的GPU和我们的清华的GM的家族和这个hugin face hugger face最重要的transformers里面的两个模块，top nier和这个model。好，这个其实就是我们整个课程的一些核心的内容，看看大家还有什么问题，我们还有20分钟的时间来广泛的来提问。
	看大家还有什么问题，我们还有20分钟的时间，然后我就可以吃晚饭了。
	我看看。对于猿飞AI领域的开发人员有什么推荐的职业路线？一天就能开发完，让我感觉没有什么壁垒。这个同学问的很有意思，就是我的第一个问题是咱们能一天开发完吗？就是你能一天开发完吗？如果不给这个代码示例的话，那我的壁垒是什么呢？我写了15年代码，搞了十年AI了，对吧？那如果你能一天开发完，那那你已经很厉害了，然后甚至你能更快的时间开发完。
	核心是什么？核心是你真的得把这八周的内容好好消化一下去举一反三的去做应用。首先这些实战项目，这五六个实战项目你吃透没有？吃透之后有没有真的理解他们能应用，他们整个大语言模型的核心就是用。对。然后这些数据变了，你是不是就得重新去想一下，怎么样去跟像数据库交互。然后向量数据库如果我们要做多种不同类型的知识库，是不是要对接多个向量数据库？多个向量数据库之间怎么样去处理，或者说一个向量数据库，但在一个向量数据库里面，如果有相似性的巴拉巴拉这种东西怎么样去处理？这里其实有很多可以挖掘的点对是一个开放式的问题。
	有的同学问这些内容可以自己创业吗？这个跟课程内容无关，其他同学可以回答他，这个我不太好回答。AIGC的创业潮，中小公司的机会点会是哪些？自己训练大模型，甚至微调都没什么意义的话，大家不要认为就是我还是劝一手啊不要天天想着创业。对，创业不是普通人能干的这是第一个结论，就创业不是普通人能干的。第二，创业承受的压力和和各种各样的付出是你现在甚至都不能想象的。第三，在创业的早期阶段，挣的钱肯定没你自己现在打工挣的多。
	这三个事情你想明白了之后才开始想，那我要决定创业了，那我要做什么事情？如果你说你想创业，你连想做什么事情都不知道，那你创什么业呢？对吧？这个就是少谈一点意义，多谈一点场景，多谈一点你怎么提升自己对。再举一个例子，就OpenAI搞GPT的时候，到今天为止都有人说GPT没有意义，所以谈意义没有价值，做出来有价值，拿到结果有价值。
	应届生面试大语言模型产品相关岗位重点考什么？吃透课程的demo能够应聘AI产品经理吗？有个同学问我觉得肯定可以。你要相信你把这几个实战的demo整明白了，你可以秒杀市面上99%的人，这个我绝对没有吹牛，对。
	Prompt需要系统的学习吗？基本的一些prompt的文档比较简单，但是需要完成具体任务的prompt比较难写，而且不稳定。这个有什么好的学习资料吗？好问题，这个我也在整理，就是prompt要怎么样去做？首先整理prompt一定是一个未来的方向，你看质谱清言就知道了。对，就是包括有很多的网站，像prompt hero，但是这些网站都不系统。
	就是有很多网站都在记录prompt，但是prompt不是一个万金油。基础模型变了，同样的prom可能结果就不一样了。所以要系统的学习prompt，最好的方法就是同学先回头把基础篇理论部分的东西吃透，然后把那三个典型的prompt learning的形式学明白，接着才会去想你要做一个什么事情。因为prompt就是一个沟通方法，根据你要做什么事情，去找一找有没有一些已经写的比较好的prompt。然后因为你又有了这些理论基础，你就知道它的好在哪，而不是直接抄。就是你要知道在没有大语言模型之前，一个好的程序员和一个差的程序员的区别就在于差的程序员只会抄抄完之后，代码稍微删减一下，能能跑起来我就不管了。但一个好的程序员是真的整明白了最佳实践和k study，还有他的一些核心原理。它其实是不需要抄的，它是消化了，它消化之后它甚至可以重写一部分功能，写的更适合现在的代码。Prom也是一个逻辑。
	老师为什么用GLM6B做embedding，还要用OpenAI的key？这个问题的上下文是什么？这个是有什么上下文吗？我不知道。
	请问在垂直应用研究中，有没有必要做一个专门的benchmark来检验各种调试结果？Benchmark的制作有什么讲究？这个可以去看一下agent benchmark的这个项目，或者你可以看看各种各样的benchmark的论文。好吧，这个我们不再展开了，这跟应用开发本身相关性不大。然后做benchmark是一个公益，我就这么讲，如果你不是一个学术型选手的话，做benchmark是一个公益的事儿对。就说一句大一点的这个发散的方式就是benchmark，就是出高考题。你说你说你然后你把你现在这个问题套成高考题，你你你你就品出这个感觉了。对。
	然后还有个同学问网络政府访问的这个事儿，我这边不展开了。因为这个问题比较敏感，也跟开发没关系。对。然后至于备案申请的这个回头看一下，那节课我讲的非常细，就是什么情况下备案，备什么案，面向什么样的群众和用户。
	还有同学问ChatGPT，我是怎么利用ChatGPT提效的，如何写PPT，写教案、写代码？这个类似就去去看一下这个纹身图纹身代码的各种事例。然后这个教不会，这是十几年的一个know how的积累。我没法说每个要怎么写，因为它是根据上下文来的，我是把GPT当成一个特殊的人类来理解的。然后你就知道怎么样跟他沟通了，就跟你怎么样跟特定的员工沟通一样。老师讲一讲info，实践角度有哪些组件不只是硬件，这个能具体说一下吗？这个是什么意思？ChatGPT plug in搞的一批创业公司没有生存逻辑了。对，很99%的创业公司是没有必要存在的。
	如何用开源的模型把业务做好，这个很关键，这个结论我不太认同。对你想象一下咱们用的macbook开源了吗？咱们用的很多东西都没开源，商业模式怎么就跟开源硬绑定了呢？然后再回过头来看一下有什么问题，大语言模型对工业自动化是否有什么帮助？例如AGVS name算法路径规划，经过这个课程看不到大元模型对这样的领域开发有什么效率提升。
	首先每个领域都有自动化的需求，我觉得马斯克有一个五步工作挺挺对的。就是这个课程第一节课我就在讲，大语言模型也只是一种技术，它没有凌驾于任何技术，他跟街边修鞋的都是技术。按照劳动人民的说法，那只是社会分工不一样，对吧？没有这个高低之分，都是劳动。那好，既然都是劳动，你得整明白你这个劳动场景，你的这个劳动技术的特点是什么。修鞋子的就是要见过各种各样的鞋，他知道这个坏了应该怎么修，不伤害这个鞋子，对吧？什么样的鞋子是压根就修不了了，已经坏了。
	那大元模型的核心能力是什么呢？大元核心的能力从GPT e bert我们第二节课的这个论第一节课讲bert的这个时候就已经提过了，核心在于语义理解。就是怎么样把人类我们讲波普尔的三个世界，讲康德的纯粹理性批判，大家回头好好看一看。我是埋了这么多精巧的设计，希望大家能够好好消化一下。对，就是你们上课40个小时，我做课肯定就是十倍以上的时间了。
	这么多人类的知识数据只是记在硬盘里有价值吗？没有价值。那大语言模型的价值在于首先把这些数据盘活了。原来人类上这么多年历史上还产生了这些数据，以前我都不会知道，我都想不到。现在我能通过大元模型给它挖出来，那怎么样挖？那就是沟通技巧是proved的这个水平了。所以从这个角度去理解大语言模型的能力。好，在这个具体问题上，这个AGV slam等等路径规划，这些都只是技术名称，要从业务视角来聊，什么样的技术有价值才是正确的一个思考方法。
	下一代GPT会不会像auto GPT1样自动执行？不好说，这个没没啥没有太多的信息披露出来。对，但是我只想说agent肯定是研究方向，而auto GPT的核心就是解决复杂问题，就是把这个多目标拆解和外部tos接的非常好用。对，呃，反正长期来看一定是这个方向，因为auto GPT的终点就是通用人工智能。对，有个同学提的很好，聚生智能，这个也是在理论课的时候有跟大家提过的，就是in body AI怎么样把其实银波DAI的核心，你把那些具象化的机械手臂换成我们蓝倩里面的toss是一回事。
	对大龄程序员出路是不是转管理？这个大环境有那么多管理岗，管理岗转吗？我觉得国内的大环境对于大龄程序员这个词提的特别不友好，就是也不知道多少岁算大龄。我记得周末的时候，我看贾扬青发朋友圈，他还提到他在哪个平台上有小红书上面有人在写他的这个履历。然后我当时还在看怎么回事，他自己说这个小红书上履历写的都挺好的。
	但是评论区有一个网友写的是，这人什么都好，就是35岁了。那你说他应不应该焦虑呢？肯定不用，对吧？他多厉害。其实所有的焦虑都来源于就所有的焦虑。我这两年我看各种各样的哲学的书籍，所有的焦虑就来源于你想要的太多了，你做的太少了。对你需要自己去综合一下他们俩的差距，他们俩差距越小你就越不焦虑对。
	然后你先想清楚你想要什么，你想要的那个，如果是钱，你就想办法使劲挣钱。如果你想要的是你的技术提升，那你想要提升哪方面的技术，去回应一个什么样的岗位，从这个视角就比较好落地的去找到你要干什么事情了。其实不用那么焦虑的对，35岁很快的。咱们国家的35岁的这个卡点，我感觉其实没有那么没有媒体夸那么夸张。对，现在大家感觉好像裁员跟35岁不是因果关系，更多还是大环境的问题。
	老师后续想继续学习AI方面的应用，如何继续跟进知识？课堂里面说了好多个可以继续跟进知识的渠道和平台。你先看看一天能不能跟下来，或者一周能不能跟下来。对，还是那个意思。对，就是肯定有渠道的对。
	大环境很浮躁，IT圈太差了。嗨这些大家都都懂。作为产品经理学透本课程以后，想做这方面的工作，找工作应该往哪个方向呢？应该怎么去突出简历的prom？如果要沿着课程内容继续深入学习，应该学哪些内容？
	很好。首先我理解这个课程，如果大家真的都消化好了，你接下来最需要学的东西就是找到一个业务场景去尝试落地。然后如果你是产品经理的岗，多看一看现在的新东西。就比如说如果你是产品经理，你之前在这节课之前有没有去用过质朴清颜，就是这么一个东西。有没有去用过它，有没有去用过原位，有没有去用过各种各样的新的AIGC的或者说大语言模型驱动的应用程序。如果没有的话，你需要反思了，为什么没有？是没有关注到这个渠道，不知道有这些公司还是因为别的什么原因，如果是不知道的话，那就赶紧去follow去关注，然后每天都需要去跟进这些新的东西。
	运是产品经理，然后这些新的东西通过学习这个课程，你能知道哪些是烟雾弹，哪些是真正的技术上的进步。就比如说这个制服青年，你说他有多少技术上的突破吗？这一堆灵感大全本质是什么呢？这个其实很有意思，就是你能看到一些没有学这个之前看不到的一些角度和深度了。然后如果还要继续学的话，产品经理我的建议就是多去看一些新产品，好的产品是怎么做的，尤其是硅谷的产品，或者说国内的这个头部的产品。然后至于to C2B to g那是看你自己的职业规划和你现有的手上的资源。这个就只能具体情况具体聊了，可以加私信，如果我有时间我可以回一回。
	对于info来说，前两天同学在群里贴了一个图，就典型的云计算或者dev ops或者dev sec ops或者说大语言模型的info。就学这个课一直在强调meta learning，就是我想告诉大家的事情是，所有的技术都是为需求服务的。所以info也是一样，不同的需求，不同的时代，不同的技术，它的info需求是不太一样的。
	AI方面招人有什么要求？如果是我的话，我会直接提一个问题，然后让他来现场解决。而且这个问题是可以通过prompt来解决。就比如说我现在面试研发岗，我甚至是允许他使用ChatGPT来解决代码问题的。就是我是不排斥任何生产力工具的，以前可能我还会让研发做这个，就在TITVT出现之前，是允许研发面试的时候编程的部分，你可以用谷歌搜索引擎查任何你想查的东西。那么现在你如果会用ChatGPT，你可以更快的解决这些lead code上面的问题，那挺好的。
	这个是一个生产力的一个变化，所以我不知道还有同学在问这个meta learning是什么意思。Meta learning的核心就是解决你们有一些人提的焦虑问题，就我到底该学什么？对，就是包括我们在开营直播的时候讲讲这个老子的道德经，包括讲这个所谓的周易，就是你怎么分配你有限的生命时间去学习一些更本质的东西，这个是没他那里。而这个更本质的东西可以指导你去很很快速的站在一个更高层次的维度，或者说更底层的核心逻辑去理解这些背后的东西。
	像现在你就是大家很多人在说100家大模型公司又怎么怎么样了，那又怎么怎么样了？技术他们有什么本质的变化吗？你看到这100家公司你的机会在哪里呢？有这么多的大语言模型公司，大家都说找不到工作，我就觉得很奇怪。
	就是你真的把这些东西学会了，你随便去敲一个这100家大元模型公司的门。然后你把这些实战的项目你忘掉这些代码你自己能自己撸出来。如果你有这个水平，我觉得你应该不愁找一个大语言模型的岗位了对。
	后面AI招的肯定越来越专，搞底层，搞应用，搞推理微调，不同场景不一样。现在一锅粥，希望面试的啥都会。所有都有焦虑的同学，就是所有有找工作焦虑的同学，我给一个建议，从明天开始去投100个简历或十个，先去面试个公司再回看看。对，再回来看看。大部分的焦虑都是你自己想出来的，你自己去做的那些事情就是具体的问题和解决方案了，对，就不焦虑了。
	对，包括还有问创业的同学，就是我建议大家把整个课程里面提到的理论OpenAI的API和年轻真的好好玩一玩。尤其是那几个实战项目，其实是挺有代表性的，包括radio对吧？我就这么说，咱们现在的同事里面有几个能够把radio男包括向量数据库这一套加加起来变成一个项目的。然后再用GPT4去生成数据，能做成一个应用形态的，就这一件事儿。这个技术点本身其实你已经站在别人的前面了。那你怎么样把课程带给你的技术上的优势逐步去做叠加。前提是你真的学会了，你真的把这个课程回看你去动手，你有这个优势了，然后你再去叠加。这个叠加就是在课程包括生态篇和这个进阶篇里，讲了很多可以扩展的思路、渠道、方法去跟进，去好好学习。
	你有个同学说的挺对的，大模型这波浪潮原来搞NLP方向的技术，现在要怎么调整，或者说对NLP技术人员的要求有哪些变化？这个是问到点上一个很好的问题。我是14年到17年，14年到16年在搞LP的。那会儿包括我们在理论课的时候，我也跟大家分享过。那会儿就是各种细分任务都是小模型或者小工具，然后也有一些机器学习统计机器学习的方法，包括前几年搞知识图谱的一些人，到现在包括很多国内的C9的教授也都认识一些人。大家会去讲NLP确实得重新做一遍了。大语言模型来了之后，因为大语言模型把很多小任务都合并了，就没有必要再去做小任务了。这些小任务以前都是人来手动调，现在大元模型直接给你吃掉了。
	吃掉之后，这个过程就特别像什么？特别像计算机视觉。大家回想一下，计算机视觉CNN火之前都是专家来做卷积核的设计。各种专家设计出来的卷积核做特定任务，这个是做边缘检测的，那个是做什么巴拉巴拉的各种各样的算子。但后来是什么故事呢？后来就是说CNN来做这个事情，我自己可以通过数据来学习出来我需要什么样的filter，我需要怎么样的feature map，就能把这个隐藏层学出来了。那那这个是一个很大的变化。甚至到了18年左右的时候，我记得有一篇文章讲这个就有点类似于attention is oil need。
	对于计算机视觉来说不需要全连接层了，有全卷积网络，我不知道大家有没有这方面的印象，全卷积网络就是我只需要卷积层就够了。所以整个你们的焦虑，你想象一下，你看的足够多了，你开始触类旁通了，你开始理解了这个计算机科学的发展这些套路，这些大的体系的东西，在这个大的体系下面玩也就这么多。玩法包括在理论篇有给大家讲过一个图，就整个神经网络三条线出去，这个神经网络层的变化有NN LSTM，然后这个STM有by LSTM双向的LSTM，然后有这个attention transformer，然后下去之后有这个GPT和bert Albert，就是双向的LSTM的思路接到transformer里面来。就是所有的原子级别的这个点，meta级别的点没有那么多的，更多就是排列组合，包括咱们学的这个计算机的数据结构，树的结构图的结构，这些结构不现在也都弄到prompt里面来了吗？对吧？数这个链的结构就是这么回事儿。对。
	有个同学说的挺好的，就道法术器四个层面。对，其实也教了术和气，这些实战代码都是术和器。但是你想要现在一下跳到道法层面或者说更高层面，不可能。你王阳明也说过，对吧？
	事上练难，上得你得练，你得多做事儿，事上面去练，然后练到比较难的事情上面，你才会有收获。你想躺平着有有成长，你就快速成长，这很难的。对你越痛苦，你其实越越有机会成长。如果你越痛苦，然后你这个痛苦又度过去了，然后你有复盘，有反思，你会有很多成长的。对，但如果你不是痛苦，你只是焦虑。那你那你越焦虑，你焦虑很久都不会解决问题的，因为你没有问题，你只是焦虑。
	老师你上课前是怎么准备的？怎么这么系统的把所有的知识串起来的？老师花了30年的时间准备。这个我就是一个大语言模型。对，然后我现在的任务是上这门课，然后我把这个任务拆成了八周，然后拆成了16节课。对我自己就是一个凹凸GPT。我一直在说，你把你自己想象成一个大模型，你考虑问题就会很有意思。甚至prompt的设计，多目标的拆解本身就是一种好的工作习惯对。
	我们到10点10分。有个同学问，焦虑的原因是技术很多贬值比较快，对比其他的例如投资管理那些能力。是的，这个我在开营的时候就提过了。这个时代没有铁饭碗。为什么没有铁饭碗？那要讲到湄潭那里了，大家怎么理解的？为什么没有铁饭碗？那是因为人变多了，地球资源有限，然后经济增长受全球化的这种好处已经到头了，阶段性到头了。所以单纯来说就是现在处于一个平台期或逐步往下跌的一个状态。
	那这个时候大家就会卷全方位的卷，从小范围的卷到大范围的卷，从单行业的卷到多行业的卷，到全行业的卷。在这个过程当中，所有的东西因为卷起来了之后，它就没有那么值钱了，很多东西就不值钱了。大家要想新花样，为什么现在一个agent要这么多agent要这么多大模型？不就这个原因。
	但是这么多大模型又回到本质的原因，你能看到本质的东西，没谈原理，就是你学到本质的东西，他们有什么不一样，他们有什么一样，谁有差异化的优势吧？谁哪个大模型最牛逼，哪个大模型在某个特定领域牛逼？这些东西是meta learning想要教给大家的。就是你看透了本质你就没有那么焦虑了。我一路学了这么多年，学了十快十种编程语言，学了TensorFlow，学了class，现在又学南茜。那你的迁移成本会降低的，因为有很多东西是相通的。
	这个同学说的很对，把自己的大脑看成一个复杂模型训练，有点像刻意练习，不断训练和微调也是的。我最近还正在看刻意练习这本书，是本好书。对，刻意练习的核心就是不对，时间不是1万小时定律，就是刻意练习设定目标，复盘中间结果，然后去模拟环境刻意练习的一个核心。
	这个刻意练习是一本很有意思的书。它的核心其实跟表示学习，跟这个deep learning也好，跟大语言很像。为什么很这就是你看的多了，你很多概念你能从哲学程度，哲学层面去理解它，你就会觉得这个东西靠谱。因为这么多哲学家，这么多科学家都用这个思路可以练习。虽然是一个教你怎么样去掌握通俗通用技能的一个数，但他跟我们表示学习有一个很重要的点，就是你在刻意练习过程当中，尽量模拟你的实际使用场景。那这个不就是我们讲度量。你不管是有监督的学习还是强化学习，你最终怎么用这个知识，你尽量学的时候就在那个环境下面去学，那那自然学的就有用了。它不管是环境的差异度，还是你最终策略的差异度，还是你的奖励机制，它它无限接近于你的使用场景，那就靠谱了。对，TensorFlow的课程，TensorFlow的课程可能需要一点点会编程的知识，但也不需要太多。因为TensorFlow跟大语言模型一样，也是一个领域特定语言，它其实不是python的。
	然后我觉得我再给大家一个缓解焦虑的思考逻辑。你们去拉一下全行业三十多个不同的这个行业，咱们写代码的已经是工资最高的了，基本上不知道在焦虑什么，就是咱们有什么好焦虑的，咱们拿这么多钱，你看看上海、北京的平均工资才多少，所以不用焦虑，也就是你哪怕换一个小公司，或者换一个一般的岗位，或者没有晋升又怎么样？你在全人类全行业来比你挺好的了。现在既然如果你能把这个事儿想明白，你不焦虑，那你要解决的问题是你要不要往上走，还是你要平衡生活和工作。如果你要平衡生活和工作，那你就去花点时间到生活上去学点别的这个去去调节一下。如果你还是想要往上卷，那你就卷，对吧？你就想办法卷。
	我看看。
	好，我这边就结束了。我们今天的课程就到这儿。回头大家有什么问题我们群里再交流。反正这个群都在有些问题，我觉得这个比较有代表性的我也都在回答。对，大家反正持续学习，都是一个新东西。我也在持续的去学习，也不一定讲的都是对的，也欢迎大家多交流，多拍砖。
	好啊，对对对，今天是教师节，感谢大家。我看群里有些有有很多同学在说，我就没有一一去回答了。对，好，希望大家都工作顺利。然后技术上我们多交流，多讨论，少焦虑。对，焦虑不解决问题，做事儿才有真正的问题。然后问题都是一个一个解决的，解决的越多你不就难上得了吗？好，那就这样。