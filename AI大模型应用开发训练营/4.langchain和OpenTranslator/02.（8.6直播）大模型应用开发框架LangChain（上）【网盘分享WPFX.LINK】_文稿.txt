	那那咱们就来，所以其实南倩这个迭代非常快。我们就在整个这个课程过程当中，通过不断的介绍它的模块，向大家不断的去展现整个南线的一个框架，它到底是怎么样的一个定位。不管这个定位是对开发者来说，对于这个应用的使用者来说，还是对于任何关注AIGC的用户来说，其实南线都是一个很独特的一个存在。
	我们两部分，第一部分讲南线的这个基础的，其实有点类似于我们经常学习一个新事物，都会有一个很核心的问题，我们叫这个5WYH，就是为什么要用它它是什么什么时候用它，谁来用它它面向的对象是什么，以及怎么样去使用它。怎么样去使用它？就是我们接下来第二部分，会详细的去讲怎么样去用蓝线。那今天可能会覆盖到它最基础的这个抽象，就是这个大模型的抽象。其实在我们六月份做这个大这门课程的纲要outline的时候，其实他还没有去做这个model IO这个抽象。那会儿他的model包括他的output puzzle和这个from都在外部散开的。其实他没有把整个这三个放到一个统一的抽象里面，叫model IO现在放进去了，但即使做了这么多操作，都还没有到0.1。
	所以大家可以想象这个框架的迭代非常快，我也会尽量在课程里面尽可能的给出一些，大家能够随着它的迭代，但是都能找到他的一些学习资料也好，他的API文档也好，它的代码定义也好。通过这样的一些方式让大家实时的跟进他的一些最新进展。我们现在就正式开始，来讲讲南区这个框架。首先要讲蓝天它是什么，对吧？我们在第一节课的时候就跟大家讲过，南券在giraud面其实是一个广受关注的项目。从左边这张图能够看出来，其实南劝这个开源的时间并不长，是去年年底到现在也就不到一年的时间，其实它的速度增长非常快，已经有将近6万颗星了。然后从右边这幅图，其实能对比的看出来，从我们的深度学习框架里面最有名的两个，一个叫sense flow，一个叫patt watch的增长对比来看，这南茜的关注度受到了一个非常猛烈的增长，就它的这个加速度明显要高。关于这两个深度学习的框架，并且它也仍然在快速增长。
	但这里有一个小细节是最近发生的，就是大家能看到在我们上第一次课的时候，其实整个连线还是属于这个创始人这个chess他的一个个人项目。但现在大家如果有关注他的话，最近他已经把整个项目放到了一个组织上下面叫能源AI因为整个能源其实已经是一个公司了，就他们对外有两轮的融资，至少公开的，但可能还会有更多。那作为一个公司来说的话，它会有公司化的一些运作。虽然他运作的是一个开源项目，所以大家后续会关注到南京的很多的链接也好，什么也好，其实都会到南苑AI这家开源组织下面去了。
	所以从第一个视角来看，南线是一个快速截取的一个开源项目。第二点来说，从他的项目介绍，包括我们很多的已经使用过南线的同学来说，南线是一个从他自己对他的定位来说，它是一个可组合性非常高的一个助手。并且这些组合抽象的这些模块都是通过我们不断的去理解大语言模型的过程当中抽象出来的一些能力，就包括右边它最新这个版本抽象出来的六个最重要的模块。
	一个是我们今天课程当中会讲到的这个model IO跟我们的一个大语言模型的输入和输出，包括这个语言模型本身，包括它跟数据的一些连接。这个数据的连接都不只是说我连接到了一个数据库之类的概念，还是说怎么样跟外部的数据去发生关系。这个数据可以是互联网上的，也可以是数据库里的，也可以是向量数据库里的那第三个就是南线的这个券，对吧？这个链条。那什么叫这个链？链是它整个南线的框架里面最重要的一个模块。因为券是实际去运作很多这个功能的一个抽象，包括这个memory，它的这个偏记忆的模块。
	然后我看最近这个呃能劝的作者自己在接受采访里面也有提到。其他是一个大家如果去那个定义上面去关注这作者的话，会发现他是一个参与了好好几次。这应该是他第三段这个AI相关的项目经历了他自己他一直在做AI相关的事情。
	但是在存储memory这一块，其实他的经验相对来说不是非常的丰富。所以memory一直是社区里面的开发者们在做贡献。Memory本身也有一些技术在和这个实现上面的一些不够优雅的地方，也在不断的迭代。
	然后还有一块就是这个agents，agents其实可以算是现在最热的一个研究方向，包括我们今天会看到auto GPT也是属于agents的范畴。Agents是相当于把我们的这个大语言模型更强化了一步。因为大家知道大语言模型本身的推理能力、理解能力已经非常强了。但是agent是真正实现了我们未来走向通用智能的一种探索方式。我们能不能通过agents像一个甩手老板一样，把我们要做的事情告诉一个大语言模型的应用，然后他来帮你实现很多的内容，call back这个是很多的稍微一些的项目都会有的这个模块，就这个回调的机制可以方便我们去做很多的操作，尤其是在这个复杂用的那所以从第二个视角来看，它对它的定义是一个通过组合各种模块，并且把这些大语言的能力抽象出来，来扩展大语言模型本身的一个助手。
	然后有官方推荐的几个不同语言的实现。比如说这个pythons，就我们现在看到这幅图里面是他pythons的这个版本的一个人命一个主要介绍，同时也支持这个javascript，最近也有这个text graph的实现，可以给到对前端有经验的同学去使用。然后除此以外还有一些很重要的点，我觉得这个REDMI是值得大家去推敲的。除了大家看到一些他的这个release的版本，包括他的这个开源协议，它的star数量以外，还有一些我不清楚大家是不是对于海外的一些M软件或者说开发者社区比较熟悉。像discard，就中间这个小人discount，他的社区其实也很活跃，里面有大量的人在讨论一些新的feature。
	The feature就包括大家看到这里有两个gift up上面的CI pad fly，一个叫南倩，一个叫南茜的experimental，这个是他最近做了一次代码的调整，他把他的一些新的比较偏实验性的一些功能放在了顶级目录experimental里面。然后他的一些比较固定的稳定的一些模块抽象放在了圆圈里面，包括他月度的下载次数是400万，这个是非常夸张的一个下载量，也充分说明这一轮大元模型，使得很多的非或者说原来不是这个圈子里面的人，也都在关注这个圈子里面的一些进展，包括去使用它的一些框架。然后这个男生AI的这个theatre也是最近我们会看到有各种各样的一些新的信息从上面放出来，但是同时还会有一个问题值得大家关注，也是最近我看有同学在发这个机制之星的文章。就我为什么放弃了两千？因为他的目前提的要求非常多，2200个，其实他的依旧很多也很正常。因为一个400万月度下载量的一个产品，一定会有很多人去吐槽。他自己也很谨慎的在用这个语义化的版本号。
	所以整个这个框架其实摊开来看很热，然后有很多的这个模块的抽象在不断的做迭代，然后使用的用户很多，因为下载量是一个更干的一个数字，有很多人在讨论他，给他提建议，提问题。针对这样的一个框架，我认为我们是应该去拥抱的，就跟我们最开始去面对tensor or flow的时候，仍然会有很多人去提他的issue。包括到今天为止，他肯定还不止2200个优秀。是开放状态。所以从这两个角度来看，这个是我们对能源的一个理解。
	关于回答他是什么，最关键的问题是说，我们知道它是一个快速崛起的项目，然后有这么多人在关注它，使用它。有这么多问题，那我们为什么要使用？为什么需要能源？一个连线作者自己回答的问题，也是我们今天会让大家体验的问题。
	就是为什么开发者要用人性，而不是直接去使用OpenAI或者packing face上面的模型呢？作者本身给了一个答案，这个答案也是我们大部分人在实际开发大元模型应用当中，我相信会遇到的问题。在huffing face上，OpenAI上或者任何其他的大语言模型这个托管平台上，我们其实都能够找到很多的方便。我们叫基座模型，或者说对应的API。比如说OpenAI开放了各种大语言模型的API，但是在产品的集成和使用当中，我们还是需要大量的工作。举一个简单的例子，就是我们基础篇的实战作业。
	Often I translator其实跟我们之前给大家看的这些课程当中，比如说怎么样去使用chat API。怎么样去使用completion ati有一个巨大的区别，我记得好像有有几个同学吧在提整个OpenAI translator项目里面，怎么感觉大语言模型的代码那么少，很多代码都不是在做大语言模型的，为什么？这个其实是大家如果做的多了之后，做这个大语言模型应用开发多了之后会发现，事实就是如此。其实整个典型的LLM的这种应用，其实它大语言模型的代码绝对不会非常多。它很少，它很精华，它它的核心的点甚至都不在写代码本身，而是在构造这个prompt。
	那我们的OKI translator去抽象一下的话，其实就是这样的一个非常简单的典型的一个大语言模型的应用实践。它的输入是一个英文的p df然后我们构造了一些prompt，甚至这个prompt是留给大家去做基础篇这个作业比较的一个关键点。我们现在的from都非常简单。大家如果看过代码，如果是纯文本的话，就是直接写个翻译对吧？如果是表格的话，我们让我们的OpenAI的这个I在翻译的时候尽量保持它的一些格式，然后就这么两个prompt，然后把它丢给了一个大语言模型。那在我们的应用里面，这个大语言模型可以是我们的OKI的API，也可以是一个私有化部署的TGLM的UIL。
	接着我们最后把它变成了一个中文的PDI。当然这个过程当中我们可以把它展开来看，就我们上次看到的是一个更更复杂的一个应用。然后我应该是两上周日的时候，我们的课程里面的一幅图，我相信大家看过课件的话应该有印象。在这个里面，其实我们的核心的这个大语言模型就只是一个很小的部分。我们的prompt其实是一个值得大家去深挖的一块。这个代码也好，或者说我们认为是构造这个魔法也好，这一块是一个目前我们还没有去深度展开来讲，但今天会教大家去用的一个部分。
	然后接着这个输入输出的处理。输入输出的处理我们自己写了很多的代码，为什么要花半节课的时间跟大家讲这个PDF的这个flower这么一个库，整个这些东西，其实我们开发这个OKHS latter现在这个版本会让大家觉得大语言模型不是很重要。怎么都在搞一些外围的工作，这也是为什么我们需要人签这个很直接的一个理由就是GPT4已经很强了。我们甚至在我们的这个翻译模型里面就可以调GPT4的这个接口，只不过可能不是很划算。因为翻译是一个相对来说比较简单的任务。只要你说清楚你要做翻译，那么GTGPT3.5也是够用的。但是核心逻辑还是在于GPT4本身无法解决我们刚刚做的这些外围的工作。
	比如说GPT4它没法帮你去解决一个PDF的文件解析的工作，他也没法帮你把一个很大的PDF去分段，对吧？那除此以外，PPT4还有很多的事情做不。我们简单看一些比较干的并且比较重要的一些GPT4能力边界外的事情。比如说数据数据GPT4是截止到2021年的Q3，我们在讲OKI模型的I那一节课的时候，我们有提到各个模型的训练数据的一个截止日期。那么最新的这个GPT4的这个版本，它的数据其实也都是两年之前的一个数据了。有很多新的数据其实他用不到，那这个时候就会造成一些事实性的错误，或者说在逻辑推理当中造成一些错误。但是它也没法联网，对吧？
	就GPT4本身它是没法联网的，它就只是一个语言模型，它能进行推理。针对他已经训练好的这个数据，这个模型是固定的。但如果你去进行这个instruction tuning，就我们之前有讲过，你去给他进行一些few shot，或进行一些one shot这样的一些prom prompt这样的一些设定，他也许能学到一定的这个参考示例的有模式，但他不一定能学到这个参考示例的常识。因为常识是很难通过这样的方式给他灌进去。那你也许可以用一些反复的方法，但这样他也就不展示一个GP44，但是说你还是得针对它去进行模型维权。同时除了没法联网以外，其他的外部工具他也对接不了。
	比如说我们想让他去帮咱们订票订酒店，那他这是做不到的对第三方的I他都无法去直接调用。同时我们在使用它的过程当中，一直都会有max token限制。比如说我们即使是扩展到最贵的这个GPT4 32K的版本，那它也有32K的限制，那我们一本书肯定就超过32K了，那怎么办？我们之前有介绍过一些思路，就是把它切成一段一段的那具体怎么做，大家也没有一个非常标准化的实现，这些工作谁来做对吧？
	然后同时我们开发大语言模型应用又不只是一个GPT4的单元模型。每个公司都在发展自己的大语言模型，开源社区天天都有这个新的大语言模型。那怎么样去对接这些大语言模型？
	我们不可能把每一个模型的I都学一遍。我们之前花了两三周的时间学会了OpenAI的PI，学会了这个GPT系列的模型。其他的大医院模型怎么办？也都是花一个月的时间去学吗？那肯定不合适。我们需要有一些抽象，这个抽象能够使得我们调所有的模型尽可能都是一套方式去调用，包括这个数据隐私问题。
	然后输出的结果也不稳定。有很多同学说这个embedding同样的输入，同样的模型，为什么输出结果不一样？这个我们已经解释过了，他可能通过了不同的AI的endpoints，但他最终在一个相同级别相同水平的一个embedding模型上。所以如果你要把这些输出来的embedding的向量去做相关的推理操作、类比操作，它即使那一个向量的值本身不一样，但是达到的效果是类似的。
	针对于大语言模型，它就跟embedding不太一样了。因为它输出的就是一些自然语言的文本。这些自然语言的文本是我们要直接拿去做使用的那如果你不只是要拿到这个结果看一眼，而是你还要拿着这个结果去下游任务里面再去做处理。那你的输出结果不稳定。这个结果包含样式上的不稳定和内容上的不稳定。如果这两个都不稳定怎么办？这些问题其实都需要你写一大堆的代码，围绕着GPT4去做服务。那这个时候其实大家就很痛苦，这些痛苦也就是年轻人想要解决的问题。
	这么多的问题说回来一句话，就是我们的大语言模型的应用，其实跟GPT的这种API的封装是完全不同的两回事。就我们要去封装一个GPT的这个I比如说我们在ChatGPT刚刚出现的时候，国内有很多的开发者，小的公司，把这个ChatGPT这些封了一个类似的UI界面，然后开始去赚钱也好，去做这个分发也好。这个是一个简单的封装，它也不需要做额外的工作。因为它只需要照着现在GPT的实现去做一个UI界面就好了。但今天其实我们讨论的不再是一个，或者说我们已经不再仅满足于一个CATGPT这样的应用了。而是我们希望大语言模型有各种各样的能力边界的拓展。我们看一个简单的一个一分多钟的短片，这个其实是三四个月前开源的一个项目，我相信不少的同学应该都了解，叫auto GPT。
	好，我们现在看到第一个例子就LGBT是一个很有名的项目。我我我理解应该大部分咱们课程的同学都已经了解这个项目。这个项目其实要做的事情就是我刚刚提到它也是agents的一种最佳实践。就怎么样能够去使用一个类似于像钢铁侠的助理，这个javis这样的一个智能助理，然后让他来帮你去做各种各样的一些工作，不只是问一个天气，不只是帮你翻译一本书，而是说能够做各种各样的工作。
	要做这些工作，首先就得克服我们刚刚说的GPT是能力不够的问题。他这儿举的第一个例子就是说让他联网去访问google的这个搜索引擎，我们可以看到其实他这儿有输出一些内容，就比如说他的这个问题是，大家能看到有一个这个屏幕上我鼠标有写，就这里有个go就是search alt GPT相当于就是去找这个项目本身，就是让他的这个RTGPT这个智能的代理去找out GPT这个项目本身通过google去找，但他没有讲通过google，不过他的实现里面能够对接google的搜索引擎。所以它的这个代理机器人，这个智能助理，它的第一步骤就跟我们之前讲的这个思维链一样。就是他怎么样能够把这个思维链里面每一步不只是语言模型的推理能力，而是把它跟各种外部的API助手对接起来。那它的这个思维链和外部的助手对接起来的能力就会被无限的放大。
	所以他的第一步要解决的问题就是能不能用google来搜索。这个时候大家能看到他的第一步其实就是在做这个command，就是google。然后arguments就是一个order GPT，学过前面这些内容动过手的同学，第一直觉就是很像一个function calling，对吧？就这个function叫google，然后call in的这个参数叫alt GPT。相当于是我这个函数的输入是一个auto GPT，就是我要带搜索的检索的这个字。那这个command其实可以被进一步抽象变成一个浏览器。然后把这个google放到arguments，就我能对接各种不一样的浏览器。这样就可以很简单的去做出一个多浏览器搜索结果的一个self consistent，对吧？就我从不同的搜索引擎里面拿到更多的这个结果，然后我再去做整合。所以整个LGBT这样的一个思路打开之后，其实我们能做的事情也非常的多。我们继续看这个例子。
	不好意思，卡住了。这里他再把这个网页的结果做一个爬取。然后这里有一个很有意思的现象是，我们人是怎么样去完成这样的一个工作。就假设我们人收到了一个要求是去搜索这个，或者是去了解什么是out TPT的时候，我们通常也是去打开一个网页。
	那这里也是一样，他去搜索了一个google搜索引擎上去查了一下auto GPT。然后这个RGBT本身这个代理他的source相当于我们在聊这个大语言模型内部发生的一些事情。而自己在想的事情是什么呢？来讲这个呃下一步，他已经从谷歌搜索引擎上找到了各种关于auto GPT的链接了。我们人通常就是对着这些结果一个一个点开看，对吧？
	那他这也是一样。他的next step他就是去访问RTGPT这个项目的get up仓库地址。这个可能会帮助他去获取关于这个项目的更多的信息，使他更好的了解这个项目。然后他的推理过程就是他去浏览这个github上面的这个RGBT项目，是获取它的最好的方式。这个可能可以在这个单元模型或者说在它的实现里面去注入，这个reasoning本身就跟我们之前讲过的这个from learning其实是很很有关联的一个操作。然后它具体的这个操作就是去访问这个当中，就我们从网页里面发取到的这个关于alt GPT的某一个链接，就是github的这个主页的链接。我们可以去永恒的。
	好，大家看到这个视频里面的左边这个部分，其实就是它打开了。这里有一个command操作，这个叫做浏览网页。就浏览具体的网页大家可以理解，前面是去爬虫，就是咱们有这个爬虫或者web开发经验的同学都都理解。我没有真实的用浏览器打开他们，我只是获取到这些数据。那现在我要通过一个网页浏览器去打开它，打开的这个地址是我们这个arguments里面显示的这个get hub的地址。然后问了一些问题，问题就是这里有写，去在去告诉我们2GPT这个项目有哪些feature，有哪些功能特性，他这边做了一些，但是做了一些后期的效果，不方便大家理解，就他在分析这个网页。
	这边分了一个段，大家注意这个。那注意这个细节的话，这里会看到其实这个death booth是工具在获取这个网页当中浏览的这个数据。然后这个数据因为它本身比较大，所以被裁成了四段。这里面有一个summary entry就可以列成。因为内容过多的时候分成了四段，这个也很合理。
	到这儿为止，其实他把这个网页基本就分析完成了。我们反正继续他做了什么事情。不好意思，卡了一下，我们可以看到最后这一步。定在这里。然后我们能看到其实到最后这一步他做了一个什么样的command。他就说这个Jason就我们要写的这个jack，其实最终他是把它写到了一个文件里面。然后写到这个文件里面第一这个文件的名字叫auto GPT点t xt，然后要写的内容其实就是LGBT1点巴拉巴拉就他问的这个问题，大家再回想一下我们这个过程，其实他LGBT的这个事例，他要做的事情一共分成了三个步骤去完成。
	第一个步骤就是他收到了一个问题，这个问题其实LGBT是什么，对吧？你帮我了解LGBT是什么。他第一个反应是说我不知道。因为这个在我的知识库以外，我就算用了GPT4，他在这个事例当中用的也是GPT4。就算用了GPT4，我的知识也只是到21年的九月份，但是这个项目是在那之后开源的那显然我的知识库里是没有的那怎么办？
	大部分的AIGC应用，我们如果现在要去想象一下自己去开发一个应用，其实第一部分都是去做这样的一个问题过滤。就我们去想现在这个问题是不是在我的这个模型的知识范畴内。如果不在的话，最直接的方式就是去联网。这也是为什么这个web pilot这个TSBT插件这么火，就是因为大模型推理能力特别强了。但是有一些事实性的知识，客观的这个呃数据他拿不到。因为这是21年9月之后的事情。
	联网一定是第一步。他在联网的过程当中，第一步查到了很多跟out GPT相关的网页。第二步通过这个raining推断，alt GPT是一个就是从网页的第一版的搜索结果里面看，它应该是一个开源项目。那他自己的raining推断要了解一个开源项目，最好的了解的链接平台就是git 2。所以他就访问了github的这个主页，然后分析了这个主页上面的内容，带着一个问题就是这个项目有哪些feature，作为一个开源项目就是最重要的。然后第三步把这些拿到的结果做了一个summary，就做一个总结。我们在之前讲playground这节课的时候告诉大家，GPT这种类型的单元模型总结能力很强。总结之后最终写到了一个文件里面。
	整个这个过程最令人感觉到神奇的是说，好像我并没有写什么代码，我只是让RTGPT这样的一个开源项目就帮我做了这个事情。这个其实是比你帮我翻译一本书看起来更让人有想象空间的。我们现在不仅是能够去做这个翻译了，而是因为这只是一个示例，我也可以让他去问任何事情。甚至说不只是对接一个谷歌的搜索引擎，对接一个导出文件的一个PI而是这种形式可以干的事情就非常多样。通过刚刚那个示例，其实我们就能看得出来，大语言模型的应用肯定不只是一个TPTAPI的封装，它有很多的想象空间，这些想象空间能够打破GPT4的一些限制，就包括我们刚刚说的联网。
	那具体来看一个典型的连线的使用场景是什么样的。我们刚刚其实有提到通常的这个使用方式。如果我们要做一个在现有应用上面去增加这个大语言模型能力的一个助手也好，功能也好。最常见的一个方式就是以提问的方式来进行交互，而不再是通过图形化的界面，通过问答的方式。我们假设现在要做一个这样的一个问答机器人，就跟我们后面的实战。比如说我们要做一个机器人，他能帮我们去卖东西，一个销售助理。那这个销售助理肯定是要跟我们最终的用户去做沟通和交互的那在这个过程当中，我们刚刚有提到，就跟问这个auto GPT1样。假设auto GPT就是我们要卖的一个商品，那那他在问问题的时候，有两种情况。
	一种情况就是说这个内容是我们大语言模型本身就知道的，还有一种是他不知道的。但是无论如何，我们能看到这里有一个标的一个顺序，无论如何我们对于用户的理解，其实大圆模型都无法直接处理。我们在讲数据表示这一节课的时候有提到大元模型第一步要做什么呢？把它变成一个高维的向量，一个embedding的一个向量。这个embedding model，比如说在我们之前的课程里面有讲过这个ADA02的这个bedding model，VV2版本的这个model是我们可以去使用的一个非常好的，又很便宜性价比高的一种invasion方法。如果我们的后端的大模型使用这个GPT4或者3.5的话，它们是兼容的。这个银背印向量是直接可用的。那么他会去处理这个真正处理的内容，其实是这一堆embedding的向量，这也是我们去做复杂应用，而不只是一轮对话的一个最大的区别。
	之前我们讲API的时候，我们都是让这个自然语言的内容直接输出到一个模型里。然后模型输出我们自然语言的结果就结束了。包括我们刚刚看到的ONI translator也是一样的。但现在有一些不一样了，为什么？因为我们要把它的内部的这个过程拆解出来。就现在我们假设LGBT，我们还不知道怎么实现的对吧？但是我们去想象一下，我们要去实现一个LGBT它内部的一些流程，其实是可以通过这样的方式来跟大家去做一个分享和拆解的那假设我们要去真正去处理的时候，我们的大元模型拿到的识别向量，并且我们需要去干预它的一些中间步骤了。
	那就比如说刚刚的这个问题我们要去查的是一个新的项目叫auto GBT。它变成了一个高维的向量，这个高维的向量大于模型本身，可能他并不知道这样的一个内容。也就是说可能对于用户来说，他的感知是他提了一个问题。
	但作为我们一个开发者来说，我们可能首先把这个问题交给了一个大元模型。就比如说这里的一个GPT4也好，或者说一个开源的大模型。对，我们其实在内部可能就直接丢给他了。那丢给他之后，他其实不知道这个问题的答案，对吧？因为这个知识是在他的去年语料范围外的那这个时候可能我们就会去联网，这个是最常见的一个场景。但是这个QA的这个问答，更多的时候我们是希望把它归属到我们的这个比较封闭的环境里面，而不是一个像RGBC这么强的一个开放式的一个场景里。
	现在我们把这个能力再往小了想，就是我们现在要做一个更小的机器人，这个机器人没有联网的能力了，我们就忘掉联网那一发是更强的能力，就他能对接到各种第三方的平台了。那假设现在连这个能力我们都没有了，我们现在只比如说在这样的一个场景里，我们训练了一个助手，他就是不会回答这些新的问题。你想象一下你招了一个销售，这个销售在回答这个顾客提问的时候，他没法去联网。那这个是更典型的一个场景。这个场景里面，如果我们遇到了这个销售知识以外的问题，他可能直接回答，我不知道，我不太懂。他可能就会回答一个，或者说用大语言模型来润色一下他说他不太懂的这个说法。那那是不会变成现在我们这幅图里面看到的一个路径，这幅图里面是没有去联网的，那么在这幅图里面我们假设我们的这个大语言模型的应用的处理能力变得更小了，没有auto GPT那么强。
	我们再回到现实一点，我们要一步一步去做这个应用开发的话。那假设就以我们要做的这个销售机器人为例，那它不能联网，那他能怎么办呢？他能有第二种方式去扩展它的这个能力，就除了联网以外，我们可以让他把一些预制的一些知识，不是让他去学，不是让他去微调，而是用一个向量数据库存下来，就是我们这里看到的中间这个部分，下载数据库。而且向量数据库也有比较有名的两个项目，一个是叫开矿，一个是叫prava。这两个项目其实目前来说在年限的这个官方文档里面也是推的比较较靠前的一个位置。这两个项目其实是用来存向量的那我们再回到第三步，我们想象一下，我们现在开始没有外部的这个联网能力的情况下，我们要怎么去处理这个问题。
	我们先假设这个问题是在我们的项链数据库里有存下来的那怎么存呢？我们先不管它已经存下来了，相当于我们是一个开卷考试的一个场景。这个开卷考试的场景，这个销售能做的问题就比较简单了。第一件事情是拿到用户的这个问题的向量，这个问题的向量是可以去向量数据库里访问的，那这个问题的向量拿到向量数据库里，我们会去做一个比较。
	大家还记得我们第一次讲这个embedding的实战的时候，我们去找了亚马逊的美食评论集，然后里面去找语义比较类似的向量是什么样的那这个时候他是不需要去大模型里面去跑，他只需要把这些相似的向量找出来就好了。那这个时候假设他找出了前K个特别相似的向量，以一个特定的相似的判断的方式找出来了。那找出来之后，他可以干什么事情呢？他可以直接首先他可以直接返回给用户，这是一种处理方式非常粗暴。但可能就不是用户的这这种方式，其实我们在embedding的实战里已经实现了。
	还有一种方式是有可能在前K个最终都结果不够好。那我们有没有一种办法去判断这前K个好不好呢？也可以。比如说我们再额外去做一步，去跟我们的大语言模型再去做一次交互，去了解我们现在的这个top k是不是用户想要的，或者说这top k有没有一些方式组合成这个用户想要的。以列表的形式呈现，或者说给他一些对比，巴拉巴拉就整个这样的一个流程，算是一个比较完整的。我们要去实现一个销售机器人就需要去开展的工作。但这些工作我们听下来好像非常的复杂，就是我们要做12345678，甚至说跟大语言模型的交互都远远不止这一次。可能一开始就得交互一次，或者说先交互一次再去找向量数据库再跟他交互，就整个这个过程非常的绕。
	这些步骤都需要我们去做，但是我们去开发这些代码都会写的不太一样。就比如说我们这个实战作业OKS slater，我们希望实现第二个里程碑。但我相信第二个里程碑大家自己去做的时候，会有很多不一样的实现方式，因为这个接口都预留的有很多种不同的写法。南茜需要做的事情就是说你们都有不同的想法，大家都很聪明，大家都会有不同的茴香豆回的写法。那有没有一种可能把这些不同的写法，我来把它定义清楚，以一个统一的方式来定义清楚。比如说跟大语言模型的交互，我把它定义清楚，一个统一的接口，跟向量数据库的交互我有一个统一的接口，跟这个embedding的这些嵌入模型我也用一个统一的接口，甚至是说跟用户的交互，我也有一个统一的结果。
	这个是南倩非常想要干的一个事情。虽然从某种的视角来看这个事儿不难，因为他好像是在干一个合适老的工作。就是我有一类向量数据库，有一类大语言模型，有一类embedding model，大家都干了是同一类型的工作。但是大家都在讲不同的方言，你说河南话，我说四川话，他说天津话，那这些不同的方言能不能变成普通话？这个是南倩干的最重要的一个工作。你说把各个地方的方言汇集成普通话这个事难不难？肯定不是一个非常难的话。比起搞文言文，肯定没有文言文那么难。
	但是要把众口难调这个事儿调整好，并且大家都服气，这个是一个非技术性的很难的活。南迁干了这么一个事儿。如果我们要做一个问答的机器人，通常来说人现在这个中心他就会干这三类工作，这个顺序不一定严格，是一次性就做好了这么一个顺序。
	好，大家大概理解这可能性要干的这个工作之后，我们再看一个它的一些基础概念和它的一个模块化设计的抽象。就是我们刚刚在那幅图里面有看到有这个大约模型，各种各样的大约模型，有这个prompt，但是没有被单独拎出来。然后还会有我们这个向量数据库，向量数据库现在我们会把它放到这个memory这个位置里。然后同时还会有这个agent，这个agent大家可以理解成我们刚刚提到的像访问浏览器，包括我们像访问这个第三方的平台，我们的这个天气查询的应用，都会被放到这个two two case。还有一个这样的抽象放到这个里面，那整体这幅图偏它的比较高层次的这个模块抽象其实想要说明什么事情，正如这里想要做的这个change everything要表达的一样，整个架构图或者说这个模块拆分的图要怎么去理解呢？
	第一我们从结果出发，右边其实是我们刚刚想要说，一个大语言模型的应用绝对不等于一个GPTAPI的封装，那如果不是一个GPTAPI封装能做什么事情？那显然我们刚刚说的QA能做去做摘要总结肯定能做，去做各种数据，私有化数据上面的聊天机器人也是能做的，包括去分析表格，去跟各种第三方的API去做集成，去做交互，甚至去理解一些特定的代码，这些都能做。包括aut GPT这样的agents都能做。那他们怎么做呢？
	通过一个线串起来，这个嵌其实就有点像我们刚刚说的那1 2 3 4 5 6 7 8 9 10 1个链条也好，或者说我们之前教大家这个思维链也好，或者说多条思维链组合出来的self consistent也好。所有这些东西都可以被他称之为一个叫做chain的抽象，然后这个券它不是一个单输入单输出的这么一个像流水线一样的这么一个概念。你可以理解成这个线就是一个顺序结构。然后这个顺序结构本身是可以多个输入，多个输出，甚至中间可以去做各种整合的那中间整合了什么呢？中间整合的就是这样的各种模块。
	那那大家比较好理解的就是大家应该都玩不过那个乐高，对吧？其实乐高是一个非常好跟倩倩去做形象对比的这么一个概念。大家可以想象南倩本身是想去设计很多个不同的乐高的基础的模块。基础的模块可能就只有那几种颜色，然后那几种不同的形状。但是通过他们的叠加，比如说我把几个不同颜色叠在一起，我可以造出一个小房子，我可以造出一个跑车，造出一个机器人，都可以通过乐高去实现。南倩其实就想干这么一个活。
	在这里面有几类图像。第一类这个大元模型的图像，把所有的语言模型放到了这样的一个抽象的统一的接口里面去。然后关于这个memory，就是我们刚才讲到的这种向量数据库，可以放在这里面。包括像一些传统的数据库，我们的关系型数据库mysql，甚至像缓存redis，像这个消息队列，都可以往这个方向去传，然后这一块，他现在理的还不是特别清楚，就是年前本身，因为他现在有data connection，还有个memory这两块。
	未来何去何从，以谁为主，谁包裹着谁，这个很难去讲。但是他有一点比较好的是说，大家你讲乐高通常就没有谁是父亲，谁是孩子，或者谁包含谁这样的一个概念。大家都是散列的，然后再散列由倩给组织起来。南倩这个开发其实也是类似的，你会发现互相之间是一个非常紧密合作，但是概念上又有分离的一个状态，包括像这个front，今天我们的下半年实战也会讲到。
	The prompt里面会有这个我们之前已经用过的。比如说我设计一段这个front，然后我发给一个大模型，让他去基于我的prompt去生产内容。Front本身我们也已经在想怎么样能够复用，能够模板化，能够节约它。这里就会有大量的可设计的部分。就比如说我们的这个front能不能是一个基于模板来生成的那它这个模板里面有一些内容是可以动态输入的，根据用户的需求来。然后这个from能不能跟我们之前学的这个from the learning，就是我们去学这个思维链结合起来，然后跟我们的few short learning结合起来。这些它都有去做一些实现，他把这个论文当中的很多想法，在他的框架里面去做了一些延伸层面上的实现。包括像这个agents，这个agents其实也有很多论文去谈，我能不能把这个思维链去做扩展，扩展成各种各样的工具，比如说这个toll case和具体执行这个tool agent，这个execute。
	这些其实都是我们后面课程当中会逐个把这个模块展开出来跟大家讲的。因为直接去讲会特别绕，就很难把一个乐高给你旅程只有一种表达方式，我想这个比喻大家应该能比较好理解。但是你去把单个每个模块去讲，细拆开讲，是能够让大家去理解每一个模块到底它的核心重点工作是什么。然后未来把他们都讲明白之后，我们用实战的例子来告诉大家在什么样的场景，什么样的情况下，应该去用什么样的模块去组合，能够达到我们一个比较好的效果。好，然后到这儿其实偏这个呃南茜的基础性的介绍就已经结束了，后面我们会每个模块都拆开来讲。
	今天我们主要讲的这个模块是它的标准化的大模型抽象的这个模块，也就是它的model IO。这个模块其实分成了三个小的子模块，然后每个子模块里又有很多的可拆解的部分。第一个就是我们的这个the temperate，就是我们的提示词的这个模板。那提示词模板跟这个事例很像，这是他官方举的一个示例。我们能看到我们以前给大元模型的这个提示词都是我们手撸的，给我们自己写了一个提示词。这个提示时里面我们教了一些技巧，教了一些tricks。比如说我们可以用一些分隔符，我们可以把大问题拆成小问题。但现在我们要开发应用了，我们不可能把提示时变成一个写死的东西。就跟我们以前开发代码，我们不太可能都写常量不写变量，对吧？这是一个最直接的感受。
	我们一直在讲大语言模型，开发这个应用要把大语言本身当成一个编程语言。它如果是一个编程语言，那它至少得支持变量的定义，对吧？那从这个类比的概念来看，其实它的prompt就是它的编程语言本身。这个prompt要支持变量的定义，那么prompt template就是一个很好的去支持变量定义的手段。比如说这里我们有一个自然语言叫做什么death x like y and y就是X和Y是不是很像，为什么？这里他把这个X和Y变成了动态输入的一个变量，这个变量是在我实际去调用这个大模型的时候才会去给它赋值的那这个事儿就很像变成了一个函数。
	之前我们几节课的积累，相信大家应该都已经有感觉了。不管是讲这个function cooling，还是讲这个ChatGPT的plug in，我们的这个的本身当然是可以像一个函数一样去运作的。这个函数可以是调大模型，也可以是调第三方的API。现在我们假设去这个大模型，他就把这个是动态的插进来。在这一次调用的时候，负和报就是西方很喜欢用的这两个常见词，就跟我们用张三李41个概念。那么这两个词去做比较和外，把这个词就给我们大元模型去做预测，或者说去做生成。
	这里它分了两类，这也是我们的模型抽象里面很重要的两类的抽象。一个叫LLLM，一个叫check model，又跟我们之前的这个课程有做结合。大家可以简单理解这个chat model就有点类似于我们的CHATAPI。他是需要去维护一个聊天记录的，更像一个对话形式聊天形式的这么一个模型抽象。
	上面的LLM有点类似于我们的completion，它是一个更简单的大约模型的抽象。就是我给你一个prom，你给我一个生成内容。这两类抽象在我们之前的这个open AS这个课程里面引用的很很常见了。这两类抽象也有它对应的一个接口定义和实现，然后通过这个单元模型我们能够得到一个生成的结果。这个结果我们会面临一个问题，就刚刚提到的样式不统一，结果不稳定。这个结果我们希望能够稳定的给到下游的应用，或者说下游的函数。
	还有一个抽象叫output partner，输出的一个解析器。这个输出的解析器有各种各样的解析方式，让他能够把我们的大元模型的输出结果以一个我预先设置好的一个样式和当然你还要塞进对应的内容。跟这个front complaint有点像反过来的一个操作。把它按照我们要的这个样式去输出，然后内容是按照我们的对应的内容填充进去的那整个的这个model IO其实就三大块。这三大块里面又会有一些小的积分项，尤其是model，应该是跟大家之前的学习最相关的两类，一个completion一个chapt。好，我们接着就实际的会去操作一下南迁的开发。到目前为止，看看大家有没有一些什么问题，我们回答这个五分钟的问题。
	有五分钟的时间，大家可以看有什么问题，我正好接杯水。对。
	对，康明老师已经回复了，有none floor这样的一些图形化的一些应用。
	这个同学问这个图里面这两句话有什么关系？这两句话没有关系，这是我们by design，就是我们设计的prompt，就是长这样的。其中的X和Y是我们的动态注入的，所以这幅图最关键的问题是要理解XY是动态注入的那这样就可以针对我们的上游的输入去做不同的按需的输出。
	这里的output powers是不是function call有一些重叠的地方，还不完全一样。就是output power更多的是对我们的结果进行一个，你可以认为对结果本身进行一个标准化的输出。在function call其实是说我们能够让大语言模型感知到什么样的情况下应该去调用。我们给他提供了额外的能力，甚至这个OpenAI的CHATAPI提供functioning之后，南倩这个社区也在紧锣密鼓的去对接这个function cooling这个API，所以我们当然今天也没有讲到，因为今天其实要讲的概念已经非常多了，在这个实战的部分，在这个check model的OpenAI的实例化的过程当中，其实他已经支持我们标准的message这三类就是我们的这个system user和assistance。但是function calling其实它也支持了，只不过他的文档没有全部都更新过来。所以为了这个方便起见，我们会在南线去调function calling的那部分，再跟大家引入这个使用方式。对。
	有的同学问output power是不是用prompt去实现的？其实这里是这样的，就是首先要做它的功能定位，就是对模型的输出去做标准化的处理。所以它怎么样实现其实都行，但它大概率不会通过大元模型去实现的，因为成本太高了。
	Model IO是封装model的差异是吗？类似做了这样的模型防腐层，不是的，就model的IO我们刚刚讲整个这个叫做model的IO大家可以理解model IO是分成了三部分，输入、模型、输出，这个过程叫做model的IO但这个名字取得确实不够好啊，他们比较着急的把这三类合到了一起。但如果只叫model的话，又放弃了这个I跟O所以这个模块的抽象大家可以简单理解成就是三部分。Model IO the model input, apple output对。
	我尽量回答跟这节课程内容相关的问题。南倩对于LLM是不是可以理解为spring？对于java？这个同学问的问题挺好的。我们待会儿后面会讲，就是关于这个model IO这个模块，我们实战完了之后会带大家有一个感受。
	南倩应该能实现plug in的类似功能，那通讯上还需要使用HTTP给做好吗？还是用pythons的内存通信更好？这里我没有太理解这个问题的场景，这个同学可以回头再看看，我们到8点25就往下面走实战的部分。在最后两分钟在使用南茜的时候，对文档的切分和向量化都是南茜给做好吗？还是自己开发？你可以理解成对于这个文档切分和向量化，南茜都提供了已经预先实现好的类和方法，可以很方便的去做实现。
	大模型回复的答案每次都不一样，怎么能标准化？输出原理是什么？首先你的prompt得用这个prompt template来生成，那自然prompt就比较相对来说稳定了。然后输出的格式就相当于你问的是一类问题，同一类问题，然后这一类问题只是这个参数不一样，这就是涉及到我们的prom的template要怎么写。举个简单例子，就是这个同学问的怎么样把这个事儿做好，觉得问的也是一个挺典型的好问题。所以我们这儿为什么要换的这个变量是X和Y是两个对象。我们我们想象一下，如果我们把这个地方的format下面的这个from改成我们X换的不是这个负Y不是换的这个，而是说我们把大和Y改成参数，你觉得能不能用一个output partner去做很好的解析？那当然就不能了，对吧？
	简单来说就是我们把prompt想成一个函数的话，那这个函数的功能主体是什么？然后它的可变的变量是什么？我们一定要把prompt想成一个函数，把大语言模型想成一个解释器或者编译器。以这样的一个思维方式去写大语言模型的应用是靠谱的那这个时候我们就会反过来推敲自己的prompt设计的好不好。
	一个好的prompt template，应该是能够很好的去抽象出哪些是不变的部分，哪些是变化的部分。就比如说我们的OpenAI translator from the template ate。咱们别把翻译这个词变成了变量，对吧？我们应该变的是输入是什么语言，输出是什么语言。这个显然应该是做成X和Y翻译也变成这个变量。如果翻译变成了变量，那输出的结果肯定就会长得很不一样。
	使用蓝线还可以快速流逝响应吗？可以，这个在model的这个底层实现。有你就知道很多同学会问，今天我们的主拍摄里面会有。对，好，那我们就直接往下走了，我们去偏实战的部分了。
	今天的实战的这个部分，我们同样是通过hitter来让大家熟悉基本功能。首先我看有同学问这个安装的事儿，因为这个安装实在是过于不太好展开讲了。我们就直接通过官方的这个文档一笔带过了，就要装这个南茜。其实首先我们这里特指南茜的pythons的这个版本，pythons的SDK你要去装它，仍然是通过这个pad去安装。然后它有全量安装和部分安装的这个版本。那我的建议是说，如果咱们机器允许，它不并不需要消耗太多的资源就去装全量的。然后全量安装这个完成之后，你不会在这个使用过程当中突然有某个包找不到的情况，这个是偏一劳永逸的。当然你也可以从这个源码去安装，但我觉得这个如果除非你是你找到了他的一些优秀，找到他的一些bug，需要去帮他去做这个开发的过程当中，也可以通过这个方式去安装，不然的话我觉得没有必要。
	然后他的官方的这个文档，其实我应该也附在了这个patter里面。一会儿我们在拍照里面可以看。然后整个官方文档，现在也做了一些更新和整合然后我们看到今天要讲的部分，其实主要就是这个model IO这个部分，几个大的输入front、输出，foot father。而这个大语言模型本身，这个语言模型language model，我们就正正去看看我们今天的这三个。今天这三个我们会从model开始，先讲model，然后再跟大家讲这个front，再讲这个out to father，正如刚刚讲到的，把这个稍微放大一点。这个大小合适吗？这个文字大家能看清吗？大家看看现在这个notebook的界面。
	好，其实我们从这个结果角度出发，这个model IO核心是提供了一套面向大语言模型的标准化的接口。这个接口就三部分，输入输出和模型本身。这三部分各有它的侧重和它的目标，就它有它的设计目标。比如说这个模型它是希望把所有的大元模型都通过这个models这个通用的接口封装好。那这样就是换汤不换药了，对吧？比如说我整个models只要稳定之后，我去调其他的不同的大元模型应该是稳定的。
	就跟我们在open EI translator里面，大家去看代码的话，我们其实也是尝试去做了这样的一个设计。大家如果真的要学好这个能线，一定要去把这个实战作业给做了。因为在做这过程当中，你就能能理解一些设计的心。为什么会在open AAI的translator 0.1版给大家留成了那样的一些设计model为什么要有一个鸡肋，然后我们的这个from为什么是能够make form，然后我们的这个输出当然没有去做太多处理，但输出其实是可以去规范化的。不过我们翻译场景比较简单。然后大家如果要保留layout，其实就需要对这个output power和这个解析做更多处理了。
	Ok说回来，整个第一部分model其实就是要把语言模型有一个统一接口标准化。那具体怎么做呢？这里留了一行，在这三个都word里面都有啊。如果我们要去装最新的这个南迁的pythons SDK，然后如果你本身你的系统里面已经有装了的话，那你通过这个杠U的upgrade参数它会去更新。今天我更新之后应该是这个253的版本了，比它的这个官方文档还要快的一个版本。安装之后。我们还是按照惯例，我们重启一下这个note，以防止有一些引用未定义的情况顺序问题。
	好，这个应该是已经重启了，它重新执行了这一段。那么它的模型抽象其实就是两个部分，我这边专门留了一个就是让大家去感知。我看有个同学在问，就是呃当时学这个open IAPI的时候，我们专门讲的completion和和这个chat confliction API。这俩API其实和这儿有一些异曲同工之妙，但是你也没法硬搬过来。
	对，简单来说语言模型是一个生成模型。就我们聊所有的语言模型，目前聊的这些主流的都是自回归的模型为主。这些自回归模型天然就是一个生成模型，这些生成模型的AIGC对吧？AI生成内容，生成模型就是你给我一个输入，我针对你的输入我去生成一些内容。所有符合这一类抽象的都可以使用语言模型，在它的这个内部叫LLM。
	还有一类是这个聊天的模型。聊天模型最大的区别在于，首先它可以从某种程度上来说，它比语言模型更复杂。它包含了这个语言模型，甚至在实现上它其实也用到了这个语言模型的鸡肋。但是它有一个最大的不同在于，它不再是一个你给我一个输入，给你一个输出这样的简单模式。而是能有上下文了，有不同的角色的上下文。这个在之前我们也已经讲过很多，这就不再赘述了。
	这个语言模型它的主要类的一个继承关系，大家如果想要去看的话，是这样的一个继承人数。就他的比如说我们的OpenAI的一个特定的语言模型，就以达芬奇的这个模型text，达芬奇003这样的一个模型为例它应该处在这样的一个位置，也就后面写的这个example处于这样的一个最最末尾的一个子类的一个时间位置。在这个地方，它的父类叫没有，它然后再往上继承的是base的一个LLL，然后到根儿上的这个继承的叫base language model的一个模型，这里会有一些主要的一些抽象，那这些主要的抽象我们目前用到的最重要的一个叫LLM的result。为了强化这个概念，我下面很多的变量也是这样取的，就是我们模型是一个语言模型，是一个你给我输入我给你输出。那这个输出是一个LAMM大语言模型结果大语言模型输出这样的一个抽象，然后包括这个prompt menu，你可以认为是它输入，这里还会有什么AI message，base message, 这些抽象是用在底层的一些实现上。我们这儿暂时接触不到，到今天这节课大家需要了解主要的就是这两个，LLM results和这个from the value下面很多是我们后面用chain，用agent，用data connection这些模块的时候，他们当然会去调用语言模型，那他们就会去使用底层的很多这些东西了。当然但是留在这里大家有兴趣可以去学习。
	这里有一个API的参考文档，这个是官方的。我们可以看到这边打开之后，会有各种的I的这些信息。就比如说我们这里只写了OpenAI的，下面只写了OpenAI的那如果对其他的这个感兴趣的话，可以直接访问这里，会比我们刚刚看到的这个文档会更直接有效一些。
	因为这份这这一份I文档是通过代码生成的，所以它的代码只要更新了，大家能看到这里有最新的0.253，它如果代码更新了，那这里的这个一定是会更新的。因为这个是直接自动生成的，所以看这个是最干净最直接的，只要你的版本号对齐。那为什么说让大家每次最好更新到最新的版本呢？因为我好像没有发现它维护历史版本。就比如说你现在装了一个0.201的那你其实是找不到它的class的。除非你去切到github上面，然后去看它的源代码，不然的话就没有特别好的方式了。或者你自己本地装一个pythons的API dog生成的一个项目也行，那就有点太折腾了，没必要。对。
	然后在读他的这些API文档的过程当中，也是有一些重点需要去关注的，并不是所有都需要去看啊。就比如说我们今天为了让大家理解大语言模型，这个最核心的是用来生成内容的这个抽象在它这个里面是怎在蓝线里面是怎么样去玩的。所以我们花了一些篇幅去跟大家介绍这个参数，让大家去理解。但实际去研究的时候，大家不需要要花这么多精力，只要把今天这部分搞明白就行了。其他的一些抽象，包括到这个终端，就是具体某一个模型的抽象，大家要去搞明白的时候，你还是先以这个开开发者文档，再是一I文档，最后再到代码实现比较好啊。尤其是本身coding基础不是特别强的同学。但如果我们本身一直在写代码，那去看源代码肯定是最好的，而且它的代码实现也不复杂。
	好，说回来最基础的这个base language model class，这个也是我们的了模型。我们这有一个checks model，也会去基于它去做继承和实现的一个最基础的模型。Base language model, 它代码实现在这儿，我这边就不再点开了。然后这个鸡肋其实它最干净，因为它是在最上面的，这个是根儿根，根上就从这儿来的那这个鸡肋其实它是为所有的语言模型定义了一个接口。这也是为什么说chat的这个聊天模型内部有有去引用这个face language model，也是这个意思。
	它其实它的定义就非常简洁，它就是允许用户以不同的方式跟模型交互，比如说通过提示或者消息就对应着我们的LLM和checked model。然后这里有一个比较重要的方法叫generate prompt，是其中最重要的一个方法。然后这个方法就类似于我们这里看到这幅图，大家可以理解这幅图的核心就是我们有一个底层的base language model，他需要去实现一个generate prompt的方法。这个方法就是输入一个prot输出一个大语言模型生成的结果。这是最底层的实现了。就跟我们今天看到的那幅我们在打开pt里面有一幅图。在PPT里我们有一页叫做给大家切过去。
	就这幅图就跟我们的OKI translator现在做的事情一样，这就是一个最基础的一个实现face language model。有一个输入，然后这个输入给到这个generate prompt这个方法，这个方法拿到这个prompt，通过单元模型生成了一些结果丢出来，这就是一个最典型的一个应用，跟我们的model l也很像，甚至都没有model IO这么复杂。有model l还能对输入进行模板化，对输出进行解析和格式化，在中间还能调不同的模型，但没有base language model是非常早期在南庆非常早期版本里的一个抽象。
	这里就是涉及到一个重点，这个generate from是直接跟大语言模型交互的，所以这个方法一定是由它的具体的子类去实现的，这是一个抽象方法，需要子类去进行实现。我们这里就不再去赘述了。所以这个best language model是最底层的一个实现，然后最重要的一个方法是这个generate from的方法。然后它是从pythons的原生的一些积累里面去做集成的，这里有定义一个language model input和output，作为我们后续今天要讲的这个from和out to the puzder这边要处理的一些鸡肋，这个大家有兴趣也可以去看一看，但是不算是特别重要，算是比较外围的一些内容。然后这里有一个具体的方法叫做predict，就是我们在图当中看到的，将单个字符串传递给这个语言模型，去返回字符串的预测，当然也可以去这个predict message，就是我们给聊天模型去使用的那每一种方法都有对应的异步方法，这边也就不再展开了。
	对然后往下看，我们可以看到其实这个语言模型开始大家有有同学提到，如果我们要做一些流式的处理，流式的响应，是不是也能用能线去实现。在最底层上，他就去实现了这些机制。比如说我们刚刚看到的异步的方法它都有，比如说这个generate prompt我没有全部粘过来。大家如果去看这个代码实现的话，除了这个基础的generate prompt以外，还有异步的generate from的方法，也有这个批处理的prom的这个方法，那这里我们就通过这个代码的链接能看到。
	稍微花一点时间，大家看到在这里有一个class的base language model的实现。然后刚刚看到的language model input out，或者是在这里定义，非常简单粗暴的一个定义，但是也很合理，一个union from the value。然后这里space message可以同时支持我们的这个input就能同时支持我们的LLM这种语言模型，或者是我们后面能看到这个聊天模型，这种base message这样的一个消息的记录，我们之前用OKAPI的时候也是自己维护了一个messages。大家如果有印象的话，那其实就是一个例子。当然最粗暴的，你直接给一个string他也是能接受的。只要是他能够序列化的，然后这个是他的实现，我们这就调回来。
	那类似的这个批处理，通过刚刚那个PTU的链接大家也都能看到，比较核心的还有一个点在于这个l am的results。这个是他返回的一个结果，就包含每个输入提示的一个候选生成列表。这个跟OpenAI也是完全对齐的。
	OpenAI本身的这个API确实对年前真的这个设计有一定的影响。然后OKI相当于也是有这么多的受众，大家理解全球可能少说有将近10亿人都大几亿人用过这个CIGBT。这个CIGBT的一些标准的使用模式，包括它的API自然会影响到大家，所以它会返回这样的一个格式也是合理的。然后包括这里还预留了一个额外关键字参数，这也是为了更方便的去做扩展。
	就是我们未来只要我们能实现针对大语言模型的各种输入和这个call back，包括它的这个材料，他的一些hope这样的一些设计，它这都能传进来。那你自己去处理传进来的内容就好了。所以这样的一个方法大家理解到位之后，其实这幅图就很好理解。就我们有一个generate prompt的方法去实现predict，或者说这个predict messages，然后就对应的这两条线，它的输出是一个language model的output，输入是一个language model的input。如果它是一个from the value就走上面这条线，如果是一个list base messages，就走下面这条线。那为什么叫base messages？我们下面讲这个具体实现的时候可以跟大家分享。
	然后接着最根儿上的这个base language model实现之后，其实接着大家能看到这里有一个base LLM，这个其实是真实的这个LI，就是我们的大语这个语言模型的抽象的一个积累。然后他去从这个base language model里面去做了继承，去实现了一些在门线里面的一些想法。包括用这个conference去定义的这个and desk这个对象的一些配置上的一些设计，包括这个缓存的一些属性，包括我们打印的这个就我们南迁作为一个开发框架，很多人在开发过程当中都会打印一些内部状态，这样的一些信息都放到了这一层。包括一些call back和call back manager的属性，包括在运行过程当中，它可以支持这个因为你每次开发大语言模型，大语言模型应用都去调大语言模型本身的话，这个token消耗是比较多的。所以它也支持这个fake LLM，这是一个假的。大家理解就是写代码当中我们可以运行假数据也好，mock运行也好。类似在运行态的时候，它也有一些管理。这些都会被加到它的这个base l2M这一层里面来。
	然后再往下是他真正实现的这个LLM这一层。这一层是为用户提供了一个简化的接口来处理，然后这一层不需要用户去实现一个完整的generate方法。然后这一层其实是我们刚刚我们再回忆一下，在这个继承关系里面，真正我们的所有实现的这些大约模型用到的副类其实是这一层，就LLM这一层。所以大家如果刚刚这两层有一个初步了解之后，定位应该也很清楚了。Base language model定义了这个三件套的一个设，然后能够比较方便的去支持两类模型，语言模型和我们的这个聊天模型。Base LLM在年轻人开发层里面提供了缓存，提供了这个运行态的一些设定，包括提供了这个config相关的一些内容。
	然后接着到了实际实现的LM，因为这只是定义了一些抽象积累，这里是实际实现的叫LLM。然后LLM的这些具体运行，就是这些模型到底要怎么样去调用，这些模型返回的结果是什么样的，apr怎么样的这些东西在最后这一层这叶子节点上的，或者说在最子类的这一层实现的。他这里就举了一些例子，比如说AIRE这家公司，然后hiking face hub上面的各种开源模型，包括OpenAI的我们的competitions API和我们的check competitions API，也都是通过在这一层去做的这个实际实现。
	那LLM也是一样的，这些方法都不变，然后我们这边输出一个M的result，到已支持的这个模型清单这里我们可以看一下，其实我们能看得到这里我附了两个文档供大家去了解除OKAI这种模型以外的那如果要开发其他的语言模型的话，可以通过这两个去做一些了解。第一个是这个开发不是这个。第一个是这个开发者的文档。这里其实有提供语言模型的其他的一些开发者文档的接口。比如说OpenAI的在这里，那如果咱们要看其他的，比如说咱们看这个挨着的OpenAI这个微软云提供的OKI，那可以去看，这里去跳不同的这个文档，可以看到比较清楚。包括像这个CHATGRM，他这边也有去做对应的支持第二个这个文档我们叫代码实现的文档，就具体这些刚刚看到这个你支持的语言模型清单具体的实现在哪儿呢？
	在这个目录下面，我们能看到这个蓝线的这个left下面有一个代码结构，我稍微跳转到这儿给大家讲一下。我们能看到整个南线的这个项目，其实跟我们现在的这个课程项目很像。就我们这个课程项目有很多的同学都遇到了这个项项目路径的问题。我们如果跳到南线的根目录上来看的话，其实这是没有什么代码的。所以大家如果自己想要在本地去开发能力也好，开发我们的课程项目也好，一定得记住把你这个pythons项目的这个项目路径给设置。
	对，那比如说这里可能就得到这个南茜的left，下面就直接有两个级别，像这个experimental是一些实验性特性。我们进到南倩的这个相对稳定的这个接口的目录里面来到年前，然后这里是他的测试相关的一些代码，和他的实际实现的代码。在这里面，它的这些模块基本是有一个相对来说比较跟文档对的齐的一个目录模块的一个规整。然后在这里面我们能看到有模型。Agents document LLM然后我们刚刚其实就是这个路径，就我们刚刚跳过来就这个路径，这里就每一个几乎对应着我们的我们的这个开发者文档里面的内容，像我们的OpenAI其实也是在这里面去做定义，包括像刚刚提到这个fake也都在这边有做这个对应的定义。所以大家如果对其他的模型感兴趣，也同样可以使用这样的方式去做访问。
	然后我们回到这个示例里面来，他在这边有两种模型可以调用。我们现在讲的这个语言模型，我们提到是类似于这个conflation API的，所以这里的实际实现就对应的这个competitions API里面的大量的参数了，这个一层四层的这个设定，应该大家就有感受了，对吧？就到最后这一层是真正的实现了我们的积累的实现。然后这个基类的实现里面有我们的具体的跟这个特定的大约模型，不管是私有化的还是这个公开的API相关的具体参数，就包括我们这里写到的temperature。刚刚大家有提到设置为零的话，相对来说就很稳定，多样性几乎就没有了。然后包括我们的这个token的数量的上限，然后我们的top p就返回这个候选的质量总概率的概率。然后包括这个是拿他的这个机翻的，回头我再调整一下，有一些这个应该是token，但这个不影响跟大家之前调用我们OKI的API是完全对齐的这个参数，所以看起来是完全一致的，大家能理解这里就好。
	那么回到实际的动手上来说，会有什么样的区别？首先人券点LLM这个是对应的我们刚刚代码里面看到的，从这个labs到年券到下面具体的这一层就对应的这个这里从这一层开始，从我们的这一层开始的一个项目路径，往下看基本上就是比较能对齐了。然后我们再再看这个怎么样去导入的，他导入了一个OpenAI这个OpenAI跟我们这看到这个base OpenAI还略有不同。这个open a其实是把把这个SOAI做了一层简单的封装。我看到这个代码实现这里，这个是我们刚刚看到的base OpenAI的一些实现。然后大家需要注意一个小细节，待会儿我会讲。就是南倩现在没有把一些比较敏感的AIP做处理，所以大家如果有提交代码，一定要注意别把那些信息打印出来了。就在这个里面我们会看到有一个叫open APIP的这个OpenAIAPIP这么一个成员变量。这个成员变量是咱们之前课程里面一直有讲到的比较敏感的信息。如果我们在patter里面直接输出这个模型的OKIHIK，它是明文输出的那大家千万不要把这个内容传到我们的代码库里了，因为只要记到这个git里都会有这个记录。
	然后实际的实现OKAI的这个model是在我们看一下在这里它是基于我们上面看到的这个被收回I对然后做了一个简难的实现。但这几乎就没有什么，它的内容几乎就是复用了这个base OKI。所以大家看这个备受KI里面是比较干的内容，然后我们要去怎么样去调用呢？我们之前调OKI的API的时候，我们用了一个completion的API，然后它是使用OpenAI提供的pythons SDK。大家如果有印象的话，然后有两个必要的参数。当然这个model其实也可以省略。我们可以默认用一个model，然后用这个text达芬奇003这个model，传一个prompt。然后这行代码就相当于是一次调用，就每执行一次，这个相当于调了一次API，让我们每次需要给一个不同的front，然后我们甚至在一次这个防汛框框里面把它构造成了一个单独用HTTP请求的一个函数。
	这个在我们去用蓝线的时候，会有一个相对来说比较明显的差异。就以前我们调大模型，每次咱们需要把不同的from path它丢进去。在大语言模型应用开发的时候，其实这个模型相对来说是比较稳定的，比较固定的。我们其实需要去玩的是不同的prompt应该怎么样去做调整，去动态注入这些prompt。这个是我故意把它拆成两部分，想要给大家感知到的一个不同，那接下来我们在prompt这边会有更明显的感觉。
	现在实际要去执行它的话，我们从这个LLM里面去import一个OKI，这里的OKI只能去访问它的语言模型，也就是我们的completion的这个接口，它的check composition要通过别的一个下面我们要介绍的一个封装来进行访问，大家可以看到这里。我们去让他输出这个一个joke，然后他是就直接输出了。就是我们调用这一行OKI completion create这个方法是完全一样，内部其实实现了一次调用，执行这一行其实他就已经调了一次text w7003，大家如果去后台去看OpenAI的调用量的话，也都能够反映出来。
	然后我们继续把当年讲这个open IPI的一些例子给大家做对比，有一些感知，然后再做一些扩展。我们让他讲十个笑话的时候，会跟当年遇到同样的问题，就是我们的max token其实是不够用的。能看到这儿max token是256对吧？但其实整个LRM这个模型它已经变成了一个实例了。跟这儿巨大的不同就在于之前这都是一些API接口，我们其实是没有一个有状态的模型维护在本地的。
	那兰倩帮你维护了一个模型，这个模型是一个text w7003的模型，然后这个模型他的max token是256，那你可以在这儿直接去做设置。当然这里的APIK相关的一些也都可以去做设置。那这个时候我们再去查看这个LM，它自然就会变成1024了，对吧？这相当于是一个有状态的模型。那我们再去输出这个结果，其实它是明显十个笑话应该是超不过的。我们可以看一下。
	Ok他给了十个笑话，所谓的这是我们都是玩过的套路。就不再展开了。然后接下来我们再看一个生成代码的应用。我们把temperate temperature作为零生成这个代码也仍然是可用的，跟之前是类似的，我们把它用XQ这个方法去执行一遍，相当于就用pythons解释器执行的这个代码了，可用的。那到这儿为止，其实我们把语言模型以OpenAI completion API为例，给大家简单过了一遍。需要注意的重点就是虽然在这个人性里面的代码和文档有大量的模型，但其实大家只要整明白它的层次设计关系，也就是我们这里看到的这四级的抽象，把这些收起来。这四级的抽象，我们的base language model，我们的base LLM，又到我们的OpenAI，其实整个四级的关系也是非常明确的那具体做的工作也是比较好的。然后我们的LLM plus里面其实是给大家打了一个样，对吧？那不用实现完整的generate，我们的fake LLM有基于他去做一些实现，整个逻辑的继承关系是比较清楚的。
	然后需要注意一些细节，是因为南倩还在快速迭代。所以在我们刚刚给大家展示的这个定义OpenAI的这个具体实现的文件里，还有一个容易引起混淆的代码。我这边正好就做一个说明，就在我们的这个。在这个OpenAI的这个代码里面，它还有一个实现。这个实现其实已经是被废弃掉的一个时限。但现在他还没有去做这个处理，就是我们的这个OpenAIr chat，今天今天应该还会有很多人会去看这些代码。
	我估计就在代码里面可能会看到这样的一个实现，叫OpenAI shut。从名字来看，应该是跟我们的shut competition API去做对齐的，包括他自己的这个注释也是这样。但其实现在这样的一个接口，包括从LLM这里去做导入这样的一个设定已经被官方遗弃了，很快就要移除了。实际上它的这个定义很显然会放到check models里面去。在这个nature in，也就是我们接下来会讲的，所以这一块大家要跳过一个被遗弃的操作。如果我们参考这个代码里面的示例，也会收到对应的警告，可能会在未来某个时间就无法使用了。
	那我们现在看看聊天模型要怎么样去实现。在聊天模型的实现里面有一个类似的逻辑，就我们基于底层的best language model，刚才也有讲了，它的输入base language inputs支持这个list messages。而同时它自己有一个类似于base LLM的实现，叫base CHA model。下面就直接是我们具体的实现了它的主要抽象，也就是我们的message里面的3种类型，然后现在也正在扩展支持这个function calling。这三类跟我们的当然这里少写了一个，应该有一个system message。大家可以理解这个base message，就跟我们看OpenAI的这个translator，就我们那个实战项目很像。
	我们实现了一个content，我看有些同学还找不到这个类。Content就是一个内容的抽象。这个内容抽象上面可以有文本的内容、表格的内容，甚至图像的内容。这也是一样的，它实现了一个基础的message叫base message。然后这个base message往上扩展可以增加肉，增加爵士的信息，就是我们的AI message，human message和他的这个system message。当然这个system message现在不是所有的聊天模型都支持，可能chair t和AI是支持，所以他没有把它放到这个主要抽象里，但我们可以放进来。
	然后这个CHAT model的对应的HI文档，也是在刚刚我们展示过的那个API文档里面，就有代码生成的这个API文档里面，我这就不再赘述了。然后ase CHAP model plus这里其实是跟我们刚刚的base LLM是完全对称的。然后在这个cheap OpenAI这个类，就是实际去实现我们的OpenAI chat petition API的实现。这里我们会发现刚刚在我说被遗弃的那个地方是另一个类，跟这个类是有所不同的，也是他们故意强化了这个chat的属性。然后这里跟这个checks confusion API的参数也是对齐的那我们就往下看，跟实际去调open API的这个OpenAI的这个HI的对比，其实通过这样的一个对比也能明显的看出来，依然是维护了一个自己的有状态的模型。
	Cheap model, 通过chat model这个包导入的。然后在这里面就是刚刚我们提到的system message，就对应着这个system role。我们在之前有这样的一个示例，在调OKI的时候我们仍然可以复刻这个示例。这个示例一个比较明显的感知就是我们看代码。把这个role直接变成了一个具体的类，把这个system变成system message，把user变成了human message，把这个assistant变成了一个AI的message，也能理解他这样的抽象。因为也许不同的大模型他们就不叫user了，可能或者说这个就不叫assessment，确实叫了AI。所以把它抽象成这样的一个类，也是为了方便概念上的对齐。
	我们之前一直讲的这个meta的这个逻辑就是在这儿，那我们从这个schema里面去导入了这三类具体的message，然后把它做了这样一个message的一个队列。我们输出一下可以看到跟我们之前去构造那个OKAI的AI的时候的message是类似的，仍然是一个财神列表，按顺序去构造了他的聊天记录。然后只不过里面存储的内容是它的schema，包括下面的一些预定义的实现，就不同的角色有不同的内容，甚至里面还会有一些其他的参数。我们后面会讲就我们的这个什么example，additional这个参数。对，然后我们把这个丢给GPT3.5，就我们这上面应该有定义，T3.5的turbo而且就会去调这个模型输出了一个内容，就是这样的一个结果。
	这个结果大家如果去对比我们的OKI那个notebook，应该是完全一样的，很稳定的只用这个连线的方式去做了调用。我们可以看到输出的是这样的一个对象，我们还记得之前在这个open IAPI里面输出的是一个OpenAI的object。大家如果还有印象，咱们现在这个OOKI也好，南京也好，大家想要去统一大语言模型开发框架，这个事儿是所有人都想干的。因为这就是一个偏开发者入口的工作，所以自然就会有很多人都去干这个事儿。这个大家要去理解，为什么不是一个字符串，当然他这样做还会有很多额外的好处，就是在他的框架内部可以去做大量的复用和功能性的扩展。好，到这儿为止我们留五分钟的时间提问，看大家有什么问题。这个代码我们课后会推到这个仓库里。因为有一些机翻的不是很好，机器翻译的注释不是很好，我要再检查一遍，对。
	还有一个同学问，还有一个CCTOKAPI是什么区别？刚刚就在讲这个区别，它就是一个新的接口，在CCT models里，在LLM里面的CCT的I已经被遗弃了对。对，function message已经支持了，我们会在用它的时候再跟大家去做这个介绍。到9.05，看看大家有没有什么这个问题。
	私有模型怎么接入？这个刚刚讲过的哟，这个同学我们私有模型怎么介入，正好就放在这儿了。大家看看这个文档，这边写的很清楚，核心其实就是实现这个具体的模型。我们刚刚这个l am是一个OpenAI对吧？所以也可以是一个chat GLM。我们的OpenAI translator的代码里面也有一个类似的这样的实现。
	大家如果还记得的话，最大的不同就是它维护了这个状态。所以像这些API调用的参数，它就直接维护在这个模型的实现里了。只要你不去变更的话，那你都只需要传入这个prom，它就能一直维持这个状态去使用。
	对，这个问题我问过文心一言的开发团队，咱们有一个同学问文心一言要支持南县的话怎么实现？应该这么讲，首先这个事儿完全取决于这个文心一言的团队自己开不开放。对，因为如果他想要积极的去去拥抱的话，肯定也很快就能够合到社区里。现在的年券还没到0.1版本，已经有1000个开发者，1000个以上的开发者的代码合进主分支了，所以社区是很开放的对，我看文心他们的状态好像现在是to b为主，我们也在用文心，没有太社区化。对然后。对，所以是这么一个状态。
	然后如果你实在想要用它的话，你可以把南线代码拉下来，然后自己本地做一些简单的改造。我们为什么要讲代码也是这个需求。不是，其实你只需要实现一个chat和API类似的类就好了。然后相当于你实现了一个文心一言的model，基于base的chat model，然后实现几个主要方法就可以了。参考着改一改，那还是这果然有同学问，当然可以，只要社区和百度愿意的话，我们肯定可以做对。
	AI translator当中的PDF解析和创建的模块如何介入到人线中？首先PDF解析的模块我印象当中年轻人已经有了，不过他解析效果没那么好啊，这也是为什么前面要讲就是PDF解析是一个上手容易，要做好上限非常高的事情。所以大家如果这块做的够好，也可以往南县市区去做贡献对。
	然后有没有人在做其他语言的版本？是想说能倩本身是吗？肯定有啊，我相信肯定有。而且现在是属于这样的一个状态。从我个人的判断来看，南券是一个很早期的项目，现在连V0.1都没有发布。我一直在强调这个点，大家看它的版本号是很很诡异的。就是正常一个开源项目是不会这样去列版本号。到了他现在是属于那个大家如果去看啊，属于这个V0.253，19小时前刚刚发布的，今天发布的那么一个如此新的版本，一个如此新的开源项目，版本变更如此频繁。
	去做多语言本身很困难，这个是一个客观条件。就是在这样快速变化的一个本身就不是很牢固的一个状态上去做多语言很困难。第二，一定还会有很多竞争者想要去自己另起炉灶。因为能却已经是一个市值好几亿美金的公司了。那么我如果能做，再多做几个语言的版本，给他做加一，我何不自己去做一个东西呢？啊，当然一定还会有这样想，所以所以一定会有很多不同的声音和不同的想法，但是无论如何，我们能看得到它的趋势。
	现在已经很难追上了，除非某一天谷歌或者阿里，或者谁谁谁哪个大公司突然举这个公司之力，就是要干一个人现在的事儿，并且还真的干出来了。我觉得是有可能超过的这就像pencil flow，虽然那会儿已经如日中天了，但他还是有一些问题。比如说的易用性他他比较难懂，他的概念比较难，还不像南茜这样说概念多复杂散。还是说tensor flow本身有很多概念很难。那这个时候cross去简化抽象了，path watch也去简化抽象了。虽然性能没有它好，但是受众就广对。
	向量数据库是怎么选择的？好问题，我们讲向量数据库的时候会专门去分享。但是刚刚那个课件里有两个这样数据库是目前用的比较广的对。OpenAI为什么要分这两种类型？这个我刚才也讲了，就是不是所有的生存需求都是要聊天的。有这种简单的生存需求的那它就应该有一个语言模型的抽象。不然我本来只是想做一个简单生成，我要去设定一堆message也不合理。
	OpenAI的每个实例维护了绘画状态吗？适合对话绑定的吗？如果是的话，大并发下面内存使用会不会有问题？首先回答内存使用应该不是啥问题，就是你想你的内存和你的钱包比起来，一定是钱包先瘪掉，因为偷看太贵了。你想通过文本这种东西占内存才占多少？你就算一兆的内存可以表达多少文本了，汉字也没几个字节，对吧？
	然后适合对话绑定的吗？应该不适合对话绑定的。如果你是指的那个聊天那个a chat的OCI的话，那肯定不适合对话。不是你说的并发的这种使用场景，因为它要有上下文聊天记录的。除非你维护了几个不同的用户，就是你你像CATGPT1样，你有很多个不同的API and the point这样的情况那有可能，但是如果是那样的情况的话，你一定是会做负载均衡的，并且针对每一个具体的模型刚刚有一些参数没跟大家细讲了，针对每一个具体的模型，它这其实是有类似的设定的。
	就我们之前的OpenAI的AI课程里面有讲过，就是针对OpenAI的服务端的设定。有organization，有API的base，有API的key，这些都是在OpenAI的服务端去设定的。然后针对同样的OpenAI的这个organization，你是可以设计一个费用上限的。所以通常来说不太会在你没有感知的情况下直接就用完就只要你在服务端设置了这些上限，它不会无限制的去调用的，包括这个代理。
	并发其实你只要不达到OpenAI的上限，你的财力够用应该都是可以的。Open IAPI的并发数挺高的对。好，我们继续，因为内容还挺多的对，然后我们接着看这个呃这个model部分我们讲完了，接着看这个prom prompt，就相对来说更花样多一点。因为我们一直在讲prom就是大语言模型开发的这个最核心的部分，就相当于它就是我们的大语言模型的编程语言了。某种层面上来说，今天一次课肯定也不可能把他的所有的最佳实践的使用方法讲完。让我们尝试把它的入门要用到的一些使用方式方法、接口都跟大家做这个分享。让大家下来至少可以玩各种各样不同的，这个是模型prompt。
	好，那么from的核心是干嘛呢？这里有一个简单的总结是模板化动态选择和管理模型的输入，这是它官方给的一个说法。我的理解就是prompt是干嘛的？它其实是首先prompt是语言模型的输入，是语言模型的需要对吧？语言模型需要这么一个东西用来生成。因为上由上文生成下文的一个智慧规的这个语言模型。
	这个prompt怎么来的很重要。就是我们知道内容是AI生成的那让AI生成内容的这个process是怎么来的那这其实就有很多种不同的做法。第一种最简单的做法就是我们写死一个form。比如说我们刚刚看的那些事例，讲十个笑话，写一段代码，这就算是写死的一个pro还可以是我们用这个参数化的方式。比如说X和Y是动态注入的，那这样的一个方式也是可以的参数化的一个模型输入。甚至这个参数化的模型输入这个XY本身的这个模板本身就这个does x like y and y就这个组合出来的这个模板本身就是就我们这是两个阶段对吧？这是没有这这是没有合并的那就这一对组合，甚至本身这个模板下面这个模板本身也都是有多个可选的那这也是一种。
	还有就是我们之前学过像这个fuel shot learning，对吧？那我们知道要怎么样能够给一些好的few shot，让大语言模型生成的内容更好。但是这个fuel shots我们之前也是写死的，我们针对一个特定的聊天记录，一个聊天窗口，我们写死的一些few shot。这个few shot能不能是从一个集合里面自动选出来的，或者说手动选出来的那就是example selector想要干的事儿。
	整个notebook我们就讲明白这两个重要的组件，一个是这个或者说这个prompt这个prompt这个组件下面的两个小的部分，一个是from the temple place，一个是这个example the selectors。Ok我们继续看这个提示模板，就我们下面的这个玩意儿，下面这句话深深绿色的这一部分怎么样去构造，使得我们的应用开发能够更简单。Prompt template, 我给它定义是它提供了一种预定义的动态注入的模型，无关的和参数化的这四个最重要的特点的提示词生成方式。
	首先它是一个提示词的生成方式，它通过模板来生成。既然是模板是预定义的是预定义的这是第一点。第二点，我们刚刚看到XY对吧？它是动态注入的，而不是一个写死的东西。它可以有预定义的下面的这个模板，也可以有动态注入的上面的参数，这两个概念大家应该理解了，我之前的这个图片。
	第三部分是它是模型无关的，简单来说就是我们换了别的模型，理论上这个prompt应该也是能重用的，这个其实大家可以自己去做实验。为什么在open s translator这么简单一个项目里面，我们也用了两个模型，也就是这个示意。我们在那个代码里面也有一个make prompt，类似于from the template的一个操作，也是想让大家去感受这个prompt不要写死，一定要能动态注入。
	参数化的这个参数化就不只是说我的输入可以参数化，它这个里面有大量可以去参数化调整的内容。就这个模板用这个提示模板去生成提示词的这个中间可以玩的内容特别多。所以你看它的类基层关系也就比刚刚的模型要复杂很多。我们在刚刚的模型里是一条像是一个链状的结构，一直到最终的具体的模型那儿，都是一个链状的结构。但是我们的from分类的就很花样多。对，还有这个pipeline，有这个street，有这个base chat啊啊啊bash chat，也后面又衍生出了这个auto GPT的，然后巴拉巴拉有一大堆，那这儿我们就不再展开了。大家可以去具体的开发的文档里面去看啊，这边就不说太多。
	然后这里会有一个像这个from the value，这个是我们在很底层的语言模型里面看到的一个输入类型，输入输输入的参数的类型。它支持两种string和这个checks，应该也很好理解和对齐了，就是两种不同的模型。然后对应的这个实现，大家有兴趣可以去看一看。
	然后我们具体来看怎么样去用这个prompt template去生成这个提示词。就prompt the template是一个类，也是一个class。那它有多种去生成提示词的方式，在这里就开始有一定规范化的设定。
	在这儿就我们之前看这个open HS latter里面有一个方法叫make from。大家如果还记得的话，那个方法是把我们解析出来的每一个书里面的content类型。这个content类型就有点类似于我们nature里面找一个对应概念，就是有点类似我们的这个base language，叫叫你们会用到的这个base ase message很类似于这个ase message，尤其是这个文本的content，这个text content就是一个字符串。那我们的人圈里面的，各种message也有一个content，也是字符串。那这个字符串，在pythons里面其实是可以被，用我们所谓叫format的方式去动态注入的那这里，有我们今天讲这个比较常用的三种方式，一种是官方文档里面首先提的。但我认为不应该是作为最主要的这个方式，也有别的方式可以去选，就不要大家只会这个就就可能会有一些特定的prompt写不出来了，为什么呢？因为比如说像这个string format这种语法去生成模板的方式，它其实是要把我们动态注入的参数用一个花括号包起来，就跟我们上面的这个图其实是一样的。这也是最常见的做动态注入变量的占位符的方式。
	然后这里面是我们的要注入的变量名两个变量。是通过什么方法呢？叫from template这个方法，这个方法就是说我给你的就是一句template，然后里面还有我要动态注入的这个变量，就通过他们来表达。然后实际去生成的时候，使用的是它的画面方法，然后动态的把这个funny和chickens住进去。这个就相当于是我们的X相当于是我们的Y那就相当于我们的for，相当于我们的bug，这个应该很好理解。然后实际执行一下，我们也重新启动一下这个notebook，让大家看见到一个干净的cell的执行顺序。
	Ok他把这个住进来了，对吧？Tell me a funny joke, chickened. 然后我们也可以干嘛呢？也可以通过这个format直接去生成这个提示。如果我们没有任何的需要动态注入的，也能通过这个方式去构造，这个是没有问题的，不会报什么错误。然后咱们去看一下这个from the template t本身是一个什么东西，我们还是用下面这个有注入的形式。
	这里是一个prompt这个template，这个是一个类，对吧？那他实例画出来了，我们有两种方式去对这个面向对象的一些基础概念，我们就不再赘述了。主要是给大家看一下这个类里面有什么样的一些关键的成员变量。
	这有一个很重要的叫input variables，就是我们的输入这个参数。这其实就是我们要动态注入的这个变量，这个是一定要在我们实际去生成提示的时候注入的。因为它是一个占位服务，它没有实际的值。然后包括它的outfits t father，这其实已经有一点剧透了对吧？就我们的output father其实是在from template里面就可以就要去做定义的。然后它的template具体的是tell me a巴拉巴拉joke about，巴拉巴拉没写。然后这儿其实它即使用的这个string format。因为是pythons 3，他也会用这个f string在内部去做的格式化的处理。
	然后还有一种方式是什么呢？就是我们使用它自己的构造函数，这个是from the claim的自己的构造函数。那这个构造函数，其实他会去实例化一个这样的对象，就我们的from the contact的对象。但他会去做一些必要性的检查，就我们刚刚提到的，比如说我们去执行这个就会报错，对吧？因为它没有去传入第二个content作为它的input variable，那它自然就没法识别化出来，就少了一个需要动态注入的占位符的变量，那我们怎么样改对呢？其实也很简单，就valid的就是把它写在这儿，自然就能够去做这个，我们可以再输出一下。
	中文，这俩其实就是完全一样的，只是两种方式。这两种方式各有优劣。一种就直接从模板来的，你很清楚你可能是之前就已经做过了，你就再移植到这个实现上面来。还有一种就是可能你自己还不清楚这template应该长什么样，你不断的在调这个template，那你就锁定好你要什么input arrive，那就调这个就好了。对，就这两种方式差不多，然后这个也更清楚的能够知道你到底要动态注入什么样的变量，他也会去帮你去做检查，不会漏掉。然后实际执行一下这个format，也就输出了。
	然后那我们怎么样去用它呢？说刚刚这种比较典型的去讲它的功能以外，对，第一个比较典型的就是我们刚刚这个讲笑话了，对吧？我们可不可以让他动态的去讲多少个笑话呢？当然可以用这个模板的方式传一个number。
	定义好这样的一个team page之后，我们用这个completion这个接口，不需要维护一个message。然后把这句话其实就是去生成了一个提示，这个提示通过后面的方法传入一个number。等于2。理论上应该讲两个笑话，对吧？我们把这个token稍微给多一点，免得他不够用。那这个时候讲两个笑话对吧？好，我们这就不赘述了。
	类似的你也可以动态的直接去调整它。这样我们就直接可以通过一个2和3这个参数的变化，就能够比较方便的能看到它的这个单元模型生成的内容受到我们的一个控制，除了刚刚提到的这两种方式以外，也可以用均价图，这个也是一个非常常用的一个模板化的方法。它会略有不同，用双括号来表达。
	这也是为什么要给大家讲一下不同的模板化提示的内容。就是大家到后面的能线的应用开发的过程当中，你会发现我也不是一个动态模板就能搞定我的应用了。我甚至我的模板，你可以这么理解，就是我现在用这种手段生成了一个动态的提示词，但可能这个动态的提示词里它会有嵌套。就是我这层提示词动态生成论里面还会有一些内容，是我想要保留成一个变量的。然后这个变量是我下一次调用我后面调用另一个大语言模型的时候才会注入的。你就可以理解成我们以前写代码的时候，我们会有一幅判断，会有循环的逻辑。那现在你就假设你现在用prompt再写一个if的逻辑，然后这个if的逻辑，那你这个条件判断是可以在这一层里面，在if条件判断语句这一层里面就注入了，你用一个画括号就好了。那假设你现在有多层，那你如果有多层的话，你那个变量要怎么留着呢？
	那一种方式就是说我不用这个检查，就是我我不用这个input arrival的检查，我也不用这个划符号，我用一些其他的符号。Ok但是不确定大语言模型会不会对你的那些其他符号有影响，那你可能就得在这个prompt里面让它忽略这些特定的你用来做占位的符号。我们在之前讲CHTTPPT的tricks的时候，我们有说你可以把它埋在这个system row的这个message里面，告诉他哪些东西是占位符，哪些是分隔符，哪些是巴拉巴拉，这样是可以的。但如果你现在只是一个简单的一个competition的ati那可能你也可以考虑通过一些其他的模板化生成的方式去来表达占位符。这样你可以操作，你可以用的这个prompt本身真的内容就更丰富了，就相当于某一个编程语言的保留字就更更少了，你可以写的东西就更多了，这样可以玩的这个手段也就更多。这里唯一的不同就是这样的一个调整。
	通过这样的方式，也可以去达到一个模板化提示生成的效果，只不过通过的是这个金价two这么一个模板。这里我们可以看一下，咱们的这个金加速的模板，它生成的这个内容，比如说是一个什么样的类型。大家能看到这里会有一个明显的参数变化，就是我们的template format，这里改成了这个均价图，然后没可以登录f string。Pythons 31个非常好的特性，然后我们只要使用这些都使用pythons 3了之后，它默认的这个string format和f screen其实是统一的，都是走的这个f string，就是这个format string能通过这样的方式去注入。大家如果有paton 2的老用户就会知道，这样要写一个format spring会更恶心一点。
	到这个地方，其实我们已经可以感受到这个prompt可以动态输入，并且有多种多样的方式可以做占位符去注入。这个是from the time template提供的第一个方便的一个设定，并且还能帮你检查你的这个动态注入的变量有没有缺漏。第二个，就是说我们怎么样去做一些实际性的应用，就除了刚才所讲笑话以外，就比如说我们要生成代码，我们可能要生成不同类型的代码，然后这些代码还可以再进一步去做单元测试。但这个单元测试就可以是一个统一的一个测试数据了，对吧？
	那假设我们现在还是生成快速排序的代码。但是，这个快速排序的代码我们用同一个prompt conflict变换的是这个编程语言，那个编程语言就变成了一个变量，这个编程语言我们第一次传入的是pythons它会去执行生成这个pythons，生成了这个pythons的代码。然后第二个我们把它它生成java的快速排序代码。
	然后我们需要知道的是在当前的jup iter环境里面，我们是可以直接通过这个SQ来执行快速排序的。再看看model里面也有了，但java不行，C加加也不行，对吧？然后大家可以看到，其实它这儿可以可以比较明显的感觉，应该是从某一个代码库里面些从训练语料里面捞出来的。甚至有中文的中文的这个呃咱们说的这个注释对吧？到这个交换数组类的两个元素等等，因为我们给了这个中文的from，大家也可以试试回头把这一段话改成英文输出来的这个快速排序，应该是没有中文的注释的，这也是很有意思大模型本身处理的一些手段，我们也可以去生成这个C加加的。
	其实这三个事例想要给大家，它生成的这个C加加代码，并且类似于markdown的处理，把我们放到了一个code block里面。这三个不同的代码其实特别类似于一个什么样的概念，就是想让大家感受。我们其实是在把这三次调用当成一个函数，有点像这个兰姆达函数，这个函数里面有一个函数就叫LLM的大语言模型，它就像一个执行函数的解释器去。那这个函数本身，其实是由short form template来定义的这是一个函数名称。那函数名称里面的这个program language programming language是你的这个函数的变量，可以注入，传入，在使用的时候LLM来执行它。那从这个视角大家再回头来看这个function calling，其实也是类似的操作了。我们在定义function calling的时候，也是抽象的定义了这个function有什么样的一些功能，需要什么样的一些变量，哪些是require，哪些是option。
	整体大元模型我们也在讲怎么样忘掉编程语言本身的语法，去专注于我们要实现的任务。然后哪些是变量，哪些是这个参数，就是这个意思。就我们把真正把大元模型当成一个编程语言也好，编程语言的接口也好，框架也好，其实就有类似于这样的体验。只不过只不过现在我们只是生成一个快速排序的代码，但你也可以生成很多其他的代码，甚至这三个可以是串联的，就是我们的chain。
	好，那么接着再看一下聊天的，就是适用于聊天模型的这种prom它要怎么生成？我们刚刚看了base language model最底层的这个定义的话时候，它有是它有一个鸡肋是language model input。Language model output大家还有印象吗？Language model input里面就可以同时支持我们的语言模型和我们的聊天模型。所以咱们现在这个聊天模型基于底层的语言模型是没有问题的。
	它的输入这个front的输入也是分成两种。一种是这种from message，就是跟上面一样完全对称。比如说这个from message这样的一行，这个里面维护了一个列表。大家细看这里有一个list的列表，专门换了行。里面每一个元组二元组，每一个二元组都是一个grow加上内容的这么一个格式。通过这个方式是可以直接去构造生成一个用于聊天模型的提示模板。
	然后具体的提示模板的生成是一个对称的方法，叫format messages。大家看上面是format方法，这里是format message方法，对吧？然后这儿有两个需要动态注入的，一个叫name，一个叫user input。
	大家细看的话从这儿开始大家就会感觉写这个prompt越写越复杂了。尤其是你写一些复杂的from的时候，因为特别多中括号是这个圆括号、花括号各种，包括我们如果用了新加数，还有两层花括号的。所以为什么说是一个新的编程语言？有一些point是在这儿就是要习惯使用自然语言来进行编程，并且还要跟大语言模型去做调试。那那这些就是你必须要去适应的，但他有很多的好处，我们也一直在体验到。
	我们把这个message可以通过这样的方式去做生成，然后把把boss和user input传进去，就有点类似于我们刚刚说的format，把pythons就把pythons java和C加加传进去。输出那输出的这个message就是我们真正的填了这两个动态注入占位服务变量的这个。那我们细看一下这些。Your name is bob传进去了，然后human就最后这个人要输入的这个有点类似我们要去访问这个CHATAPI的时候，最后一定是人输的。然后这个人说了一句什么呢？输的什么修炼，相当于我们问这个系统，问这个大模型叫什么名字，他应该叫bob。
	因为刚刚才告诉他，你可以看到human的最后一个content叫watch your name，那我们也可以输出这个message，以一个比较简单的方式，我们可以给大家打印一下。比如说这个system message是第这个列表里面的第零个，第零个就是一个system message的变量，然后再输出它的content。然后我们的这个human是他的最后一个，对吧？那我们就把这两个message都输出出来了。What's your what's your names？最后这个输入的u man输入的，然后这个是这里的第一个，我们从这儿用刚刚的方法去实例化一个c chat NAI的这个模型的PT3.5的模型，那我们把这个输进去，看一下它的这个结果。
	然后返回了my name is bob，对吧？How can I assist you today？这个就是他的一个GPT3.5的回复，他记得他是报，因为我们告诉他的名字。好，除了这个以外，我们之前playground里面的各种trip也可以从通过这个方式去玩。第一个就是我们的这个摘要总结，我们之前有写获得几篇文章，让我们的system来写，system role来写，然后用什么来做分隔？每篇的这个总结要总结出来，解释原因。
	然后最后有一个human的user input。这里插了两个参数，一个叫number，一个叫user input。具体我们在这儿就可以写number等于3，对吧？User input if string的这个要直接传这个多行字符串的方式就输入进来了。那我们来实际执行一下，print，我们把这个front改成front，这个应该是最后他输出的这个内容。这个是一个负一，这个应该大家都知道的，就是操作pythons字典，就是pythons列表的这个操作。好，那这个就是user inputs，那我们再实际执行一下，看看能不能总结。
	文章一、文章2、文章三的论点，然后谁是谁是最好的问更好的问题。对，然后我们这儿输入了两篇文章，但是输了就是这个number二这个prompt应该就是我们前面有写获得两篇文章，然后用这个分隔看他会不会看这个大模型会怎么处理。是说这个。
	稍等一下。大家可以看到虽然我们在user input里面输入了三篇文章，大家还记得这里是三篇文章。但是我们number给的2 system role的这个机制是非常明确的，只会对两篇文章进行处理。所以大家可以看到这里第一篇文章，第二篇文章最后巴拉巴拉进行了一个总结。这些都是相当于给大家一个概念，就是虽然有两个参数，就相当于定义了一个函数，叫做摘要总结的函数。它有两个参数，一个是number 2，一个是user inputs。
	但是我们的大语言模型在执行这个函数的时候，显然是这个system row的这个代码逻辑也好。你可以认为或者说语言推理逻辑也好，是优先级更高的。所以他必然会先去考究这个system role里面的信息。然后system role里面明确说了，只有这个传入的这么多篇文章，number篇文章。所以他就只需要总结传入的这个两篇文章就可以了。即使你传十篇文章，大概率上只要你不用一个特别差的语言模型，它应该而且它对于这个语言模型本身有system role这个功能的话，他都会更好的去执行你要的这个诉求，也从一定层面上节约了你的token，对吧？因为最终生成的这个内容是计算你的token的。
	然后这个是到目前为止，我们prompt基础的功能。就是我们怎么样为我们的语言模型和我们的聊天模型生成这个prompt，用prompt template的这个类来生成，或者说用我们这儿看到的这个chat from the template这个类来生成。然后一个是维护提示，此模板用来生成提示词。一个是用这个聊天提示词模板来生成这个聊天记录，这个聊天记录本身也可以去针对不同的角色去做设定，这跟OPI的这个功能也是完全对齐的。然后如果大家要使用其他的这个语言模型，也只需要按照这个需求去做微调就好了。好，那我们再留五分钟时间提问，看看到目前为止大家有什么问题。
	看大家有什么问题，我们五分钟时间。
	这个同学说的很对，把from the conflict当做函数占位符是函数的参数的话，大模型就是函数的运行时环境。这个一点没错，就是这个意思。如果我们在聊所谓的大语言模型的这个编程的话，其实就是这么个感觉。
	老师一直不理解为什么几百亿公司的模型可以跑在咱们个人电脑的本地，是什么意思？这个我一直有讲，从讲OpenAI的API开始就有讲我们本地没跑大模型，我们是把我们本地构造好的这个front发到了OpenAI的这个服务上。OPAI执行完了之后，把结果给我们。对，南倩也就是这样的，南倩也只是说我们一直在聊，南倩也只是把这个模板的事儿整明白之后，让你更好的去构造这个prom，更方便的去构造prom，而不是说在你本地跑大模型。
	提示工程如何设计是另一种技术了。我们暂时不能在这一门课里讲所有的知识，这门课或者说这次课我们都是在讲棉签。然后后面我们在实际案例里面会有一些特定使用场景和最佳实践的提示词设计。
	然后有个同学说这个深有感触，但是基于自然语言编程，感觉编程效率变低了。怎么写出来稳定的这个比较难，有没有调试的工具？我觉得这个对比是不是特别公平和客观？因为你要这么想，咱们现在写代码写了多少年？我们现在就某一个特定的编程语言开发框架投入了多长时间了，然后学了多久，生产当中使用了多久，然后咱们现在实际写了多少个from，咱们把现在自己手上写过的prot拿来梳理一下，有没有这个我们不说多了，有没有1万字吧，有没有1万字的from？其实都没有。对，我们说的是那些正经的内容，不是那些参考事例，不是我们接下来要讲的这个feel your shot的一个selector，是正经偏功能性的，是代码逻辑，没有那么多的那我们得学，学的好了才能够把prot用好。然后调试工具的话，有有关于chain的调试工具。
	我看刚刚康明老师也有提到name floor。本来我们是想讲name floor在101的，后来想了一下这个概念太多了。而nme floor的重点是把年券的各种模块呈现给大家。所以我们到后面把agent把change讲了之后，我们再给大家看net floor是什么，大家会更直观一些。现在一下给大家太多概念，容易搞晕掉。对。
	Tensor or pro和patrik企业封装大语言模型开发了吗？这个我还真没关注，直觉上应该不会，因为做的是不一样的事情。然后经常看见公众号上面说手机马上可以运行大模型了。这个大家见仁见智，就关于大模型的定义这块也是见仁见智。比如说6B这种60亿参数的算法模型，不好说对吧？在手机上跑一个6B的量化或者算大模型，跑在手机上了也不好说。
	QA中需要统计之类的问题，具体是统计什么问题？这个南迁怎么怎么走？还有一个同学问code interpreter开放了标准接口会不会对南迁有威胁？这个不是个技术问题，这我回答不了。坦白来说，因为威胁这事儿干啥都有威胁，就是南谁啥也不干，南南倩就是没有人干，南倩自己啥也不干，南迁也会有威胁，对吧？然后我觉得是不一样的事儿，因为上次有讲过前台GPT想做前台这个入口。但南倩是想说你们谁做入口我不关心，我方便你们开发，我做N个入口都行。就有点类似于阿里的淘宝和京东的关系，来感受一下。模型使用什么方法去做量化？这个应该自己的大模型，就大家在用大模型的时候，应该官方项目里都有写量化的方。
	有人说这个统计总共有多少个匹配的数据，用一杯顶在向量数据库搜不太合适。这完全取决于你的使用场景。首先你在统计什么？你在统计如果是统计这种，像我们上次说美食评论里面这个是好评还是差评。那这种偏语义的信息，你就是得在向量数据库里去做搜索。只不过看你做的有多巧妙了。
	比如说你搜了一次，你把这个情感词存下来，再存到一个关系性数据库，存到一个缓存，存到一个其他成本更低的数据库里去，把这种结果存下来。那这样下一次同样的情感不就可以不用去扫销量数据库了吗？对吧？
	然后第二就是说向量数据库的扫描成本并不贵，因为你在建建立那个向量数据库的时候，已经把它们都存下来了。那你在扫描的时候，也只有你现在这个待检索的这个是一个向量，其他的都已经提前向量化了。所以最多就是一个搜索的成本，不会有大模型的成本。
	OpenAI在国内还是限制太多了，能不能针对百度和阿里这样的大厂的模型讲如何做应用？您提的这个问题问题不在我，问题在百度和阿里他们自己有没有开放和开源的更多，让大家能够给予他们去讲。有个同学问这里的量化指的是什么？量化是指评估吗？不是，量化是一种手段，可以理解成以比较低的损失去极大的降低模型的运行成本。模型量化大家可以说一下这个关键词就能知道。
	南倩不会sars化的，因为南倩她不开店。南倩就是淘宝她自己没开店，对吧？他上面有卖家，但他自己不是卖家。
	目前有哪些模型可以做到离线推理的？这个离线推理怎么理解？就是所有那种针对国内大模型，想要做单独的尤其国内大厂的闭源模型想要去做分享的同学，这个可能暂时这门课是满足不了的。因为它受众太小了，而且他的申请也不是那么好，申请成本也很高。我训练出来的文心千帆的模型部署一天的成本，一个实例就500块钱，我部署两个实例就要1000块钱。这个成本我不觉得是大家这个课里的同学愿意承受的。但如果有特别的需求，可以单独联系咱们的班主任或者浩哥。
	还有同学问7IGBT服务海量用户，找一台好一点的机器是不是可以部署一个GBT模型？首先一台机器可能部署不了这个GPT模型，这个模型挺大的对，然后上万亿的这个模型参数，你可能得要上百万的机器才能部署下来。那你想象一下你要调用多久才能调用上百万的开销呢？对吧？这个是第一个逻辑。第二个逻辑就是说他也没有开放私有化部署的权限，特指针对咱们。但是在海外一些美国的一些公司，他们是有提供这个私有化的版本的。
	好，那我们就继续往下了这个问题先回答到这，我们继续往下看看还有这个few shot和这个out of the mother，这两块就相对轻松一点了，概念没有这么复杂。第一个就是我们的这个few shot prompt from template。上面我们讲了prompt template c chat from template，现在我们讲few short的这个from template，这儿大家应该也就整明白套路了，对吧？未来肯定还会有叉叉from template去服务于各种front template，为什么会这样呢？
	今天一直在给大家灌输一个概念，因为from template本身是干的类似于函数的事儿，那函数肯定是好用的。函数大家都会用，最常见的排序类的函数，所有的项目基本都会用，只要你涉及到排序，快排、堆排等等。还有一些比如说图论里面的一些方法，比如说单元点最短路，多元点最短路，求top排序。像这些函数一定是大家都会用。类似的我们有一些front的典型的一些写法，也都是大家会去用的。但是又跟直接写一个具体的函数略有不同，对吧？因为我们fun template其实是说我们能针对一种范式来造这个函数，就比如说有语言模型的from template，有这个聊天模型的front template，还有这种few short learning也好，或者说未few short learning的场景服务的front template。那现在这个就叫few short的prompt tempt。
	那么构造这个future short front template方法通常有两种，也是比较典型的两种方式。一种我们叫从示例集参考示例集里面手动选择赛道of example就我们手动定义了一堆example。还有一种就是通过这个example selector，也是他专门做了一个抽象门前叫视力选择器来自动的选择。就这两种方法，那这两种方法我们一个一个看，首先从示例集里面怎么选，我们从这个定义来看，首先我们定义了一堆的事例，这个事例就是一堆的问答队。这个问答队是事实，就是我们给了一些事实，并且这个事实还是一个step by step推理的一个事实。
	大家去细看一下这个example，谁活得更久，穆罕默德阿里还是艾伦托尼？首先这里没有任何偏向性，就是来自于南京官方的一个maple，我是把它做了一个翻译，所以大家不要好奇问为什么是他们？然后这里需要进一步的问题吗？是的，追问趋势是多大了？中间答案趋势是74岁，他趋势是多大？去世时41岁，所以最终答案是他为什么？因为他们去世的时候一个其实是一个四十1，那不是很简单，对吧？
	这个过程就是我们之前讲的一个典型的思维链过程。对他通过这个思维链在不同的分支里面总结出了两个知识点。一个是它的趋势时间，一个是图灵的趋势时间。最后得出了一个比较得到这个答案。
	类似的这个crack list是一个海外非常活跃的一个交易平台。这个care list里面这个创始人什么时候出生的？谁是他的创始人，然后是由谁创办的？然后追问是什么时候出生的，然后出生于什么时候，最终答案是这样的那这个其实像不像我们开始看的这个auto GPT的一个傻瓜版本？然后所有的个人，其实大家后面理解都是在干这个事儿，欠agent都是在干这个事儿。
	就是我们之前为什么在理论的部分讲的这个各种链，就思维链，然后包括这个self consistency，包括这个思维数，包括其他的这种构造，都是想说大模型的一个核心能力是推理。这个推理没有办法一下搞到那种复杂问题的推理都能做的很好。那就把复杂问题拆步骤简单化，子问题化，然后再一步一步的去推理，然后甚至说三个臭皮匠，顶个诸葛亮就多个推理，然后再合并。
	整个南茜也是想把prompt learning这一套搞法，能够在它的框架层面上延伸的去做很好的支持。所以fuel shot这么一个重要的东西，在GT3的论文里面我们也看了。Fuel shot learning明显是优于one shot和zero shot learning的。大家如果忘掉的话，可以再回去看看论文和我们的这个课件。怎么样去搞这个few shot，定义了这么多个？四个这个example他实际想要表达的一个例子是什么呢？
	就是我们要从这四个example里面，我们能不能首先变成我们参考的示例。然后同时这个参考示例是就跟我们论文里面讲的一样，作为这个fuel shot，然后让我们的这个大模型有一个参考，参考这个short的问答，然后去回答我们现在要问的这个问题，无非就做这么一个事情。那我们具体执行一下，先定义好这个example执行，然后这里我们定一个一example的front，然后它是一个输入一对问题和答案，然后template的这个形状是这个question问题换行，输出一下，可以看到这里有一个这个question。
	谁活得更久，找到了这样的一个sample的示例。因为我们传的是第一个example，这里用了一个pythons的一个小的语法糖，两个星号是什么意思呢？先看这个示例的解释，大家应该就明白了。首先这里传的这里换了一个函数，对吧？我们同样是传了这么一个参数，example是0。就这个example列表里面的第一个这个事例传进来，就我们把这个传进去了。但是前面加了一个星号，星号是什么意思？你看它会自动把这个key value这个建设队，就这个字典里面的建设队去做解包解剖之后传递给这个函数。所以你看这段代码就应该很好理解了，就是我们把同样的这个东西传给了printing info。
	那printing info需要什么呢？需要一个question，需要一个answer，把这个question跟answer对，好，answer就正好对应在这里。Question answer, 你可以理解成通过这个方式，我们把一个字典在里面的key value传到了一个函数里。这是非常常用的一种手段，那它这就输出出来了，对吧？
	那跟这儿其实是一个意思，你把这儿的example from the format换成这个print info来理解一下。然后这个example的from the format，我们一直说把它当成一个函数对吧？那这已经够直观了对吧？如果我们假设他不去给大模型使用的话，那么这不就一模一样的函数。但他现在不会这么简单，这个只能输出一段自然语言。但是我们把这段自然语言交给大模型，它能生成更多的内容，这就是巨大的一个区别。
	我们把这一段prompt刚刚生成出来的，去把它丢给这个大语言模型，但这里我们需要去注意一个点，就是先运行一下。这里我们其实做了一个特定的生成，这个生成是干嘛的？我们刚刚看到这里导入了一个few shot的这个form template对象，传入了四个比较重要的参数。这四个重要的参数，第一个example就是我们上面定义的example，大家还记得吗？这里有一个examples，这个东西我们定义了四个参考示例，我们同时又定义了一个参考示例的prompt，这是我们这儿定义的这个参考示例的prompt。然后这个参考示例的prompt我们可以给大家看一下它的类型是什么。
	还有印象吗？来自于这个地方，它就是一个普通的front template，对吧？就到这儿为止没有什么新鲜的东西了，它就是一个很普通的prompt template的实例，只是我们取名叫这个而已。
	我会让大家理解这个过程，把他的生成这个prompt这个方法也跟之前讲的一样，format通过这个format的方法能生成一个具体的提示词。但我们现在传的是这个example prompt本身它是一个模板的实例，它不是这个提示词。这儿大家一定整明白，什么是prompt的这个template，什么是这个prompt。对，但这也是他定义的这个参数名称本身，这儿其实是一个example的这个from the conflict。但因为他取的这个参数名叫内容，我们就取了一个类似的名字。但其实这是一个template，不是一个具体的实际的from。然后这里还有一个后缀的模板，然后这个input会被我们实际替换，我这会被实际替换，这个input variable类似的来做检查，我们把这个新定义的prt或者我们把它取个名字，以示区别，这个few shot的这个prt。
	执行一下，然后我们看一下这里发生了一些什么样的事情。首先我们给他把这个输进去之后，这个一个map全部都作为了案例，1234，这四个都作为了这个案例。然后question这个是我们输入的马里波尔的那个是谁？对，然后有一个什么样的区别？我不知道大家看懂没有，到这儿为止有点迷惑一个最简单的逻辑就是大家有没有发现example是一个这样的格式。Example的这个prompt我们把它相当于规范化的这个prompt。所以这一堆examples都参考的这个example的prompt的方式，变成了这样的形式，这是一步操作。
	然后第二步操作，我们把这个input切换成了这句话，然后通过这个方式我们能生成fuel short的brand，对吧？感觉很神奇对吧？其实简单来说这就是一堆字符串。我们想怎么样把这个字符串造的更好看一点，然后在造的过程当中，我们给它赋予了一些概念，就这么简单，其实就是一堆字符串处理的手段。
	因为整个from就是字符串，所以大家首先从这个视角来理解form的不神奇，只不过这个字符串它有语义。这个语义就跟内容有关，跟格式有关。这个格式通过我们上面这两个方式，一个作为内容，一个ample，一个作为格式，一个example的prom。好，把这个内容few shot example造出来了，现在要问的问题跟刚才的那个最初级版本的prom times lays是一模一样的，一个input。那这个surface因为它前面已经有few shot 1 example了，所以他造了一个概念，就不是我们的prom，而是它1 example后面的这个suffer后缀。
	对，就是把它拼起来就变成了这个file short的这个prom，就是我们的带有这个feel out out的提示模板，把它的封面动态注入一下，就生成了这么个玩意儿。这玩意儿也可以丢给我们的大语言模型，它就能把这一堆内容丢过去，那就完了，这儿这个应该没什么复杂，对吧？就是构造这个front怎么构造的问题。
	还有一种方式就是我们说的，我不是手动的一个一个去弄这些，我能不能自动的去选。因为现在的问题就是我的example全丢进来了，我有多少一个sample就全丢进来了。这样的话如果我一个sample特别多怎么办？就超过这个filing的线去。那我是不是最好还是根据我的实际情况去选我要用的一个example，对吧？因为这是这因为这些还相对来说不是同一类问题。有的是问的活得更久，有的是问的是什么时候出生，有的时候是问的亲戚关系，还有问这个是属于同一个国家，其实严格意义上来说也不算完全同一类问题。他们放在一起做fo example，few shot的example也不不一定很好。
	这里就涉及到一个新的抽象的叫example的selector，大家慢慢有体会年前在干什么事情。就是把大约模型里面我们需要去做的抽象，他都给你去做了一些抽象的定义。当你用起来的时候，有一个现成的工具可用。这就有点类似于最早人类干活的时候没啥工具，后来开了五金店，那什么东西都有现成的工具了。甚至你越到后面这些五金店里的东西都是电动的电钻，对吧？然后这个电动的各种玩意儿，其实就是这么个逻辑，这就是南倩想干的事。
	你要真的把南倩作为一个现实生活中的类似的工具箱的话，那就是个五金店。这个五金店里面的所有工具都有一类使用场景，然后你也可以把这一类五金电子和配件整合在一起用，那当然就效率那么高。但是看成同样，比如你同样都是要造一个椅子出来，然后你的五金配练用法可以不一样，组合可以不一样，顺序也可以不一样。最终造出来的东西功能性上可能是一样的，但是实际体感用户体验上也许就不同了。
	这个就是南迁干的事儿，我们如果要跟现实生活中做对比的那我们再看一下这个example selector它的定义或者它的设计目标。它就是说我有太多的参考示例的时候，我要选一下哪些是好的参考示例，跟我这次问题相关性比较高。最好就是在选的过程当中，是根据某种条件或者规则来自动选择的。这里大家会发现又一些类似的概念冒出来了。就是某种条件或规则是不是跟我们之前说的语义搜索找到一个特定的度量方法有关，其实就是这个意思了。
	你想你的few shot一般是有问有答的，然后你要问的问题是只有问的那就是在你的问题里面找一堆以前的参考示例里相似的问题了，作为你的参考示例。所以这里它提供了一种手段，相当于给了你卖了一个五金配件。这个配件用的顺不顺手，好不好，就取决于你的首先在五金配件零配件店里开的怎么样，对吧？其实就是你会不会选，这儿的参考示例也是一样。
	如果参考示例特别的多，那你也许通过这个方式能选出来一些刚好匹配的file shot。但如果你是一个完全全新的问题，没有什么可参考的file shop，那你就要数注意设置一下这个阈值。也有可能你选出来的top 5的photoshop，他都跟你这个问题相关性不大。但是他也是这一批参考系列里面的最高值了，但可能这个最高值的相关度才0.2、0.3。这个时候你最好就不要给复有效的，因为反而会给成会变成误解。这里我们在逐步的把一些用到的概念拆解给大家。
	就在我们之前讲base language model时候，有一个鸡肋叫ABC。这个看着可能有的人会觉得很奇怪，这个也是pythons当中的一个语法，它的全名叫抽象的基础类型，抽象的鸡肋，是一个ABC模块当中一个缩写。我们在使用abstract measure的时候，也是来自于这个ABC模块，就是pythons当中的面向对象的一些，原生的一些抽象。它的主要用途就是为了方便你去实现这种面向对象的设计，就是你我定义基类，定义抽象的基类，然后子类再去做一些实际的实现。它有一些特定的特点，包括它能实现抽象的方法，它本身不能直接的实例化，要强制子类去实现。那这样就比较好给一些统一的接口，那这样的设定在能线里面也是随处可见。大家如果在看代码的时候，可以关注一下。
	那我们实际来看看这个exam selector怎么玩，举一个例子，比如说我们要去问这个问问题，这个问问题要找参考实例，那这里就给了一些假设我们要找找一个特定的单词，我们就先这样想，找一个特定的单词，我们直接看到下面，找一个特定的单词。然后这个特定的单词我们希望能找到它的反义词或者说近义词都行。看懂看我们的设计就好了。
	我们这里设置了一些example，这个example是什么样子呢？很明显，一个sample是一组相对的概念，比如说开心、伤心高，这个矮或长或者短，然后windy或者cop就这就这种类似的词，然后有的是两级的，有的是感受，有的有可能是跟这个程度有关的，这样的一些词汇。然后他们的表达的这个内容其实本身也有一些差异，维度上的不同。然后我们提问的时候，就从这个example里面自动的去找要参考什么，就干这么一个事儿。这些都跟之前我们定义的套路差不多。
	然后唯一的不同是我们在这引入了一个还没介绍的概念，就是我们这引入了一个叫我们的systematic similarity example selector。就这个玩意儿，它是一个已经预定义好的一个example selector。大家可以先不去纠结它怎么实现的，你把它理解成一个实现了语义相似度搜索的一个类就好了。那那它的实现的核心就是说我能够去找出相似的这个语义的内容，就跟我们之前美食评论这一趴要讲的内容是一致的。类似的一个ample selector，还有都在这个模块下面有做实现，大家可以从官方文档里面去查阅，根据你的需求，还有两个东西，一个叫perma这个是一个向量数据库像数据库在这个蓝线里面放在weft store下面。还有一个是我们之前用过的invading，这里用了OpenAI的embedding，放在embedding下面。
	Ok那这三个东西是一体的，下面这两个是作为它的参数传进去。我们可以看到这里创建了一个语义相似性的一个选择器。你先执行一下，创建了一个一相似性的选择器。这个选择器输入的是example，就可供选择的列表，备选列表。然后用什么方式去做这个向量的生成。我们在一开始讲这个南线的时候，有一这图就是一个典型的QA的最佳实践的示例。一定要先向量化，向量化之后去这个vector store里面去找类似的问题，top k的问题，那这个方法就几乎是完整的复刻，那这就是top k到底要几个，待会儿这个参数我们也能去做调整，比如说我们可以看看拍摄的这幅图。大家还记得这幅图，对这幅图，大家再回想一下，刚刚我们那个应该叫systematic similarity example selector。
	就这么一个语义相似性的示例选择器的实例化的这么一个方法里面传入了四个参数。一个参数是我们的候选集examples。这个examples就是我们的自然语言，就我们刚刚看到那55段儿词。第二个是我们的这个OpenAI的embedding，就我们这的embedding model。第三个是这个chroma，就这个向量数据库开源的。第四个是我们的这个top k的K，我们要把top几top KK等于1 top几跟它最相似的这个向量给它弄出来这么四个东西一起构造出了一个选出最相近的prompt的这个方法，那看我们开始讲这儿的时候有提过the prompt to LLM with the tough case Victors，这是不是就能理解？这不就是把这个选出来的prompt作为它的few short example，这就是一种典型的使用场景。
	好吧，我们回到这儿，让我们重新定义一下这个example selector。然后这个example selector要传给我们的这个few shot的这个prom template，对吧？那大家还记得我们刚刚这儿是直接给一个examples，然后由这个example的prompt来构造对应的这个future。
	现在我们不是传全量的示例集了，我们传了一个example the selector的实例，这里就是一个实力。然后这里我们是定义了一个模板，这个模板是之前已经定义好的，在前面这就是一个普通的from template这样的一个构造形式。然后有前缀，有这个后缀，最后是输入变量的名字。这些套路都已经很清楚了，就赘述了。
	我们可以看到我们输入worry的这样的一个问题的时候，他找出的参考示例是这个担忧的说参考视频是这个。我们输入胖的时候，这个地方出问题了，他找的是这个pathy和sad，这个是不太对。我看看。看换一个词，比如说tall short。换成这个，那他就给了这样的一个词，这个就是我们刚刚说的比较典型的问题，就是这个只是一个功能的演示，但实际情况下不希望这个example太少。因为这五个其实你可以某种层面上来说一个特定的维度，它就只有一个人，所以容易选错。但是如果你有比较多的examples，比如说同样一个方向上的问题有那么两三个、三四个甚至10个，那它就能更好的去比那个向量。
	你可以简单理解成你在这个高维空间里面，因为我们用的OpenAI的evading 1600多位才五个示例。这个空间是相当稀疏的，给它去比的时候会比较抓比较抓不到这个关键点。然后又是一个单词，他也可以试一试后面把这个一example变成一个句子，然后再去找。
	整个example的selector，其实就是这么一个方式，核心来看我们就是想要去做这样的一个工作，就是我们如何去用蓝茜实现few shot这个prompt learning，那还有两种方式，一种是通过这个示例器手动选择，一种是通过example的selector来自动选择。那手动选择就是我们直接传入一个ample，你自己选你要什么，然后通过example的这个prop来构造它的样式。自动选就是我们通过一个特定的一个选择器，然后通过这个选择器来实现我们的这个选择。有一定的风险就在于，如果你的example给的不够多，那去构造这个选择器的过程当中，向量维度很高，量力很少就很稀疏，稀疏的时候你连这个K都没法填，因为我把K填成2的时候必然找出来的。另一个就不太相关了你至少要准备一个可候选的这个样示例，在五个甚至10个以上算是及格，能能稍微用。最好这个势力是在这个千或者万这个量级。如果你要构建一个真实的很稳定的应用的话，那这样是比较靠谱的对。通过这两种方式我们能构造这个prom，我们接着再花一点时间去讲讲那个outboard father。
	最后大家的提问了，outfit the partner要干的事情就对输出进行处理，对吧？那我们前面已经预告了，在在from template里面甚至就能去做设定。我们这里举了简单的两个例子，这个就更粗暴的直接了。因为整个auto partner是对输出进行一个规范化，然后那其实还是字符串处理，对吧？为什么呢？因为语言模型核心输出的目前就只是文本。虽然GT4在发布的时候提到了多多模态，能够生成图像什么的，但它目前还是只输出文本。现在我们无非就是希望这个文本能更加的结构化，按照我们的预期来进行输出。这个其实就是off to他的这个输出解析器它的价值。
	我们具体来看怎么做。第一种方我们举了两个例子。第一个就是说我们假设想要把输出的结果都变成一个列表，那我们应该怎么做呢？从这个output path里面来，跟我们刚刚一个maple selector非常像，所以有两个放在一起讲，就是我们可以从这个output partner这个模块里找到各种各样的怎么找。我们前面讲这个model的时候，已经给大家说过那些路径了，大家可以通过类似的方式去找。一个是通过这里面导入一个通过逗号分隔符，然后并且以一个历史的方式去输出的这么一个输出。解析器导了这么一个玩意儿，同时我们还导入了几种不同的from template。那具体怎么实现，我们也重新启动一下，让它回到这个干净的一个内存状态。
	我们定义了一个output father，通过这样的方式实例化多用的默认参数。获取了一个叫做format instruction。这个其实就是一个对输出进行格式化的一个指令。
	然后大家想一想为什么会这样实现。我先咨询到这就你们可以去看看源码，去想一想这部分源码我没有细看，但是逻辑上来说很清楚，就是我们讲OpenAI translator的时候是一个环，就是从左到右再从右到左。那为什么会这么画？也是因为我们要获取标准化的输出。我们当时讲过有很多种方式，一种是在大模型层面上去做处理，一种是在后处理层面上去做处理。在这儿的实现其实也是一样的。大家可以想象，如果我们想要获取一个标准化的输出，是不是最好的方式也是通过print去告诉大模型，你要按照一个特定的方式输出呢？那其实两千也是这样实现的。
	就我们看开始有人问这个action variables干嘛的对，之前没有用过这个参数，现在我们把它用起来了。他传了一个format instruction，就是我们的规范化的一个instruction，或者说我们的这个格式化输出格式化的一个instructions，是这样抽象定义的那怎么来的呢？其实就是我们刚刚从这儿获取出来的对吧？Output father get format instruction, 从这儿获取出来的传到这儿来了，那么可以执行一下，然后通过这样的一个方式，我们接着传了一个input的，这个就相当于是模型的输入模型的输入就是我们的from实际生成的一个form。按照我们之前没有定义它的时候，那么我们的prompt应该就是这里，然后把我们的subject替换过来就结束了。
	但实际情况上不是这样，实际情况下我们还有一个format instruction，大家看这儿还有一个format instruction，然后我们只传了一个subject，这其实跟我们刚刚理解就会有出入了。大家如果想象一下，如果我们把这儿的这个发球给注释掉，大概率这里要报错，大家想一想为什么？然后如果我们不把这个传进来的话，那么这个prompt也不会有我们的这个output，就是我们的这个输出解析器当中的这一部分获取到的内容。所以不管是这个input的变量也好，还是我们解析器的这这个输出规范也好，都是在最终丰富这个给模型的front？都弄到这个模板里来了。只不过一部分我们动态输入的，一部分通过这个output father里面获取的，根据我们的需求获取不同的解析形式。那我们实际输出一下就明白，忘了执行这样对这个部分list by ice cream favor就是5种冰淇淋的味道风味。
	是前面这一部分，包括这换行符。后面这部分format instruction就从这个里面获取出来的那是个什么意思呢？其实这不就是一段字符串对吧？那看这个好像就感觉这个抽象很没用，很弱对吧？但是就是这么一堆抽象构造出来的这样一个prompt template，能够层层递进，最终又被change，又被agent去不断调用的这么一个人群的框架。所以你单看这个，比如说你有一个五金店的一个榔头，你感觉好像这玩意儿榔头也可以。你拿个扳手硬敲这个钉子也能敲进去。那后面你会发现榔头定这个锤子各种东西现在都有电气化自动化了，那不就是agent和chain。
	我们找一个现实中类比的概念让大家去想象的话，就是这些底层的东西就是我们一个小配件。但这些小配件我们玩的好，变成auto GPT了，变成LL chain，变成这个self search chain这样的一些chain之后，那不就是我们的电动的这些工具了，那它效率就有一个很高的提升。所以去看这些过程当中，也要能理解它的这些抽象的价值所在，而不是为了封装而封装。当然做这种事情就一定会被人吐槽他的封装有点过于了，但这个取舍，这个trade off本身就是一个非技术性的问题。我们就不在这儿讲课堂了。我们这OpenAI大家应该熟悉了对吧？这应该是调了默认的这个达芬奇003，然后去执行一下output输出，输出了5种风味，参考了这样的一句话对吧？
	其实就是output partner就是给大模型一个特定的。但是我们以前你想象一下，我们让你们来写这样的一个按照特定样式输出的话，是不是很痛苦？至少他能给你一些反复适用的英文的of the brother，对吧？就像这个快排一样让你去用。虽然它不一定在每个模型的每一种任务上都稳定，但是至少这还是不错的对。
	并且理解了这个囊县的这个基础概念模块架构实现之后，就不会发出那种你完全不知道他内部是怎么实现的这样的疑问。因为我看那篇文章，我为什么放弃能源？最大的一个冲击就是这个南迁有很多底层的实现构造了front，但他不知道在哪儿构造的。但你把model IO这个模块，我们今天讲的你明白之后，最核心的最底层的不就这么构造。然后你再看上面的chain，你可以最方便的通过verge。我们前面有讲，因为他们最终这些欠A键都调的是我们今天讲的这些东西。那你就能看到他们是走不掉的那这些东西都是能输出出来的，所以核心还是把这些底子基础整明白，就没有那么多不明白了。
	对，那么这儿输出一下这个pass，它就能变成一个这个是output poser的另一个方法对吧？整个poser就俩方法，一个叫get format instruction，就跟我们instruction tuning一样，变成prom的一部分丢给大模型。第二个就是pass，把我们的输出结果通过plus因为他自己知道怎么处理这个东西，变成一个list。刚刚有同学问，其实问的就是这个pass是不是通过prompt实现的？不是，这就是一个简单的一个文本处理的一个手段。对，当然我们不排除你玩花儿活非要定一个再调大模型的plus也是可以的，但是就没有那么稳定。
	类似的这个daytime，就我们用这个日期解析也是一个类似的问题。就是我们把output father变成一个特定的daytime的这样的一个格式，这里引入了一个chain叫LLM chain。这些都是留了一个口子方便大家去试，然后去看文档去感受。
	然后在我们的星期三讲的时候就会更轻松一些，套路也是完全一样，定义一个partner，把这个partner作为这个format instruction。这里大家都熟悉了，我们就直接这样传了，因为反正也就用这么一下。然后运行，然后输出这个prompt对吧？那这是我们会实例化一个券，这个券的输入也很，其实也就这三件套这三件套里面的两套input output都变成了这个front。然后模型就是我们要用的默认的一个text w7003对吧？我们model IO今天学的这三个内容可以拿来初始化一个最基础的LLM chain，就是大语言模型的chain。然后这个券的方法就变了，因为这是我们年前自己定义的一个抽象run方法。Rain方法传入了around when was be come on funding，然后他会问这个问题的问题就会就到这儿来替换，咨询一下。
	Output, 给了一个时间，刚才说了这个output通过pass方法变成了这个daytime对吧？这个为了输出的更好。对，我不知道这个能不能接。对，应该是可以写print，那么就可以变成一个特定的一个时间了，这个就是我们的这个daytime partner。
	然后南茜的这个LM chain，大家可以看看文档，其实没有那么复杂。大家仔细想想它怎么实现，就是把我们每一次去调LLM船prompt封装起来了，变成了一个圈，然后里面还有一些小的东西。我们留个悬念，下节课再讲。我们再花五分钟讲一下PPT最后的一部分，就是想跟大家做的一个分享，也是一个简单的开了一个口子，后面我们再去跟大家讲这个。其他的模块的时候就比较好了。我们刚刚有提到这个蓝线的模块设计，有这个prom template。大家现在在看这个就比较直接了，就有这个input，然后有output这个input。
	Output我们主要指应用的原始输入和原始输出。原始输入我们在里面可以做各种处理，包括去做embedding，包括去把它变成from the temptation当中的一部分。然后在大约模型里面输给他，然后可以让他去做这个搜索。
	我们下一节课就会涉及到一个简单的notebook的示例。怎么样去搜索，需要去用第三方的API搜索引行的。然后这个搜索结果可以给到from template再到大元模型。这个是不是就是我们auto GPT当中的第一部分的内部逻辑的一个事例了。大家想想最开始的那个视频LGBT，问问什么叫OGPT，他想了想要搜索一下，得到了一堆网页的结果。这堆网页结果是不是可以再去构造from template，这就是我说的为什么后面这个front这个template就会变成函数一样的东西。这是一种实现联网查询的逻辑。那么能不能结合联网和这个向量数据库也是一样的。
	大家想一下问auto GPT是什么？如果我第二次再问他的时候，假设他之前有把这个结果存下来，那是不是可以直接通过vector store就去获取到结果了，这个结果就可以丢给这个front template，然后组装一下就可以给到这个输出了，或者说能拿到部分结果给下游的任务去做扩展，这个都是比较常见的一种操作。然后这个是课程项目，我们今天晚上来得及的话，会把这几个notebook上传。然后最后还有一个小的观点，我们还有五分钟时间给大家分享一下。比较这个公益欲善其事必先利其器，就跟那个五金零配件我们一直在讲这些事例一样。就是你是用用自己的拳头去敲那个钉子好呢？还是说你拿了一个榔头去敲，还是拿了一个电动的工具去敲，这就是一个巨大的效率上的区别，对吧？
	那我们可以看得到作为参考深度学习和这个plan native云延伸，其实它的开发生态已经非常成熟了。这里大部分也都是我曾经使用过的一些操作系统也好，这个开源项目也好，或者说开发框架也好。大家可以看得到其实操作系统层面上，我们现在不管是服务器端、桌面端还是移动端，都已经有非常好的面向商业用户和开源社区的各种各样的操作系统。比如说我们的大家都在用的这个手机，移动端的android然后IOS，可能还会有一些很小众的一些操作系统，大家有在用的也许也有。然后这些平台上面一直都有一些智能化的框架在做开发。比如说这个TESSOFOW night，比如说这个onion，OPENENO这些都是为了在arm芯片上面去更好的实现智能应用的一些生态的一些联合。
	再往上看一些服务器端的，比如说我们的这个ux及其他的一些各种发行版无方图，我们的red head 3头OS，包括一些其他的一些mix的发行版都有。包括windows和micro s这种win或者unix类unix的这种操作系统。这个生态一定会逐步的健全起来，大家才能在上面做各种各样的应用，而且也会有应用层面上的一些需求，倒逼过来我们的操作系统去做迭代。
	在中间这一层的云原生其实也是一样的。就我们为什么把OS和云原生放在这里。就是今天我们很多同学都在问什么样的开发环境，什么样的部署环境，什么样的机器？
	其实我们能够看得到的是，未来操作系统会越来越不重要或者不可见。Cloud native已经有很多框架去掩盖了操作系统。不管是早期的open stack还是wm war这种虚拟机的形式，还是说这个gobal native开源以来带起来的这种容器化的形式。所谓的操作系统已经变成了一层镜像。通过我们的docker也好，通过我们的mobile也好。
	虽然这个项目最后做的不是很成功，就多ker的商业化版本和社区版本。但是整个容器化已经是一个不可替代的趋势了。这些趋势会使得OS变得不重要，我们的这个云云上面的各种应用会变得越来越明确。并且这个地盘也是很清楚的。然后每一块地盘干什么事儿，有什么样的插件项目也都是很清楚的那再看到deep learning也是一样的，就我们通过deep learning的框架，这十年来的一个这七八年来的一个发展，我们能看得到就是不同的框架有不同的受众，这是因为deep learning本身这个蛋糕做大，它的市场变大了。开源的市场、商业化的市场，还有集成它的应用的市场都变大了。但是到今天为止也有很多框架，慢慢就没有那么多人关注了。
	我们不去说是谁啊，但是明显看得到他在慢慢的退居二线。那还有什么样的一些应用一直在活跃呢？我们再看AIGC的应用开发生态。我们简单想一想，就首先在应用层，我们经历了从web I到mobile，到现在我们聊这个大圆的这个，其实这三这个阶段是都没有逃离开后面三层的支持，我们能看得到未来一定这个deep learning club native和OS都是偏底层的一些应用，它在支撑着我们的这些应用层的一些应用。然后我们在web AP看见了有非常多的框架，非常多的语言，包括像这个pythons的同学用的这些flash接这个江构spring这个java的。然后包括这个偏前端的，我们用的这些react到mobile端，我们还有这个react的native，还有各种各样的，包括像苹果公司出的swap等等。
	但是到大语言模型的应用开发框架，目前我们还没有看到太多的杀出一条血路来。只能说在我们寻找这个大元模型开发框架的这个过程当中，视野当中有一个避不开的框架叫年前。但是它会不会是最终的形态，我们不知道，也无法去做预测。但是我们能看得到的是这样的一个技术站也好，这样的一个开发生态的圈子也好，一定会有这么一个席位摆在这里，就是我们的大语言开发的框架。至于里面是谁，我们可以且走且看。但是在这个过程当中，我们就跟学南茜一样。
	我们要整明白这个框框里面它的抽象、他的设计目标，他服务的用户，他应该是什么样的。我们又在讲这个meta arned节目叫meta learning这部分到底是在做什么事情？就跟我们今天把prom的这个感觉很神奇的东西整明白了，就是字符串都在围绕它做工作，因为它就是大语言模型沟通的桥梁。我们现在目前跟大语言模型沟通就是靠语言，那就是要把它做好，并且要把它复用起来。然后能够把它跟现有的我们这张屏幕里面看到的所有东西都能够结合起来，那就能长出各种各样的应用，你可以把它作用于这幅图里面的任何一个模块框架，或者说部署的系统。Ok我们今天的这个课程内容就到这，看大家还有什么问题。
	大家可以提问，我们到10点40。
	3.5又便宜又轻，为什么很多地方还是默认使用003这个模型？这是因为这些框架都应该有很多原因。这个同学问的第一是因为3.5的全面开放也不是很长时间，也就是最近几个月的事儿。但是这些框架都已经开发很长时间了，他不可能把这些代码都扔了。因为在这些代码上已经做了很多的工作了，就是基于这个completion的这种API。
	第二个也没有很多地方都还默认是5003，只是说003这种形式是非常好的。因为它不是为了聊天设计的，是为了生成设计的。003本身的能力也挺强的，所以大家会依然使用它。对，还有003可以做，反正这也是很重要的，3.5和4都不能做翻车。我们关于今天的问题，大家多问一问。我们到10点40。
	这个同学问COT很好，中间巴拉巴拉还要给出相应查询。查询中间答案。没有太get到你的这个点。对，就是中间答案你也可以存下来。对，然后看你问的具体问题的复杂度了，是可以直接处理中间答案，还是要怎么样处理？
	南茜的功能会不会被大模型的实现替代掉？这个问题不好回答。对，因为现在的大模型本身是个语言模型，而南线的设计目标不是做语言模型做的事情，而是给这个语言模型增加很多的能力。对这个能力就跟我不知道大家小时候看过一个电影没有一个动画片没有这个叫什么战神金刚。
	我来组成头部。对，OpenAI干的事情是头部。对，但是还有组成这个躯干、手、腿什么的事情，是语言模型本身不干的。是我们这张图里的很多很多，就我们现在这张开发生态的图里面要干的。但是他们怎么连在一起呢？不过连倩来连在一起。
	老师能把今天的课再总结一下吗？这个同学是想要总结什么？哪一部分？
	看大家关于今天的课程内容本身还有什么问题吗？
	以后会讲连线结合具体私有化模型的部署训练和省会会讲结合CHATGLM的部署和训练。就是所有的大模型跟南倩他都不是一回事儿。举了个例子，就是大模型是组成头部。对那其他的这些我们这里看到的各种各样的应用框架组成了躯干。南倩是这个关节，对，把大家连在一起，然后很丝滑的连在一起，这个是年前想要干的工作。
	大模型接收的入参可以以向量方式传吗？还是只能传字符串？这个问题问的挺好的，目前来说应该只能传文本对。
	这个同学说听了3个小时前后连不上了，想总体再过一下挺好的，请使用视频回放功能。
	刚刚的example里的few short，针对具体公司的一个产品来说，是不是每一类的问题都需要提供一个fuel shot的例子。如果客户问到的问题，你没有类似的few shot例子回答，是不是会错new short的使用场景最佳实践之前理论课有讲过，这个大家可以再看看理论课的部分。然后file shot是一种范式，它不是一个具体的方法。所以你的产品到底要问什么问题很关键。对，是问产品的使用手册，试问产品的问题，故障常见问题还是什么的。然后没有的例子，这就是我前面就讲过了没有的例子。那你就需要把语义相似度的阈值设置一下，不能说找出一个top one的例子就往上甩，低于一定的阈值是不能作为参考的。
	还有同学说南倩是在OpenAI上面开发的，这个全错。之前一直在讲，没有什么OpenAI上面开发的说法，南倩跟OpenAI没有一毛钱关系。明天OpenAI公司关掉了，南茜也能正常运行，这个讲了很久，专门在讲这个类的继承关系，这个open也只是最后那个叶子节点上的一个子类。这里的向量数据库是该开源的，当然是在本地安装，而且这chroma是南茜的全量数据包当中的一部分，是它的依赖。所以你只要papp store全量的男权就有了。
	还有同学甩了一个链接问是不是二开这个数，确实难回答。没看过这个项目，可以自己再研究研究。我估计这个项目确实看不过来了，后面如果有时间的话，可以我会抽空去看看。对。感觉南茜可以实现大模型平台无关性。对，我们今天的notebook里面都有写。南茜包括这个最核心的from template设计的目标，就是为了实现模型无关系。
	感觉南苑封装大模型这接口还是有点耦合了，这个我是完全不同意。我们前面花了很长时间讲这个类的继承，这个同学要再看一看这一段的视频回放，这个OpenAI不是接口，OpenAI是最后你实际调用的那个实例，跟我们刚刚看到的output powers和example selector没有本质区别。微调有哪些方式，我们到讲的时候会去讲的。然后好，我们今天的这个直播就到这儿。然后回头大家有什么问题可以再继续在群里提问和讨论。因为时间也比较晚了，今天开始的稍晚了一些，就到这儿，辛苦大家了。