	就正式开始，今天已经戴上耳机了，应该也没有噪音，对吧？现在应该是那个风扇的声音也听不见，还是比较安静的。好，那我们就正式开始我们的今天的这一节课程，叫大模型应用开发框架。
	蓝圈的下上中下一共讲了三次课。整个南线的这个框架，我们通过这三次课基本上就覆盖到它的核心模块了。我们之前有讲过，第一次课有讲过它的model IO，就是model input output。然后第二次课我们讲了chain和memory。Chain是组织能欠的所有模块的一个抽象，券本身是一个很有意思的模块，它把我们的蓝券里面的所有的抽象和模块串起来了，变成了一个很基础但是又被广泛使用的一个模块。我们还讲了memory，在上一次的课程里面，就我们的券能够去执行我们一些既定的操作了。比如说我们有sequential change是按顺序执行，我们有router chain去按照条件来判断我要执行什么样的具体的chain。
	这些操作设计好之后，我们还设计了一个叫做memory的模块，就给我们的大语言模型的应用增加了记忆的功能。使得我们的大语言模型在跟真正的应用去落地的时候，去设计它的时候，既能够有一个模块能够去读取我以前的一些聊天天记录也好，我以前使用的比较好的一些prompt也好。也可以在记录聊天记录的过程当中，使用一些预制的或者我们自定义的一些memory的手段。
	来节省我们的token的开销，或者说能关注到我们更加重要的token的内容。这就是我们之前两次课讲的几个大的模块。今天我们主要把后面两个很重要的模块，一个叫数据处理流的一个模块。这个是年前最近把大部分跟数据处理相关的已经实现的功能，把它归类归整到了这个叫data connection这个模块下面。然后同时我们还有一个模块，就是我们整个南线最重要的一个很复杂应用，要如何去设计，有一个抽象叫agents，就我们的代理。那代理系统它有它的理论基础，它也有它的一个实际的实现。并且到这个阶段，就我们今天讲的这一次课覆盖的内容，大家的实际体感就会跟model这个课不太一样，跟OpenAI的基础篇也不太一样。因为整个南劝，不好意思，我最近感冒了，这个声音比较听着比较难受。
	整个南券其实到今天我们这一次课就把它的基本的概念模块都讲完了。但是接下来我们不是还有六次课吗？这六次课其实是更多的去实战和使用，所以就算是我们这三次课把它的重要概念都讲了一遍，大家也会发现不可能覆盖到它的每个API。所以我们更多的是以这五个模块为一个切入点，让大家理解每个模块的功能定位是什么，他有什么样的一些典型的使用方式。然后除了这些典型的使用方式以外，我们还能在哪去查取他的代码，定义它的API文档和开发者文档。
	就今天也很典型。今天我们讲的这个数据处理流，我们会把文档的加载器、转换器，包括我们的文本向量模型和向量数据库都给大家做一个展示。然后检索器，我们的这个recover，我们今天不会有实战的代码。因为其实retry val是一个也是一种抽象方式，是用来读取我们的这个数据的方式。会在后面的实战里面，根据我们实战的应用去介绍具体的这个recover。
	然后agents也是同理，我们今天会给大家讲agent理论，同时也会去讲能欠的agency到底是怎么设计的。然后我们在这个年轻的agents这个模块内部，哪些重点的模块需要关注，就这个agents模块内部最重要的三个agent tools和这个tool kits这三个小的子模块，它们分别是什么样的概念，有什么样的作用，它的设计目标是什么？然后agents真正运行它的时候，是一个agent的一个运行时，叫agent run time。这个agent run time有很多种不同的类型，我们正常默认使用的这个agent execute是一种类型。但除此以外，大家要理解能欠这个框架不是石头缝里蹦出来的。就像agent有理论基础是因为有react这篇文章。后面大家也知道我们展示过auto GPT这样的一种agents的典型事例。Auto GPT也是一种agent的运行时，除了这个以外，还有会有像一个很有名的项目叫baby AGI，就是通向通用人工智能的一个小baby，对吧？
	我这么理解这个名字的话，baby AGI他也提出了一种可以去当做agent运行时的这个execute，叫做plan and excute。这些其实都是我们可以去延展选读的，如果有机会后面我们也会在实战里面去给大家深入去讲。然后实际上我们会去看两种不同的agent。一个是如何让我们的谷歌搜索引擎和我们的大语言模型结合起来。那这样的话，我们的大语言模型就有联网的能力，就有实际的数据的更新的数据加入到我们的应用里面来。这个是我们今天会做的第一个跟agent的相关的实战。第二个就是react这么一篇文章，它的一个实际实现其实是一个很典型的一个现在在agent里面使用的一种范式和方法了。我们也会把它做一个介绍，其实大家会发现agent是一个很抽象的概念。但实际去使用的时候，只要抓住了几个关键模块和本质，它也就不复杂了。我们尝试把它能够由浅入深的跟大家做讲解，我们再来简单回顾一下这张图，这张图其实就是我们学南圈的一个核心模块的一个过程。
	前两次课我们已经搞定了上面这个红框里的内容，就是我们的这个大语言模型，有语言模型，有这个聊天模型对吧？分别对应着生成类的和聊天类的。同时我们有这个prom作为模型的输入，我们设计了这个prompt template作为这个模型的输入的模板。然后这个模板本身就像是一个函数一样，然后这个from template里面的这个input variable，就这个输入的变量，就像我们函数的入参一样。这个是我们把模型和模型的输入整合到了一起。他们两个合在一起被我们称之为一种最简单的券，叫做LLM券，对吧？就是大语言模型的券。
	这个券也是大部分复杂的其他的券的调用的基础，它作为我们乐高的第一个小模块，然后在LM chain上面我们可以组装出signal al chain，router chain, 然后这些chain可以给它赋予一个记忆的模块，叫做memory。我们上节课涉及到了conversation buffer memory，当然还会有一些其他类型的memory。比如说这个面向向量数据库的作为后端的这种memory。这也是一种整个这些模块之间都像乐高积木一样可以互相组合。我的agents里面可以去调一些特定的memory，然后我的chain里面也可以去调这个memory，然后我的agent里面有一些这个tour。
	所以大家整个学南圈的过程就是把它的概念每就像我们用乐高一样，它有不同的形状不同的颜色。我们把不同的形状不同的颜色整明白之后，不管他怎么组装，我们都能够把它捋顺。那他们怎么组装的呢？就是这个拼接的这个接口本身大部分都是通过这个券来实现的，把它串起来的。即使是用了agent，它下面也是用嵌在实际的执行，这个我们待会儿会去讲。
	今天的第一个部分，我们要学的新的东西叫data connection。Data connection对于能券来说是一个能券native，就框架原生的一个数据处理流。在整个这个chain是把大家连接起来了。但是连接起来之后，我们有不同的这个模块之间有数据要去做传递、做交换、做读写。这些东西总要有一个地方把它管起来。我们把data connection作为管理他们的一个大的模块，把它都框在这个框框下面。那么data connection具体可以分成这几类。
	当然不是说任何一个应用都需要这么一个流程，只是说这样一个流程是符合逻辑上的顺序的。但你说这个有没有可能不按照这个顺序来操作呢？也是可以的。比如说你的应用特别复杂，你不只来自于同一个source，有很多个source。有的source就是不止来源于同一个数据源，那有的数据源需要你去做这个加载，在transform，在做这个嵌入。有的可能就直接拿来之后就嵌入了，或者你直接就从store里面读出来的，它可能是取了当中的一部分子集，但是这样的一幅图，这个date connection是希望尽可能给大家呈现出来这几个抽象之间的一种比较常见的连接关系。我们待会逐逐个的会去讲每一个部分有什么样的一些典型的使用方式，以及它的一个功能定位。那这个流程这个data connection的流程，也就对应着我们的五个重要的抽象。
	在我们的这个目录里面刚刚有讲到，就我们的document node、document transformer, 我们的text invading model。这个我们之前在OpenAI的基础篇里学过，待会儿也会去实际使用到。包括这个vest store，我们之前有一个notebook的示例里面用过这个roma这样的一个向量数据库，今天我们也会继续去使用它作为我们的这个向量数据库的bacon，作为这个实际的向量数据库后端引擎，这个retry就这个检索其实就是怎么样从我们的向量数据库里取东西。
	最近有几次课上完之后，有同学也在问。我们到底要往向量数据库里面存什么？要怎么样从里面取东西？取出来的是什么？这些其实都是case by case，根据你自己的实际情况来的。
	好，我们首先看这个document loader，就是一个加载器。那这个加载器它的核心最最重要要解决的问题是什么？我们把它简单拆解来看这幅图其实要讲的是左边来自于各种各样的不同平台文件格式的数据来源。就这个source我们能看得到，典型的我们可以分成两类，就是这个source。
	这一次我们按两个维度去分，一类叫做数据格式，比如说我们已经处理过的，我们使用过pandas，处理过这个CSV，我们使用过之前的这个PDF的plapper，处理过这个PDF的格式。我们也在这个OpenAI的translator这个项目里面，我们基础篇的结业的这个项目里面，导出过markdown这样的文件。然后我们在向这个服务端发送请求OPI的时候，我们处理过杰森这种数据格式，对吧？这些数据格式其实我们都已经处理过了，那么HTML可能我们还没有处理，它是一个标记格式。理论上你可以理解它跟markdown是一类格式，只不过它是在互联网这个发展过程当中，所出现的一种静态网页的这个标语言格式而已。
	然后这里还有一个比较特别的叫做file，就是你可以理解这个文件目录这个其实是干嘛的呢？他就说我现在一个处理文件，我也可以处理一个目录下面的所有的文件。在你的这个目录下面，它自己实现了一个叫做on struct这么一个loader。
	在年线的这个框架层面上，这个是从数据格式来分。但我们还可以有其他的数据格式，这个可以根据实际情况一直去写。这也是大部分男券的开源贡献者们在做的工作的一个重点。就我能够针对不同的数据源去做接入。
	然后这里其实有一个比较大的风险是什么？就当大家去看这一部分框架层面代码的时候，比如说像哔哩哔哩这个数据源，今天我在使用它的loader的时候没有成功，我没有去深入研究到底他是什么问题，但可以想象得到所有这种平台，他们都不希望自己的数据轻易的被扒走。这个是一个很很直观的一个感受。就我作为一个平台运营方，我的内容被别人拿走了，我的价值就没有了。所以就能想到这里会有一些挑战。
	我们需要做的事情就是根据我们的实际需求，看看哪一个loader是符合我们的预期的。比如说我们现在要查论文，今天会给大家讲这样一个论文平台，对吧？那这个平台我们可以直接使用loader加载进来。比如说这个哔哩哔哩是一个视频网站，它也有一个loader但目前没有生效。那如果它这个是一些特定的bug被修复了，那它就能从哔哩哔哩加载字幕。那类似的视频网站像youtube它也有自己的loader都是开源贡献实现的。
	然后像这种IM的平台，比如说discord，我们的mid的这个纹身图的这么一个产品，最早就是在这个disco上面去进行展示生成的一个大的平台。它是一个类似于微信，类似于飞书企微的一个IM的一个平台，不过它具有一定的匿名的特性。像如果咱们有产品经理的这个学员，那西格玛是我们产品经理一定会使用的一个设计平台，也有loader去实现了，包括get up。但他可能需要你提供key去访问这get up上面的代码，直接把代码拿下来。像redit这种QA类型问答类型的网站，包括像传统的深度学习，tensor fo w的data这么一个抽象，也能够直接被它接入进来。
	所有的接入最终都会变成一个统一的格式，不然接入就没有意义了。不然就只是一个加把数据打开的这么一个工作，它就不是一个接入的这么一个定义了，对吧？那一定要把各种敞口的数据来源定义成一个统一的抽象才有意义。这个抽象叫什么呢？叫做document，我们叫document loader对吧？是load进来变成了document。我们看到整个document，其实它的这个定义非常的简单，它基于一个base model。这个大家可以去看，base model下面就没有太多的定义了，到document这一层就够了。
	首先我们要知道的是，刚刚看到的那么多不同的数据来源，都有自己对应的loader数据。如果是文件格式就是文件格式的，如果是平台测的就是平台测的loader这些loader都会基于一个类叫做base loader。大家应该已经比较熟悉我们的这个课程的一个教学方法了，去找到代码的本质就比较好去不管是d bug也好，因为他可能也有bug。我刚才提到了平台会去更新，去解决一些数据被拉走的问题。但是我们也可以更新去重新拉出来这个数据，对吧？
	这些不同的类型的loader，其实它的代码实现并不复杂。因为我们的这个base loader的要求不多。大家看到这里有一个abstract measure，对吧？这已经很很常见的套路了。
	在python里面的这个面向对象的写法，我们对于任何的具体实践的loader都需要实现至少一个最关键的方法。就这个load的方法，就把这个数据加载成一个document objects，就把我们的数据从这个数据来源的样式变成一个document的抽象，这个document是一个列表，这个document本身的定义也很简单，这里有几个关键的我们也会用得到的。第一就是我们的这个page content，就我的加载出来的这个内容可以当成一个文档一样的。这样的就有点类似我们在OpenAI translator这个项目里面，我们不设计了一本书叫做book，那个book里面我们也设计了很多的pages，每一个pages里面有不同的content。大家如果动手做过这个作业，同学应该知道的。这个document的抽象也是这样的，就我有一个document，一个document里面有很多的配置。但是，这个page和page content的抽象就留给大家去做这个操作了。所以它的page content你可以简单理解成就是我的配置下面没有再细分了，都是文本，没有把表格和这个图像或者其他的多媒体格式放进来，我就是放的文本，所以它的page content就约等于我们的那个抽象里面的text content。
	他每一页有他自己的content，但这每一页怎么来的？因为我们的不同的数据来源都不只是一个文本格式，对吧？你是全量的弄进来，还是只是取一个每一段的段首的几句话。就比如说我们在讲transform chain的时候有这么一个操作，取每个段落的前面几句，这就是一个transform加这个load的一个操作。到这儿，其实这个page刚才就是把我们数据来源转换成一个document对象之后存的这个内容。至于具体要取什么，是我们下一环transform要干的事儿。我们这就只是当成直接全部存进来就好，存到page content里面就好了，就是干这个事儿。
	好，page content是需要我们能够去熟练看到和掌握的那还有一个很重要的就是这个meta data。这个meta data跟我们看一个正常的，不管你是用的什么操作系统，一个正常的文件的这个meta data是类似的。那我们的document的meta data里面会根据我们这个loader实现的不同，给它写入一些不同的数据。因为它实现一个factory，其实就实现了一个字典的方法。你可以比如说我们今天会看到这个阿凯加载论文的这个loader，那他自然就会写一篇论文的发表的时间，他的作者是谁，他的摘要是什么，这是他写的meta data。但如果你针对你的特定的要处理的对象meta data不一样，那你都可以去重新去实现它。
	这个都很简单，不是一个很复杂的操作，往这个document里面写就好了。它是一个它的一个必要的基础属性，是可以被复写的，这个key是可以自定义的那除此以外，还可以实现这个look up的方法。你可以理解成我变成了一个document，变成了一个文档。这个文档我们希望能够去检索这个文档里面有什么内容。最常见的就是我们在使用电脑的时候，我们用control f或者用这个command f来检索我当前这一页，或者整个文档里面有没有一个关键词。这个操作通常叫做look up，它也支持这样的一些操作。如果你一些特定的类型，可能你需要重新去实现这个look up在你的这个document里面。
	好，这个是最基础的两个抽象，就是关于我们文档和这个加载器。所有的加载其实都是基于这两个基础的类再去累加和自定义的。所以把这两个的重要的几个属性搞明白，你去看后面的代码就会很轻松。
	刚刚我们还有提到就这个document transformers，就是他框架原生的一些数据转换模块。除了我们之前已经使用过的，就是把一个文本拆分成切断。比如我们把一个长文本切成几段，或者说把一个代码切成几段以外，它的integration就是我们右下角这有放这个链接，inspiration的文档里面还有放一些其他的document transformers，大家有兴趣可以去研究研究。那这儿我们就不再展开了。他要做的主要工作就是除了把我们的不同数据源的格式转换成document这个类型以外。我们的page content里面具体要存什么，也可以通过transformer这个模块直接来处理。
	再往后走，我们就是之前有已经用过和学过的一个概念，就是我们的embedding，我们的text embedding。之前我们在OpenAI的基础篇的时候有讲过我们的embedding a打就ADAV2这样的一个模型。就我们的OpenAI提供的一个API，是非常好用的一个embedding的模型。最近这个智源也开源了一个in bedding的一个模型，大家也可以通过它去调用。
	如果我们要做私有化的话，那么任倩目前已经支持的tex ebel ding model，我这边有罗列出来。就是目前在平台上已经支持了，把他们对应的这个详细的API和参数，也能通过右下角的这个链接去访问到，包括像这个moc ML embedding这家公司也卖掉了，他们是一个很成功的一个创业公司包括像这个elas elastic search这种比较老牌的这个方法也是一种evading方法。这个其实对于男性来说，它是来者不拒的，他并没有说我只能使用某一个特定的evading方法。
	Embedding的核心。我们在理论片的时候，我们在这个基础篇的这个embedding理论部分已经给大家讲过了。它就是一种表示学习的方法，核心在于降维，然后在降维的过程当中保留了语义，所以in bedding本身并不是一个黑科技，也并不是一个很奇妙的东西。了解它的作用，就能理解为什么会出现这么多不同的invading模型。核心要用它的时候要保证你的in bedding向量，在做检索推理相似性搜索的时候，是用相同的一倍定模型去生成的这个向量。同时在后续去做大语言模型应用的时候，也保证他们是使用可以共享的这个embedding的模型。
	我们在讲这个embedding的时候也有提到，ADA的这个02的这个OpenAI的embedding模型是能够跟GPT3.5和GPT4会互通的那其他的一些invading模型通常也都会把这个跟什么大语言模型可以生成类的大语言模型是能够直接互用的。这个信息应该会展示出来。这个是大家用媒体模型之前一定要去看清楚的。之前我们其实在讲embedding这一节课的时候，跟大家提过什么是in bedding。这里我们通过一个短片，没我们没有说要用腾讯的这个校园数据库的云服务，他这个短片比较好的去跟大家介绍了什么是向量数据库，然后向量数据库在检索的时候，除了我们简单去比较一下距离之外，还有没有一些别的方法给大家开做一下思维。通过这个小短片大家来看一看。
	大家能听见这个视频的声音吗？我现在是耳机能听见。
	这有点问题，他现在是在我的耳机里面播放。对我看看这个怎么样让它能播放，然后把耳机断掉，然后给大家播这个视频。
	的向量数据库。腾讯云自研的向量数据库turns and cloud vector DB采用的标量加向量的混合检索模式。好比我们去超市买一个大的红色的陕西苹果，要先在水果区定位到苹果货架，然后通过大的红色的陕西的检索标签找到符合要求的苹果。
	腾讯云向量数据库单索引支持10亿级向量规模，数据接入AI的效率也比传统方案提升十倍。企业可以更快速的将向量数据库与AI模型相结合，实现更高效的在线推理。当这样的象征数据库与大模型相结合，还会产生什么其他的化学反应呢？不如我们将它接入到大语言模型的各个环节来一探究竟。业界的AI大模型采取预训练的模式，使用收集好的数据进行训练，知识库难以做到实时更新。而接入向量数据库后，就像给模型插了个高性能外接硬盘，支持学习互联网等最新信息。企业也可以注入公司信息、产品手册等私域数据以供模型进行推理，比重新训练、模型微调等方式更加便宜高效。
	在线推理时大模型其实是没有记忆功能的，只能通过重新输入之前的问答实现短期记忆，而且输入的内容也是有长度限制的。腾讯云的向量数据库可以充当AI的海马体，让人工智能拥有记忆历史，问答能够当成新的训练语料进入向量数据库永久储存。甚至当用户提出了重复相似的提问，向量数据库会直接给出缓存答案，这就让AI大模型越用越聪明，越用越迅捷。
	此外面对希望进入AI大模型的企业，腾讯云的向量数据库能够提供一站式的解决方案。除数据存储管理外，还可以帮助企业完成私域知识库中的文本分割向量化两大复杂环节。比起传统的企业自己做饭，接入时间可以从30天缩短到3天左右。
	当人类想到向量，可以让计算机理解复杂的图像、文字、音频等信息时，我们似乎找到了AI理解现实世界的钥匙。不断进化的向量数据库与人工智能这对黄金搭档，还会给人工智能时代带来什么惊喜呢？让我们拭目以待。
	好，我们重新接到这个耳机上。
	好，现在大家应该能听见这个声音，对吧？没有杂音，是接到耳机上。好，其实刚才这个短片里面有很多的细节是值得我们玩味的，我们缺少一些关键的点跟大家讲一讲。其实不管是腾讯也好，还是讲我们的这个能劝也好，大家都在尝试使用这个大语言模型，然后把大语言模型做的更好用。这个是咱们在学习这个课程也好，或者说咱们继续实践也好，都在做的一个事儿。
	我们今天要讲的这个data connection，其实跟现在我暂停到的这一幅画面是很像的。就我们看到，像下数据库这样的云服务厂商也在做类似的一个流程。这个流程其实很像，就我们这儿看到这个地方有个文本分割，这文本分割就等价于我们刚刚的这个流程图里面的加载和转换。我们把这个document loader和document transformer做好之后，其实就等于这一部分，我们把文本分割好了一条一条的内容。那分割好的这些文本就分割到足够合适的尺寸。比如说我们一句话，作为一个分割的单位，或者说我们这个固定长度不超过这个长度，都可以按它来切。都可以这样切好的文本通过一个向量的这个模型，就我们的text in bedding模型就变成了这个向量高维的向量。就比如说我们的ADA in bedding 02这个模型出来就是一个1500多位的一个向量。
	这些向量化的结果，其实它是带有语义信息的，只不过我们可能人看不到。他通过一个比较巧妙的方式，比如说这里放到了一个高维空间，但是其实是降维到三维的一个空间里，我们能看到他有一些关联关系了。他当时还讲到两个点就除了这个流程，其实大家都按照这个流程在玩以外，还有一个很重要的点，他有提到。会越用越聪明，有些答案不用去销量数据库里面查。这个也是不用去大模型里面去实际运行，去销量数据库里可以查出来。
	这个也是很典型的一类应用，就是我们假设我们在做这个QA问答。比如说我们后面要做的一个实战的作业。后面几次课程里面会有一个实战，用QA的这个机器人的形式问答机器人的形式。这个问答机器人如果已经问过同样的问题了，那么我们是完全可以不用大语言模型来跑的。现在数据库那就应该是能记下来这个Q就这个问题，这个问题一定是一个尽可能我们理解成就是一串数字，一串向量的这个数字，这个数字是我们的问题，这个问题可以被存在向量数据库里。那下次有新问题出现的时候，我首先就能去比较这个问题是否被问过。
	那被问过是可以被度量的对吧？那以前我们去度量它最简单的方式就是这个文字跟文字之间是不完全一样。那变成向量之后，可以是看这两个向量的距离是不是很小。通过类似的一个度量衡或者一个阈值，我们就能判断这个问题是不是曾经问过。如果曾经问过，他的对应答案我们也存下来过，那也在上那个数据库里，那可以直接把答案给到用户，那就不用去大模型那转一圈，降低一下这个运行的时间和成本。
	还有一种情况就是我们之前讲过memory这个模块。有很多用户的这个聊天的结果记录，中间结果不一定都要存到向量库里，是向量数据库里。还可以先放在内存里，放在memory里，或者放在一个其他的memory的存储的后端引擎里。但是这些结果我们可以用来再学习，就是相当于我们已经部署好的这些应用，它在生产环境里面的很多数据。不管是问答也好，还是连续的对话也好，还是一个好的prompt和它对应的生成结果也好。这些都可以变成一些好的prompt learning的simple，对吧？就我们提示词学习的这个样例，变成这样的东西是可以也存到向量数据库里的那也就能为我们所用。
	所以这两个点是非常值得关注的。第一个就是我们的这个流程，几乎是所有的目前的这个大语言模型的应用，都是这样的一个流程。从自然语言的文本分割变成向量，再进到向量数据库里。它可以作为我们大语言模型的第二大脑。还有一个就是我们的很多的数据是可以不用通过训练这个模型本身，就可以直接给到我们的这个问答的用户。因为我们向量数据库里也有一个缓存的能力，把我们的一些以前运行的结果或者相似的答案能存下来，然后这个既能够为用未来的学习作为一个存储，同时也能为线上的服务提供一个缓存的结果，这个是比较常见的一种运行方式。
	目前我们看到南茜支持的，向量数据库，其实非常多。在这个过程当中，其实我们就是把这一个的高维向量，就这个高维的向量直接存到一个数据库里。我们的数据库里，大家都用过mysql这种关系型数据库，我相信像一个这个表一样维表一样把数据存下来。那现在数据库里面存的都是我们的这个高维的向量，也就这么一个不同，就存的内容不同，可能你没法直接读出来。看着这一串向量理解它是什么，跟我们的这个存储内容的样式会略微有不同。但存的其实也都是这样的一些数字。这些数字支持的这些数据库，目前能确有我们现在屏幕上看到的这些。
	比较典型的像我们这个课程里面已经使用过的chroma，还有这个PG vector，还有这个packing，这都是用的比较多的向量数据库。除此以外，像一些国内的公司，比如说像阿里巴巴，像微软的edge，包括我们看到这个MongoDB，其实他们也都有出去出这个对应的一些产品大家如果在自己的实际使用场景里面有这个需求，那也可以把它存下来。这个向量数据库也就只是一个数据库，没有这么黑科技。对那么一个典型的向量数据库刚刚有讲到，我们通过前面这一部分，把这个数据源做了加载转换，再做这个文本分割，然后变成嵌入之后存到这里了。那实际用户要去使用的时候，他是去query就来提问题，对吧？
	像我们的这个应用提问题，大语言模型应用通常是文本交互的一个方式。那提问题这个自然语言会变成一个向量，就我刚刚提到的这个地方的in bedding model一定得和这边的in bedding model是同一个model，对吧？不然出来的向量维度都不一样。可能如果两个维度不一样的向量，它是没法去做运算的，这个大家可以理解对吧？如果它是同一个嵌入向量模型的不同版本，可能它的维度一样，但是这样也是不太好通用的，这个我就不再赘述了，这个是一个很重要的使用它的前提。
	好，那么把这个问题变成一个嵌入向量去输入到向量数据库之后，会得到一个结果。拿这个结果的过程我们可以叫做retrial。Retrial的不一定是最相似的，这里有两个关键点。第一个点就是说这个相似，就我们在向量的比较上相似是最常用的。就这俩结果是不是最相似的，就变成了一个排序的顺序。但是除了最相似以外，还会有很多别的度量方式，所以打个引号的第一个点是在于这个相似本身。什么叫相似？是可以被定义的，第二是不是只用相似也是可以值得探讨的。
	第三，去retried的方法，我是问一次问多次，一次拿多个。Retire的不同就造就了我们不同的retire的一些设定。这个是一个向量数据库的一个最佳实践的一个流程，也是它的一些注意点需要去在使用的时候去注意的。然后除了那些以外，我们还可以看到有一个很关键的模块，在我们的data connection里面叫recover。这里应该有个拼写错，应该叫recover数据检索器。
	数据检索器就是从向量数据库里去检索数据的。这有很多种不同的已经实现的方法。包括amazon的这个chain desk的，这个是应该是一个搞区块链的。然后包括像这个ChatGPT plug in coil对等等这样的一些已经实现的检索器会在我们具体去做应用的时候去展开讲的好，总结我们整个data connection，其实到最后这一步，这个检索器整个这一条链路上都会是一个根据用户需求来制定的自定义去实现这个流程里面每一个模块的这么一个操作方式。而不是说一个既定的套路，跟我们以前写这个web开发和APP开发还略有不同。它是根据你数据的特点来定制化你的loader，你的transformation，你的这个向量数据库的选择。包括你最终要用什么样的方式去retrial，它完全是根据你的数据和任务来的的，所以它不是一个一招鲜的东西。但是这个流程本身是可以被复用的，并且是它一个最佳实践。
	这个是我们讲这个data connection的部分。到目前为止看大家有没有什么关于data connection的抽象的一些问题。我们五分钟的时间回答一下到目前为止的这个流程上的问题。然后五分钟之后我们就实际的去展示一下这五个这四个就我们的document loader、transformer、embedding model和我们的vector store。具体它的API是怎么用的，以及它的文档要怎么样去读。
	有个同学问store是不是要和recover配套用，不是强绑定关系。
	看大家还有什么问题可以在评论区提出来。对。
	文本分割需要怎么分？这个同学问的很好，我们待会儿会讲一讲这个文本分割应该怎么样去做对，然后漏的是爬虫技术吗？这个问题问的挺好的，首先漏的是一个基础方法，就我们刚刚在这一页有看到base loader这个鸡类里面有定义一个抽象方法叫load。Load的核心是说把一个特定数据源里面的数据加载到我们的人里面，变成一个符合document这个抽象的数据类型。我说数据对象，那么document里面要怎么样叫做符合document的抽象呢？右边就有写，他要有他的page content，他要有他的meta data。
	这个过程当中，如果你是从互联网上取数据，那么你的loader自然就要实现一定的爬虫的技术。但你也可以不是不用爬虫，因为有可能这个平台本身它开放了API，他的数据是可以让你去对接的那这个时候你就是通过对接他的API了，所以为什么说整个这个流程是任务驱动和根据你的数据要求来呢？就是这个意思，不同的数据来源它的loader自然就不一样。
	有个同学问想要锁定某篇文章的内容的in bedding怎么做？这个是锁定某篇文章是什么概念？有的同学在稍微解释一下，没有太看懂这个问题。
	然后有个同学问retry ve与现在搜索引擎的底层实现有什么区别与联系吗？第一，retry ve就是为什么今天不想把把这么多概念一下一股脑丢给大家。首先retrial是一个去向量数据库里检索内容的方法。然后它不是一个方法，它是一种抽象，就是我们怎么从一堆数据里面一堆向量数据里面去检索数据的这么一个抽象的方法。根据你的需求，你可以实现你自己的driver。而每一家搜索引擎他们自己的这个倒排算法和后面的优化算法，包括接了广告之后，广告要怎么插进来，这个都不太一样。
	首先这是两个不一样的东西，因为你的这个搜索引擎具体怎么实现？每家都不同，所以很难直接说现代的搜索引擎。但是你说从一堆大的结果里面检索内容，那他们是有一些关联的。因为他们核心都是从你的问的这个问题出发去找结果。那你有些搜索引擎也好，或者说这个企业端自己的类似于这个偏搜索的功能也好，用这个E用ES那其实也是在一堆结果里面去找数据，建索引。这个比对本身找索引和找向量是一个语义级别的差异。因为建索引它不一定能拿到语义信息。
	而我们现在聊的这个recover通常是基于大语言模型的in bedding的结果就是embedding之后变成一个带语义的向量。那这个时候他是靠大语言模型所把抽象出来的这个语言能力，就是他把你给的这个文本里面的含义变成了一个语义，语义变成了一个向量。然后我下一次问问题的时候，他也是一段文字。然后通过这个语义的比较来实现了相似性，或者说其他度量的比较和查找。所以会略微有一些不同。一个是纯粹的优化查询技术，一个是把语言模型的语义抽取能力融合进来的一种技术。
	Memory部分有跟data connection对接吗？是可以对接的，因为我们的memory有一些实现是跟这个vector store相关的。我们上节课其实有提到memory只是存在了内存里，但是我们实际要用它的时候，memory一定要找一个持久化的后端把它存下来。那现在数据库可以作为memory的一种存储后端。因为memory本身你就可以去存文本，你也可以存向量，对吧？那如果你要去存向量，那你找一个向量数据库把memory存下来是很合理的。
	比较好的markdown loader是啥？官方有提供。对，南茜官方有提供，待会儿我们可以看一下文档。
	如何爬取ChatGPT的数据？我不知道你是要爬谁的，是爬别的用户的吗？那这个是违法的。如果你是爬取自己的ChatGPT的数据，那OpenAI官方是有提供导出功能的。它它已经做好了，你点一下就出来了。Retrial不只是能用于相似的查找，这完全取决于你怎么去实现它。
	我理解像都是。我理解向量数据库存的向量是多篇文章的向量，我想锁定某篇文章的向量怎么做？这个问题问的挺好的。其实刚刚如果是这个意思的话，其实刚刚的那个腾讯云的那个视频，他做了一个介绍。首先这个问题没有标准答案，就看你怎么去设计它它是一个技术问题，需要一些技术设计方案来回应它。
	在刚刚的腾讯云的短片里，他有一个介绍是说把我们的传统的关系型数据库和我们向量数据库去做一个，结合。就比如说我要去一个超市，超市里面的所有的这个商品都变成了一个向量，那这个时候我要去找一个红富士水果，红富士苹果在哪？他首先得知道这苹果是在水果区，那这件在水果区这个事其实就不是一个去比较商品每一个商品了，它其实是把里面的商品分了区的那我们也是一样，我们可以去把我们的每一篇文章带上一个标签。相当于有些向量它就是有归属的那这个向量它是属于这一篇文章，另一个向量是属于另一篇文章要分区了一样。那么当你要去检索的时候，你可以去指定这个分区，当然这个分区不是指这个数据库的分片，而是指就类似于一个标签，对，那这样就可以去做锁定了。
	是不是embedding library的版本升级了，store里的内容都要重新生成。看你的这个升级是什么概念。如果你是说像OpenAI的那个ADA in bedding，从01版本升到02版本，那那是的，就是你的，你如果要用更新的GPT4模型，那原来的那个invading的结果就不太好用了，你得拿新的invading的ADA02去重新过一遍才行。
	Transform到invaded，invade的转化过程有什么规则吗？这个我没有太理解，因为transform就是对文本的处理，你可以理解transform是在是在针对你的具体要求去把你的document去做处理。我们变成了document的之后，我也可以再把它切成一个的文本段。因为document的文本也可以很长。就比如说我们今天会看待会看阿凯的这个document，它就很大。
	向量数据库里面数据的时效性怎样维护？这个时效性我不知道是什么概念，是指他是什么时候写进去的吗？还是什么？这个可能需要这个同学再再详细写。
	如果向量数据库里没查到答案，还会去查大模型吗？这个是你自己通过agent去实现的。同一段text，同一个invading模型，每一次计算出来的向量是一样的。这个问题已经问过好多回了，我就不再解答了。我是。
	大家问的问题好多，对关于向量数据库，大家对这个很关心。
	请问向量数据库的维度是不是越大越好？不是的对，因为向量in贝定的核心不就是要降维，把维度降下来。维度越大越好的话，那不就应该是one hot，对吧？
	带着标签进行in bedding吗？不是带着标签进行in bedding。对，你是要用这个向量数据库本身的一些机制来实现了，就是在外围做一些工作了。这可能要看你具体用的是什么向量数据库了，如果直接带着标签来in背景不是很稳妥。
	但是如果你是文章这种跟刚刚的那个水果区域还不太一样，因为文章太多了。我不知道你的最终的问题是10万篇文章吗？还是说十篇文章就够了。那如果十篇文章的话，完全可以分开存，就是十个不同的科尔玛都可以。但如果是10万个文章的话，那你把文章区分开来，这个本身就有点难了。对，你其实丢失了很多语义信息了。
	向量数据库搜索的时候，搜索最近的结果还是搜索完全匹配的结果？这是一个挺好的问题。在我的经验里面，向量数据库的时间戳不是一个很重要的信息。因为向量数据库存的就是那个文本本身，它不像我们存的关系型数据库。就关系数据库里可能会去存一些原数据，这个原数据是什么？我插入这个数据的时间，创建人权限等等。那现在数据库里存的就是这个文本本身的一个向量。对。
	好，那我们先切到展示这个demo的环节，然后接着大家再回答后面的问题。
	我们我们把这个拍摄下面的目录结构做了一个简单的调整。前面几次课里面的这个model IO chain memory都放到了对应的目录下面。然后今天的两个模块有点小看这个字。今天的两个模块会分别放在data connection和这个agents下面，现在这个大小应该可以了，对吧？好，我们看到这里有几个对应的，我们把门分别打开，older, transformed, invading和vector store。
	这个字号大家能看清楚吗？就我们现在的这个字号在notebook里。
	我看大家没有说看不清楚，就当你们能看见了首先第一个部分就我们处理的加载不同来源的数据的抽象叫document loader。然后在这个notebook里面我们会展示一下去加载这个TXT文件的这个文档加载器，以及加载这个active论文。或者说我们想要加载任何网页里面的这个静态文本内容。我们会把这三类作为一个比较常见的事例，在这个notebook里面给大家做一个展示。
	然后需要注意的是说，加载出来的这个结果都会被放在一个叫做document类的数据抽象里面。这个数据抽象通常就包含它的正文，就他这个document里面存的这个正文文字和他的一些相关的原数据的一些信息。对，刚刚我们在课件里面也有展示这一段代码定义，这我就不再赘述了。比较关键点就是说这个document类是所有的这个loader加载之后所存储的一种数据抽象。然后我们在这个document这个类的定里面也能看到还有一些常见的成员变量，page content和meta data是最关键的。
	除了这个以外，我们刚刚还有提到base loader这么一个定义。它是所有不同的加载器，就XX loader。我们自己写一个叉叉loader，都需要去基于这个base loader再去做实现。实现里面最关键的方法就是我们的这个load方法。我们要能够实现这个方法才能够称之为一个自定义的loader，那他就是在里面去写，我们到底要从这个数据源里面怎么样去把数据取出来。当然我们还可以看到它有一个其他的方法，或者说这里还有一个鸡肋里面实现的方法叫做load and split。就是我们现在看到这个部分，那这其实跟我们的transform有一些重合。
	我们看到这个transform，因为我们知道transform其实是在讲要把我们的这个文本去做分割，分成不同的文本，或者说分成不同的文本块小的文本。在这里面，其实有一些loader也把这件事给做了。就他在他的loader里面就把transform的这个切分的工作给做了。如果我们的transformer只是单单做一个切分的话，那是可以去这样去做的。但如果你你的这个transform工作更复杂，不只是一个简简单单的分割，那么你就可以在transform那个部分去实现对应的一些split。他这只是用了一个叫做默认的一个recursive的一个文本的一个分割器。如果你想要实现更复杂的那你可以再去重载这个node and split方法，或者去专门把这个定一个自己的split，然后让他去用，这样都是可以的。
	好，那我们看一下际上要去加载这样的三类。一类叫TST文件，一类叫a cap论文，一类是任何网页的文本内容。要怎么去做呢？其实它的抽象的最大的好处就是我们可以把很多重复性的工作封装起来。那就比如说在南区里面，我们要去加载TXT这样的文件，这个state of the union很有名的一个文这个宣言。那么呃加载的这个TST文件的loader就叫text loader的名字也很好记。所有这些loader都放在了这个document loaders这个包下面。我们从里面去加载这个text load，让我们重新还是重新启动一下这个。
	我们使用这个text loader加载这个文件对吧？我们看一下加载出来之后，这个地方应该是一个list对吧？我们回忆一下这个地方，它的结果一定是一个document的列表。实际上来看也是如此，它这里面是一个列表类型，然后里面存了document这么一个数据的抽象，它的page content是它的成员变量，里面存的是它的正文的内容，我们把它这个先收起来看一看我们的。
	这个类型是不是是一个列表，对吧？我们再看一下它的。第列表里面的第零个元素是我们刚刚看到这个正文的内容。他他把它加载到了一个document里，然后他的数据定义符合这个nance下面的这个document的这个类的定义，跟我们刚刚在看到这些实现是完全一致的。然后我们我们要想看一下它里面存的内容，当然它全部打印很长，你也可以把它直接导到别的地方去看啊。那我们也可以直接使用一个简单的一个python的去查看它元素的下标的操作。就看它的前100个字符是什么，它其实就是它节选的前100个字符串。
	这个里面其实就是我们刚刚看到的这个document，它被text loader这么一个方法，从一个TXT文件变成了一个男券的这个document。这个document可以在后面继续去做一些操作，比如说就可以直接去做这个切分去做这个embedding。但因为这里我们没有去调这个load and spray的方法，所以它没有切分，它放在了一起。我们因为要介绍这个概念，我们故意把把它放到了这个transformer里面。待会儿我们会跟大家讲，这个是最最简单的一种low的方法。
	类似的假设我们要load的不是一个TST，这个就是从文件格式的角度来做区分。假设我们要load的是一个markdown，我们刚才有同学在问，我们可以看到在南线的文档里面，都都写的很多了不去做粘贴的工作了。我们在documents里面我们可以看到document loader有CSV，比如说有CSV的loader，从这个document loader里面去import这个CSV的loader，可以用。然后包括像我们的这个Jason，有这个Jason的这个loader，像我们的这个markdown也有对应的markdown，它叫constructed markdown loader，那这些方式都是按照统一的漏的接口来实现的。大家如果仔细去看这个代码应该能明白，就有一个输入文件的路径，文件的路径传给我们的这个loader。然后这个简陋的方法，但它也可以一次性像我们刚刚写的那样，把这个loader点loader方法直接把这路径丢进去就好了。这个是一个比较，然后它里面的内容也都是这个page content这样的一个格式。大家如果对这个其他的loader就从文件格式这个角度对其他的loader有一些想知道怎么去处理的话，就访问这个APAPI文档和这个开发者文档就可以了。我们把这个附在这里，给大家做一个参考。
	好，我们除了基于文件格式有不同的loader实现以外，其实不同的数据来源也会有不同的load。就比如说我们这里可以看到在这个模块里面有这个RQ，就我们的这个论文，我们有不同的同学在讲，其实我们的这个translator这个项目也可以去翻译RQ上面的论文。是的，这个是一个很值得去做的一个事情。我们在在这个蓝圈的这个代码库里能看到，其实archive loader它的代码实现本身并不复杂，就只是一个重写了这个漏的方法，这个是必须要实现的一个方法，就没有别的一些内容了。它的其他的这些loader，都在我们刚刚讲的这个模块下面，我们的document loaders下面，有各种各样的不同的自定义的loader，大家如果感兴趣的话，可以去看一看你想要去使用的这个loader。如果它不能成功运行，出现了什么样的bug，包括像刚刚看到这个markdown。回到这儿，我们继续看这个archive loader，它实现的这个方法其实主要就是这个load方法。这个active loader他给了一些比较重要的参数，是base loader里面没有提到的。
	第一个就是我们的这个query，因为正常我们使用阿Q我不清楚有没有同学去用过active这个网站。我们再稍微点开一下这个网站给大家浏览一下。这个是阿cave的这个网站，阿cave这个网站应该是在我们的课程非常早期的时候就讲过了。它是一个用来降低学术垄断，让大部分的人都可以去学习最先进的学术论文的一个平台。这个平台其实除了CS就除了计算机相关的文章以外，也会有物理、数学相关的一些内容。我们刚刚看到的那个链接，其实是GPT3这篇论文的一个链接，在阿Q上面他也遵循了这个学术论文的发布方式。他会每篇论你可以简单理解成每篇论文有一个自己的ID，那这个ID其实就在这个UI这里就能直接获取到这个独一无二的这个ID。在这也能看到，就我们要去其他的论文，如果要去引用它，可以通过这个方式去引用。
	那么在这个平台上我们要怎么去搜一篇论文呢？通常也是两种方式一种是说通过这个论文的名字，或者说直接找到一个比较主要的论文作者的名字就能去做检索。第二个最直接的就是直接找到他的ID去做检索，当然你也可以通过搜索引擎，再加上阿KV这个关键字来搜索都是可以的。但在这个平台内部来说，其实最核心的就是这么几个点，作者，然后我们的论文标题和我们的这个ID我们实际上out of loader也是遵循这么一个思路来做的。
	在archive loader里面有一个query这么一个参数，它其实就相当于我们刚刚提到的，用来在a cap这个平台上查找文档的一个文本。那它可以是ID，也可以是标题，也可以是作者。比如说我们要去实际使用这个active loader的话，首先需要安装两个依赖包，一个叫active，一个叫潘PA是up of loader去处理下载好的从这个平台上下载好的论文，因为他下来的是这个PDF的文章，去处理他的一个包。这个是这个loader内部已经实现的，我们这儿就不再去复这个纠结用的那个PDF的这个解析的包了。安装好之后，同样是从这个document loader里面去加载这个archive loader。然后我们设计一个query叫2005.14165，也就是我们刚刚看到的这个T3这篇论文的这个ID，作为我们要查询的这个query，这里有一个第二个参数叫load max stocks，它这个参数是一个optional，一个可选的参数，默认值是100，简单来说就是返回top多少的这个文档。因为他无法保证他给到的结果就是对的，所以他通常会给你一批，这一批里面你再根据一些特定的方式去查出来哪个是你真正要的那这儿默认会有100个，这个是比较大的一个文档数量，下来会比较耗时。
	那你可以给它设置成一个，比如说top 5或者top 10，如果你不知道他这个IQ LID的话，当然你实际实际在写代码的过程当中，你自己去构造一个应用。比如说你要去做这个RK的这个中文版的一个网站，那你可以遵循着按照作者来梳理这个网站，然后把这个作者相关的所有的都当下来。这个是一个很常见的一个使用场景，loader这样的一个使用场景。那我们这里就实际去根据这个ID找出5篇跟这个ID相关的在阿卡姆上面发表的论文，他这里正在下载这个PDA文档，大家能看到左边这有一个一秒钟之前接下来的一篇文章，就我们这个是在后台发生的一些工作。然后我们能想得到他下来之后应该是load成了一个document，之后还会把那个清除掉，不占用我们的这个结果。
	这里还会有一个参数叫做load or available meta。默认情况下它是一个false的值。简单来说就是我们刚刚有提到，加载完成之后变成一个document。Document里面有正文，也有我们的原数据，那原数据到底要存什么？结果是完全自定义的那当我们设置为boss的时候，也是它的默认值的时候，它只会在RQ上面去下载它的发表日期，发布时间和最后更新时间，包括标题、作者摘要。如果设置为数的话，它当然就会下一些更丰富的一些meta data过来。
	那我们通过这个嫩方法知道，其实也就找到两篇相关的，因为我们这个很精准，这个ID但是为什么会是两篇而不是一篇，对吧？这其实很有很值得我们去看看，这是什么原因？第一个就是通过meta data是比较好去找出这篇文章到底是什么的，这也是一个为什么要用archive loader作为第一个数据源分享的这个自定义loader的方式，因为它的实现是一个比较符合我们最佳实践的一种实现。因为我们加载出来的正文通常是乱七八糟的，因为你你的数据来源很多。但meta data的作用就是说我能不能够不看正文，不看我的page content，就能知道我现在这个document是什么。一个好的document的meta data应该做到这一点。
	从这儿我们能看到，首先这篇这个documents 0，它是一个2023年，就是6月9号今年6月9号才发布的一篇文章。它的名字里面也没有这个GPT相关的一些内容。但是在他的summary里面在他summary里面他有引用到我们的，举例到我们这篇文章，大家能看到我现在框下来这个部分对吧？他有写这个基础模型，像这个大型的神经网络预训练的，在大的语料和文本上面的这些有革命性的NLP的这些基础模型，可以被直接的去使用。其中就举到了这个GPT3这样的一个模型，所以在他的文章里面，其实正文里面有提到他，所以被他拎出来了。
	那么还有一个有没有找对呢？我们可以看到第二篇确实就找对了，我们的这个一共加载了两个出来。那第二篇找对了，找到了这个large models are future learners，就是我们的这篇GPT3的论文，然后怎么样去使用这个loader，其实我刚刚有提各种各样的使用场景。就比如说我们要去做翻译的时候，基于作者，或者我们要去基于一些关键领域。比如说我们最近正在研究这个language model，那可能就可以通过language model去把很多语言模型的文章都下载下来，然后加载出来。加载出来之后你可以分段去存，也可以直接去做翻译。因为在content里面，其实这篇文章都已经拉下来了，对吧？你不需要再一篇一篇的通过爬虫去获取，这个是啊比较典型的一些使用场景。
	通过这样的方式，我们把R上面的论文能够通过这么一行代码。其实你加载了一个arve loader，然后你获取了一个关键词，这关键词其实可以填进来，就这么一行代码，你把这个论文拿下来了。拿下来之后，因为archive loader本身实现的比较好，你还能够通过meta data去做一些筛选，那这个是加载这个论文。
	那类似的其他的网页我也想加载怎么办？就比如说任意一个网页，只要它不是通过这个javascript需要去动态调用的，静态的这些内容我能不能拉下来，那也是可以的。有一个叫做constructive URL loader，这个constructed在loader里面会广泛出现，作为很多非结构化的数据来源的前缀。就比如说我们刚刚看到这个markdown也有这个constructed markdown loader一样，所以不要被前缀迷惑，关键得看后面。
	在这里面，它核心是要说检测我们的给到的这个UIL的类型，然后去做对应的加载。然后里面有两个关键的参数。第一个就是我们的UIL，它可以首先它可以一次加载一批UIL。我不是一次只能加载一个网页，你给我一堆UIL我都可以来做这个加载。
	第二就是说他做了一些从事的机制。包括这个加载的模式上的区分，这个continue on family，这个方法主要就是说我这一批里面有一个失败了，还要不要继续。然后mode是指我的这个加载的模式有single和elements两种。这个就是涉及到一些前端的标记语言，HTML的一些基础知识，我这边就不再展开了。但是他的实现方式比较简单，我们看一下我们用它去加载一个接下来我们讲agent的时候会用到的一篇论文，他是在这个github上面使用的，这个github免费提供。不好意思，多了一个回括号。这是这个UIL，就我们的这个react这篇文章，打开是一个静态的网页，这个网页里面有图有表，其实这个其实也是一张图，有文本，然后有图片，然后我们可以回到这个事例里面来。这个其实就是我们要去展示的这么一篇论文的文章。
	在这样的一个地址里，那他会怎么样去加载呢？首先我们只放了一个，把这一个在这个列表里面传进去。UIL那会这忘了是什么意思。对，然后点一下这个加载，其实它会去加载这个内容速度很快。因为它只是去请求这个静态网页的内容，能看到这个data里面其实已经有内容了，那我们看看它的meta data是怎么写的。
	这里有写一个source，所以对于UIL的这个loader来说，它的meta data比较简单。只有一个就是你给它的这个内容，就这个UIL是什么，通过它能去校验这个data 0里面的这个配置。是如何的那我们能看到这里有大量的换行符，它也是符合我们的这个格式标准的。
	这个就是我们把这个网站的UIL里面的文本的内容能够拿取下来，当然它还有可扩展的空间，就比如说这里有一些图片，那这些图片能不能被我们的这个loader给加载下来呢？如果我们要去做这个扩展是可以去做的。但是为什么这个UIL没有实现？我们其实在之前有提到我们的这个大语言模型处理的还是文本为主。然后这些图像如果我们把它加载进来了，然后我们把它以一个document的一个特定的成员变量放在那儿了。但是后续不管是embedding也好，还是我们的vector store也好，暂时没有太多的施展空间。所以大部分的loader你会发现即使是加载的各种非文本类的多媒体的资源，但最终加载完之后，存在document里面的还是以文本内容为主。
	这个就是它的论文标题，作者和下面的这个摘要和文这么一个关于loader的一个事例。我也希望大家能够根据自己的情况，多去这个年轻的文档里面去看看各种各样的那大几十个loader有哪些是使用适用于你的场景的。然后可以积极的去做这个实测，还有什么问题我们可以再交流，也可以去查阅这个人券的官方的这个社区，里面也有很多的反馈和更新。
	这是loader，我们看一下transformer怎么做的那transformer通常我们在讲transform这个操作的时候，我看之前那个hugin face也有一个模块叫transformers。就这个transformer这个词出现的非常高频。那在我们data connection里面去讲transformer的时候，通常是指我们刚刚已经把各种数据源变成了一个document。但我现在需要把它变成一个向量。但是这个document跟向量之间差了一环，差一环什么呢？就是document有可能太大了，或者说他需要被操作组装一下，要么是去掐头去尾，要么是保留这个头，把尾巴删掉，或者说做个什么其他的操作都有可能。
	但他最终的目标是想说通过transform这一个步骤，变成一个可以丢给我这个embedding ing模型的输入。就把我的原来的document切成符合evading模型的这个样子，切的时候就各种各样的方法。在这儿这个document类放在这里，让大家能记得住。这里我们能看到整个notebook举了两个例子。一个例子是文本的切割，这个也是我们之前有涉及到的。第二个就是我们的代码的切割。
	文本的切割通常来说是因为文本太长了，我们需要把文本变成小的文本，因为太长的文本语义提不出来。他可能那段文本太长之后，本身就有好多个不同的主题或者说表达的含义，交给一个向量模型，它是很难变成一个统一的向量的。它可能就会表达能力不足，而且本身多语义的情况下，它很拧巴，它不知道应该表达成哪个语义。这时候就需要我们去切，最好切的每一段都是一个有特定含义的，单一的这种主题或者情绪，或者各种，反正是有一个框框能去表达它的是最好的。那这个时候我们需要去做分割。分割有两种方式，一种是基于你猜这个文字的方式，一种是基于你猜的这个数量的大小，这个文档的描述不是很清楚，就我们直接取了上面的官方文档，那看下面的代码是比较明确的。
	首先我们从参数的角度来看这个方法。在我们讲base loader的时候，它有一个load and split方法，就已经用到了这样的一个text please text splitter这么一个文本分割器了。这个文本分割器，它主要就接收四个参数。第一个就是我们的这个function，我们的这个nurse function，就是我们计算这个计计算这个切它不是要切成很多块，这个长度的方法就是按长度来算。还有第二个，就是如果我按长度来算，那么怎么样保证我每一块不会太长？因为你不可能切出来刚好每一块都一样长。那它能做的事情就是我设置一个最大的值，保证我切出来的每一块都不超过这个值，就这么一个意思，这里会有一个最大值的大小，就这个chat size可以认为每一块有一个最大值，由他来确定由他来确定这个值。
	还有一个就是说我们切的时候肯定不能每一刀都切的那么精准，刚好就是把那个语义切的干干净净的。就比如我们讲之前最早讲这个喝咖啡的例子，对吧？我去这个商店里面点了几杯咖啡，几杯拿铁，一杯美式，喝了几口之后送给朋友还剩几杯，你很难切出来，你很难说切在哪一刀上面刚好就羽翼是干净的。所以会有一个叫overlap的一个参数。他就说我把这个长文本切成了4份，但其中每一份之间会有一些重叠。这样保证我在语义的提取的时候，用in bedding的时候不会造成一些问题。这个也是一个最佳实践，就我们这个overlap。第三个就是说是否在原数据中包含这个起始位置，相当于给一些额外的index的信息，让我能够去回到这个切之前的这个位置，比较方便我后面的一些应用的工作的开展。如果不需要的话，就设置为这个boss也是OK的那我们在这个事例里面，我们能看到，我们要加载一个长文本，这个长文本我们待会可以直接输出出来看一眼。
	点错了，我们可以重启一下这个notebook。
	我们可以首先看一下这个文本是什么内容。
	是这样的一个关于新冠的一个TT文件。我们可以通过python这个python本身的read方法先读进来。读进来之后我们导入这个text。然后把它实例化，相当于这个相当于实例化了一个你可以理解成有一个自动的切割机一样的东西。然后它需要有一些输入参数，我们刚才讲到的就这四个参数。
	然后实例化之后，现在问题就来了，我现在有一个切割机一直可以去切割，然后我需要递给他这个递给他我们的对应的这个待切割的长文本，对吧？那待切割的长文本支持给一个一个列表的长文本，所以他这边输入的是一个列表，列表里面其实是带切割的文本，我们相当于就把刚刚打印出来的那这一长串文本，其实是个字符串丢进去了。它也支持有多个这样字符串去做切割。那我们把它丢进去之后，变成了一个document，然后这个document就这个text，我们可以先执行一下，给大家感受一下。
	首先这个create documents方法就是把我们的输入的这些字符串也好或其他的格式也好，按照我们的需求切割成对应的这个documents。然后这个documents里面的这个内容，其实就是我们的这里打印出来这个page content。所以我们直接按照这个类型输出给大家看一下，这是一些list对吧？然后我们看看每一个具体的输出出来的是什么。这里的概念稍微有点绕，是南券设计的这个角度。那现在我们能看到通过这个切割，我们的这个tests就已经变成了刚刚我们的loader加载出来的那个数据类型。就是有一个list，list里面的每一个元素都是一个document。
	那么这个过程当中可能会有同学会很疑惑，就是为什么这儿没有使用一个loader？就是在这儿我们直接通过这个python去读了一个文件，而没有使用这个text loader。而是把这一堆字符串传到这里来，这个就是我们可以理解成是为了用来更灵活的去应对各种情况，首先就现在这个事例来说，就TXT这个文本类型的事例来说，在text loader的这个内部，它有一个方法叫load and split。所以如果我们使用text loader并且又调用了它的漏单的spring的方法，然后用这几个参数去做初始化的话，它就能够直接形成这样的一个结果。
	但是还有很多情况，他其实它的输入源不是一个简单的loader就能描述的，他就是通过一堆复杂的操作，变成了一堆python的字符串。那针对这样的情况，可能他一个loader就搞不定了。那那他就只能说通过loader变成一堆字符串，这个字符串再去接一个这样的分割器也好，或叫transformer也好，就我们的文本的这个transformer转换器也好。然后再变成我们最重要的这个document这个形状。并且里面的每一个document的这个语句长度，其实也就是直接可以转换给这个向量模型的长度，所以才会有我们看到的这样一幅画面，就是对于一些非常简单的数据来源和数据类型，这个transform甚至可以包含在这个loader里面。
	因为本来它这是一个数据的状态，通过这几个操作变成了不同的状态。对于一个简单的数据来源，它的loader就包含了load和transform，这样去讲大家应该能理解。但是对于复杂的场景，可能我们的source来源各种各样，它甚至都不是同一个来源。那他前面就需要多个loader，就跟我们上节课讲这个多输入多输出router chain的这个实例有点像，多个数据来源多个数据来源统一不同的loader整合好之后，再丢到transform这个过程里面来。那这个时候他自然就需要你去拼成这个字符串的列表。然后最终把这些不同数据来源经过不同loader来的字符串，通过统一的transform变成我们要的可以给到invading向量的这个一段的document。那这样是以另一种场景。所以我们单独把这个拎出来是想给大家做这样的一个示例，但我们现在不去故意造场景去演示这个功能，我们在后面几个实战里面会去展示这种具体的功能。
	当然我们还有一个刚刚到这儿为止，我们可以看到原文，就这样的一个原文，它输出的内容是切割后的。除此以外，我们还得去讲一下，就是通过transform的方式来造这个document的时候，它的meta data是可以符合我们document的定义，能够以这样的方式传进去，就共享这个meta data。就比如说我们刚刚有一个我记得有一个同学提问，就是我们的有怎么样锁定这个文章的embedding。就我有好多篇文章，然后我只想在某一篇文章里面去做向量的查询。那这个某一篇文章本身，一篇文章是哪篇文章，它可以变成一个meta data里面的信息，就跟我们刚刚讲这个R也好，或者说我们这里去记录一个文件路径也好，记录一个UIL的地址也好，这些东西都可以放在meta data里面。然后当我们从向量数据库里直接去查的时候，也许不一定能把这个信息存到向量里。但是在向量数据库的外围，比如说我把这个结果拿出来了，我是能够跟外围的meta data去做匹配筛选的，所以这个是一种处理手段。
	就回答刚刚那个同学的提问，那这里我们能看到相比于刚刚这个tax 0，我们的documents 0把这个meta data传进去了。所以这里的这个meta data只有一个start index 0。但这里有一个document，一当然我们这个事例里面document是一样的，其实都写一是才是符合他的要求的，或者说我们写一个document name都行，这个只是为了演示工。然后这个start index为什么会有？是因为我们把这个add start in index给它做成了这个true传进来了。好，那这儿我们就把这个分割文本的这种transformer应该跟大家讲明白了。
	那还有一种操作是什么呢？就是说去分割代码。因为我们像有同学问这个loader是不是爬虫？Loader可以是爬虫，那么这里就举了一个类似的例子，假设我们现在把一些网页的内容加载进来了，在文献里面它有一个预定义的变量，全局变量叫language，就是编程语言。我们可以把它导入进来，在text bleeder上面，speed split er上面有一个这个language。
	然后我们去看这个language是一个什么概念呢？他就说我现在能去支持的编程语言的这个列表是这些，其中就有我们要展示的这个HTML，我们有讲HTML本身就是一个标记语言的这么一个类型，那么这个标记语言大家如果打开过网页的源代码的，就会看见这个HTML文件，差不多就是通过这样的一个，一对一对的code block包裹起来的，然后里面表达了不同的含义，这个是南茜的这个网站的一部分，那从这个HTML文本我们怎么样去使用它？就相当于这个是首先这个是文本，对吧？这是理论上，这是语言模型能够处理的一些内容。但是你直接把这个丢给语言模型，感觉直觉上就不太靠谱，对吧？
	第一件事情要做的就是说怎么样去读进来loader loader进来，刚刚的这个UIL loader就能干这事，这就不串起来了。第二就是说拿到了这个HTML了，我怎么样去transform，是怎么样去切切完之后怎么样变成向量？向量是下一个事例我们要讲的那现在就回到怎么样去切这件事儿，那怎么样去切呢？能欠给了一些预定义的这个方法，还是这个resist，这是一个默认的一个分割器，能切多个的默认的一个分割器。它也有这个不带reserve，就这个character taxes splitter也是有这样的一个定义的，我们待会也会用到这个，就是为了处理有多个输入待切的时候用这样的一个split，那有个from language的这么一个方法，那这个from language就是这个language就对应在这里。
	所以如果我们要去处理代码的分割问题的时候，我们可以去检查一下你现在下来的这个能确的版本，它支持的编程语言是不是有你要处理的这个代码。就假设我们要去做一些代码的一些应用，或者说再训练也好，翻译处也好。那这个时候from language就能干这事儿。你可以从language里面直接就可以点HTML对吧？
	那它是一个这样的预定义的一个语言类型，这个和我们后面去用agent很像，然后我们这俩参数应该比较熟了。第一个就是我们切割的这个块的最大的这个大小。第二个就是我们的这个overlap设置为零，就不要重叠，因为我们代码就不重叠了，然后我们去执行一下。把它打印出来，大家能看见首先这里是有很多个document，对吧？去输出一下它的长度。13个，他把这里他其实相当于把这里的每一行切成了一个。
	我们可以实际来看看出来，这里其实就是一个这13个具的document，然后里面他做了一个什么样的操作，就我们刚才讲它是一个code block的一个设定。那这儿大家看他第一个document把HTML就是相当于把它这一行和我们的下面这个HTML放在了一起，就这两个放在了一起，我们的这个head，然后title放在了一起，就这里放在了一起。那这个是按照它的切割长度来的，就60，然后没有这个重叠，把这个style也放在一起。然后这个切割方式首先是南茜已经提供的自定义的一个方式，然后其他的编程语言，它也有提供这样的一些切割的方法。
	那这个方法本身好不好用，或者说符不符合你的这个实际场景，需要你自己去实际像我这样跑一下之后才知道。所以多去用这些内置的这个分割器也好，或者说这个splitter也好，是很重要的。因为第一，如果他能够直接用会很省事儿。第二就是说如果它不适用，其实你需要做的第一件事情是就是你如果他不适用，你需要做的事情是去看看这些参数的设置本身符不符合一些要求。就比如说60是不是一个过小的一个数，或者说相对你的爬起来这个网页，60都有点太小了。因为它这个网站很简单，但如果你你排除那个网页里面充满了各种大量的文本，那这个地方可能就得设置的再大一点。
	这个全是实战经验得来的一些效果，不能通过一个API你就能一下学会太多，所以这个是很重要的，也是把这个留了大量的留白，是希望大家能够基于这个方式去试一试。咱们也能够自己去找一些自己写的代码也好，或者网上的其他代码也好。然后只要是在这个列表里的，看看它的一个切割结果是不是满足你的这个要求。那这个是对于代码的处理，它有提供一些类似的方法，甚至大家如果有兴趣在这个列表之外的我们想去处理，然后找到了一些最佳实践的切割方法，也可以向社区去做这个开源贡献。
	第三个就是我们的这个in bedding的部分。In bedding其实我们前面在学OpenAI的API的时候就已经学过了。它核心来说就是一个嵌入。这个嵌入的点就在于把原来的复杂的自然语言，它本身是一个很高维的一个空间，能够降维。然后降维之后，还能够保留它的语义，并且能够在这个降维的空间的向量里面去做一些操作，比如说推理，比如说寻找相似的，这个都可以通过向量的操作来实现。
	那么在南线的这个embedding类里面，有两种方法，一种是对文档进行embedding ing，一种是对查询就是query去进行evading，区别就在于他们的这个类的使用方法上。对于文档的embedding，你可以输入很多内容，就是我可以有多条。对于这个query通常是单个文本，这个也跟我们的使用场景是很相像的。就比如说对于我们一直在讲这种问答型的应用来说的话，通常它是后者，就是它是一个问题。那这个问题会变成一个embedding的向量。对于其他的场景，更多的更广泛的场景，可能你可以使用前面这种方式。
	好，那我们看一下这个使用方式，使用这个银北京的方法很简单。第一个就是说在南线里面有这么一个模块叫一杯子，里面有很多embedding的方法，OpenAI的embedding是最常用的，我们在课件里面还有给大家看到其他的一些embedding方法。同时，我们在官方的这个网站里面，这个项目列表里面也能看到。是当然他首先是推了这个open OpenAI的in培训方法，但是在我们的这个integration就在这个集成的目录下面，里面还有大量的embedding的模型，大家如果感兴趣的话，可以在这个部分去做进一步的研究，找一个符合你自己的使用OpenAI的这个in bedding。
	我们还是重启这个notebook，具体要怎么去用呢？就跟我们调OpenAI的API1样。但他这里把OpenAI API相关的调用都已经封装到年前的这个模块里了。所以你只要在环境变量里面设置了这个OpenAI的APIK，它自然就会通过这个OpenAI embedding这样的一个类的实例化过程去找，这里它会。不需要你输入任何的内容，只需要你提供一个open API key就好。这个时候他把你给了一个多个文本，对吧？
	我们刚刚提到的invade的这个documents的方法，这个方法会返回五个向量，相当于这里的这个列表里面的每一个文本，都会被它当做一个单独需要去做in bedding的这个结果。有点类似于我们刚刚看到的page content，应该跟这儿接起来对吧？就我们在刚刚在这个transformer里面看到了这里的每一个page content，就可以组装成我们要拿去做evening的一个列表，这个列表就可以直接丢过来。那么这就得到了一个OpenAI的ADA的in bing的结果。还记得这个ADA的这个inviting向量是1536位，可以在这做一些结果。当然也可以对这个query去做一个embedding，就使用它的embedding y query方法就好。这个是非常简单的一个实现。然后in beating的这个方法也是符合统一的接口。
	如果我们使用了其他的一些in bedding的，就非OpenAI的embedding的模型的话，也是支持对应的这两个方法的，只要他按照我们的这个实现，对应的这个代码也都在南茜的这个github上面，我们这就不再赘述了。我们接着再快速走到这个way store，就是这个向量数据库这里来。Vector store我们在这个notebook里面，我们会去讲怎么样去加载一个数据源的数据，然后做转换，然后变成一个嵌入的向量存下来。接着我们会去对它进行query，拿到一个结果。这里我们可以使用这个chroma这样一个被广泛应用的一个开源的一个向量数据库，安装一下它还是重启一下这个notebook。
	好，我们把这个收起来。好，我们这儿就把它串起来了。刚刚那三个notebook其实是为了让大家去做各种各样的测试，没有把这些各种API串起来，也是为了讲的更明白一点，尤其是这个loader和这个transformer之间关系。场景不同，简单场景一个load就够了，然后这个复杂场景你可能需要单独处理，这些都是根据你的需求来的那这儿我们其实真正要用的这个向量数据库，就是把刚刚那三个notebook里面的每一步都可以跟着对应起来。
	第一个就是我们用text loader去加载一个TST文件，对吧？刚才也在loader里见过了这个row documents，就是我们的经过转换之后的这个长文本。然后我们还可以使用一个直接执行下来了。还可以使用这个transformer，就我们把一个长文本怎么样做成一个短的符合我们要求的文本呢？就是把这个原来的长文本在我们刚刚定义好的这个文本分割器里，这里有做这个对应的定义。我们没有去把这个index给它加上了。然后经过这个分割器，就变成了一个短的文本，然后我们把它打印出来，给大家有一个比较直观的体验。
	这个documents其实就是切割之后的这个文本。然后我们这的这个设置的是1000，所以感觉看着很长对吧？我们假设把它设置成100，然后我们再来做一次这个切割。
	这个100设置的有点太小，这直接就超过了100，所以报了一个警告。就说明他这个实在是有点过小了。它自己的这个默认的切割器不太能接受小于100的操作，所以就做了一个告警。但这个问题也不大，因为我们只做这个示例，我这边就不再再去调整这个数字了。这个意思大家应该能理解了，这个documents其实就是我们通过这个文本切割器切出来的这样的一个内容。这个内容是我们即将要丢给我们的这个向量词嵌入的这个向量模型。
	向量模型变成一个向量之后，存到我们的向量数据库里面的一些结果。就比如说这里我们实例化这个，我们导入了这个chroma，这chroma里面有很多种把我们的这个结果存到roma里面的方法。那最常用的是这个from documents方法，这个from documents方法他接受的是什么呢？你可以简单理解成我们这儿已经把它切好了，然后我们把ebing这一步和我们存进去这一步，通过这个from documents方法都封装在了一起。那他会去把我们的documents，就是这儿一段一段的文本，通过我们的OpenAI emb ign就我们刚刚在这个事例里面有看到，通过这个OpenAI in building会变成一个一个的向量，然后把这一堆documents通过这个open add in bedding，想在这里每一个都会变成一个list，这个list会一一对应的被in bedding模型变成一个in bedding的向量。然后这些向量最后就会存在一个向量的列表，然后把这个向量的列表全部都存到这个chrome里面去其实这一步就干了。
	从我们的这个transform之后的结果到invade的，然后再到我们存进去这么一个结果。好，这已经存好了。那怎么样去验证它存存没存对呢？我们可以进行一个相似性的一个搜索，我刚刚切的比较短了，所以跟开始的这个示例会略有不同。因为它只有这个长度为100这么一个圈。然后这里首先我们先不关心这个下面的不同，我们看一下这个地方的实现，这里对于我们的chroma这个向量数据库，这个DB是一个chroma这个向量数据库，跟我们之前例子里面用这个circle light有点类似。DB其实就代表的这个向量数据库。
	那这个向量数据库要去对它进行检索查询的时候，它实现了两个方法。一个方法就是similarity search，就这个相似性的搜索，这个方法的输入是一个自然语言的文本。第二个是我们的基于我们这个in bedding向量的一个搜索，就是相当于我们把这个文本，就我们这儿选中这个文本框，变成一个嵌入之后的一个向量。然后拿这个向量再去向量数据库里面查。所以我们能看到这里的这个in embedding vector，就是把我们的这个query，通过embedded query方法，我们刚刚有讲到这种问答类的，通常会使用这个方法，变成了一个向量，这个可以直接我没有再去定义一个invading模型了，就直接实例化它。然后这个地方我清楚大家拍熟不熟，这个地方其实就等价于我们这里的实现，就在这儿我们定义了一个模型叫invading model，就是是一个意思。对，为了看着这个代码很简洁，因为他只用了一次这个示例，其实就等价于这个，然后再用它来进行这个。
	其实是这个意思，但我们注释在这里其实一个概念。然后用这个in bedding的向量的这个结果去做相似度的查询。然后查出来的这个结果，我们把最相像的那个输出来，用的这个similarity search by vector多了一个by vector，我们执行一下，输出来结果是一致的。因为开始的那个长文本是之前的notebook按照1000的这个track拆出来的向量，所以它自然查出来结果是比较长的。因为它一开始存的就是长度为一千的这个文本块。那你变小之后，那个向量数据库里面的结果都变了。所以自然也就会有不一样的结果，是按照我们新的切出来的这个documents的结果来做的这个检索的答案。这个就是基于向量数据库的一个语音搜索。
	其他的这一部分把我们整个前面讲的三块都串起来了。怎么样去把一个外部数据源变成南线可以使用的document这样的一种数据抽象。然后这个数据抽象如果它本身是比较长的文本，我们可以去给它做一个文本分割器，这是一种做数据转换的方式。
	这个文本分割器里面有比较重要的两个参数，一个是切的这个文本块，还有一个最大限制的大小。但是我们其实会发现它是一个比较软的一个限制，一个soft的限制。因为它为了保留语义，就比如说你设置成一，那肯定是不合法的。它为了这个splitter仍然是有用的，它会有一些语义级的判断。就比如说他会实际抄一点点，但他会保证这句话完整，不会把一个单词给切断，或者把明明是这个副总统，他只切了一个VICE和president都没了。为什么会出现这个情况？还有一个原因是因为我们把这个overlap设置为零，就没有重叠，所以它就一定得保证切出来的每一段尽可能是没有丢失信息的。这些单词就不会被它切断，它就会略微有一些错，这都是使用这个分割器的时候一些实际的技巧，需要大家多去改一改这些参数去体会，这些也不会产生token的费用，都在本地进行的对，然后把它切割好之后，就变成了一个document的列表。
	然后这个document的列表，就我们这里看到这个document列表里面的每一个document，就这个page content，每一个document就有会变成我们evading向量里面的输入。所以这个地方大家去看代码的会去看代码的话会发现其实他掉了这个OpenAI in bedding里面的embedding的这个document方法，就是我们这儿的这个方法，调了这个方法在内部。那这个方法需要的输入是什么呢？需要的输入是一个documents的列表，那不就是我们这里传给他的这个document的列表，对吧？就这里整个这个方法是一个非常简约的一种实现形式，你输入一个你要的嵌入模型，输入一个已经处理好待嵌入的文本。然后用这个chroma的from documents方法就把这一批数据都存到了这个向量数据库里。
	当你想要去对这个向量数据库里去进行搜索，检索你要的内容的时候，使用它的两个方法。一个方法是这个similar search，那你就可以直接去基于文本进行查询。第二种方法就是你要指定这个嵌入向量的这个模型，然后把你的这个query通过你指定的这个embedding model变成这个向量，然后拿着这个向量去直接用这个向量跟我们的这个chroma里面的所有的结果去做相似性的搜索也是可以的。然后输出的结果，其实就是我们直接按照这个相似度的最相似的排在这个列表的最前面，作为了这个dox的一个结果。这些函数的定义都在我们的instagram里面有对应的方法的定义。大家能看到我们这有对应的roma的方法，这里面会有一些其他的一些使用场景。我们会在具体实战这个QA机器人的时候再去逐个去讲，这边就不做这个API的翻译工作了。好，那到这儿为止，其实我们data connection要给大家讲的这些概念模块和最典型的用法，就差不就是这些内容。然后看看大家还有什么问题，我们再花十分钟的时间来回答问题，接着我们就进入这个agent的部分。
	然后我正好去倒点水。
	大家可以提一提问题，我们九点半的时候就开始这个agent。
	我看一下大家之前提的问题。
	对，切割不需要单元模型，这个大家有写。切割是不需要大语言模型的切割就是切割。对你这个切割也可以自己去看一下它的源码定义，实现的也很简单的。
	对，这个同学有个同学说切割就是简单的字符串处理，体感就像正则，甚至还不如正则。你说的是对的，就是我刚才有提到切割这个事儿，或者说转换分割只是它的一种形式，完全取决于你的业务。就是现在大家简单来说，我们在座的各位应该没有是搞基础大模型的这个训练和迭代的对吧？这个本身这样的公司和从业者都很少。那么在基础大模型一样的情况下，大家怎么样卷出差异度，我比你做的更好，其实就是在这些细节上，就是在这个data connection的各种细节上。就是我比如说我的loader要做的任务应该是保证我的内容原封不动的都下下来了。对于URL loader来说，我别把一些该加载的内容没了，或者说我要去实现一个更高级别的类似于爬虫的loader的时候，我还要去执行这个页面上面的JSA jx这些东西。我能正常的执行把我要的row data。
	其实我们看到最后这个事例叫road documents，这些原数据都拿回来。原数据拿回来之后，transform就是根据我们特定的需求来做的了。切割只是分割，只是最简单的一种方式。
	然后。正则匹配需要解决的问题是说，如果你的正则匹配写的不够好会漏掉东西。对，然后有同学问这个分割组件GL6B是想说那个质朴的GLM的6B是吧？这个可以的对，到这儿为止都跟大语言模型没相关的，对吧？然后你如果你想要去用他的那个6B的话，那其实最核心的是你切割之后，你的向量模型用的是什么，你用这个GRM的这个大家看到这个in bedding model里面，我们去这个integration，我印象当中GLM是有没有看一下。
	没有直接显示，但我印象当中GGLM的那个ebel ding是兼容性好像还行的，这个可能得其他同学查一查了。Mini max是有的，就是国内的一个初创公司。然后mini那个GLM因为本身它没有提供，我印象当中他没有提供公开的API，所以你需要私有化部署之后，去调用它的那个in beading。然后只要符合这个self posted的embeddable就好了。我印象当中可以通过这种方式去处理。
	卡了吗？应该，就是问的比较早的。
	有个同学问示例中的chroma存储的是数据是在本地磁盘吗？然后不需要单独部署，然后像常规的数据库一样，通过数据库链接访问吗？首先常规的数据库也不是非要通过链接访问它它也有这个OIM对吧？然后向量数据库，我们这没有做持久化，就这个事例没有做持久化。就我们现在给大家看的这个实例没有做持久化，也不在我们这张图里面对。就是说你要持久化，这个向量数据库肯定是要要持久化的，不然你这个重启不就没了吗？你这个应用一旦重启就没了吗？但是他不在我们今天的讲的这个主要内容里面，在我们实战里面用这个向量数据库去实战的时候，我就涉及到这部分内容。
	今天的讲课的重点是想把这个data connection，这个数据流跟大家捋清楚。就是我们这儿有一个数据流，这个数据流很关键，这个数据流是一个处理大语言模型应用的一个最佳实践的流程。欧式距离还是鱼弦相似度比较好，这个很难回答。这个就跟甜豆花和咸豆花一样的。这个其实看人就看你这个问题。对，然后理论上鱼弦相似度会在高维向量里好很多，因为欧式距离不太有这么大的通用性。
	有个同学一直在刷那个网页内容，什么loader爬下来很多什么JS正文包在里面，很难分辨出正文。对，因为那就只是一个UIL的loader，他也有request loader，也有一些其他的加载特定网页的loader。因为爬虫这件事儿本来就是一个针对特定网站平台去做的对，然后如果你只是想要把GS区分出来，那也可以做后处理，那是transform该干的事儿。对，这个text leader就是一种典型的transformer操作。
	那好，我看好像大部分的同学问的问题。有哪些好的开源中文私有化in bing模型可以试一下GLM的这个就是清华的这个GOM的这个开源的模型是可以的。因为我记得我今天刚刚看了一个排行榜，就是关于in bedding的。在开源模型里GOM的这个第二代还行，是排的很靠前的。比起这个闭源的来说，但是GPT4还是遥遥领先的一个状态。这个我找一找，回头发到群里面，就关于这个排行榜。
	行，那我们就到下一趴就讲agent。