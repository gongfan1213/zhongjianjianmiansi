	咱们就开始今天的课程，可能偏开发和应用实践多一些。然后我也留了一些时间，待会儿我们可以在这个直播的课堂当中大家提问，让我们及时的通过这个不管是通过这个prompt的方式，还是在我们的这个demo的项目里面，以及我们最终这个实战类的家庭作业怎么提交，然后怎么样去共同维护这个OpenAI quick star这个开源项目，我们都会在这节课做一个解答。好，所以今天我们的这个题目叫OpenAI的大模型开发与应用实践，主要分成三部分。
	第一部分是跟大家分享一下OpenAI的大模型，它有一个语言模型的一个总体性的介绍，然后我们把语音识别的和图像生成的放到后面的内容里去了，今天我们主要聚焦语言模型。然后就语言模型本身来说，我们之前在理论部分已经向大家介绍了，就是从GPT1到GPT3，一直到3.5GPT4。那整个OpenAI，其实现在在线上能够让大家使用的到的语言模型。除了我们上一周介绍的这个应该周三介绍的这个embedding以外，还有这四个我们今天可以一起看一看。然后还有一个比较重要的问题，就是这个token的计费和计算。就我们现在都在用这个OpenAI的API对吧？然后他每次调用到花钱，那他怎么去计费的？我们能不能在调用之前去大概算一下我们这次调用多少钱，这个还蛮关键的对吧？
	然后第二部分我会跟大家简单的介绍一下OKI的API。就是我们的刚刚上面提到的这些语言模型怎么用，对吧？因为这语言模型本身是一个大家开始通过理论已经掌握的一个有有大体认知的一个东西。但真正要把它用起来，其实是通过API调用这种方式。API其实是大家可以理解语言模型是一个最底下我们可以随时去去替换的。今天我用这个模型，明天我用那个模型。
	API统一了这个调用的接口，所以它的输入和输出的这个格式是一致的。所以我们掌握API主要是关注它的输入和输出。就我们调用它的时候要给它什么参数，要用什么样的调用的URL，什么样的路径，然后我们拿到返回结果的时候，要怎么样去解析它，然后从这个解析结果里面去获取到它的内容。
	然后我今天在我们的实战的这个notebook里面就加了很多的prompt。然后是希望大家把这个代码拉下来之后，能够看一看这些prompt到底都是怎么样去写的。然后我们在自己实战或者说自己在课后去手动去跑这些代码的时候，也能用类似的prompt去解决咱们的问题。因为我看群里有很多关于这个代码的问题，不管是这个怎么安装，然后调用的时候这个被限速了，或者说调用的时候有些东西拿不到等等。这一系列的问题其实都可以通过GPT帮我们去回答和解决。如果实在解决不了的，我们回头也在今天会发布一个在这个OpenAI的这个quick star的开源项目上。
	就我们这个课程项目上，我们一起去把这些难的问题就提交到我们的这个医学列表里。我们提问题，然后我们在那上面去做解答。然后我们觉得很有意思的问题，我们会把它加到项目的这个FAQ里面。其他的同学就能在那儿去持续的看到一些问题。
	包括我们从开课到今天，已经是第二周的第四次课上课了。这个过程当中有一些我们认为很典型的问题也会收集起来，放到我们的这个开源项目里面去。大家可以在里面通过文档的方式去看以往的问题OK。然后最后一部分是我们的文本内容的生成和我们做这个聊天对话的机器人。我们会有一些简单的demo，为什么叫初探呢？也就是因为今天更多的是向大家去介绍大模型和OpenAI的API结合的这么一种包包包装我们功能的方式。后面我们会有一个OpenAI的translator，会更系统的去跟大家讲怎么样用open I的API去做一个应用。所以今天我们更注重两个部分。
	第一个部分是我们的open I的大模型的语言模型部分到底有哪些？怎么去选，怎么样去算我们的成本，有一个大概的认知。第二个就是说这些API怎么用。然后通过这些prom的这些实战的代码，让他大家能够在这个已有的代码上去做各种各样的改造。这样我们就可以去根据我们各自的需求去玩我们这些API了。
	好，首先第一点就是说刚刚有提到OpenAI提供了很多的模型。我这里是筛选出来的这个语言模型。当然它还有别的这个语音识别的模型，生成这个图像的模型，暂时我们没有去把它放在我们这一节课里面。我们上一节课其实是有讲这个embedding的这个模型，就我们用的这个ADA的这个in bedding的模型。除了embeds ging以外，其实更多的语言模型就是我们这里能看到的GPT4、GPT3.5和GPT3。
	还有一个可能大家用的不多的这个moderation，它是做这个监管的。就是我们包括大家国内现在也知道，我们做AIGC现在是有一定监管的要求的。我们不能把这个GPT的生成内容直接原封不动的就甩给你的C端用户或者说互联网用户。因为这个生成内容如果没有监管的话，会带来大量的问题。Moderation就是一个已经被微调好的，专门用来检测有没有问题的一个模型。待会儿我们会细讲。GPT的这三个版本，我们在理论部分已经有过介绍了，这就不再赘述。我们继续去细看一下，在OpenAI的这个层面上，第三个模型有什么区分。
	首先第一个，我们介绍GPT4的模型。GGPT4是今年应该是三月底的时候，正式开放给这个一部分的用户去使用的那GPT4相相较于这个GPT3和3.5，它的常识性的知识就我们之前理论课学过，对吧？我们有去比较这些大语言模型的能力的时候，有一个常识性的考试。并且它占了九个benchmark里面当中的9分之5。所以常识是很重要的，它在一些广泛的常识和领域，专业知识这一块做的非常好，相对于GPT3.5有一个比较高的提升。能够去遵循一些比较复杂的指令，就是我们之前提到的他能去做这个COT，他能去做这个TOT。因为它本身具有这样的参数规模，以及它本身能够去通过它的训练去把这个步骤逐渐去做拆解，所以能够更好的解决一些困难的问题。
	但是相对来说也有一个点，就是我从我的了解来说，好像GPT4的API还没有对所有用户都提供权限，我不确定是不是，尤其是咱们没有去注册OpenAI，而是通过一些第三方的方式去获取这个APIK的话，就有可能拿不到这个GPT4的权限。所以我们的demo考虑到这一点，是以GPT3.5为主要的使用的模型。但是大家如果有这个权限，把它改成GPT4的模型就可以了，这倒不是什么难的事情。
	然后第二个就是说它的训练数据是最新的。但即使是这样，也都只到2021年的九月份，也就差不多两年之前的一个时间，然后他的这个版本也有一些不同。就我从这个梳理open I的大模型的过程当中，我能感觉得到大家可能觉得它的这个模型的代号，或者说这个模型类型、模型名称特别多。你看GPT4在这儿就有列出来了。这四个然后我相信之前用过它API的还会看到，还有这个03开头的。我们这里会有03开头，这是6月13号，也会有这个03开头，就三月份的版本。那么大体来看，它的命名规则也比较简单，这里简单跟大家介绍一下。
	第一个就是说GPT4，你可以理解它有一个标准版和一个长上下文版本。标准版就是支持8K的token，就我们8000个这个头，我也不知道token翻译成什么比较好啊，大家就理解成翻译成这个8000个token的这个长度的上下文是它能支持的一个上限了。这个上限其实在我们后面去讲这个对话补全，就这个chat completion的这个API的时候很重要。因为对话补全需要把历史的聊天记录给存下来发给他。如果上下文的窗口的宽度不够大的话，其实它不能解决很很复杂的聊天问题这个是一个限制。所以它推出了这个32K就是32000的这个版本OK所以这是是两个大的区分，就在这个token的长度上有一个区分，四倍的一个差距。然后这个标准的GPT4这个模型没有带任何后缀的，它是两周做一次迭代，如果大家买了这个GPT p这个ChatGPT的这个plus的服务订阅，大家能在那个聊天窗口，就OPI那个聊天窗口的最下面看见它有一个当前这个模型的版本的时间。那基本上这个GPT4默认的这个模型，它是两周一个迭代。
	然后GPT40613这个模型是遵循了OpenAI一贯的一个命名风格。大家如果使用这个模型的话，它通常是一个季度更新的快照。就是大家可以理解成每一个季度三个月的时间，可能它代号的这种模型会做一个更新，所以简单来说就是GPT4和这个GPT4 32K这个长窗口的版本都是两周一个迭代。然后带了这个日期的都是一个季度一个版本。
	它也有它的场景和好处，为什么要这么做？首先我们在使用一个基础模型去做我们的应用的时候，我们当然希望这个模型它足够稳定。它不要老发版本我们都不知道，而且它发完版本我们不知道要有什么新的特性，它的输出结果也许就不太稳定了。所以针对这种情况，它提供了一个三个月周期的一个长版本，针对这种我要尝鲜的，我要用它最新功能的，它给了两周一个迭代的版本。这个是他做这样的一个四个不同模型的一个意图。希望大家能够在使用的时候，根据自己的需求去做选择。
	第二个就是说现在GPT的这个API调用可能用的最多的这个GPT3.5。GPT3.5官方也给了一些建议。就比如说首先我们在这里有一个标红的就GPT3.5，它是专门为了对话去做过一些优化的。我们之前大家还记得理论课的时候，有一张OpenAI的从GPT3到3.5到4的1个迭代图，对吧？包括下面我还会再跟大家回顾一下。这个过程当中3.5是一个蛮重要的节点。并且ChatGPT的正式发布也是基于3.5的模型。
	这个模型，其实它的价格相对来说是一个比较折中的，然后官方也给了一个比较好的推荐，就是从ROI从性价比的角度是比较推荐这个GPT3.5 turbo这么一个模型。然后它为对话做了优化的同时，一些传统的文本生成任务，包括代码的生成，它也是能够去实现的，而不是说它只能做对话。然后他能以一个比较低的成本去生成一个还不错的结果。
	然后这里就会有一个我看群里有人在问的问题，就是我们上一节课有一个生成in bedding的一段代码，就把我们这个美食评论生成1536位的一个高向量。然后美食评论的文本没变，为什么生成出来的向量变了？这是因为本身这个语言模型就有一个很重要的调用参数，叫做温度值。你可以这么简单理解，这个温度值。这温度值其实它是用来去因为语言模型本身是一个概率模型，它在去做这个生成的时候，会有不同的多样性的选择。或者说你可以认为他他自己是想做一些有创意的效果。那他设置了这么一个temperature，这么一个纸，这个值的设定会决定它到底是遵循一个不变的结果，还是多给你一些多样性的结果，是这么一个设定。所以通过这个temperature的值的不同，它确实会有不一样的效果。
	对，好，那么还有一些这幅图里很重要的模型，就左边这一部分我相信就不重复介绍了。这个命名规范是完全一致的，它的标准版，它的这个加长版，以及它的两周正常迭代的版本和这个季度版本。右边这一部分其实是，我相信大家如果了解这个理论部分的同学还有一些印象，就是我们的这个达芬奇002和达芬奇这个003，以及这个code达芬奇0022的版本。他们其实也是属于这个GPT3.5家族的，那他们是有什么样的一些特点呢？首先这三个模型他们的训练时间比较早，它比我们的GPT4和左边看到的这个turbo模型，他们的训练数据其实是有一些不同的。大家能看到他们差不多要更早几个月，是更早一批的模型，然后没有加入一些特定的训练。
	那具体是什么训练呢？我们来回忆一下第二次课的内容。我们在第二次课的时候其实有给大家看过这幅图，我不知道还有多少人记得，就我们的这个GPT的一个训练迭代的一个发展史，或者说它的技术迭代改进的一些技术点。我们能看到从这个code达芬奇002，就我们的去年3月份到这个text达芬奇002，一直到我们的这个text达芬奇003。这个过程当中其实不断在加我们的黑科，对吧？就我们的黑魔法。
	这个扣达芬奇002是把这个代码训练，把github的这个代码库给加到我们的这个训练集里了，然后去做了训练，然后加了一些指令微调，变成了我们的这个text。然后后面加了基于人类的人类反馈的强化学习的训练，变成了我们的这个dex达芬奇003。然后这个模型又为了让人的这让人用起来更舒服，在这个GPT的返回结果上面去去挑选答案。我不知道大家还记得那个t igbt训练的这个三步法，就最后一步我们是为了把这个所有都是正确答案。但是人在听这个正确答案的不同描述方式的时候，有人更喜欢听的这种描述方式。最后这一步就是为了让这个回复对人类阅读更友好。最后变成了这个GPT3.5的turbo。
	所以从这个角度来看，在对应着刚刚上一页的那张图，大家应该就能了解这个GPT3.5家族大概是怎么一步一步过来的。然后这里面的不同模型也有它不同擅长的点。然后并不存在说你的一个任务在最新的模型上就一定好，尤其是你的任务是一个比较细分领域的任务，就还是需要你去做一些尝试。
	那怎么样尝试呢？从GPT3.5的turbo开始是一个比较不错的选择，因为他把很多黑魔法都加进来了。但即使如此，上上一节课我们有看到现在的GPT3点捂的这个ChatGPT用的这个GPT3.5，现在仍然使用的是这个tex w7002，没有用这个003，这是为什么？大家也可以再去研究研究玩一玩。这可能是OpenAI自己发现002的性价比更高，所以可以用更低的成本去服务更多的免费的ChatGPT用户。因为3.5是开放给所有用户免费使用的。Ok这个是我们GPT3.5的family以及它的一些使用参数和关键点，包括它的一些技术点。这些模型现在也都是能够开放给大家去调用的，我们待会儿通过model的API都能拿到这些模型，也能够去使用。
	这些模型。
	然后接着就是我们的这个GPT3对吧？GPT3其实很重要，但是它也很tRicky，就很很很微妙。为什么？首先我们知道OpenAI一直没有把微调的能力开放的非常的充分。所以我们目前所有的用户如果要去微调GPT的模型，还是只能在基于GPT3这个系列来做微调。连刚刚看到的T3.5，我们都是没有权限去做模型微调的当可能微软有这样的权限，那我们没有。那对于我们普通用户来说，唯一如果想要做微调，就只能基于GPT3的这个模型。
	但是不幸的一点是什么？就我们刚刚那幅图也有看到，包括这里GPT3的整个模型家族都打了这样的一个lexi的这么一个标签。我再回去给大家看看3.5，在3.5这里我们的GPT3.5的turbo是没有这样的标签的。但是这三个模型打上了这样的标签，大家能看到对吧？我的鼠标这里有晃。对，到了这个GPT3的模型，就全量都打上了这样的标签。
	那这个标签是什么意思呢？这个标签你可以理解成就是打上这个标签就意味着这一批模型是很快会下线的，他可能不会再长期支持了。我印象当中应该是应该是在明年的一月份，就2024年的1月20多号，这些模型应该都会下线了。我们后面有一页会专门去讲，所以这里对很多人来说是一个需要去权衡的点就我们很多人希望说能在GPT3上面去做微调。然后按照现在OpenAI的这个协议，咱们是在GPT3上面做完微调之后，我们拿不到这个模型。我们只能拿到这样一个模型的ID或者说这个句柄，或者说一个钩子。
	就相当于OpenAI开了一个餐馆，GPT3是唯一一个可以在上面做定制化做菜的。你希望说这份菜按照你的这个数据去做，但是做完之后，你每次只能去把这个做做的这个成品的这个菜你拿来吃。但是怎么去做的，以及这个的做法菜谱这些秘方啥的你都拿不到。所以你只能提供你的数据，你的原材料，把你的家乡特有的一些原材料，比如说这个食材他们都没有，只有咱们这儿有。但跟他一融合之后，在咱们这个小地域内就特别受欢迎。因为这个风味更适合本地人，对吧？
	但是你就没法去拿到到那个厨房和这个菜谱，你就只能说需要的时候你找他要，然后你每次给他付费，相当于可能你最后做了一个本地化的餐馆。然后这个餐馆的一道菜卖100块钱，其中有40块钱是。分给这个OpenAI的，你可能赚的是这个差价。然后你始终无法自己去做出这道菜，因为他没有把这个微调好的模型给到你那现在是open I是这样去服务的那从这个角度来说，大家就要去考虑了。
	因为从明年1月份开始，你微调好的这些模型你可能都用不了了。你花了很多自己的数据，花了钱，花了时间终于把这道菜调好啊，然后你这餐馆也开起来了。然后明年1月份的时候，会员跟你说好了，这道菜我们也不做了，你也别做了，那你这个餐馆还开不开了，所以我们也没有在这门课里面急着去讲这个模型微调。
	就模型微调我们可能会在生态片里面基于这个私有化的模型，比如说这个charge LM26B这样的一个国内的开源的这种模型，并且它的协议也比较友好，是可商用的那基于这样的大模型我们去做微调是一个比较合适的一个选择，所以GPT3的微调我们暂时不讲OK，然后这里还会有一个，大家的在使用GT3这个系列模型的时候，包括我们在做invading的时候，会看到有ABCD4个模型对吧？然后这四个模型从价格来看就是A是最便宜的，这个D是最贵的对吧？一路上来它的这个性能如果大家真的去调用过的话，会发现这个达芬奇的模型显然是比ADA要好很多的。然后我们刚刚从这个模型命名你也能看得出来，code达芬奇002对吧？Text达芬奇002，然后text达芬奇003。所以这个达芬奇跟GPT3.5基本上共用了很多东西，这个是我们从命名上就能看出来的。但是在实际过程当中，他也提供了其他三个选择。
	那那这个具体我们要用什么样的模型，因为这个跟我们的开销是直接相关的，跟成本是直接相关的。我们怎么去选？一种方式是你在你的具体问题上，你找一批样例数据直接去做测试。然后你看这四个模型，哪个模型能满足你，最坏情况就是这四个都不够，你得用3.53是不行的对吧？那比较好的情况可能是这个达芬奇或者说这个ADA就已经满足你了。那你就可以用非常低的成本去完成你的任务，这个是一个很很值得大家去测试的一个事儿，我们今天给的这个notebook，大家也能在上面去改模型，去做不同的尝试。
	然后这个训练数据他们就往回倒了两年了。大家会发现首先它的token数又更小了，GPT3的token这个窗口数只有两两K然后它的训练数据是还要再往前两年，就直到2019年3.5，我们知道是到了2021年对吧？所以它的数据也会有一些缺失。当然这些我们都可以通过。
	后面就如果我们发现ADA是可用的那这个成本可控。我们又希望说它的ADA模型本身的能力够了，但他需要补充一些外部的新鲜的互联网的数据。这个时候我们可以通过南茜去调用一些外部数据，或者说通过南茜去调用一些数据库里的数据。这样是一个极低成本的ROY极高的一种大模型应用的开发方案，这个也是OK的。
	好，除了我们刚刚说的这种方式，就是在你自己的数据上面去做测试，然后你自己去评估以外也有人去关心我们刚刚提到这个问题，当然OpenAI它自己是没有正式公布过这四个模型的，这个模型大小的据我所知，然后呃呃但是我们又特别想知道他跟他论文当中，因为GPT3是有论文的对吧？跟他论文当中的哪些模型是比较一致的？所以有人去做了这么一个测试和实验，在一些公开的benchmark上面，叫on the size of open OpenAI APM models，有这么一篇文章大家可以去有兴趣可以去关注一下。然后它的测试结果在一些公开测试集上面测出来结果，像这四个模型对应的这个模型尺寸，差不多是以上的数据。但是它还有一些其他的GPT3的小版本，大家能看到这个ADA和我们最终的这个达芬奇，其实差了非常远。一个是3.5亿，一个是1750亿，差了都不止两个数量级，对吧？这个是非常夸张的，所以它的性能有差距也是正常的。
	然后大家再回过头来想一想，现在我们用的这些国内在大模型，比如说我们这个质朴开源的这个CIGLM6B对吧？然后这个百川智能就王小川之前做了一个7B然后前两天这个lama开放了第二代，然后有这个7B有这个70B所以其实模型现在大家也没有说非要一定要搞到几千亿。因为通过我们在使用AI的过程当中，有人始终会觉得慢。这个慢也是有一定原因的。就是因为跑一次这个大模型，它就是要花很多的算力，尤其是GPT4，我们知道有好几个大模型，然后并行在跑跑完之后还要再对这个结果去做整合。我们讲了TOT之后，大家应该能理解这个逻辑，对吧？包括这个self instancy，就是这个多条路径去做推理，然后把结果做整合加权也好，所以它消耗的算力就是非常多。
	那有没有可能大模型轻一点，就我们不要搞几千亿，我们搞个几十亿或者大几十亿的参数就够我们用了。这个也是未来我们大模型要真正推广开来的一个很重要的点，因为大模型真的要用起来。你最好是能在手机上就能用，对吧？或者说你在家里面的笔记本上就能跑。那么大的，一个几千亿的，你肯定跑不了。那比如说一个几B的、6B的或者7B的模型，你一张的卡还是能跑的那这样是对未来我们的这个大模型推广是有非常好的一个积极的作用的。
	所以在这个过程当中，大家也可以去了解，试一试ADA和B和C这两个小模型。然后去感受一下我们的19年训练数据的一个结果。然后正好就可以去跟我们后面的ChatGLM1代和二代再去做对比，吧？我们在同样的模型参戍上面去做比较是比较有意义的然后这里有提这个时间是2024年的1月4号。我们的这个find tuning，就我们的这个GPT3的find tuning这些模型就会被回收了，就关掉了。大家就无法再用这个GPT3的反听力。
	但我相信OpenAI自己按照他们的这个说法，他们也在紧锣密鼓的把GPT3.5和GPT4的这个繁重的权限开放给大家。但是我能想象得到，应该不是什么太大的技术问题。技术问题肯定也有，这里也有很多成本的问题，包括我们开放之后要去反去用这个3.5和4，是不是会把他们的一些优势给逐渐抹抹杀掉，这些其实都是蛮多非技术的手段能够解决的问题。
	但fine unity本身的好处，我们是大家通过前面理论课应该都大概有所了解了。就它比起我们用这个instruction的tuning，或者说zero shot、one shot、few shot这个tuning还是有好处的。我们从两个角度来看，第一个就是说首先fine tuning它的学习，它会对我们的模型参数本身去做调整。
	我们而我们刚刚说的那些prompt的engineering的手段，它都是不会去修改我们的模型的。通过find tuning，我们可以把我们的模型天然就变得更适应于我们的特定领域任务。这个是它的第一个好处，并且也是比较直接大家能想到的那第二个也是跟我们的成本直接相关的。因为短期来看，OpenAI是不会让大家去私有化整个GPT模型的那从这个视角来看，每一次如果我们都使用这种fuel shot这个learning的手段来说的话，它会有很多。因为你每一次聊天，我们在做这个聊天应用？每一次都会带上这个内容。这些内容都是按照token要算钱的那你其实花销了很多不必要的成本。这是两个很重要的事儿。
	就是如果我们从积极一边来看的话，five tuning还是未来的一个在领域做特定应用用的时候很必要的一种手段。但是对于我们开发人员，就是我们在学习这个技术本身来说，我的建议是现在不用太花太多精力去搞这个fine tuning。除非你现在有公司有特定的任务，或者你有手上有很好的数据，那你可以去做这个事情。后面我们也会去讲，就是怎么样去造这个反应通灵的数据。
	通过GPT本身就能帮我们造这个数据，包括造给其他大模型用的训练数据，做这个的数据OK最后我们除了GPT3.5和4以外，其实为了让这个AIGC的应用更多的被政府监管机构所接受，OpenAI也推出了一个专门用来做模型监管，模型内容监管的一个模型叫moderation。这个模型就如它的这个描述所说，它也首先它也是两个版本，一个版本是一个相对稳定的版本，一个版本是快速迭代的，这个就跟大家做这个信息安全网络攻防一样，他就算训练了一个模型用来识别这里写到的这个包括一些偏仇恨威胁，一些赌辅读相关的内容。他他虽然一直在训练在监测这个内容，但是我们人的这个思路是非常开阔的，总是会把格局打开之后给一些奇奇怪怪的描述。就比如说今天刷刷屏的这个刀郎的歌，我不知道他有没有被刷屏，这个歌词就很有内涵，对吧？你把它交给这个moderation模型，让他去告诉这里面有没有什么问题。首先他肯定没问题，对于文化人这个就是不一样。所以这个很很模糊很灰色，但有些比较明确的这种不太好的这种内容，一定会被它识别出来。然后除了讲它识别这些类型以外，首先moderation是会把你生成的内容去打上标签并且做过滤的。
	就比如说我们这儿有这些不同的类别，还有一个比较好的点，就是这个moderation首先它是一个API，然后这个API如果你是用来做OpenAI的这些API的输入输出处理的话，它是不要要钱的，它是免费去使用的。但也是同时它不允许你拿去干别的事情，这是第一个点就它是免费可以用于牛棚AI的输入输出处理。然后第二点就是说如果你有一些很长的文本，这个也是我们上一上一节课花了一些时间跟大家讲这个invading。就这个embedding它不是可以无限长拿去做in bedding的，为什么有这个word to work？对吧？就是他先把一个单词这事儿整向量，然后表达的比较清楚了。后面有这个短语，有这个就一个短语。大家学英语的时候学过短语对吧？
	然后有这个句子，这些东西其实是越做越长的。但是即使是现在的这个GPT4或者GPT3.5，他也不可能是说你一个几千个token的词，这个一段内容，它把它分成这其中的一个类别，这也不太现实。所以他给的建议是说比较长的，比如说超过2000个这个character，超过2000个词的这种，你最好就把它切一切，切完之后丢给他去做判定，比较稳妥。
	然后接着就是大家可能会比较关心的一个问题，也是我之前使用的时候，在反复看的一个。就是我们这个计费就是你你用这么一个玩意儿你是要花钱的，那么有可能你一下就给它用光了。那我们之前有说过可以去设置那个使用上限，对吧？
	然后我们具体来看一下这些语言模型的价格差异。首先我们能看得到这幅图里面把我们刚刚讲到的语言模型基本上都抛出来了。除了GPT3，GPT3在这个位置有翻出你的价格，也有对有usage的价格。首先它的find的价格和这个usage的价格是不一样的，对吧？然后我们仔细来看一下，现在这里最便宜的是哪个呢？就是我们大家都跑过那个embedding n了，如果大家把那个限速的问题解决的话，1000次调用ebel ding model。这个ADAV2大家应该都调过了。那它是0.0001美金，1000个token花这么多钱，它的计费单位都是每1000个token OK那么这个是0.0013个零对吧？
	那么到了GPT3，这里是GPT3的这个usage。我们看右边这一列，我们的ADA的这个GPT3的模型的使用，其实是0.0016，就已经是这个ADA invading模型的16倍了。然后在24倍对吧？然后120倍1200倍我应该没有数错对吧？所以其实GT3的这个达芬奇的模型就已经是的模型的1200倍的价格了。
	这个是很夸张的，大家我不知道大家平时买东西，买网上买这些这种付费服务，或者说买一些电子设备倍的时候，什么时候出现过1000倍的价差，对吧？那这里其实这个调用就是很夸张的那为什么要把嵌入模型单独放到前面去讲？就是因为第一它真的成本很低，我们跑了那么多的这个文本用来做embedding生成之后其实也没花多少钱。大部分人的体感就是花了一毛钱两毛钱人民币。
	因为embedding作为一个很重要的技术手段，在一些大部分问题上面都能得到很好的产出，就比如说我们看到的做这个聚类，就我们把文本聚类成几类重要的这这个类型。有的是做负面的评论，有的是正面的评论，包括去做稳基于语义的搜索。就比如说我们说其中有不少评论是在讲狗粮，那我们通过这个dog food能搜出来这个狗粮相关的评论，这些都是一次性生成embedding之后，你基本上就不需要花更多的钱了。只需要把你每一次要搜的那个内容去做一次，对吧？那你要说的那个内容文本不长的，对吧？那更小。所以说这是一个极低的，在大多数场景就能work的一个方案。
	大家一定要把这个in bedding作为一个蛮重要的一个技术手段。并且我们后面像这种特定领域知识库上面去做这个销售问答之类的场景，其实也都是在使用embedding大语言模型。尽可能我们把它作为这个困难问题的时候才会引入的一种解决问题的手段。
	第一它贵，第二它慢。这个是一个价格的比较。我们的达芬奇首当其冲的成为了最贵的选手，这个0.12非常贵，这个价格已经到什么地步了呢？我们看左边这个GPT3.5和GPT4，GPT3.5和GPT4最贵的价格，其实也就是我们的这个GPT4的长窗口的版本0.12。
	所以从这个视角来看，我们又回过头来看GPT3.5的标准版。GPT3.5的这个标准版的，你看它的这个4K就是我们的四千多个token长度的上下文，它的价格是0.0015，这边是0.0001对吧？15倍的一个价格，相对于北京相对来说还能接受。而如果我们是这个output的部分因为我们在使用这个对话模型时候，你的你丢给他的信息和他回给你的信息，这两个加起来算钱的，只不过它的单价可能会略微不同，但这个也很很好理解对吧？它返回给你的内容是生成的，所以它收的稍微贵一点。
	好，接着我们再看一下，怎么样我们知道单价了，单价是按1000个token来计费的那我们怎么知道有多少个token对吧？下面是一段描述，来自于OKI官方提供的一个网页小工具。这个小工具我们直接切到下一页叫token lizer。这个token ized里面有一个示例，我把它专门考进来了。首先这个左边和右边是来自于这个网页的截图，这个网页的地址就在右下角，我们可以大家在课件里面到时候自己去访问一下。就这个OpenAI的这个官方网站的这个platform下面有一个token ized的小工具。然后这个小工具，我们能看到它能帮你去算你输入的这个内容它有多少个token。就比如说这一段英文，它翻译成中文是这个意思。
	就是说我们每个模型都有不同的功能和价格，然后价格是按照每1000个token计算的，你可以将token作为这个单词的一个组成部分，所以这里很重要，就一个token不代表一个单词，当然如果能是那样最好。其中1000个token大约相当于750个单词。这是因为大家会发现有一些不是单这种标点符号、空格、换行符，也会被当做token。因为它需要被表达，就有空格和没空格，有没有标点符号是不一样的，对吧？所以它需要被记下来，有一些特定的token ID，那为什么被记下来记成什么东西了呢？
	大家就可以简单理解成这里不同的这个token会具有不同的token ID我们刚刚讲的这些标点符号也就会被记录成不同的ID有一些相同的ID就说明它们是相同的这个词，相同的token，那不一定是这个单词，有相同的一个token被提取出来的那这个是一个很有意思的点。这段话是来自于刚刚那个报价单里的。大家如果访问那个OKI的pressing那个网页的话，就有这么一段话。他自己会说这个是包含35个token，也是可验证的，然后大概是这么个意思，大家大概能理解的对吧？但这有就有一个问题，就是说你不能我现在写一个代码，然后我要去做应用了。我每次都把我的这个内容跑到这儿来去复制一下，然后自己再盘算一下，说要不要再优化一下我的这个输入或者什么的，这个不太方便是吧？那其实有这个写代码的方式去算的。我们在在上一节课的时候，其实已经用到这个python包了，叫tik token，它是一个快速用来计算这个BPE的一个分词器，所以BPE就是bad pair encoding，就是字节对的编码，我应该这么执意是没错的。
	用来干嘛的呢？它是用来跟OpenAI的这个模型API能够紧密协作的一个python包。这个python包可以使用一些特定的编码方式，然后把你的内容分成不同的分词，你可以分成不同的token。这个token那就简单了，它分完之后你就算有多少个算它的长度就知道它大概有多少个token了。
	待会儿我们的今天的实战demo里面还专门有一个notebook让大家去试，然后去比较不同的大语言模型。它的这个token ize算出来之后，不同的数量，然后他的这个包也做了一些性能上的对比。因为这个事情很直接，你可以想象得到，如果我们真正要去开发一个大语言模型的应用，那一定会去算这个tiktok的。它有两个角度要去计算，第一个就是我们上一次in bedding，如果太长了，超过我这个要调用的模型API的max token，就是这个或者超过这个context，那根本就掉了，就会报错，对吧？所以你可以用它来做过滤，这个是第一个点。如果太长了，你要么就是放弃它，要么就是把它切成几块，切成几段，然后再重新切成几段，分开去调用。这个是第一个视角，就是为了我们这个应用能正常的运行，你需要去做这个token的计算。
	第二个就是说如果你对成本考虑的比较多，比较敏感。然后你的每一次调用你希望能算出一个ROY的值，那你通过它也能够设置一个阈值。如果这次调用你发现是很很奇怪的，正常你可能一次问询，就是在这个小几百个token突然来了一个问了四千多个。那这个就虽然没有打爆那个context，但是可能你就会去考虑这种东西是不是先这种query是不是我就先不问了，我先自己做一个分析，存个档，然后告诉用户超长了或者别的一些原因。所以它作为我们很多的AIGC的应用的前置环节，它是会被反复调用的。所以它的性能也就很关键，也就会来做这个bench mark。这里是跟这个hugin face去做了这个性能的对比，那能看得出来它的速度是比较快的，差不多有3到6倍，跟一些开源的token niza去做对比，当然这个是他自己提供的数据，我没有去实测。大家如果有不同的一些最新的benchmark测试结果，也欢迎提到我们的在项目里面来让大家看到最新的，也许有比他做的还好的。
	好，刚刚其实我们把大模型语言模型有什么主要的几个版本，我们的43.53，包括我们做监管的moderation，以及我们的token怎么样去计费，然后怎么样去计数。然后我们的计有网页的小工具，大家用ChatGPT的时候就能够去用。因为我们用ChatGPT的时候，他有时候也会告诉你这个书的内容太多了，那多少算多，对吧？那我能不能把我的直接使用ChatGPT的时候这个token拆一拆，这个也是有网页小工具可以支持的那我们写代码的话，有这个tik token，我们今天也会去实际去用。这两个其实就把我们的怎么样用语言模型，然后它有什么样的功能，有什么样的版本，然后在用的时候你应该选择哪一个具体的模型做了一个分享。
	接下来我们再看一看写代码的层面上，我们要怎么样去通过open API来帮助我们去完成很多的工作。具体来看，我们会有三个大的部分。一个叫model s的API，一个叫做completion的API，就我们的文本补全的API，还有一个就是我们的这个chat completion的API，也是现在用的最多的。我看统计数据UPI给的是97%的OpenAI API调用，我现在都是通过chat completion这么一个API，那么首先要用这些模型，那首先得能拿得到这些模型对吧？
	那第一个就是这个list models，这个API的名称相信也很直观通做这么一个简单的调用。OpenAI这个python库的调用，就通过OpenAI的下面的list这个方法，它其实实际上会去大家看起来好像是在访问一个list里面的一个内容，但其实会去调用一下这个OpenAI，返回它现在能够给你提供的所有模型的这个列表OK然后第二个就是说这个列表里面有什么内容，我们可以通过这个呃，retried, 或者说直接展开刚刚的列表来看retry ve你可以变成获取一个具体的模型。然后，这个模型里面有一些相关的信息，然后你还能拿到它的一些句柄等等。那这里我们就不再展开了，这两个很简单。但是我们刚才有提到这个更新换代的事儿，就是1月4号，OPI自己有一个模型启动的计划是1月4号。那这1月4号你的模型就会发现，原来我一直开发的某一个基础模型。
	就我们刚刚提到了，你知道你在调哪个model API了，然后你也知道这个model ID是什么了。但是1月4号的时候，可能这些模型就不会再支持了，那你需要去做一个替换。他给了一些替换的意见。首先我们能看到大量的之前的老一点的3.5系列的这些小模型，都会被这个3.5 turbo的instruct指令微调的模型去做替代。然后之前的那几个GPT3的模型也会有002的版本。然后这些做这个invading的模型，之前有很多都会被统一的替换成我们上一节课用的这个text 0ADA002。所以不管是基础的语言模型还是说嵌入模型，在1月4号的时候，就明年这个元旦之后就会去做这个迁移，所以大家在这个开发的过程当中，也可以去做一调整。如果你之前已经有一些应用是基于它的某个基础模型了，可以看一看是不是在这个列表里面适当的去做一下迁移，甚至对比一下结果。
	好，那么completions API是干嘛的呢？它是一个文本补全的API，这个比较直接，你可以简单理解成什么呢？就是我们一直在讲大语言模型是一个生成模型。然后这个生成模型就是你给一个prompt，它生成一个结果。Competition API是最早开放的一个API，它应该是在我印象中，在2020年还是2021年初的时候，OKI就已经把这个API开放出来了。所以是非常早的，大家可以理解成就。GPT3出来的时候，3.5都还没有都还没有去出那堆魔改的各种什么code达芬奇002，包括这个text达芬奇002，text达芬奇003。这都还没有的时候，就我们GPT3这个论文刚发出来没多久，这个API就有了，所以这是一个非常老的API了。
	所以如果像海外一些AI的创IGC的创业公司，大部分好多都在用这个。然后也有一些IG的创业公司被这个GPT4直接就给干没了。就是大家最近看到什么AIGC的创业公司已经开始裁员了。有很多原因就在于那一帮海外的公司做的比较早。所以他们有很多是基于这种早期的GPT的模型上面去做了大量的工作。这也是为什么朱小虎和福生他们在那argue说现在的这个基础模型的创新很难。然后在基础模型上面去做应用的价值很薄，很难找到护城河的原因。
	现在我们到今天这个视角来看，completions API虽然是一个很老的API，并且也很快会也会下线了，他也会在明年下线。但是它帮助我们理解这个API和怎么样去用这个OKI的API是一个蛮好的一个帮助。并且它的下一代就是这种对话补全的API也沿用了它的很多输入参数。
	所以我们先从他讲起，他其实就是给一个数，就是你可以理解成它就是一个机器人，然后你告诉他一个事儿，你告诉他一个prompt，它还你一个结果。当然他也可以给你多个结果，然后附上概率，是这么一个API就做这么一件事儿。不能支持这个聊天记录的，这个就是把你之前聊的记下来，它不支持这个。你如果想让他支持这个，你得把之前的聊天记录你自己换种方式秒描述，然后变成下一个问题的这个输入。它不存在连续的提问，就从它的设计之初来说的话，OK然后比较关键的输入参数就在这儿，我们能看得到，像这个model和这个prompt的这俩是一定要给的。这个输入参数这边有明确标出来，是必要的输入参数。还有一些其他的参数，我们可以待会儿在实战的时候再去细讲。
	像这个max token，就我一共要生成的这个token，这个数量可以有这个上限。然后除了这些以外，他还给了一些其他的这些参数，那这些参数我们就不在课上这个直播课上去浪费时间给大家一个一个去做这个分享了。大家可以到时候把这个GPT的输入参数的文档甩给GPT4或者GP3.5，让他给你去做讲解。甚至可以让他给你去生成这个示例的代码，告诉你这些值取值的不同，变成一个的请求。然后你自己可以去请求一下，看看它到底有什么不同，好吧。然后他有一个很小小技巧，这边可以提一嘴，就是我们在自己做这个prom engineering的时候也可以用这个类似的技巧。
	核心的意思就是说我们正常的时候，我们给这个prom的早期，就像20年刚出来的时候，我们它是一个从左到右生成的一个自回归的模型，然后生成了一个文本序列。所以那会儿大他想法是说我说上半句你说下半句，那这个时候他会生成一个结果给你。但是有没有可能我说上半句，我说下半句，你告诉我中间那半句对吧？那其实这个inserting text就这么一个思想，大概能理解对吧？就给更多的上下文，让它生成的质量更高。然后为了实现这个inserting text，他还给了一些最佳实践的建议。比如说尽可能多给一些上下文信息，把这个max token不要弄太小，就要大于二百多，甚至可以让它多生成几个，然后你在当中选多生成几个在里面选。跟我们之前说的这个self consistency，这个自洽性是不是就结合起来了，这是一个逻辑。
	然后是这个chat completion API，就我们的对话补全的API。这个对话补全的API或者叫聊天补全的API，跟刚刚那个很早期的版本一个最大的不同，我印象中这个API应该是今年才出来的，但当然如果我时间说错了，可以下来指正我一下。具体时间我不太清楚了。
	这个API很重要的一点是说，首先我们的ChatGPT就是基于这个API去做的，就大家在用的这个ChatGPT是基于这个在做的。然后第二个就是说它有一个巨大的好处是我们一直人聊天，不会是我说一句你回我一句，我们聊天结束了，那这不就这个聊天终结者了，对吧？那就没啥可聊的了。实际情况是说我们俩反复在沟通，然后我们在沟通的时候，我们会一直深入的去探讨一些问题，或者说连沿着一个问题越聊越多，或要么就往深了聊，要么就往广了聊，对吧？反正都在这个话题上面聊，那他就需要去记录你前面说的内容。
	这里就有一个很重要的点，就是我们通过这个chat completions，它是可以让你记住上下文的。但是这个记住上下文也不是说它自带的，因为我们人会出现一个什么问题，就是换个话题对吧？但是我换个话题我还可以换回来，就我们正在聊这个chat confliction API，然后我突然聊到什么一个创业项目了，然后他就跟我们现在聊的上下文无关。但是我可能聊完那个上下文，聊完这个创业项目之后，他结束了，我又回来接着聊这个。所以他不可能帮你直接去记录你之前聊了什么。这个既有我刚刚说的那个现实问题，同时对于OpenAI家公司来说，他去帮你寄，他会花更多的钱，他也不知道应该寄什么。所以对他自己来说是造成了不必要的麻烦。
	所以他提供的方式是你告诉我之前我们聊了什么。就是你调用我API的这个客户端，你来告诉我之前我们聊了什么。那这个时候，就变成我的这个调用方，或者说我这个我其实是在大家理解，我其实是想做一个AIGC的应用。然后底层是调了这个OpenAI的大模型的能力。那我自己也会对我的用户去提供各种功能，那我这个开发AIGC应用的人就要来维护到底我们在聊什么？
	我们的上下文是什么？那通过什么来维护呢？就有一个比较重要的参数叫做message，就是我们的这个消息列表。在这个消息列表里面，你可以去传这个历史信息，就是我们的之前的一些对话的内容。然后这个时候你就可以自己去宰，因为你自己可以判断你的用户正在跟你聊那个话题。如果切了话题，那这一部分的上下文就可以不用了，就不需要再加载进去了。也许他直接就开了一个新的话题，那你就可以省下一大堆的token对吧？所以这里就给了AIGC的应用开发者很多的自由度。
	但也是给了这个自由度就能看出水平了，对吧？有的人可以把同样的一个功能做的成本很低，因为他知道在什么样阶段不必要再给这些message，甚至他通过不给这些也让问题变得更干净。那我们的我们的这个大模型他也就清楚你现在要问的问题是只给的就是这个新问题，跟老问题无关。只要你不给这个message，这是一个很重要的一个点，也是我们在实际使用这个chat completion的时候一个需要注意的一个很重要的点。
	第二个就是说，那好，那你要记这个聊天的记录，那我们聊天记录是两个人在聊，对吧？就是有的就是相当于我这个用户，我是给大模型提问的人我希望大模型能跟我交流，那大模型也会输出一些内容对吧？那身份不一样就需要记得不一样，所以它会有一个身份的参数叫这个肉现在是支持这个function call出来之前只支持三个角色，就是这个system user和这个assistant。大家简单理解成user就是用户，就是我们这些用大模型的人。Assistant就是GPT，大家可以看到这个颜色不一样，右边有一个图，就GPT输出的内容。
	然后system是什么呢？System是你可以简单理解成就是我们希望告诉这个assistant有一个上帝视角。上帝视角是这个系统外的一个角色，他能够告诉assistant你现在主要工作是做什么，然后这个就不是assistant的输出的内容，对吧？其实是向一个上帝视角的人输出的这个内容。所以这里就衍生出一个点，就我们在用这个g ChatGPT的时候应该怎么用。其实你会发现这个参数，那你直接在你自己去用它，去用ChatGPT的时候，你的prompt也就可以按这样的一个格式去做整理。
	像我现在很多的prom，我都会带上这个肉，然后一个冒号，然后写要是一个什么样的角色。然后最好你还能给他一些不同的分隔符，或者说提示词，告诉他这是你的肉，你得记住了。然后接下来你就可以只丢这个内容，你都不需要反复的去把这个肉的这个。系带上，并且实测这个是可用的，所以这些都是一些prompt的技巧。大家待会儿后面今天留了1个小时的时间，大家都可以去来交流这些东西，也可以在线上提问。
	到时候最后再来做一下对比，就是我们有这个completion，有chat completion。为什么要把那个completion干了？是因为这俩其实是等价的，只不过chat completion更强，他把这个completion给包含了，并且更便宜。
	大家用过这个open API的话会发现completion是不支持调用GPT3.5，它只能用GPT3这个时代的模型。3.5的接口它都调不了，更别说调GPT4了。它有一个URL，大家还记得的话对吧？那么你要调这个3.5及其以后的模型，你都要用chat completion。这也是open I很很讨巧的一种设计对吧？你自然而然的新用户自然用新模型的新用户就不会用老接口。那你那些从20年、21年、22年开发的代码就都需要去做迁移。
	这里这一页是想跟大家讲一个简单的等价。就是说我们用这个翻译，就是这个机器翻译。我们整节课这四次课都有讲到机器翻译这么一件工作交给我们的大模型去做的时候，我们要怎么给他写这个prompt。如果我们用的是最原始的completion，可能我们会用上面这种方式。左边是这个指示词，就我们要让他干什么。右边可能是指我们最后会输进去的这个实际的内容，这个是一个用completion的很典型的一种手段。
	然后下面是如果我们用chat completion的这个模式，首先role是user对吧？就是我们用户user，然后内容就是我们要告诉他的内容。然后甚至如果我们要做的好的话，甚至可以把这个roll加一个system。然后告诉他你是一个翻译专家，你的主要工作是翻译英文到法语，对吧？那这个时候这个肉就只需要输text了，那是不是就不需要每次都去做这个传递了，甚至在ChatGPT使用的时候，是不是就不需要每次去用传递了。这些都是一些我们了解它背后机制的时候，不仅是做AIGC的开发，包括我们直接去使用ChatGPT的时候，对于prom的都会有更深的理解。
	OK现在是1个小时的时间，我们休息五分钟，大家来提问题。对，还有什么问题？然后我没吃晚饭，我就吃个桃子。
	来来来，看看大家有什么问题。
	对我觉得我有点低血糖了，实在不好意思。
	大家有什么问题吗？
	我们看看。
	有个同学问为什么不用assistant？Assistant是GPT4返回给你的结果。所以你要是把你的这个翻译指令甩给assistant，其实是不好的。你甩给user或者sist system都好，但是甩给助手的输出是不好的。
	有个同学一直在问英语不好，GPT能学会吗？我不知道是说GPT的英语不好，还是他英语不好，但我觉得首先GPT的英语应该挺好的，然后如果咱们自己的英语不好的话，GPT支持中文，所以咱们就用中文就好了。
	请求中max token的设定会涉及调用费用吗？我理解一下这个问题。首先max token是去说我们要生成的内容有一个上限，你可以简单理解成它就是控制费用的，或者说他就是控制内容的长度的。所以你设定了之后，它一定不会超过那个内容，它就不会超过那个token的这个上限了，那那自然就跟费用直接相关了。
	有的同学问这个肉要自己训练吗？肉是不训练的，肉其实是属于我们给GPT的输入。所以应该在第二节课的时候讲这个prompt learning的时候就已提过了。其实我们给的所有的消息列表都是给到GPT作为它的提示词的那这个时候我们通过不同的肉，其实是为了让GPT理解清楚之前聊过什么，他的任务是什么，你现在让他做什么。
	Chat completion这种road的设定本质上是拼了一个prompt，把之前的信息写给到GPT。是的，就是这样实现的。包括我们用ChatGPT的时候，其实我们的这个网页就ChatGPT这个网页上面，它也是不断的把之前的信息又存下来。但是我不知道大家有没有去用过它的这个结果的编辑功能。就是我们有一个删除笔记的网页，这个网页生成出来内容你不满意，然后它有一个按钮叫重新生成。也有一个地方就是你可以去把你的输入的prompt直接有一个编辑的按钮，然后你可以把它重新编辑一下再输入。
	这个时候经常会触碰到一个bug，就是他忘记了上下文。我理解可能是ONI在实现这个内容的时候，它如果你要编辑了这个prompt，然后重新输入的时候，它对于你直接很流畅的往下去跟他连续对话，它传递的这个message是不一样的。所以在这种情况下特别容易出错，这个是我自己的一个切身的一个体会。但是拿不到这个开发者日志，我们也没法证实这个事情。
	然后第二个同学在第二个问题，completion接口什么时候下？好像繁重的model只能用completion，我印象中也是这样的，就是我们刚刚有看到在那个价格那一页，我们回到那一页，稍等鼠标。在这一页的时候，其实我们有看到这个GPT3.5和GPT4都是写的这个input output去算这个不同的单价。就我们的这个token，我们的这个find tuning这个model，其实是有training和usage的。因为GPT3的这四个模型是支持fine tuning的，它也支持fine tuning的模型去使用。那确实像3.5和4还不能去支持翻译to，然后我们的GPT3的model是通过这个completion去调用的，所以这个结论是正确的。
	第三个问题，肉可不可以在普通的对话框里去做设置？我不理解这个普通对话框是不是是是指什么？如果是指的这个ChatGPT的这个网页端的话，那这里我印象当中好像是不行的。但是你可以通过你的prompt直接去写，就是你在你的prompt里面写一个roll，它是有它是能理解的。只是说它可能不一定有直接植入的那么强，但是它是能理解的。对，英语不好编程都能学会。是的，我想说英语不好不是一个问题。英语不好是一个顺手应该要去解决的一个事儿，但它不应该成为你学很多东西的障碍。
	第四个问题是有什么好的办法能够识别到用户在同一个station当中切换话题了吗？比较难，就是比较直接拿那个有一个一招鲜的办法做到这一点。如果可以的话，就ChatGPT就很轻松了，它可以很低的成本去去解决这些问题。但实际情况是，要实现这个的话，它肯定不是根据你这最新的一个问题，而是根据上下文。然后上下文可能还需要去做这个语义提取主题理解，然后提取出一个主题来，然后主题不断的再去做比较。比如说它可以基于时间窗口，一个固定的时间窗口就对应着一个固定的context的窗口。然后基于这个滑动窗口可能可以去看你的话题的最终提取出来这个话题的相似度是不是有大的迁移。
	这里需要做很多工程上的设计了，并且跟你具体的应用场景有直接关系。有不同的应应用场景，它的这个query的长度也不一样，回复的长度也不一样。所以这是一个case by case的问题。但是大的思路是有的，基于时间窗口，包括最新的这个问题，然后去做这个相似度的比较等等。
	每次发第五个问题是每次发送content，就每次发送这个message里面content是不是都会消耗一个token？每次调用都会消耗，都会都会计费。这个应该上节课有给大家看过open I那个计费的页面，他不是说一次调用一个token，他是说一次调用根据你的内容的数量算出来有多少token。
	看来这个计费确实是一个问题，很多同学没理解到，就是你调用了API，然后你调用它的时候，你会带上你的请求，你的请求里面是有内容的。比如说你的内容是现在这一段话，那这一段话是消耗了多少个token呢？是35个token。
	那为什么是35个token呢？因为它分词出来就是35个，然后你怎么知道它是35个呢？有两种方法，一种方法是网页端的，你能在这儿直接看。一种方法是我们待会儿会讲的，就是用python的这个软件包。这个token它是能帮你去算的。也不是说同样一个内容，它出来的token数都是一样的，它跟你用的模型有关，跟你编码方式有关系。好，我们差不多十分钟的时间回答了大家一些问题，我们接着再进入到我们的这个实战的环节，然后大家可以再多提提问题。
	这里我们再讲一讲这个课程的项目，这项目我们上一节课发布下来非常好，大家能看到有一些变化。第一个就是说我们有76个star和53个folk了，大家还是有很多很积极的同学的，也欢迎。然后也有十多个pull request，我合并了一些pull request。所以大家能看到这里会有一些同学已经是项目的贡献者了。然后也期待大家做更多的贡献。就在这个项目里面让更多的同学都能学到不一样的知识的这个切就切入的视角不一样，大家能看到的这个重点也不一样，但是能够分享出来。
	然后还这一点就是我会在最快明天会把之前我们的理论课一的homework也传上来。然后理论课二我看到有分同学在问，有没有类似的这种选择题形式的作业，那我也会做一份，然后放到这里来。这个会是一部分新增加的内容，回头大家能在这个项目上直接看到。
	然后第二个就是说我看到群里面有很多同学在提问题，提各种各样的问题。有些问题是以前提过的那我们会做一个FAQ，然后我看已经有一个PR提了FAQ，是把7月12号的一些问题做了一个汇总。那我会在明天新建一个怎么样提FAQ的一个格式。然后大家就可以把一些常见问题提过来。我们做一个大模型应用开发100问对吧？这样很多新来的同学，都能够通过这些100问快速去找到自己的问这个问题的答案。
	然后第三个就是说我们会有很多同学还在问这个prompt应该怎么用？各种各样的prompt。第一我们会在接下来的实战的这个代码里面，就会新增加很多的这个prompt的一个介绍，怎么样去实际用。甚至在我们做这个demo的过程当中就遇到了一些问题。我也会用这个GPT4去帮我解决这些问题。然后这个解决问题的思路希望更多的是分享给大家，让大家在改这个notebook这个示例代码的时候，能够自己再去做一些延展，这个是最重要的，要授人以渔对吧？这个我们在理论课的时候也讲过了，然后这个是一个点。
	第二就是说我会把一些很典型的prompt，也会把它做成这个分享的文档，放到这里面来，我们做一个prompt 100个分享对吧？让大家在各种各样的这个问题场景里面，都能够通过一些特定的prompt来解决问题。做一个proof的cookbook。好，也希望有更多的同学，因为我们这个课应该是有一千多个同学，也有不同的同学都能够在这个github上面拉拉下来最新的代码，然后去看我看看这个里面的prompt的分享，看看我们的家庭作业。然后一些实际的代码自己再改一改，根据自己的需求。因为后面我们的这个更更完整的这个实战的这种项目，也会在这个github上面对。然后在这个OpenAI API的这两周的课程里面，我们都会在这个目录下面去增加我们的这个示例代码。
	就比如说上一节课的这个in bedding和这一节课的这个models，和这个count tokens with tick token这两个新的notebook，其实我在直播前我就把它提交上去了。所以大家如果现在拉那个代码的话，就可以看到我正在待会儿要去给大家演示的那个版本。但是我们待会儿执行完之后，那个notebook就会有这个执行。它每一个叫sale这个单元它的执行的时间不一样。它前面会有一个数字，是指在这个kernel运行状态下，我是第几次执行的这个sale，它的这个时序。所以有的同学会问说我的同样的内容怎么sale跟前面的数字不一样？所以那个不太重要，对，大家可以忽略掉。对。
	然后大家在提交这个代码，提交这个pull request的过程当中，也需要注意一下。就是说第一提的时候看一看前面有没有人提过类似的东西了。就比如说我们给这个read me做中文版对吧，可能就有两三个类似的。然后这种大家可以要注意一下，就尽可能我们去不做同样的这个内容的提交。把一些不同的，包括我们说这个prompt的分析，讲问题的集锦，我们都会做上来。这个大家看到不同的可以贡献上来，我们一起为这个代码仓库做贡献，大家都能够去提好对好，我们接下来看这个实战的这个。浏览器大家。
	能看见吗？
	先把这个调大一点。看一下大家能看见这个，大家能看见这个大小吗？应该能看见对吧？我们把这个尺寸做的现在这个大小OK吗？会不会太小？行，那我们就直接开始。对，刚刚提到的这个FAQ，我们再到这个项目里面跟大家实际看一下。
	我们的两位助教的老师，后面也会在这个pull request里面会去做这个意见的修改。比如说像这个PR就这个request大家提交之后，我会去做这个评论，然后大家需要关注一下，我不知道这个github可能大家用的平时没有那么多，但如果用的多的话，会知道他会给你提醒的。这边会有一些这个消息提醒，然后也会给你发邮件。
	像我可能同时还在维护一些其他的项目，这边就会有一些其他的项目的一些内容。比如说就这个项目来说的话，如果看到了这个新的提示，大家可以来回复一下。就比如说我们这个同学加了很好的一个功能，就是给我们的上一节课的in bedding加了一个radio的这么一个GOI的一个界面。如果把这两个小的我们一起确认了没有问题，我们就可以把它合进来。
	其他的也是类似的，大家如果有一些比较明确的共性的问题，包括这个代码方面的也可以在这提。在这儿提完之后，我会来看。然后如果我我认为这个问题确实我们需要去解决，我们就写代码把它解决掉。或者说我们有一些其他的同学动手能力也很强，就把这些优秀解决掉了。我们一起可以让这个代码仓库变得越来越健壮，越来越友好，新同学都能够学会。好，我们回到今天的这个实战的demo里面来。首先今天会有两个notebook的这个demo要跟大家去做演示，第一个是。
	是有个什么谁啊谁把什么意思？没事，我刚看到这个大家提的问题，以为是有什么问题。我们这两个就对应着我们刚刚讲的这个PPT里面的一些内容。第一个就是我们models的API，我们会去讲这个把它收起来model API。第二个就是我们的这个怎拿的，然后把它搜一搜太。
	拿到了这个model之后，我们怎么样去做文本的生成，以及做我们的这个聊天机器人的这种多轮对话。这个是我们的第一个我们要实战的这个notebook。第二个是我们怎么样去用tiktok这么一个库来帮助我们去做这个计算。就我们去算一下我们现在这次调用应该是多少个token，然后帮助我们去做判断，就接下来要不要去切分或者说抛弃这一个询问这个文本，或者说再去做一些别的操作，好吧？
	好，那我们先看这个model的API。Model的API我们可以看到，它的这个调用其实比较简单。就是这里看到的这一行就通过这个OpenAI这个库引入之后，我们会在环境变量里面通过这个OpenAI的API key把它配置好。
	这个我再稍微唠叨一下，我怕有同学不知道，朋友在问。在这里我们有写的很清楚，我们可以跳到中文版本的文档，在使用之前一定要在你自己的这个环境变量的文件里面，看你用的什么样的shell。如果你是用的这个linux或者unix的话，如果你用的shell，用的batch，用的ZSH，用的不同的这个shell，它会有对应的用来配置环境变量的文件。然后你要把你的APIT放进去，放进去之后你也可以直接在终端里面导出，那就会变成环境变量了。然后如果你是windows的话，也需要通过这个命令提示符或者说这个，我印象当中我有十多年没用过windows了。这个应该是在13年最后用过的时候，它是有一个专门的环境变量的一个配置窗口，可以去配置这个OpenAI的key的。把你的这个key配进去，那你就可以去用这个OpenAI的API调用它。好，如果你配置好了之后，其实我们实际来运行一下，我们重启一下，让大家把这个数字回到这个开始。
	好，大家看到这个重启之后，整个notebook里面的内存就相当于全部清掉了。我们就从头开始运行整个项目很有意思。我们拿到了这个API的调用方法是通过OpenAI点model点list。
	很多同学其实有看到刚刚我们的这个API reference里面的一些内容，我们这里再简单讲一讲。就是我相信有些不是python的，用比较熟练的同学可能会有这个疑问。我们在OpenAI的官方文档这个API reference里面能看到，它这其实是提供了一个简单的一个示例代码，并且这个示例代码也是能调的对吧？你能通过这个CURL相当于直接去访问它，通过这个HTTP的接口，然后带上你的这个OpenAI的key，在这个automation的这里面就可以了。然后其实实际访问的是这么一个UIL，当然你也可以用python对吧？那python它相当于就帮你封装好了。
	那这个OpenAI点model点list这么一个方法，它内部就会去调用刚刚的我们在这儿看到的这一串UIL，并且把这个从环境变量里面取到的APIK变成它的一个参数。他去构造了一个请求，通过python的request这个库，然后就可以类似于CURL的方式，你可以就去请求了一个HTTP的这个请求。也有note GS，就是前端的同学如果比较熟练的话，也可以用这个note GS。我们也欢迎我们的这个示例的这个notebook给出这个note GS的这个版本。这样有一些前端同学可能看的就比较习惯了。
	它的返回都是一致的，返回的都是这样的一个结构。这个返回的结构大家会感觉有点复杂，我们实际上大部分的这个请求返回都比较陌生的那我们把刚刚执行到的这个结果存到了models这个变量里面。然后我们把它打印出来，可以看到它其实是给了一个data这么一个数组。这个整个是一个Jason吧？一个数组，然后这个data里面其实存了应该是可以通过电这个键，我们可以看到完整的就是是一个非常多的返回。然后这个返回的内容其实参数也很多，对吧？我觉得一看就已经看晕了。
	那首先怎么办？给了第一个prompt，就是大家在解决这种新的API的时候有什么样的办法。搁以前可能我会说去看一下API的reference，就这里我们刚刚打开那个窗口去看看他的描述，这是一种方法。但是他会花你很多的时间，尤其是如果返回很多的话。那对于我们来说，现在我们想要知道的是什么呢？我们其实是想要知道现在这个models就这么一个API它到底支持哪些模型？这个至少是我们目前最关心的问题。
	所以给了第一个prompt是做什么呢？就是这样的一个内容。我把它实际上这种markdown的这个格式，你双击之后，它会给到没有渲染的这个markdown的原本的内容。然后这里做了一个块，那这个块里面的内容其实就是给到GPT4或者GPT3.5的完整的prot。那么来你点一下这个运行这个mark down就被渲染了。
	那它实际这个prompt大家分析一下，其实就是两部分对吧？第一部分，因为这个问题很简单，其实就是让他从这个一堆数据结构里面就返回的数据结构里面找出来我们关心的那我们怎么干的呢？我们告诉他下面的这个Jason的这个数据，你希望存到一个存在这个model s的变量里面，就正正如这里所所打印的对吧？然后我们希望干什么呢？我们其实已经知道它每一个模型就是这个data里面的一个队列里面当中的一个元素，那我们希望能够便利这个models，就是这样的一个models，然后获取当这个里面的每一个ID这个是一眼能看到的，这个ID其实是实际的模型。
	将每一个ID都取出来，把结果放到model list里面，然后生成一个python的代码，然后给了他一个示例，这个示例我故意给了他这个两个，就没有只给一只长，相当于这个数组的长度是两个，对吧？至少给他两个模型，没有只给一个。好，然后他给了一个就GPT4给了一个代码，但他又有一些赘述的内容，我就去掉了。他给的这个代码是通过一个lambda表达式来获取的。熟悉python的同学应该很清楚这段代码的逻辑。
	如果大家发现这个代码还是看的很复杂的话，可以再给GPT1个prompt，让它变成一个更简单的形式，就是不用这种number达表达式的形式写成这个for循环的形式，那其实就是去把models data这样这个数组里面的每一个，相当于就是把这里取出来，变成了一个model。那这个model里面，就变成了一个字典。那这个字典里面的ID，这个model就等于这里的一个字典，这个字典里面的ID就是我们要的内容。对于这个model data，就相当于对于所有的模型都来一步这样的操作，都把这个ID取出来。取出来之后，最外层大家看到涛了一个这个list，对吧？取出来结果最后变成了一个list，那就是一个model list。我们执行一下能看到这个是我们目前就到今天为止，此时此刻我们的OpenAI支持的所有模型都在这里了。我们可以看到多少个？
	56个这么巧的数字，56个模型都摆在这儿了。但是其中有很多是即将要退出历史舞台的，包括我们随便一看像这个text search，这就是要结束的对吧？它很快就没了。像我们上一节课用到的这个text embedding ADA002，这个是会至少到目前为止来看会长期维护的。
	这个是一个获取模型的最简单的一个API。其实就是通过我们调调用这个OpenAI model这个封装里面的这个list就能拿到了，对吧？那类似的假设我们现在不只是说我要看现在支持什么模型，或者说我的模型到底现在还过没过期。而是说我要具体的去分析一下这每一个模型里面的具体的内容了。
	那怎么办？这里其实是有一个完整的一个model的示例的内容，它包含了什么样的一些信息。这个通过官方给的一个翻译就够了。然后我实际试了一下，把这个内容直接交给GPT4，然后告诉他这个是OpenAI的model API的返回。你告诉我一下每一个key是什么含义。我印象中这个就是这么来的，而不是我自己去输出的。所以这个prom的我没有贴在这里，大家也可以通过类似的方式，类似的思路就是可以去尝试一下。
	因为本身这个大语言模型已经对很多的代码库都做了训练了。如果你去请求了一些别的库，然后你可以告诉他你请求的是什么样的一个代码库，或者什么样的一个API。然后这个具体的是哪个API或者函数的返回结果，然后让他来分析一下这些内容。这里对于一个特定的模型，我们可以去拿到这些结果作为一些关键因素。刚刚我们看到的其实是这个ID就是这个模型的唯一标志符。然后包括这个模型的创建这个模型的时间，是一个时间戳算出来的一个增量时间，以及它的一些权限。就比如说这个permission里面有是否允许创建什么引擎，是否允许反应确认之类的，我们就不再展开了。
	那这里就是说我们去取一下这个text，达芬奇003它会给对应的一些包括权限，使用者的一些信息。那我们可以看一下这个GPT3.5的。也会给出对应的信息。
	我们能够看到的一个典型的点就是说，首先这个已经把API的调用变成了一个python的函数的一个调用了。其实对于很多对于网络调用什么比较反感的同学，这已经非常方便去使用了。所以整个前半部分我们的model API其实通过两个简单的python的函数调用，就已经能够去实现对OpenAI模型API的一个使用了。
	第二就是我们的completion的API对吧？就我们前面在PPT里还有讲到的就是怎么样通过文本补全的方式，也包括代码。就是文本和代码补全的方式，让我们的这个大模型，让我们的这个GPT给我们生成我们想要的内容。这里有一些主要的请求参数说明，就是我们在刚刚PPT里也有看到的，做了一个翻译，包括这里有它的这个默认值什么的。最重要的两个必填的参数这个应该是翻译错误，对这个prom应该是必填的。
	然后我们能看到第一个简单的示例，就是我们用这个completion的API去生成文本。这个是最简单的，就跟我们在刚刚在示例里面看到他帮你做翻译或者做别的问题一样。那怎么去用？上面这是为了让大家能够直接从这儿开始的话，也能使用留了这个内容当然这些其实在上面都已经执行过了，真正核心的其实还是类似于这样的一个函数调用。这个函数调用里面其实有一些关键的参数，最重要的其实是这两个必填的对吧？
	第一就是指定你要使用哪一个模型，这个模型就是我们刚刚看到的model ID？我们有56个不同的模型，然后这56个不同的模型他们都有一个通过ID来指明的这个模型类型，然后要填到这里来。然后relation这个API它并不是支持所有的56个模型，只支持其中的一部分，像我们提到的GPT3.5和GPT4都不是他能够去直接调用的。会报错，待会儿我们也可以试一下他会给你报什么样的错误。
	然后这个prom就是我们平时说的提示词填在这里，然后你也可以去设置这个max token，这里我们有写，它是指补全时要生成的这个最大token数，然后包括我们刚刚说的这个温度值。如果你使用一个较高的值，它会输出我们说了多样性对吧？让它这个结果更加的发散。然后如果值比较低，它会给出相对来说更确定性的结果。我印象当中好像设置成零的话，基本上这个就不会变了。对，OK这是最重要的。这个四个参数平时也是我们经常相关的这两个必填这个决定了生成的一个上限，这个决定了一个生成的是否发散和结果的稳定性。
	OK那我们让他就说这是一个test，这个是OKI官方给的一个示例的一个文本，我们试一试看能不能跑通好，他没有报错对吧？我们把它打印出来说this is a test。我们平时在用ChatGPT的时候，其实它给你的内容很干净。就他你给他什么样的结果，他就还你一个对应的答案。但其实我们开发的时候会给你各种各样的返回，对吧？就跟我们刚刚用model API list API的第一个问题一样。类似的我们可以同样的让GT来帮你分析这些内容，让我们一眼能看得出来这个其实是我们要的这个text。但它结构有些复杂对吧？它先是有一个外层的Jason，然后套了一个choice，然后choice里面有不同的text，然后你就会发现这个choice为什么是一个宿主？
	因为我们说了completion的API可以返回多个答案，可以返回多个结果。那多个结果就在不同的choice里面，那么你可以在不同的choice里面去做选择，然后通过一些你自己的度量手段去做选择，这个是没有问题的。那么我们如果没有去特殊设定，它会给你就选这个排在最前面的答案就好了。
	我们这个答案在深圳生成的内容在哪呢？其实在这个choice 0的这个里面，那你有多个choice都会在这个text的这个关键词里面，对吧？那我们要去获取这个内容，把它放到一个单独的变量叫text里面，就从这里面取出来，然后存到text里面。然后我们告诉这个GPT4，他其实就很快就直接把这个结构解析出来之后，他告诉你在这个返回结构我们放到了data对吧？放到了data里面。然后这个data里面的choice，因为它整个是一个可以像字典一样去访问的结构。然后当中的这个choice的第零个，我们从零开始数的第零个。
	的text这个字段的内容就是我们要的结果，那我们执行一下。他把这个说出来叫this is indeed a test，其实就是把这个结果取出来了。然后这里还多了两个换行符，大家如果有注意到的话，这里有俩换行符，然后他把这俩换行符也输出了，所以完整的这个xt是俩换行符，再加上this is indeed a test。
	OK这个是最简单的一个调用。首先我们这儿就可以来做第一个小实验，就是我们能不能把这个换成GPT3.5，做这个completion API的这个接口，对吧？可以试一下。这个过程其实就是想告诉大家，这些所谓的很新的东西其实就是你把它拆解来看，也就没有那么复杂，要改的也就是那么点内容。然后大家如果对这些API不熟悉的话，刚刚也给了一些prompt，大家可以自己去试，可以去问对，刚刚没有体感，就感觉这好像是在本地拿数据一样。但其实不是，他就是在调这个OpenAI的请求，调他的API，那最后有写这个提示对吧？就是。In value the request error, this is a chat model and not supported in the paba and the point. 
	刘明，你看他其实是告诉你，你在调这个V1 completion这个API，但是这个API是不支持我们的这个GPT3.5的，因为它是一个对话的模型。然后你要调用它的话，你需要使用VE chat completion。就我们下面会去介绍的这个API OK，那我们就知道它不行，对吧？
	OK那就放在这里，我们接着来看看，它能不能生成中文的文本。要让它生成中文的文本，首先要改第一个我们要去动的参数了，就是我们的max token对吧？因为七是生成不了什么笑话？我们可以先不改着试试，还是7。感觉这个话就没说出来，对吧？那我们把它改成一千试试。
	为什么程序员最喜欢穿灰色的衣服？因为他们不想被d bug，这个我没听懂，我们把这个ten free改一下。我不知道有没有听懂这个冷笑话的，这个灰色衣服不想被debug是什么概念？因为看不出bug是吗？
	我们看调这个temperature之后，它没有那么死板了。因为零的话你可以理解成就是训练语料里面有什么我就尽量给什么，可能就是命中了一个很很冷的一个东西，0.5的这个程序员和经理在一起吃饭，经理问程序员你最近在做什么？程序员说我正在写一个bug free的程序。经理笑了，那你现在还有多少bug？程序员笑着说目前还剩下两个。我很好冷，对，很冷。但是point是什么呢？就是我们要让这个completion这个API去做各种各样的事情。
	玩这三个参数是最关键的，也是你一开始想要上手的话最需要去使用的。我们单纯从文本生成的角度，上面是这个英文的生成，下面是这个中文的生成。然后你都可以通过这三个参数去达到你想要的效果。Max token去确定了它的最大的生成内容的长度，temperature是确定了它的多样性。在这儿你也可以给它一些更宽泛的一些指令，比如说讲十个对吧？
	然后他应该也都能生成，因为没有超过这个max token，我看看会不会有点耗时。
	居然有这么久。
	好，他把这个答案拿过来了。我们让它生成十个的时候就能看出一些问题来了，对吧？第一个就是说你看他这儿这个是有套路的，大概率可能就是之前的训练集里是有一些这种类似的内容的。因为讲笑话对于其实讲笑话对于GPT来说是比较难的，因为要理解笑点是挺难的。
	但第一个其实还是不错的，这个结论就是什么是最慢的编程语言，人类语言对吧？执行的慢，没法交给GPU去执行。对，然后这里我们能看到它生成了什么，123456789，然后到这个到这儿的时候就停下来了。理论上来说这个应该还没到max token，但是它居然停下来了。
	我们可以实际来看看这个数量到底是多少个token，正好给大家看下这个网站platform。应该是在token那一个。还真的抄了。中文的这个字节数比较大，所以跟大家直观想象的不太一样。就刚刚咱们这一点内容消耗了1000个token，所以到这儿的时候就998个token了。
	就第九个问第九个笑话都没讲完，他就超了。然后你其实减掉一个换行符，它是要消耗这个token的。所以这些都是一些可优化的技巧对吧？就是从输出的角度来说，这换横幅好像没那么重要，但是你这就就少了九个token了，类似这样的特殊符号什么的都是一样的。那最好这个正好就是一个实测。
	其实人家的这个max token设定是没有问题的，这个就是998个token了，所以后面就直接截断了。你可以理解成就是在生成在它大语言模型的内部结构里面。因为你把这个max token数给到他，所以它在生成过程当中，他是知道我现在生成多少token的，所以下一个就直接不生成了，这一个也就直接中断了，因为是概率模型，一个一个往外吐的对OK。
	这个是我们同时验证了两个事情，一个就是max token是work的，然后第二个就是这个热度值你去调整的话，它确实会有不一样的。就是你会感觉这个笑话它有硬不硬？是很冷，还是相对来说要柔和一点。
	这是从文本生成的角度，我们去看了一下这俩API，那我们能不能生成代码呢？其实是可以的对吧？我们上面就是用ChatGPT生成的代码，包括去取这个text的内容，去取这个model list，都是通过ChatGPT来的那我们自己能不能写代码来干这事儿呢？肯定可以对吧？因为大语言模型它自己宣称它可以，那我们实际来试试是不是现在GPT这个是一个虚假的这个操作，肯定不是，那实际怎么操作呢？一样的修改一下这个prompt，然后我们把这个max token稍微调的长一点。然后我们看到这里，我们让它生成一个可以实际执行的一个快速排序的一个代码，这个是一个比较典型的面试可能的考试题。然后这个快速排序本身是一个n log n的一个很经典的排序算法，然后它里面涉及的知识点也比较多，那我们看看它能不能生成。
	然后这个可以复用，因为text没变，对吧？我们来输出一下，大家能看到这个是一段text里面的内容，就是我们的这个我把这个完整的data打出来给大家看一下，有感觉一点。这个是他输出的一个内容，就是我们相当于用tax达芬奇003这个大元模型，然后通过这一段prompt，然后让它去生成的这个结果的一个返回，然后我们取出了这个text。这个text的其实是有换行符，有格式的。
	就是我们我不知道大家有没有get到这个点，我专门提到可执行的快速排序的python代码。大家如果了解这个python的话，其实它的这个缩进还蛮关键的对吧？你要是这个缩进不对的话，可能就出问题了。它的这个作用域范围就变了。跟有一些非常随意的语言也有一些不同。这个时候他给了一个可执行的python代码，要怎么样去验证它，对吧？
	我们看看同样的这个问题就来了，你有两种方式。第一种方式是你把这个代码复制，然后复制之后可能格式还乱掉了。比如说我们这儿去复制，它很好，它这个格式没没变，然后你再去跑一跑这个，然后去看看它对不对？很多代码都可以这么做。但是你如果想把它变成一个自动化的过程，你应该怎么办？一个比较直观的感受就是我们让PPT帮你想想怎么办，对吧？那你看这里有一个prom的是什么呢？
	它的目标，我的所有的这里的注释性的语言，这个prom的都是想告诉大家这下面这prom的目标是干什么的，而不用输入到这个GPT里面去，真正输入的内容是下面的部分。那目标是什么呢？我是想说能不能直接在这个notebook里面去直接执行这个生成的代码。有没有一些什么好的手段？而不是像我们刚刚想的那样的，人手动的把它拷贝出来，然后再手动的执行。因为实际上这些东西都应该写代码自动去解决。
	就相当于你写了一个生成代码的prompt，然后去调用了这个OpenAI的大模型，然后生成了一个结果，结果存到了这个text里面。然后你先要去验证这个text是不是真正能运行的，最好能让GPT再帮你生成一个样例。其实就是想干这么一个事儿，那它就能完全自动化了。包括它验证它生成的代码是否符合你的这个需求，都是一个可自动化做自动化测试的一件事情。
	那怎么办呢？这里我们问这个GPT，我们说我们现在想用，我们现在已经用这个completion的API生成了这个python的代码。因为我一直在用的是这个GPT4，所以GPT4是肯定知道这个completion API的，但GPT4不一定知道这个function call，这个是有一些天然的优势，他知道这个内容。但如果你用的不是OpenAI的这个come KI，你用了一个第三方的库。我刚刚也有提到，你最好就是把你现在用的什么库，最好什么版本都写清楚，你现在用什么东西。然后如果你发现这个代码很新，是在21年9月之后的内容，那么也可以用ChatGPT的插件，它会有一些网络访问的功能，它会自动去判断它，然后它就能去访问你的这个UIL，就能去看一下你的这些相关的引用的内容和资料，也一样能生成类似的效果。
	如果不是很复杂的内容解析的话，那么我们回过头来就是我们现在是用的这个open I的completion API。然后告诉他我们生成的这个python代码存放在了一个text的字符串类型的变量当中，实际上就是这样对吧？我们把这个text打出来了，它是一个字符串，然后如下所示，这里就写了把上面的这一个cel里面的内容贴到了这里，然后把它的打印机结果也附到了这里。最后我们给了一段我们想让他干的活，就是如何在jupiter notebook当中执行text中存放的这段代码。有这段代码对吧？所以其实这个prom的需要GPT4去理解，哪一部分内容是text里面的内容，他理解到是这个才行，对吧？并且要保证它的格式没有问题。
	那他给的回复是使用这样的一个函数，这个函数是python的一个内置函数，python native的一个函数。这个函数主要的功能是执行，这个就相当于把我们这个需求完美的满足了。就有这么一个函数，很巧妙的直接把这个EXEC就是execute这个函数的输入是一个字符串，把它输入的内容就当成一个python的代码，然后它就会去直接执行它。但是也是因为这个功能，所以其实是一个非常危险的操作。
	那GPT4也给了提醒，这一段注释是GPT4输出的，包括下面这一段示例的这个测试样例也是他他给的。他又提到这个功能的用途以及他的一些注意事项。因为这样的一个函数其实是在普通的这个情况下是不会去使用的，因为会被注入病毒，对吧？就很很好理解，你可以让他放心的去执行任何的内容。Python的那他如果要让你输出你的open IAPIK，那其实也会去输出的，我这就不展示这个内容，反正大家都知道这个key了。那实际上我们可以去执行这个text。
	但我们回到这里，它其实是要验证一下这个快速排序生成的代码是否真的可用。那实际上是可用的对吧？它其实已经执行了这个代码，所以换句话说，它其实做的事情就跟我们刚刚手动想做的那个操作是一样的。它相当于就把这段代码放到了这个python的解释器里，然后去跑了一遍。跑了一遍自然就定义出了这个quick shot，对吧？为了以示清白，我专门留在这里，我们没有执行这个quick shot对吧？这个地方是空的。这个快排的定义是通过刚刚的这个EXEC来执行的。
	好，那到这里为止，其实我们把completion的两个最典型的场景，一个是文本生成，那一个是代码的生成，都已经跟大家做了一个分享。我不知道到这儿为止大家有什么问题吗？我抽三个问题。
	我看一眼。
	抽三个问题，大家可以现在问一问，刚才有一点没听懂，想按暂停，这个我解决不了。
	有一个同学问的很好挑。第一个问题就是通过API和直接通过ChatGPT生成的结果有什么区别？这是一个挺好的问题。
	首先通过completion API和ChatGPT生成的结果是一定不一样的，这个是非常好理解的。因为ChatGPT并没有使用completion的这些模型，就completion用的都是老模型对吧？用的GPT3的模型。而ChatGPT至少也用的是GPT3.5系列的，就包括text的达芬奇002，这些都是3.5系列的了。也不对，这个completion可以支持可以执行这个达芬奇002和003。那那刚刚那个那个不能作为一个直接判断依据。
	但是有一个第二个很大的区别在ChatGPT是更像chat competition，就我们接下来要讲的这种基于聊天信息的。所以它是有上下文的，而你completion是没有上下文的。你只能把你的上下文一种你的这个GPT3模型好理解的方式。因为上下文的好处是说我只要记录一下我说了什么，GPT说了什么，系统想要GPT干什么，然后就跟记账一样的，记日记一样的，然后丢给chat completion就好了。但是competition你要是这么丢给他他可能会错乱，不知道你要让他干什么事情。所以chat的这个就是聊天补全的这个API天然要更好一点，所以区别在这儿。呃。
	有一个同学第二个问题，在java的后台，如何针对不同的service control和DADAO去生成代码并可执行？这个问题问的很具体。首先这个问题太泛了，就是你你包括我或者你交给PPT都不知道你具体想要生成什么东西。所以一个比较好的建议就是看一看之前我们讲prompt的这个课程。然后包括我们接下来要做的这个prompt的100个典型分享，对吧？
	就是要问具体包括官方给的这个几个策略怎么样去提问，就是一定要问一个具体的，就比如说快速排序对吧？比如说你这儿不同的service control，它这是一个范式，这个编程范式不是具体问题是方法论对吧？你具体要问什么问题？所以这个是一个蛮典型的对，就是这个是一个点。
	然后如果我理解的另一个含义，就假设我现在在像GPT1样，我理解另一个含义。你现在不是在问要生成一个代码，而是说实际去执行。那那这个问题是一样的，就是你应该问一问GPT你的执行环境是什么？Java是在JVM里面去执行的对吧？那你能不能在JVM里面提供一个入口，然后这个入口是把你生成的代码丢进去的，那这个就跟notebook一样了，对吧？就是有一个java的XQ这么一个类似的入口，让他去执行你的代码。但这种方式不推荐，是比较危险的，一定是在可控范围内去做这个事情比较好啊。
	要么就是你的权限，你的这个接入的权限是比较健康的，不太会有这个外部的接入，那这样的方式是还OK的。第三个问题是如何监督结果正确。这个我理解是一个软件工程的典型问题。就是怎么样知道你的代码是对的，就是不是说生生没生成，而是生成了一段代码是不是对的。这个问题人写的和GPT写的都是一样的，这就是一个最简单的方式。比如说我们这儿有一个快速排序对吧？然后它有一个排序结果。但正常人来做测试的时候，就你想象一下你公司的测试工程师是怎么干的，他要写测试样例的对吧？那这个测试样例就是用来判断这个对不对的。
	如果有参加过这个OI或者NIP或者SM的这个比赛的同学，都知道我们做编程题，包括need code对吧？你刷过net code都知道它会有样例点，有simple，有input，sample output simple. 那你跑一遍的一些测试点，就基本能够知道你的这个代码有没有问题了。如果跑完测试点都通过了，但是代码一上线有问题，说明这个测试样例没写好，对吧？就测试工作没做到位。所以怎么监督它的结果是否正确，就不再是GPT这边的问题了，而是一个测试问题。这个测试问题涉及到怎么样去做测试样例和测试工程的问题。再挑一个问题，我这么多问题，那我们再挑三个问题。
	如果文本切分的比较大，其中包括查询的关键字，这个关键字很短，怎么样进行向量计算？是一个大的embedding和小的embedding in向量空间计算吗？怎么检索出关键字？这个同学要好好复习一下。上节课我理解一下，就是你你表达的意思是说，你理解成了一个文本被切分了。切分之后有的关键字比较短，有的关键字比较长。他们的embedding的向量空间不一样，或者他们的embedding的维度不一样了，这肯定不对的。
	我们在上一节课的这个notebook里面，还专门去做了这个事情的。第一，不管管你输入的是一个字母，还是说很长的一段一段话，你embedding过去的向量的维度都是一样的。换句话说，你的内容不管怎么样，只要你没超过它的这个上下文的长度，它都是会变成同一个向量空间，同样维度的一个embedding ing的vector的。所以你输入的内容长短不会影响这个事情。
	然后只不过是说你输入的内容的多少，最后embedding出来的这个向量到底准不准的问题。因为有可能你的语料，你一开始做的这个训练阶段的事儿，你的这个长度就是在这个100个token或者说500个token这个尺度范围是最合适的。当然如果100个token做的好，通常可能十个token它的语义T恤也没问题，应该是有这种向下兼容的能力的。但是如果你一下整了一个3000个token的一个query，那这个时候确实容易出问题。因为本这么长的query就有可能有多个含义，所以你要切。那切出来之后各自会对到不同的问题上去，因为他们可能表达的这个不一样。
	对，回答一下这个问题。我们看一下连续聊天，待会儿再再这个回答还没讲到这儿。对。
	有个同学问这个有什么应用场景？这是个好问题。理论上来说，如果你现在生活当中都没有问题，你就没有应用场景，对吧？那那证明你活得很快乐。但大部分的职场同学都是有很多问题的。
	我觉得一个最朴素的问题，也应该是大部分人都会遇到的问题，就是时间怎么样高效的利用。这对于我来说，我要做很多的事情。再跟大家去做这个demo，demo的视频，demo的代码、demo的课件的时候，能不能尽可能用AIGC的内容？那可以的。比如说这里我们就授人以渔对吧？也是自己的一个学习，就把这些GPT4直接就用在了这个notebook里面，大家也能看到是怎么用的那通过这样的方式，既可以让大家理解这个prompt是怎么用的，有什么样的应用场景。同时也能够接下来我们在自己去跑这个样例代码或者说玩别的代码的时候，通过类似的手段，类似的prompt能解决你的问题，提升你的效率。所以应用场景应该是跟你现在遇到的问题是直接相关的对。如果你找不到你身上有什么身边有什么样的问题需要你快速去解决，去提升你的时间利用效率的话，那那也挺好的对。
	第三个问题，是否有对应的开源的前端代码来接受python调用返回的信息。消息就像POE那样的页面。我是后端不怎么会写前端代码，非常好的问题。
	首先我建议你尝试直接用GPT4来生成这个前端代码，我之前也干过这事儿，就刚刚开放这个chat completion这个接口的时候，首先你不需要特别复杂的前端页面，是有的，就是有这样的开源项目你可以去找，有国内的，有国外的都有。就是套一个OpenAI的壳。然后你就相当于把你的APIK放到那开源项目里，然后你把那代码拉下来。但是你可能自己得做一下那个代码审核，看看有没有一些不是很安全的代码在里面，我相信大部分应该没有，大部分开源项目的这个作者还是很良心的。
	第二种手段，其实我还是建议你既然在学这个课程，去尝试用GPT4来生成这些前端代码。就跟我们那个OpenAI的translator这个项目最重要的一点就是我会去给大家分享一下，怎么样去把那样的一个开源项目，用GPT4的prom直接来生成这个完整的开源项目。这个还是挺有挺有挑战的。并且你要是把它掌握了之后，其实挺有用的。因为它涉及到多轮的对话。这个多轮的对话既是对chat GP的挑战，也是对我们使用这个prompt的这个人的一个挑战。但是一旦你整明白了一些套路之后，其实是还蛮提效的对。
	最后我看好几个人都在问，这个choice 0是要写死吗？这么回答这个问题，第一就是如果你不知道如何在多个生成内容里面做选择的时候，你就只生成一个。因为生成多个不都是token，要花钱。对，那你就只生成一个，先把就生成一个token，跟你现有的应用场景结合起来。然后第二步，你现在的这个应用场景，你验证了是有价值的，并且愿意长期投入了之后，你接着因为你这个应用也跑通了，对吧？你无非就是说你希望生成的质量更高一些。那这个时候就两种手段。一种是去调整你的prompt，让你的这个prompt表达的更准确，然后更全面，然后它能够生成更优质的内容。第二种手段也是官方推荐的一些手段，就是生成多个choice，生成多个内容，你自己再去选。
	那怎么选呢？可以基于你的用户的反馈，你可以把多个choice直接给到你的终端用户，让他们去选，这是具体问题相关的，写不写死取决于你现在在什么阶段，在干什么事情，好吧？好，那我们接着就是往下面这个环节来走了，待会儿我们还有提问的环节。就现在实战开始之后，我相信很多同学都有不同这个角度的问题，我们尽量多留一些这个时间，然后也让大家都能听到这些问题的答案。
	接着是这个chat completions API，我们也把它叫成聊天机器人的初探，就是因为多轮对话，多轮对话本身有很多的值得探讨的技巧在里面。首先大家能看到这里大部分的参数都是一样的，就几个主要的model，然后stream，然后max token temperature stream，这个后面有机会再单独讲。现在跟大家大部分涉及不到，我们就先跳过。如果大家真的有问题，可以在衣袖上面去提他们的衣袖上面关于这些特定参数的应该怎么用，到时候看看有没有已经用过的同学也可以分享。
	然后大家有一个变化，就是我们这个print没了，变成了一个message，对吧？这儿我不知道刚刚大家还有印象吗？我们刚回答了半天问题，就回到这儿来。
	对我们调用这个completion API的时候，model prom两个最重要的参数对吧？然后下面是这个生成的总数和这个temperature。那么到了chat completion的时候，我们不再用一个简单的prom，因为它不再是一个单独对话了，而是说我们用一个message。这个消息列表就是用来维护我们从跟这个模型聊天，从开始的第一个问题到现在我们一直在讨论的这个内容。然后你可以清空，你可以每一轮都清空，那就相当于你在用一个completion一样。那你也可以记录下来，然后记录什么你自己来做选择。
	对然后你甚至可以做什么事情呢？就是你不只是有角色，不只是有内容，你还可以有名字，你还可以让他记下来。因为你可以扮演有多个人在提问，对吧？比如说user可以是学生学生A和学生B或者说学生A和老师B然后这个assistant就我们的这个GPT，他更像是一个参考资料一样的角色，他给的是客观的一些意见，然后是两个人在辩论，让他来仲裁对吧？你说有这个有有这个student a说这个我是我在对吧？然后这个teacher b说你好好学习吧，别天天别整那些乱七八糟的，你让assistant来做评价，说你认为谁说的对？对吧？那这样肯定是可以的对吧？大家都可以用这个API来自己去做试验，我们这就不再展开了。
	然后我们接着就来实际看一看这个chat completion，首先我们要定义一下，就跟定义这个prompt一样，我们现在要主动的开始去维护这么一个message。就我们的这个消息列表，消息列表最朴素最简单的形式就长这样了，对吧？一个用户rose是user，然后内容hello对吧？就跟你第一次写代码都会有一个hello word一样，对吧？这是我们第一次正儿八经的跟这个chat completion，就类似于这个ChatGPT的这个hello world一样。你要对这个ChatGPT的这个模型，就GPT3.5 turbo这个模型，我们知道有这么一个模型，对吧？刚56个模型里面有它的啊不要忘了是我们这个变了一个模型出来。然后这里的接口也很简单，大家如果仔细对比的话，我把它拷贝下来，我们可以看一下。
	刚刚上面的这个接口是completion create对吧？下面这个是chat completion create，所以这个接口基本上没有什么变化。大家主要注意这里变成了chat completion就好了。
	然后模型我们选择这个GPT3.5 turbo，然后message就是这个message，然后什么参数都可以先不改，都用它默认的。我们先跑一下，看看会出什么结果，把它打印出来。打印出来结果是结构上也比较类似，仍然是有多个生成内容可以去调整的。有一个message不再是有一个text了，而是一个message。然后这个message里面会有这个内容，然后有它的这个role。这个也是为了方便你自己在基于它的返回去维护的。就它的生成始终都是一个assistant的生成，然后这个assistant会生成一个content，然后你从这里面去取。但是也是因为这个结构变了，所以我们刚刚的那个解析结果就变了，对吧？
	你不能再用刚刚那个获取text的方法去获取了，那怎么办？你还是一样的，你告诉他，你甚至首先告诉他，我希望维护一个保存聊天记录的一个messages的变量，对吧？就是这儿按照我们刚刚的说法，我们应该怎么做？我们应该把这个message输入和这个输出，就是我们现在GPT3.5第一次给你的返回。跟你说hello，他说hi there，how can I assist you today? 那你要把这俩拼在一起，然后变成一个新的聊天记录，再问他下一轮的问题才对，是吧？所以我们把这儿变成了我们把这个收起来，免得过于复杂。
	我们希望是说要维护一个messages的变量，用来保存聊天记录，像下面这样，这是我们的期望。下面是这个open I chat completion API的返回结果，我们希望将这个生成的这一部分，就是我们希望把这一部分的message追加到这个messages结尾，就是这里的聊天记录，然后把这个prompt丢给我们的GPT4，他给了一个什么建议？首先他把这个给调整了，对吧？就我们之前这是text，大家如果有印象的话，那现在这变成了一个message。然后把它放到这个new message里面，我们打印一下，这个new message确实取到了，然后有content有肉，然后我们把这个new message追加到这个message这个列表里面来，吧？然后我们输出一下。出问题了，果不其然没有这么顺利，对吧？不然这个GPT3.5就被拿下了。
	这么简单。问题在哪儿？其实也不复杂，大家不要感觉很复杂。第一个就是说他跟之前不一样在哪儿呢？之前是给你一个text的对吧？那text就是可以直接变成字符串的那现在给了你一个肉，给了一个相当于给了角色，给了内容。他为了返回结果能够方便访问，它定义了一个叫OpenAI的object。
	就相当于首先我们用的都是这个OpenAI的库，那OpenAI帮你去做了请求，然后也帮你去拿了结果。然后对于这个chat completion这个调用的结果就是我们这儿，我们这个结果是通过OpenAI的这个库的chat completion这个类的create这个方法帮你构造了一个请求。这个请求的目标是去调用我们的API使用的参数是GPT3.5 turbo这个模型的类型。带上这个模型的message，同时再带上你自己的open API的key对吧？OpenAI的API key。然后这三个主要参数当然还会有一个默认的参数，然后丢到我们的这个请求那边去。然后他拿到了结果，他拿到结果之后做了一次处理，然后这个处理之后放到data里面来了。
	我们直接看这个print出来的，打印出来的这个节省数据。看不出来什么区别。但实际上它为了方便访问结果，它把它变成了一个OpenAI的object。
	那OpenAI的object的一些优势是什么？大家这边有看new message是一个OpenAI的这个object，他为什么做这个事儿呢？也是为了方便快速去取数据。就比如说我们看到new message有这么两个内容，然后我们就可以通过这个肉可以直接拿到这个肉，我们也可以通过这个。Content去取内容。它其实有这样的一些面向对象的取，这个相当于边成员变量的一些优势在这儿做了一个封装，但也没有那么复杂，所以大家不用感觉那么头疼。
	好，我们现在知道这个不对了，加进去的这个不是我们要的这个内容，而是这样的一个形式。那那你显然要做一个数据类型的转换，对吧？那第一步是要把这个错误的OpenAI的给它移除。那就移除的方法可以，因为message本身是一个python的这个列表，可以用pop方法就把最后加入的这个给它弹出去。
	这个时候我们再看message就是干净的，就是我们一开始的hello，然后这个hi there还没进去对吧？那我们接下来想要做的事情是把这个OpenAI的object变成一个python的字典，因为这是一个字典对吧？这是一个字典，然后怎么弄呢？然后我们就告诉这个GPT4，打印了这个print message之后，发现数据类型不对。Message的输出是这样的，就把我们刚刚的这个困难报给他。
	然后我们希望将这个open ID object转换成下面这样的一个数据类型，然后他给了一些建议，那这个建议就是什么样的呢？首先这一段就不用再执行了，这段应该本身刚刚执行过了message。然后我们把这个new，我算了还是留在这儿。万一大家改过这个new message上面一堆操作之后，我们重新去从这个data里面取一下，重新取出来。然后取出来之后，把这个字典数据类型的放到这个new message dictionary里面。然后具体怎么干的呢？就是刚刚上面的代码也已经给大家展示过了，对吧？
	因为message的肉就直接能去取这个roll，这都打印过了，然后content争取这个content，构造一个这样的字典，然后这个字典的这个类型我们再输出确认一下，执行一下，变成了一个字典对吧？然后我们去打印一下，这是一个字典，没有问题。然后我们重新把这个new message的dictionary添加到我们的这个消息列表聊天记录当中，添加进去，然后我们输出一下聊天记录，大家可以看到这个聊天记录很多，这一次的这个notebook我就没有每一行都加注释了，因为未来代码会越来越多，大家如果有不明白的某一个某一行代码的话，就用我们这么多的prompt对吧？叫GPT4给你做回复，让他去解释这个代码或者让他加上注释都是可以的对。
	然后接着我们又开始新一轮对话了，因为我们其实刚刚就只完成了一轮对话，就跟completion一样，对吧？你就跟他说了一个嗨，然后他跟你说了一个hi there，吧？我能帮你什么？
	新一轮对话我们取了一个新新的new chat，让大家方便记录，跟上面区分出来。New chat其实就是我这一轮想要去跟GPT的这个3.5 turbo这个模型去交流的内容。然后我的角色是user对吧？那内容是三个问题，第一个问题是讲一个程序员才听得懂的冷笑话。第二个今天是几号？第三个是明天是星期几？
	然后我们把这个新的内容也加到这个message里面，这个就是有一个跟刚刚调completion API不太一样的地方。因为completion API就是每次去里面取结果就好了，对吧？但是现在是你要维护一个聊天记录，甚至你最好把你的这个message就提前构造好，最好还检查一下，看看你的这个有没有加进来，然后数据类型是不是对的。
	好，发现这个没有问题，他就是这样一个一个的聊天记录，所以其实也很简单，你的上下文就是这样丢进来就好好了，最好是按照顺序，按照时序来的。然后目前你就能看到这里，我们引入一个打印的美观一点的。应该是有这个。OK这个是一个为了输出一些结构化的python的数据内容的时候，他帮你做的这个更这个布局更好看一点。一个perfect print这么一个包。
	然后大家能看到，这个其实就相当于三轮的交流，或者说一轮半的交流，有有来有往的。一开始是我说hello，然后这个assistant就回我，后面是肉assistant就回我。然后我现在又要准备去问他问题了，问他的问题其实是三个问题，一次性，然后我们对他进行请求。这个是接口调用的形式，其实非常简单，速度也还行，就一秒以内差不多。我们重新去取这个结果，这个地方不是打印它的列表，就是粘贴错误。
	然后大家可以看到一个很有意思的现象，我们刚刚没有什么问题吧？Print都很正常roll content。但其实我们跟这个completion一个不一样的点在于，completion就是一个字符串。所以我们print的时候它会自动unicode帮你解析好，对吧？
	那现在new message是一个什么呢？大家还记得的话，new message是一个OpenAI的object。所以这个OpenAI的object在于print这边就没有去做很好的，你可以理解成没有做很好的适配。理论上其实做的好的话，OpenAI的object也可以重写。这个string方法就是一些python的东西，它能让这个print适配的更好，就不会出现这个中文没有被转译解析出来很好的一个显示的一个样式。
	其实应该两个拼一个变成一个中文，那怎么办？不懂也没关系，还是交给GPT对吧？我们问他这个，我们现在希望他能解析这个中文的输出。因为理论上来说，你问他中文，他肯定回你中文。而且有点经验的人一看这就是中文的这个东西，然后你就告诉他新一轮的对话返回结果没有解析成中文内容是这样的，打印了一个new message，输出了一个巴拉巴拉这样的内容，然后这个是GPT4的回复内容。他说从你提供的信息中看，这一段文本是经过unicode编码的。中文我们上节课有讲过unicode的编码，对吧？从asic ic扩展到UTF8 unicode到one hot，到我们的深度学习的embedding那么简单的编码去储存我们的内容。通过uni code基本上是最通用的一种手段，就只是为了储存交换信息的角度来说，这是非常通用的。可以使用python的这个print函数将其打印出来以获取原始的中文字符串。
	说了一大堆车轱辘话，对吧？最后给的代码是什么呢？最后给的代码是直接让我们输出这个content，这个内容。这里很有意思首先GPT是理解了你的需求，但是他没有去做太多多余的动作。就比如说他没有把这个new message的数据类型给你变了，而是说他理解到你最关心的是这个content这部分，它没有变成正确的解析成中文的结果。所以说它就只让你聚焦输出这个content，然后你输出的话你就能拿到这个结果，就这么简单，只给了一个办法，然后他给了一个答案，大家可以看到跟之前那个生成结果不一样。因为我们默认的这个chat confliction的temperature是一，我印象中，所以它会每次生成结果不太一样。
	对，然后三个问题。第一个问题是当程序员去海边度假的时候，他们喜欢在沙滩上大喊hello world，这个很程序员对吧？第一个问题是冷笑话。第二个是今天是几号，他很主动的问你所在时区的这个是这个你所在时区的这个日期是几号？反问你跟我上一次的提问不太一样，上一次的那个提问他就直接说我是一个AI助手，我不知道时间。然后第三个问题，明天的星期几取决于所在的市区，但其实也不是只是所在的市区这么简单，对吧？然后你在哪个地区呢？
	大概这么一个逻辑，从这个角度来说，其实我们已经完整的把这个对话的内容给复刻下来了。就是从第一轮的我们简单的打个招呼，然后我们的GPT回回你一个内容，你把它加到消息记录里面，变成聊天记录。然后你把你的新的问题添加进来，然后你去问他他给你一个结果。
	其实前面那个上下文对于你来说，你可以说他没有太大帮助，对吧？那这个也是没有问题的。但是你给了这个assistant，它其实是一种形式，让你有理解这个过程。然后这个过程你可以无限的去做叠加嵌套。因为到这儿为止其实回答了三个问题，对吧？但是你可能对后面两个问题不太满意。那你可以把这个首先你可以把前面这一轮对话这两个content都拿掉，这个是没有问题的。因为messages是由你来控制的，你想要给他什么都行。第二个就是说你问了三个问题，有两个问题你都不满意的情况下，这个时候你的选择是可以。
	如果第一个问题你满意，你可以把第一个问题的结果直接拿走，然后你就着后面两个问题再继续去做研究。就比如说你觉得后面两个问题是他能够回答的那你就单独针对后面两个问题再去做新的messages，然后让他跟你去做交互。然后你甚至可以把这两个变成两个请求，就是几号和星期几，你也可以把它变成一个先后问题。比如说先问今天是几号，然后通过几号能查出明天星期几，那这个都是可以的。就是留下了足够多的自由度，大家可以在这个代码片段到这儿为止再继续去做扩展是没有问题的。
	好，然后我们最后再看一下这个多种身份，就是怎么样给多种身份，因为支持三种身份，对吧？System user和assistant然后给了一个简单示例，这个也是官方给的一个示例。这个示例就同时用到了三种身份。
	第一个就是我们的这个system，大家可以看到它的它的这个内容就很有意思。就是它的这个输入的点叫做你是一个能力很强，能helpful能帮助大家的这么一个assistant一个助手。这个其实是告诉我们的大模型的。
	然后user是在问什么呢？就是问谁赢了这个比赛？那么在2020年，我们的assistant是回了一个信息，就是loss engineers的赢了这个比赛。然后user接着又去问在哪儿比的？这个过程其实是有点模拟我们在这个GPT在这个ChatGPT去聊天过程当中的一个对话。我不清楚这个意思，大家看得到没有？首先这个system是我们ChatGPT看不见的对吧？你可以理解成我们的ChatGPT这个网页，这个应用程序，它收到的system的指令，大概率就有有点类似于这样的一句话。
	就是整个ChatGPT，因为它是一个语言模型，大家为什么要做理论课，到现在为止就是让大家不断去理解ChatGPT是一个应用，GPT是一个语言模型，而GPT这个语言模型它就是用来生成文本的，生成代码的。但是你要是让他只干生成文本和代码的话，它可以乱七八糟，怎么样都能生成。ChatGPT是为了让它生成的内容对人更友好，人看着更舒服，更像是一个人回复的内容。那他也可以去干别的事情，说我们这儿就让他不要做这个helpful的assistant。
	要我记得ChatGPT刚刚出来的时候，有很多的人去对他进行这个身份攻击，就是说强行植入。那个时候这个应该叫做prompt injection，就是大家可以去搜索一下这个词叫这个提示词注入。提示词注入其实我们今天学了这个理论就知道，其实它属于什么呢？属于这个zero short learning，对吧？就属于这个样本的一个提示词的学习，或者提示词的微调。比如说他就给了一个ChatGPT。在很早期，在今年年初的时候，就说你是一个天天劝退人，不要让人工作的这么一个专家。然后你现在给我的所有的这些问题提供回复，当时这个ChatGPT还没有做很多moderation，就我们今天提到这个专门做监管的内容，没有做moderation的时候，他真的会给出很多让人很退不是很积极的回复。
	这就是因为为什么要搞这个system的点在这儿。就是首先在之前大家这个system相当于是开放的那所有人都可以给ChatGPT去做这个注入。那对于对于从这个内容的友好度，这个健康的角度来说，其实是不好的，对吧？所以后面就封闭起来了，然后做成了一种专门的一个入口，就是通过这个stem road的方式去告诉这个GPT模型你要干什么。因为我们GPT是一个底层模型，除了用来做ChatGPT，也可以用来做XXX GPT，做什么都行，因为它只是用来做生成的，所以你在system这个层面上一定要说清楚他的一个身份到底是什么，就是他身份是什么。然后assistant这个role更多的是他回复的内容，对他说了什么，对他告诉了你什么。在这里首先从这个历史记录里面就已经知道谁赢了这个比赛，然后是在哪儿赢得这个比赛。
	所以在问在哪儿举办的时候，他能给一些额外的信息，就是去去查答案也好，就是他应该是去其他的一些信息，综合了以上的这些内容，就知道他应该要去帮忙去回答这些问题。然后同时他也能查到这个2020的巴拉巴拉这些信息。所以总体来看我们到这儿为止，就到chat completion这个API为止，有一个很重要的观点和这个视角要告诉大家。就是说ChatGPT是一个应用，GPT是一个核心的基础模型，而我们的目标是做出好多个ChatGPT，然后它可以是网页的形式，可以是APP的形式，也可以是小程序的形式都可以。然后底层的这个GPT的这个模型，是可以被上层的各种API包包来用的对吧？那么OpenAI包出来了这个chat completion的API，包了moderation的API。假设我们不用GPT这个大模型，我们用其他私有化的大模型或者一些国产的大模型，那相当于就是这个框架是不变的。
	还记得我们一开始上这个课程的时候有讲过一个AIGC的技术站，下层最底层是这个基础模型层，中间是这个AIGC。C的中间件就是我们的南茜，我们的向量数据库，包括我们其他的一些在中间跟大模型和上层应用打交道的这些软件技术站。而最上层的才是我们真正的应用层，应用层可以是各种各样的形态，是百花齐放的。所以把这个三层的架构理解清楚之后，我们再回过头来看。
	其实像这些肉也好，包括我们去做聊天记录也好，都是为了让这个大模型的能力能够被多挖掘一些。但是它的能力边界是什么，其实我们现在也都不知道。并且因为OpenAI做了moderation，甚至有这个隐含的moderation。就是他自己过滤一些内容，你也没法完全的去触碰到它。所以才会有国产大模型，有那么two有这个私有化大模型的机会在。因为它给了你更多的灵活的自由度，让你去调。你可以调的不只是上面的API了，你不只是能调这些肉了，甚至这个肉本身这个机制都是可以在基础模型层去做设定的。O那到这儿我们就把这个chat completion基本上给大家过了一遍，让大家理解核心照这个聊天记录，然后有三种角色，三种角色不断的给到我们的这个chat completion每一个模型里面去，让他去做记录，然后让他能够去理解你的问题，知道自己的身份。对好在开始这个tiktok之前，我们再回答五个问题。
	再回答三个问题，我好像已经九点半了。对。
	有个同学问chat completion API为了保持聊天的上下文入参，每次都要传入message。这个网络数据的传输量越来越大，最终的大小将不能承受我的理解。他问的肯定不是网络带宽不能承受，而是指GPT模型的这个context的上限你说的很对，是的。他他现在的解决办法就是它的模型层去扩大这个context的上限，或者你自己在聊天记录的维护上做的更巧妙一些。
	对，第二个同学问老师的GPU选型，这个课什么时候补？是这样的，我们最开始给外面排的这个课表，内容是很全的，但是顺序上没有最终定下来。后来我们随着上课的过程当中，我们会发现大家的对于提问的关注点等等都不太一样。GPU的选型是跟私有化的大模型部署直接相关的。我们会把它放到这个生态圈里面去，就是跟我们的私有化的这个大模型部署一起来讲。
	所以换句话说换句话说就是其实到私有化大模型部署之前，你都不需要GPU，大家不要有GPU焦虑。对，就是你除了自己要部署私有化大模型以外，你是不需要GPU的，包括用蓝茜、南茜也不需要GPU，只有大模型需要GPU。如果你不私有化大模型，你就不需要GPU，这个逻辑应该很清楚。
	第三个问题，有没有什么好的办法或者好的prot能让模型高概率的输出特定格式的东西？只能通过SFT吗？SFT就是supervise的fine tuning，这个我们在第二节课的时候有讲，大家可以回去看一看。就讲我们的这个GPT的family一路风云变幻和各种演变，就prompt learning这个理论课的第二节课有讲，大家可以有这个概念，大家首先能整明白。
	然后回到这个问题本身，这不是一个整个AI大模型。为什么说现在拼的是工程化的能力，产品化的能力？就是因为他没有一个一个办法就能解决所有问题的。就没有银弹。你按照这个计算机的经典语言，就是没有银弹，就没有一个子弹，那能解决所有问题，就没有免费的午餐。
	然后能想到一些思路可以分享一下。第一个就是说你首先得至少出现一次，或者你能复现这个高概率的这个问。就因为你要输高概率输出特定的格式，所以你首先能输出这样的格式，你你你至少能输出一回，或者你能稳定，你能复现它。在ChatGPT这个页面，或者说你通过大量的调用这个API chat competition的API或者competition的API，能够通过这样的方式能至少模拟出来复现它。然后如果你能复现它的话，这个时候就很好办了。第一，你在调用这个大模型的时候，你可以让它的temperature的这个值调的尽可能低。如果你需要的是一些特定的内容方向上的话。
	然后第二就是说这个大模型其实它也有所谓的随机种子的。我不清楚这个大家试试过没有。我是有看到一些prom engineering的一些分享，讲到给GPT一些prompt的注入。就是你可以问他你现在用到的这个随机性的种子，就跟类似于你用这个stable diffusion，它有一个随机化种子一样。大模型也有类似的东西，然后你可以去跟他交流，问出这个关键的key，就是用来描述这玩意儿的。然后在每一次的调用的时候，都把它作为这个zero shot这个prompt turing的一个输入的内容，就把它当成这个pro告诉他你现在就用这个随机化的种子或者用几号模型，大概是这个意思，可以试试，这也是一个思路。
	然后第三个就是说用能欠的话，能欠的解决方案是他自己去做output的part，他会把输出做做一个格式化规范化。就是不管你的输入是怎么样的，我的输出规范化在中间这一层去做这个操作，这是一类。还有一类办法就是类似于我们做这个COT或者TOT逻辑。就是你让他多生成几个，然后你再来挑。你甚至从多生成的几个里面，从内容层面，从语义层面再去整合。然后整合之后接一个能签的output partner，然后变成一个特定格式。对，就各种办法去去解决它。一个做NLP早期阶段就是这样的一个状态，现在已经比那会儿好很多了，语义的部分大部分解决了现在各种NLP的pipeline串联的问题。
	同有个同学问，传输它的context的内容进行调用会消耗更多的token吗？首先不存在消耗更多的token，我们一定要整整明白这个逻辑，就是token你只要整明白它的单价，整明白它是怎么算的那它就是这么一个计费方式，所以它就摆在这儿了。你传的什么东西它会用来算，对吧？那么在completion API里面是你的prompt，在这个chat completion里面是你的messages，就是这么简单了。那你传的这些内容，然后唯一可能还要有点细微区别。就是你用的是这个chat completion的话，它的input和output的单价不一样。Input稍微便宜一点点，output会贵一些，就这么个逻辑对。还有个同学问南茜怎么解决无线上下文的，南茜不解决无线上下文，对。
	生态片可以讲lama的私有化模型吗？这样我回头我首先我会建一个衣袖，让大家去github上面投票，就是我把我会把几个私有化把模型放在那儿，让大家去点一点投票，看看哪個最高我们就优先讲哪个，这样就应该能覆盖最多的人群了。还有同学问接口里都是直接返回所有信息的，如果想做到像k GPT网页那样一个字一个字的输出，有接口能实现吗？好问题，这个同学问的还挺好的。我们之前有说有一个参数还没讲对吧？就是这个参数stream，这就是用来实现刚刚那个同学问的问题的。不管是我们的chat completion，还是我们的completion接口都有这么一个参数。这个参数设置为true的时候，就是以流的方式返回。简单来说就是不是说生成完了才给你结果，而是实时的给你结果，就是它会持续不断的给你生成结果所以当你把这个stream设置为true的时候，就可以实现刚刚那个效果了。
	不是前端实现的，是服务端实现的OK，那我们就进入到第二个的demo。第一个我们花了四十多分钟的时间，第二个demo跟大家也直接相关。很多人在问就是我这个token要花钱的，尤其是上一节课是in bedding对吧？便宜的现在换成这个GPT3.5，或者说尤其是这个我为了演示效果，给大家用的是这个，尤其是completion阶段用的是这个text的win 7003这个模型我印象当中应该是最贵的。我没有记错的话，我们再来看看价格，给大家有体感一点。OpenAI pricing. 
	这个是报价对吧？我们刚刚后面chat completion用的都是这个GPT3.5 turbo，就用的这个标准的模型。大家有GPT4API的也可以换这个GPT4的这个模型，通过我们最上面的model API这个list models，应该就看得到有哪些模型了。Model ID搞对就行。
	然后我们的达芬奇003是最贵的，这个毋庸置疑对吧？0.03每1000个token，然后我们的ADA的这个V2，就我们上一节课用的这个invading是0.001。然后这个是training的价格，这个使用更贵。对，就达芬奇那个是0.12，对，是它的这个1200倍，这个体感很强。但是相对于这个ADA，就这个evading的这个ADA，我们使用这个GPT3.5，其实它的价格还好，对吧？大家能看到也就是20倍的价格。然后如果是输入的话，也就15倍，还行，不算特别离谱。对，然后GPT3.5和GPT4的这个差价也是很大的。
	我为什么开始在这一页花了很多时间让大家用GPT3.5？我作为这个开始是因为它的性价比真的是最高的。我们反复来看，就这个GPT3.5 turbo这个标准版的价格跟ADA是一个价，甚至更便宜，便宜了一美便宜的这个也不叫一美分，就比一美分的这个单位还小一个。对，都是这个价格跟ADA1个价，更便宜。所以这个是大家要用的话，用GPT3.5 turbo不算是一个很贵的方案，是一个非常适合大家去用的一个方案，好吧？
	OK那我们回到我们的这个代码这来。然后我们接着就讲，就是有有这个网页端的token inder去帮我们检验这个token是不是超了或者多少个。然后我们同样也有一个代码库叫tik token，是一个bad pair encoding的这么一个分词器。它也是能够让我们用代码的方式来计算我的这个token数量的，这个我们刚刚在PPT也有讲过了，这就不再赘述。然后我们上一节课用了这个text的embedding ADAV2，那他用的编码名称就是这个CL100k base这个编码名称。我相信上节课真正跑了代码同学还有印象。
	然后一些老的模型，包括我们刚刚看到的这个tex w7003，然后还有这个code x用的是这个P50k base。然后还有一些更老的，比如说这个GBT two或者达芬奇这样的模型，我们用的是这个R50k base。好吧，我们待会儿会对比一下，就同样的输入他们的分支，分支分出来的这个token数量会不会不一样。OK然后上节课代码里面其实也有这样的一行内容，就是叫encoding for model。就是它用来干什么，获取一个模型的编码方式。就比如首先它这个代码库，就这个tiktok这个代码库是跟OpenAI，就我们刚刚看的那个notebook里面那个OpenAI的代码库是有整合的。所以你直接输入这么一个字符串，这其实输入一个字符串，它也能拿到它的encoding。
	它相当于你输这个输右边这个OpenAI的模型。我们通过model list ID拿到的模型，然后他能告诉你他现在用的是哪个encoding，对吧？上一节课我们就是这么干的，大家如果还有印象的话，上节课我们输入了一个text embedding ADA002，它给了一个CL100k base。对，其实是可以这样的。然后还有一些扩展阅读，就比如说我们不是python的同学怎么办？对吧？那么他也有一些其他语言的支持，巴拉巴拉我们这就不再赘述了，节约时间。
	然后在英语当中，这个的长度通常是在一个字符到一个单词之间变化的。然后在某些语言当中，token的字符有时候会比它短，有时候有可能比一个单词长。这就是一些OpenAI的分词器，就是我们刚看到的那个内容，我们这就不再讲了。
	然后我就再讲一个细节，也是在我们正式跑这个代码之前，很多同学在问的。就是上节课其实我有跟大家在那个代码里面有讲。通过这个感叹号，然后再加后面的pip install，可以安装这个python的包。
	首先这个指令的意义意思就是说通过这个感叹号，我们可以在to pitter notebook这个交互式的环境里面直接去执行shell的一些命令。Shell是什么？大家如果不知道就再问问GBGT4，然后这个是一个很强的一个手段，就相当于有点类似于我们刚刚那个XQ函数可以执行python的这个代码一样。通过这个感叹号可以直接在这儿执行任意的这个需要脚本，需要指令。
	那百分号是干嘛的？百分号是一个按照这段话也是GPT4生成的。这个解释说明叫做magic order或者叫这个magic命令。就是什么意思？就是相当于to patter专门为这个百分号做了一些手段，通过这个百分号PIP，这个magic命令就是干什么？就是他自己内置了这个指令之后，通过它能够确保在这个指令后面的pipe安装的库和当前的这个就拍的kernel的python环境是一致的。
	也是现在就拍的社区更推荐的一种手段，但是用百用这个百分号如果你啥也不干你就只是安装一个包没问题的。如果你还想干一些别的事儿，比如说你想在这儿去X你忘了设置这个APIK在环境变量里。那其实你通过这个感叹号也能导出一个API的key在环境变量里去。但是如果你要更安全，更简单，那么用这个百分号可以让你的这个当前的坑就是指这个就我们上节课也有给大家看过，就是不同的kernel对吧？我们实际上就patter notebook是可以支持不同的kernel的。我甚至可以在这个notebook里面跑python 3，在这个model这里面跑python 2，不同的turn l这个多环境，当然这就不再展开了，所以它能确保你这个kero是一定匹配的。
	OK然后我们如果需要的话，安装一下，在这儿可以执行。它是用来检验的，你有没有安装对应的包，都已经装好了就没有问题。包括它的依赖项，然后我们现在导入这个take token对吧？导入之后我们要去取一下，按名称进行加载编码，然后我们接着往下执行，这些都是可以，这个两种方式都是可以的。
	然后拿到这个encoding之后，我们现在实际要干什么呢？相当于我们已经找到一种特定的编码方式了，对吧？没有特定的编码方式了。然后我们去把这么一段话要包含这个感叹号，就这个tiktok is great这么加感叹号这么一段内容。
	假设这就是我们要问这个GPT模型的这个内容，我们想知道它有它会被编码成一个什么样的向量，以及也不知道编码是什么样，它它会被分成什么样的一个词。这个词其实在这个python里面是一个list，对吧？然后它这个分层的这个词是长什么样子的，然后它的token数量是多少。那我们运行之后它变成了一个这样的内容，一个数组。这个数组里面有六个不同的数字，对吧？这个数字其实是它的ID，我不清楚大家还记得我们刚刚看这个PPT里面有一页，就讲他的那个事例的时候，我们可以现在在实操一把。
	比如说就这段话对吧，他自己明确说了，this paragraph is 35个token，我们来看看是不是，很显然不是，对吧？是哪里出问题了呢？这也是我刚刚发现的，第一，这个没有自包含对吧？先把它干掉，40个还是不对，对吧，把它干掉。把它干掉，这有个空格，看不见的空格35个。所以我们刚刚PPT里看到的35个就这么来的。然后这很吊诡的是，这个逗号其实是用来做这个数值类型的分隔的，为了显示美观。但其实它也是要占这个token的。
	然后我们刚刚看到那些ID，其实就是这个ID，然后我们可以做点小实验，比如说我们在这儿刚刚不是有逗号吗？我们整很多的，他没有按顺序来这个十一。看来我们还不能直接这样直观的看出它的区别。但我们想表达的意思就是说，你的这个token ID就跟他这儿的分词结果一样的。他不一定每一次刚好都把这个严格的按照一个逗号分出来。对，然后这个token ID就对应着这里的这个token的ID只不过他这没有做这种美观的不同颜色的划分，然后通过这个不同颜色的划分，你就能知道它一共有多少token。
	刚刚那个颜色是为了区分前后的不同token，所以做了不同的染色，然后我们怎么样去知道它有多少个token呢？其实就是算这个长度，对吧？那不是挺简单的，就是算这个的长度。
	所以这里大家可以看到，我们把刚刚上面整个这一部分变成了一个简单的函数。这个函数就是说第一去取这个编码方式，然后把它输入的这个字符串。因为这个函数有一个输入，把这个函数输入的字符串给它编码了。编码之后就变成了这里的内容，对吧？就变成了这个了。再把它这个列表的长度统计一下，那不就是它的token数量了。
	那其实就是这个token ID的个数，就有6个OK。那那这段代码大家应该也清楚了，那我们接着往下走，有没有可以反向转回来的，对吧？就比如说我们刚刚看到这个地方，这个地方我们都已经是token ID了，那这堆token ID能不能对应着一个一个的token呢？这个当然是可以的，这不就对应着一个一个的token。然后大家知道这个四色定律就知道这个不同的颜色它是能染出来的，然后他就能看到对应的这些token，就跟这里是一样的。
	我们从token ID回到token也是OK的，用这个抵扣的方法就好了，你把它转回来就变成了这个抵扣的。然后这里我们能看到什么？他这边有讲，就是我们的这个抵扣的方法可以作用于单个token。但是这里有一个问题，就包括刚刚我们在那个事例里面加那个逗号，加了一堆逗号，没有直接变化。因为抵扣的大部分时候它是应用于单个token的。你可以理解token ID和token是一一对应的，但如果你用的UTF8的编码，大家还记得上面那个例子吗？我们看看在这个地方还能不能找到，有一个中文的输出的时候。
	这里就这里其实UTF8就是中文，包括一些有一些其他的语言，它不是一个单字节的语言。它一个字可能就是用unicode里面就是几个unicode就是几个字节，然后你加了一个逗号，可能这个顺序就变了。那他在这个做分词，做这个token ized的时候，他那个token的顺序就变，相当于以前没有被分在一起的，因为你的调整分在了一起，然后原来在这个顺序变了之后，自然token ID就变了。所以它的变了之后，那个编解码也会出问题的，所以没有那么的稳定。你可以这么简单理解。
	然后我们也可以把它安全的在每一个单个的上面去做解码，那就会解码成这样，就是大家可以理解成就是一个一个的token涨涨回来，一一对应，就是长这样的。但是如果你直接decode，它就会拼起来。但拼的时候如果刚刚我们说的拼错了，那不就整个那一串都解码错了。就比如说假设这是中文，那我们把把刚刚那一串unicode当中一个拿走，那就错乱了，这个意思我不知道表达清楚没有，OK然后我们再往下看啊，这个执行一下。
	对，然后我们也可以比较不同的编码方式。好，就直接往下看。比如说针对这个example string，就是我们要去做编码的内容，对吧？我们有说过不同的不同的encoding出来的结果其实是不太一样的。
	到这个地方来看就比较明显了，就是这个GPT two，然后这个P50k base我们上面有看到这一类P50K的base是什么模型？是code x text达芬奇002。包括我们刚刚那个completion API里面用到的text达芬奇003，用的是这个P50k base的这种编码方式，然后也有这个CL100K其实他们出来的结果就不同。大家有注意的话，直观的感觉好像是这个新的版本的分出来的这个token数更多，至于这里为什么它更多，它更这个也没有什么直接的答案，这个大家可以接下来再深入研究研究这个方向，我没有去做太多研究。然后我们看到包括这样的算术运算符，不同类型的语言，他们去做对比，你看在这个日语这一块它就不一样，对吧？
	所以其实核心问题在哪儿呢？还是在他在学的这个语言模型本身对于这个语义的理解。因为从他的视角来看，token就是一个的语义信息，所以并没有一个直接的他在哪一个领域它分出来token会更多，哪个领域会更少的这种。如果有类似的benchmark，大家可以去去找一找，也可以分享给大家。因为这个事情会直接对我们做这个研发有帮助，对吧？就比如说你的某一个模型在特定的领域的知识里面的token分出来就很少，并且它效果还好，那就挺牛逼的那那这样的模型就更适合做这样的领域的事情。这个逻辑大家应该能明白。然后我们最后会可以看一下在这个chat completion call里面，就我们刚刚在调的那个多轮对话里面，他是怎么样去算这个费用的。这个代码我就不再细展开了，大家可以下来再看一看。
	反正核心逻辑跟刚刚一样的，就是我们要去把这个输入的字符，这个文本先给它编码，用它对应的编码方式。因为我们要对比不同的模型，把对应的编码方式编码一下，然后把它的每条信息有一个message。Message我们刚才也有理解，它会有空格，有这个换行符，这些都要占token。那我们把它的形式在同样在类似的模型上，我们就做成一样的这个messages的形式，它的规范化的格式，不要因为那些空格什么的导致它很多。然后把这儿这个执行一下，然后给了一些示例，这个就跟刚刚我们看到的message很像，这个大家应该比较熟悉了对吧？有system的role，甚至还有名字是干什么的，我们把它执行一下，能看到对于不同的这个，因为我们这儿最后我筛下来选的这个大应该是一样的编码方式。我看。
	大家回头，好，很好，这个可以留一个家庭作业。大家把这个函数扩展一下，扩展到能支持另外两种编码方式，也就是我们的上面写的另外两个，一个P50和这个R50，然后对应的这个encoding出来的结果。然后我们再去看看他们和completion，这里不是一个简单的添加，对吧？我们看看他们如果用了completion，然后把这个东西做一个换算，我们可以从结果的视角来看看。比如说他们输入的是message，然后出来了一些这样的结果，然后我们把它转成一个可以等价的给completion用的这么一个prot。然后它的token是多少？如果输出效果还差不多的话，那说明你找到了一个等价转换的办法。
	在不复杂的多轮对话的场景里面。从这儿来看，其实因为他们用的这个编码方式是一样的，所以他们输出的这个token很稳定，都是129。因为我们上面也看到了GPT3.5，然后GPT4以及这个text的ADA的embedding v2都是用的这个CL100k base这么一个encoding的方法。所以针对于同样的输入，他们输出的这个token的数量肯定是一样的。这个也很稳定的复现了这个结果。好，那么整个怎么样通过token去计算token数量的这个demo也都展现给大家了。然后我看今天群里还有人在讨论这个in bedding和这个大模型然后向量然后新新的数据来了，或者说大语言模型变了之后，in bedding是不是都得重新训练之类的一些问题。
	这是我我的观点是这样，因为首先这个问题我看蛮多人在讨论的，我分享一下我自己的一个观点。首先大大语言模型和这个invading模型，它很像这个DNA的双螺旋一样。通过这个大家也能看得出来，大语言模型在迭代，然后invading的方式也在迭代，然后encoding的方式也在迭代。所以整个整个这个过程其实是你可以理解那个训练集，那个训练语料。其实从知识学习的层面来说，大部分都已经学的大差不差了。我理解很多这种企业数据里面，更多的它不是知识变了，就他要学的常识什么东西他都没有大的变化。所以他对那个语言模型本身更多就是一些反应图，就是一些反应通领了。
	所以针对这种情况下，首先语言模型本身不会怎么变的，所以上一节课我们才会说这个text embedding ADA已经有半年多没有迭代了。所以大语言模型如果本身不去做这种训练集，就这个训练语料有比较大比重的变化的话，其实它的embedding都不会有比较大的调整的，所以这个大家要放心。然后第二就是说一定会有一些人去做类似的工作，就跟image net数据集大家需要去照这个图片然后去做标注一样。未来这种人类的知识变成一个embedding的向量的数据库，这样的工作也一定会有越来越多的人去做，不管是开源的还是商业化的。
	然后像我今天在群里有提到Alexander这么一个项目，是0000后他们发起的一个项目。这个项目的目标就是把archive这个数据集，我们有提到active这个论文的数据集，全部变成向量化的一个项目。这个项目大家可以先去搜一搜，回头我们也会在合适的时间去跟大家讲这个属于生态篇的内容了。就是我们这个行业其实除和技术的视角，在开源生态，在各种各样的不同的团队，都有人在为这个大模型的应用也好，技术也好，学术也好，在为它的发展做很多生态的建设。好，实战就到这儿，我们接下来看看大家的问题，我们再回答十个问题。
	要有个同学问，如果调用国外的大模型接口，然后弄一些应用上线使用，会不会有安全的法律风险？好问题，这个我们会专门有数据安全的一个课程，那一一节课程。然后我们这里简单回答一下这个问题。首先国内的要求是网信办提到了我们需要做AIGC应用的话，需要去做备案。所以你需要备案，如果你做的是AI生成类的应用。第二就是你直接调用，如果你自己不做任何的审核，你会有安全的法律风险的。因为最新的这个政策是，平台方如果不监管平台上的内容，不管你是AIGC的，还是UGC的，还是PGC的，平台方都有责任。所以如果你是搭一个平台，然后让AI和用户在上面生成内容的话，你自己需要去做监管和审核的。所以跟是不是AIGC也没关系了。对。
	然后第二同学说如果用redis去存储上下文，理论上来说是不是可以支持无限的上下文。那问题是GPT怎么找到上下文哪个角色哪个问题呢？不是这样的，就是我们现在的问题不是说上下文存不下来，而是说GPT1次能够处理的上下文有上限。你可以简单理解成就是你脑容量有上限。你今天就只能学这么多东西了，你再学学不进去，学不动了，简单就是这么理解。你不可能说因为人类的知识都写在那儿了，你就都能学进来了。不是，人类的知识都写在release里面，你也处理不过来吗？
	数据中台中的数据也做embedding以后，直接面向向量数据库开发。这样是否可以，看你是开发什么内容。简单来说就是有无数的研发到这一辈子都是面向circle做CRUD的开发的那你说他可以吗？也可以，也是一份工作对吧？这个可不可以？
	这个事儿是不太具体的，所以我只能答可以针对平台监管这一块。老师后续可以详细展开吗？可以，这个我也欢迎去提github上面去提一秀，你可以详细展开一下你要问什么。对，因为平台监管这个问题太大了。
	以后上下文会不会索引化，只用传递索引即可，这样就可以指数级别扩大上下文了。这个索引化是比较抽象，我理解的是说首先上下文窗口一定会往大的扩展的，因为我们上节课上一节课就有讲过这个non net就是这个长的网络。微软的发布的论文是可以把上下文弄到很宽的对，但是任何事情都有两面，你上下文越搞越大之后，那是不是上下文就没有意义了，那还怎么搞注意力呢？对吧？所以不要大家不要老想着这个地方有一个好处，这个好处就要把它无限放大。这个思维习惯不好的就是任何好处，任何技术它有它的应用边界的。你如果能找到它的比较比较合适的切分手段，那肯定是切掉比较好的。
	让它更聚焦。
	关于向量数据库和AI应用整体架构这方面有课时来讲吗？包括向量数据库的评测，我们的南茜就是一个从0到1的AI应用的开发，然后会有AI应用架构的整体的这个课时来讲，向量数据库的评测，跟刚刚那个问题一样，在需求上面给他be要求上面去提一提问题。具体想要评测什么，我们可以针对具体问题来做。
	还有同学问如果使用GPT做服务，如何做稳定性的监控指标？怎么定义？这是一个挺好的问题。首先这件事儿比较麻烦，在OpenAI如果明天关机了，你做什么稳定性监控都很难，对吧？所以你只能做一些预警性的东西。那预警的那跟传统的做web的这些接口是一样的。
	就是你给你的这个，因为首先你不会直接把TPT的API暴露给用户，你自己会做一个服务，你的服务对于GPT的这些API的调用是一对一的，一对N的，N对一的。然后转换过来之后，首先应该看你自己应用的这个服务指标。比如说你有TPS，你有QPS，然后你有并发量，然后这些东西定义清楚之后，反转过来才是你对于GPT API的一个底线要求。然后如果你发现一个账号搞不定，因为我们能看到这个GPT API是有并发的一个指导的。我们看看在是在pricing还是manager outcome rate limit。就是。
	我们能看到。
	的是，OpenAI是有提供一些这个指标的，这些指标是至少是他承诺的，然后他达没达到我们再说。然后这些承诺的指标是你可以作为一个参考，而如果你能够这个指标都远远超过了你需要的这个数量的话，那那问题就不大了。但如果这个指标都还满足不了你那可能就是一些传统手段。比如说你用多个客户端来分流降低压力。因为这个时候open I的API变成了一个后端服务，那这个时候单用户对于他来说是有他的服务上线的那你就只能把你中间这个中台变成多用户。
	呃。
	还有个问题，问题是讲搜索引擎从海量数据里面检索GPT，每次访问有token的限制。能不能用搜索引擎的算法从海量数据做到最强关联的上下文，再用这个上下文去利用大模型的理解能力去干什么呢？这个同学能想写清楚吗？就是你说了一个过程，但是你最终要干什么呢？你都把上下文做出来了，你要用大模型的理解能力干什么？
	用gbt接口做的in bedding向量结果不能用到其他模型。能不能这样说，embedding结果不具备可移植性，更换模型要重新做云北鼎重新存储。如果你的基座模型没换，那就不用换。我们刚刚不是看到了吗？你的GPT3.5，我们再回到这儿，GPT3.5、GPT4不都共享了text embedding NADA002，他们训练集没怎么变，大家这个应该是通了才对。大家再想想，今天我还专门讲了这两个训练集的时间，GPT3.5 turbo训练集到什么时间？到2021年的july对吧？我记得是然后GPT4是到2021年的september，就差了两三个月。GPT就加了一些互联网数据，然后还有一些其他的评论数据，人类的数据。然后这些数据加进来之后，他们没有换编码方式，没有换in bedding方式，这不是说你稍微用一下image都变了，不是的，对。
	有模型可不可以将复杂语义的句子拆分为多个有关联的不同语义的简单句子？只能大模型实现。你先举十个这样的例子，然后你再拿已有的大模型试试。然后你你你在还解决不了问题，你就在一手上面提问。对，因为这个太泛了。有类似于南茜的java框架吗？据我所知，目前南茜没有支持java。
	如何让ChatGPT读取完整的项目代码？还是那句话，读来干嘛？对我觉得我前面四次课都是在告诉大家一个很重要的道理，把GPT当成一个人。然后现在你你现在问的这些问题，你问他他也答不出来，对吧？你问人人也答不出来，就是目的性一定要非常强，这也是prompt生成类的最重要的一个点，你让人家生成什么你说清楚，对吧？
	读没有意义的，就是为什么很多人学代码，就是看代码看了很久他没有个目标之后，他他他没有那个抓手。所以这个同学最好你要想清楚让他读完整的代码要干嘛。如果要干的那个事儿，他需要读完整的代码，是不是不需要？是不是只需要读部分的代码？那读哪一部分的代码。
	对吧？
	我们最后再挑三个问题。我看一看。
	第一个问题，我看到老师find to one未来是不是就不存在了？以后的趋势都是prompt engineering。不是的哟，这个我们刚刚有讲，在这个find tuning of GPT three那一页有讲，就find two还是有它的固有优势的。第一个优势就是它能对模型参数进行调整，所以对于特定的领域数据不需要每一次都进行few short prompt learning，对吧？第二个就是说也因为这个好处，我们可以降低单次或者说每一次沟通的这个上下文的token的数量。因为你不用给他feel sure让他学没有future al learning就已经学到那个fine tuning的模型本身里去了。所以你在给他去造这个输入输出的时候也要简单一点，所以fine tuning肯定存在的，不然也不会有这个GPT3.5到GPT4的迭代了。
	老师之前说的三层底层大模型，中间in bedding上层内南迁的应用框架。不是的，底层大模型是对的，中间层是南茜，embedding n是一种手段。南茜你可以理解成embedding ing本身是一个也是个模型，对吧？它它用来干嘛的？我们上节课有一页很重要的图，in bing是表示学习的一种形式，表示学习就是用来干嘛的呢？用来自动去学习，从数据里面自动学习一些特征表达的那in bedding一个重要功能就是它也是去学的。但它能把一些高维的空间降维到一些低维的空间，还保留他一些很好的特性，对吧？所以in bedding本身是干这事儿的。
	你通过embedding可以做一些什么事情？刚刚提到的降维是一方面保留它的一些特征对吧？同时通过in beading还能获取到一些语义的关联性，语义的相似性，去做一些语义的操作，这些操作就能体现出他学到语义了。所以我们用embedding通常用干什么？做聚类、做分类、做搜索、做语义查询，这些事情都是evading可以做的。
	所以从这个层面来说，你可以把这个embedding变成南茜可以调用的一个模型，然后他那幅文恩图里面evading和大模型和语言模型是有交互的哟。Embedding和语言模型的交汇点，包括深度神经网络的交汇点，那三个的交汇点是我们的大语言模型。然后中间层就是南茜，上层才是那些应用。因为南茜它也不解决应用层的事儿，南茜是一个跟大模型沟通的桥梁对。
	老师有没有遇到有些问题是怎么调prompt的，都调不对，比如说输出格式。是这样的，就这个问题，你写代码的bug的时候也会遇到你去教这个新手研发工程师教学生的时候也会遇到，会肯定会遇到，然后怎么都没法输队？我们之前也教过很多技巧，不管是理论层面的还是实战层面的。比如说这个chair thought，你这问题是不是太复杂了？这个输出格式是要一把输出的吗？能不能拆几段输出，然后用chair source的方式去搞。
	第二就是说你输出的这些东西是不是太太冷门了，甚至是寒门？都没几个人干过。那可能你就给他一些few shot learning，让他去学，这些都是理论层面上可以干的。然后如果你这个问题就比较常见，那在网上搜一搜，应该都能找到好的办法。最后一个问题。
	请问后面课程中有关于AI应用生态方面的内容介绍吗？咱们是想听什么？公司还是产品开源项目数据集？AI应用。
	生态是什么？
	好，我看好像差不多了，我们就到这儿。今天回头大家有什么问题我们群里沟通，然后也可以在这个github的一手上面去提问，也欢迎大家去提这个quest。