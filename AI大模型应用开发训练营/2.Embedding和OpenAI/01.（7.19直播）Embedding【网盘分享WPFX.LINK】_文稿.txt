	言归正传，重新回来让大家这个思绪回来。你看现在还是需要这个通用人工智能的对吧？不然这种软件问题居然还会卡住我们时间。
	通过人工智能，其实最关键的一个问题就是解决这个智能的问题，就是我们到底什么样叫做智能？这个智能其实大家每个人的定义都不太一样。但是在解决智能的问题之前，我觉得我们需要先搞清楚一个最基本的问题，就是什么叫通用的人工智能，那要搞清楚这个问题，其实我们得先整明白什么是人的智能。就是我们的我们每一个人到底是怎么认识这个世界的。这个其实是一个蛮关键的问题。我相信可能有一些同学，不是非计算机科学专业的同学，所以我简单花20分钟的时间跟大家分享一下，就是我们讲人工智能的前提是我们理解人的智能。人的智能就是人对于这个世界的认识和那人对于这个世界的认识。
	怎么让计算机去认识？那计算机又是怎么样一步一步从一个计算器，就我们在二战的时候发明的这个计算器，然后后来在这个annic，逐渐变成现在的这个计算机。然后计算机通过互联网产生了很多的数据，然后我们有了深度学习，有了大语言模型。一直到今天为止，我们开始发现，咱们是有机会迈向这个通用人工智能的。
	在真正到达通用人工智能之前，我们再来回看一下两个问题。一个叫三个世界，一个叫图灵测试。我相信图林测试很多人都听说过，但是今天分享的这个概念很有意思，也是一个哲学命题。
	就是一个奥地利的哲学家叫卡尔波普尔。在1978年的时候，他提出了一个观点叫做三个世界。这个三个世界其实是一种观察和定义我们现实世界的方法。我相信这种思想实验大家都看过，包括刚刚大脑。但我们这里聊的落地一点就是我们对于这个世界的认识，就是人对于这个世界的认识越清楚，我们就越有可能去造出一个通用的人工智能，对吧？因为我们不可能造出一个比我们还要聪明很多倍的生物。
	在我们无法认知这个世界的前提下，三个世界是什么概念呢？首先波普尔提出的这个概念很有意思，他提出了这个世界由三部分组成。第一部分是绝对客观的世界。就在没有假设我们这个地球上没有人都是动物，都是一些微生物植物，那这个世界仍然是存在的。它是由物理客体和事件组成的一些世界。包括我们的刚刚提到这些物质以及一些能量，比如说我们能看得到的能量以及一些暗物质、暗能量。这些东西这些有机和无机的无机的这些物质就组成了一个哲学当中的客观世界。这个是无论如何没有世界2跟世界3，你都可以独立存在的一个世界。
	然后世界二是什么呢？就是人类开始产生了文字，开始产生了文化。我们会发现我们想要去表达，我们想要去语言模型重要的一个能力，就是把人的说话的这个能力好像给学到了，我们开始表达了。这个表达是怎么来的？其实是叫做主观世界。简单来说就是我们每一个人都是有心灵感知的，有情感波动的那我们感知到的这些世界意识，这种心理状态和过程，我们叫做主观世界。
	主观世界其实笛卡尔就是一个很很有名的数学家和哲学家。笛卡尔其实把这两个世界的边界叫做心物问题，就是2.2元元论的问题。这个世界一和世界二的划分也有一个很有名的哲学问题，也是笛卡尔提出来的。
	在1637年的时候，就是再往前倒两百多年的时候，三百多年的时候，笛卡尔在1637年有一本很有名的书，叫谈谈正确引导理性在各门科学上寻找真理的方法，简称谈谈方法。这个书，或者说这本文章，提出了一个很有名的观点，我相信很多人都都听说过，叫做我思故我在，对吧？就是我思考，所以我存在。然后笛卡尔也引出了一个很有名的东西，叫笛卡尔坐标系。我相信学习过初等数学的人都听过这个词，笛卡尔坐标系。这个东西后面我们会逐渐的去了解坐标是什么，高维向量是什么，对吧？
	那么世界三是什么呢？就是我们人有主观世界了，我们也看到了这个客观的世界。那这个时候我们人开始干嘛了？我们人开始写书写内容，包括我在这儿上课对吧？我们哔哔哔在这说话。那这些东西叫什么呢？就是我们人的主观的这些情感波动，我们希望说出来，我们让别的人也知道，我们开始跟别人交流了，这时候我们把我们的思想的内容物质化，变成一些有形的客观知识。比如说我们人造的产品，文化的产品，语言、艺术品、图书，以及我们现在这会儿大家正在看的手机、电脑、电视，这些东西都是客观知识组成的世界。
	因为人认识了世界一，然后我们觉得我们也可以作为一个创造者，然后我们在创造一个新的世界。这就是我们现在看到的很多的生活当中，这些物体都不是来自于自然界，是由人创造出来的那这些有形的物质，还有一些无形的，比如说我们无法表达的，目前脑科学研究还没有办法整明白的这些东西。比如说一些问题、一些猜想、一些理论，这些东西也都是客观知识组成的世界，只不过这些知识我们还没整明白。
	这里又提到一个很有名的人叫做康德。其实康德是一个很有名的哲学家，他有一本书叫纯粹理性的批判。他其实是在研究世界2和世界三的一个界限。
	那为什么要谈这三个世界呢？其实跟我们息息相关，给我们回想我们要做一个通用人工智能。那通用人工智能做出来干嘛？解决什么问题，吧？那通用人工智能是怎么样去感知这个世界的这是一个很有意思的话题。也是今天我觉得整个这2个小时的时间，大家都可以去去去思考和去领悟的。
	再看第二个问题叫图灵测试，图灵测试其实比波普尔的三个世界还要早。图灵是很有名的一个人，现在计算机科学领域最最高的奖项就是图灵奖。我相信大家应该也都知道这个奖项。
	图灵在1950年的时候，他发表了一篇划时代的论文，然后在这篇论文当中，其实跟通用人工智能沾了一些边，或者说其实是有重叠的。他提出了什么？有没有可能造出一种机器，它具有真正的智能。这个其实是很有意思的。真正的智能他又用了另一个词叫做智慧。真正的智能，其实就是我们现在很多人在探讨的这种通过人工智能。即使现在有一些大语言模型，通过了一部分的图灵测试，但大家觉得它还是跟这个通用人工智能有一些距离。
	通用人工智能其实从现在我们来看，它的技术包括一些什么样的技术能够造就这个通用人工智能呢？我们需要有自然语言处理的技术，对吧？因为我们自然语言处理是为了把人类造出来的这个客观知识的世界这个世界三能够让我们的计算机去学学会这些知识。同时学完之后我们知道人的主观情感也好，人的这个主观世界也好，我们是有一些推理的能力的那现在的大语言模型可能推理能力比较弱，我们也在上一节课讲了，为什么弱，对吧？那能不能让我们的通人工智能的这个程序也好，这个机器也好，具备这种推理的能力。然后更进一步，一个通用人工智能不能只有一个大脑，对吧？我们希望它能影响这个世界，他能像我们的人一样去造这个世界山，他也能够去有有手有脚。
	所以现在我们看得到最新的这个科研领域，把这个大语言模型跟计算机视觉去做结合，跟机器人去做结合。比如说李菲菲老师这个imagination的发起人，他们现在就在研究的这个新的科研领域叫聚生智能in body的AI就是把我们的大语言模型相当于人的脑子，然后计算机视觉相当于人的眼睛，然后机器人学相当于人的手臂四肢，把这三块拼图补全之后，能不能让我们的这个通用人工智能更进一步，这个是目前业界最新的技术。但是回看，其实图灵早在1951年的时候就已经提出了一个很有名的图灵测试。并且在1956年的达特茅斯会议上，他也参与了这个会议。然后我们才第一次有了人工智能这个概念，对吧？这些是我们对于通用人工智能从哲学层面上有一些思好，然后哲学层面上的思考怎么指导我们去做科学。
	这里就引申出一个很重要的一点，就是世界和世界22不是我们所能改变的，也不是我们计算机科学所能去干涉的。就是这个物理世界、客观世界和这个主观世界，心理学、脑科学都不是计算机科学研究的重点。那计算机科学在研究什么呢？在研究的是客观知识组成的世界，就是我们人类的这些思想，这些产物，最终变成了这个客观知识的世界。这个客观世界怎么样能够用计算机去理解他？这个就很有意思，也是我们今天想要分享的一个重点。它也跟我们的大语言模型和我们的嵌入，就我们的embedding和我们的深度学习都有关系。
	我们先来看看这个世界有些什么样的东西。这个世界无非几大类，一类是这些我们能看得到的这些图像，比如说小猫对吧？我们看见的这个东西，一些文字，我们从最开始看图说话，古人没有文字的时候，我们看见了很多内容。然后我们出现了象形文字，出现了甲骨文，然后变成了现在的文字。那这个文字变成了书，变成了知识，那就是人这个世界从世界2到世界三的一个变化。怎么让我们的计算机也能一步一步的从我们的像人一样，从世界2到世界三让计算机能理解和表示这个客观世界的知识。
	这里就很有意思，我相信所有人对于计算机更底层计算机科学的理解都是在有一张很有名的黑客帝国的字符化，就全是010101对吧？好像很多人一听到计算机，第一个反应就是010101这样的一些编码。而实际上计算机的早期就是这样的。
	计算机刚刚诞生的时候，我们也不知道怎么样去表达这个客观世界，我们也不知道能让计算机干什么事情。一开始它就是用来算数的，就是帮你算加减乘除，后来他能做更多的功能了，就是因为他的数据表示能力变强了。他是一开始只会有零一这样的表示，那怎么怎么一步又一步的表示出了更多的这个内容，相当于他能把这个客观世界表达的能力越来越强，这个过程其实很有意思，我们简单来看看。
	首先我们刚刚看到这些图像分成两类，一类就是彩色的图像，一类是灰度的图像。我们最早我相信大家应该如果跟我年纪差不多的话，是有看过黑白电视机的那黑白电视机这种灰度的图像是怎么表达的？其实我们从这个0101怎么又开始能够表达图像了？在计算机，因为最开始计算机是没有这个显示器的对吧？大家都是拿这个波形的显示屏来做显示的，连这种好看的这种液晶显示器都没有。
	那怎么样去表达一个灰度的图像呢？最简单的方式，这是一个m list数据集，这个手写体数字数据集里面其实有一定的一个洞见，就是什么让大家能够去了解我们最早是怎么去表达这种图像的灰度图像？我们通常用右边这个，大家能看到这个颜色，我们现在用很多这个软件都知道可以调这个颜色。调灰度这个东西叫灰度值。我们如果把这个灰度值以一个最小颗粒度，比如说我们以这个手写体数字这个别为例的话，它可以分成256个级别。就我们把这个从纯黑到看不见的这个白分成256个级别。那么这个256个级别就表示了它越来越黑，对吧？那这个颜色就有区分了，就能表示出黑白了。
	最开始的灰度图像其实就是这样表达的，并且我们可以用一个矩阵来表达这样一幅画面。就比如说这幅画面是一个28乘28的这么一个宽分辨率，那这个28乘28就是变成了每一个我们这里看每一个数字，都能表示出它的颜色深浅，也就是我们的灰度值。这个灰度值我们刚刚提到0到255，对吧？那0到255可能我们为了存储方便，我们可以把它规范化到0到1之间。最后你能看到这里其实是一个0到1之间的值，用来表示这么一张灰色灰度图像。
	那怎么样去表达一个彩色的图像呢？其实也很简单，我们刚刚看到灰度值是用来表示亮度的，简单来说，就是我亮不亮，这个是黑色的还是看不见的，这个纯白色的、透明色的。那么学过这个基本的彩色三原色就知道，我们有这个RGB。当然可能在这个计算机里面它用了另外三种基色来表达，但是这三种颜色它也可以有深浅，对吧？我们把这三种颜色的深浅可以表达出来，0到255也行。我们为了更细去细分，也可以把这个256个级别变得更多，这个是没有问题的。然后同时同时我们可以把它用不同的颜色来表达它的深浅，然后这三个颜色叠加到一起就变成了五颜六色的这个世界。这个就是我们人看图像到我们用计算机来表示图像的一个过程。
	图像我们表达清楚了，我们怎么样去表达文字，对吧？因为计算机的发明是从美国，欧洲这样的一些西方国家开始的那我们可以简单讲一下，就是一个很有名的叫做asic就是美国信息交换标准代码这么一个东西。计算机科学的同学可能对这个很熟悉了，它其实是很有名的。这个HRPE就是美国电气和电子工程师协会。大家可以简单理解成在计算机科学领域，这个HAPE就跟中国科学院一样重要。HRPE的fellow就相当于我们的院士。那么HFE这个协会，他们有一个很重要的里程碑，就是发布了这个asic编码。
	这个S的编码最早是在1963年发布的，它分成两部分，一部分就是我们看得到的这些字符。比如说我们有这个标点符号，有空格，有百分号，有26英文字母，有数字，对吧？还有一些看不见的字符，我们去控制计算机的。比如说我们的键盘上面的各种字符。那这些东西，都是通过asic c这个编码，第一次让我们计算机知道原来人的这些数字文字是可以被表达的，并且还能控制它。这是西文对吧？
	那什么是中文？又是怎么样去发展的那最早的中文，我相信有一些看国内教材的知道这个C语言最早有烫烫烫烫烫的一个梗，为什么？就是因为我们用计算机去表示中文的时候，那会儿还不是很先进。我们一开始是通过asic这个编码的扩展来进行表示的。就比如说我们最开始的这个SK码只能表示西文，就我们的英文后来能够表示拉丁文，表示各种国家的语言，但是混用的时候会出现问题。因为大家可能用了同样的一个字符集，用了同样的一个认为在计算机里是同样的一串01。但是它在不同的这个语言的时候，它表达的意思是不一样的，那大家就会有冲突对吧？
	那怎么办？后来出现的这个unicode和这个UTF8到这儿，其实跟pythons就已经有一些关联了。我们知道pythons 2和pythons 3有1个很大的区别，就是在这个代码的表示上，pythons 3是支持这个unico。
	Unicode一个很重要的价值就是它把全世界我们几乎可以表达的字符都通过unicode表示出来了。这些东西第一阶段和第二阶段的发展，使得我们能够把文字，不管是西文、中文各种文字存下来了。但是存下来也只是相当于我们有个记事本，记事本并不能帮助我们更进一步的去给他赋能了，对吧？我们就相当于我们现在刚刚人学会了写竹简，然后这些竹简上的这些字，我们还没有造出文化，还没出现所谓的哲学科学。
	更进一步是什么呢？出现了这个文化编码有词汇表。在这个文化编码一开始想要做的事情就是我们把这些文字用一个统一的编码把它表达下来，就是把它存下来，存下来通过文化的编码，我们还可以去做统计，就比如说这个磁带模型，我们把所有的一本书里的文字，把它存到把它想象成一个袋子，就是你去购物袋，购物袋里面存的不是商品，是一个一个的单词。那个单词我们可以统计哪个单词出现的多，哪个单词出现的少。这个就是最早的一种大家可以认为一种编码方式，one hot它就像一个词汇表一样。但是这个东西可以满足我们统计的需求，但是无法满足什么问题呢？就是我们假设现在想要去搞深度学习，对吧？我们学过了要去从数据里面去学习内容。
	王浩编码有两个最大的问题，第一个就是说他非常浪费资源，就是他把我们的每一个就相当于我们如果有一万个单词，那我们表达的这个词汇表就得有长度为一万的一个向量，然后每个单词占了一位，所以叫one horse。就只有一个维度是一，其他维度都是零，对它非常浪费资源，不太火了屏幕的保护程序。大家有看到屏幕变变颜色吗？
	对，那先这样有关hot编码之后，我们后来发现有有这样的问题。第一个就非常浪费资源。第二个就是说他把词与词之间的关联关系给抹杀掉了。就是我我们都是不同维度的，大家都学过这个线性代数对吧？我有1万个维度，我这两个维度之间是直接没有任何关系的，我是无法对它进行计算的那所以后来我们发现其实这种编码方式还有可进步的，还有可迭代的点。那就开始引入到我们现在的这个词嵌入。
	包括基于深度学习模型，我们会发现像word vector，global vector，我们之前提到过的那这些编码方式就出现了。他们是希望做什么事情呢？就是我不再人去编这个词汇表了，我让这个计算机通过一些巧妙的设计，让他自己去学习我们这些文字语言，然后找出它的关联关系。这个其实是一个由我们只是纯能够把这些人类的文字存在计算机里，到我们做一个词汇表开始统计。到最后我们让计算机去学习我们写的这些内容的一个演变过程。这里就衍生出一个很有意思的点，就是我们已经把它存下来了，然后能够统计了。
	最终我们希望能够把这个物理世界都能够存下来，不只是我们刚刚说到的文字，还有图像对吧？前面提到的甚至有一些语言学的东西，我们表达的这些文化、概念等等。怎么样去表达？然后我们有这么多的知识，就我们人类天天都在造数据，我们这这是一个在微视百科上拿到的2012年的一个热力图，大家能看到这里的这这个点是一个时间的波动。就是随着每一天我们能看到有很多的设备在全世界范围内。然后它月亮就表达它产生的这个热度，你可以简单理解成它产生的数据越多。然后随着时间大家会发现24小时全世界范围内一直都在生成我们的这个客观知识，对吧？
	那有一个量化的数据是什么样的呢？2023年的四月份有这么一个统计，就是现在我们通过移动互联网，通过各种各样的设备，人类产生的这些数据信息是非常多的，那么这些数据我们怎么样能够让我们的AI让我们的人工智能或者让我们的模型去学习他们，学习我们这个世界。肯定不可能首先全部简单的记下来，肯定是能记的。
	然后统计但是是不好统计的，因为大家的数据表示都不太一样，对吧？你是在一个APP里，然后可能你有你自己的数据格式，然后他是一个移动互联网的其他的应用，或者它是一个网页应用，或者是一个物联网设备，然后大家的表达方式也不一样，还有不同的语言。那这些东西怎么样让一个AI去统一的学习它？那这里就涉及到一个很有意思的概念，叫做表示学习。我们今天这个主题叫embedding我们现在发现其实embeds和表示学习开始有关联了。
	表示学习其实跟我们刚刚讲的哲学面积是一样的。就是表示学习其实是希望让我们的算法，让我们的AI能够自动的从一些原始数据当中学习到一种表示的形式，或者说一种特征的表示。简单来说就是说人话就是这有一堆的数据。然后我希望有一个算法，你别管他具体是怎么学的，反正他把这一堆数据里面的这些重要的东西学出来了。并且学出来之后，他把它变成了一个数学问题，一个高维的特征空间。然后这个特征空间里面的这些不同的首先比如说我假如是一个一千维的空间，这1000维空间当每一个维度都学的很好。那这样的话大家知道如果这个1000个维度每个都学的很好，那么我们就可以通过这个里面具体的某一个向量跟向量之间的关系，去找到他们的一些相似性的关系，或者做差异性的关系。
	这个是表示学习的一个概念。那什么叫embedding呢？其实embedding是一种表示学习的形式，并且我们很直观的可以想象，刚刚我们看到这个世界每天都在产生这么多的信息。我们有了表示学习，我们能学。但是它可能不是一千维的，它可能是1000亿维度的那一千亿的维度怎么样能够比较好的一个比较好的成本，或者说性价比高的方式为我们所用的那这个时候就有embedding，有嵌入的很重要的这个概念，或者说这种形式出现了。
	Embedding我们谈到这个概念的时候，它最重要的第一个特征特点就是说把一个高维的数据能够映射到一个低维的空间当中。比如说我们能不能把这个1000亿维的东西变成一个一万维或者说十万维度的，给它降低很多的维度，并且它的特征还保留着，这个就是embedding的一个价值，表示学习和embedding，我们可以把它简单理解成都是希望能够去表达我们这个世界，只不过表示学习的特点是我要把它学好。Embedding的特点是说你学好。但是我的我想我希望他以一个比较低的成本能够表达出来，即使他可能会有一点点的损失，这个是他们的不同的切入的重点。
	Ok我们来看一看表示学习是怎么样让我的计算机能够逐渐的去学这些客观知识。首先我们分成三类，还是刚刚看到的。第一类是我们的图像数据，我们知道有很多的图像识别模型在过去十年使我们能够分辨出自然界的各种各样的物体。包括不同的人脸的识别，物体的识别。然后文本识别的模型使我们能对不同国家的语言，甚至一些很小众的语言，一些古文都能够进行识别学习。然后我们这些互联网的数据造成了我们的造就了我们的这个语言模型以及我们的大语言模型。这个是最终都学成了一个高维的特征空间，对吧？我们都知道这是一个结果了，这也是一个流程。
	那怎么样去学呢？这里有一个小短片是腾讯做的。我觉得他的视觉化的表达。
	为什么说用好跟大家分享一下不开向量的数据库呢？回答这个问题之前，我们先来理解一下什么是向量。这是一个苹果，但在发明苹果这个词之前，人们怎么描述它呢？颜色、大小、形状、纹理，找到更多的特征，就能对苹果的定义更清晰。把这些特征用数字表述就可以得到一个数组，就是向量。当复杂的图形变成了计算机熟悉的数字，他就认识苹果。了当新的苹果出现，计算机还能认出来他吗？当然，因为这些图像在向量空间中离得最近，相似性最高。
	今天让我们惊叹不已的人工智能往往通过上千个向量维度来学学习训练他们就像是AI大模型的眼睛。当AI大模型遇上庞大的向量数据，这组黄金搭档如何让硅基生物更聪明呢？以大语言模型为例，简单来说，在训练时未给它的词句都会先转化为向量数据。当训练数据里出现多组类似的语言时，在向量数据高倍空间相近的词汇就会距离更近，语言模型就可以逐渐捕捉到词汇间的语义和语法。比如他会更明白苹果和西瓜语义上接近，但是和公交车相差甚远。
	接下来模型需要对上下文进行理解，此时transformer架构就开始发挥作用。从每个词自身出发，观察和其他词之间的关系权重。例如这句话里很好吃，和我关系权重最大，权重结果被当做新的维度记录下来，一句更复杂的话也转化成了带权重的向量语言。模型经过查询计算生成权重最高的答案输出给你一次问答就完成了。
	实际上大模型训练推理过程更为复杂，他们需要处理如文本、图像、音视频等大量非结构化数据，并转化为向量数据进行学习。这些数据的规模动辄过亿，向量维度可能高达数千。在选择数据库时，传统数据库只能进行行列检索，一一对应，再说出精准的答案。但向量数据库则是专门为非结构化数据检索而设计，它将向量数据组成一个立体高维空间，在空间中进行模糊检索，能够快速输出权重最高的答案。
	好，这个短片很有意思，它其实像。
	我们为什么说用好大幕。
	展现了一个很有意思的点。就是说刚刚我们讲的那些内容，就是我们人认识这个世界，是有很多你说不明白的，这个脑科学也没研究清楚的。就我们人去对一个我们叫客观世界的认知，是很聪明的。
	就比如说这里看到的这个苹果，我们人是怎么样对苹果进行学习的，我们会去描述它，对吧？因为我们人要跟人交流，我们会找到什么呢？找到颜色、找到大小、找到形状、找到纹理。假设就这四个维度，这四个维度其实就是一个四维的空间，对吧？那这个四维的空间可能就能够区分出苹果梨香蕉这种不同的水果了。但是当我们想要表达水果和蔬菜的时候，也许这四个维度就不够了。我们为了去学习这个客观世界，这个维度就会越来越多越来越多。
	然后变成这些维度是一个一个的高维空间。这些高维空间当中存的其实就是我们这些向量数据，对吧？那我们人其实是完成了这个特征的学习。但是怎么样计算机去完成这个特征的学习，是表示学习研究的一个重要内容。然后在这个过程当中，其实我们除了刚刚看到的这种苹果以外，这种图像以外，大家的还会有文字，找到这个比如说有文字，这个文字它也可以表达成我们看到的这样的高维向量。这样的高维向量关键是要把这个语义给学出来，对吧？
	我们要去学习这个语义语法，所以我们有了各种各样的语言模型。然后这语言模型经过我们前两节课的学习，我们也知道了有注意力的机制有3。Former他开始不只是学这个词本身，就是还会去学他的上下文。这个上下文的信息引入进来之后，逐渐的我们发展出了这种大语言的模型。
	然后大语言的模型现在只能解决语言对吧？语言和图。比如说这里的苹果和之前看到的那个图像的苹果，能不能变成同样一个我们理解的语义。就比如说最经典的我们谈苹果的时候丢失了上下文，我们不知道它到底指的是那个水果，还是指的是那台手机或者是这台苹果电脑。那这个时候我们能不能把非结构化的数据也拿去学习。
	通过一步一步的演进，我们让计算机越来越能够表示和学习我们这个客观世界和我们的客观知识组成的世解，对吧？那这个是一个过程。表示学习其实他的一句简单的话来说，他核心的目的就是为了让计算机更好的去理解世界。那怎么样理解？我们前面有提到还要能够自动的去从原始数据当中去学我们的重要特征和结构，这个是非常重要的。因为这个世界太复杂了。我们在学大语言模型的时候就已经讲过了无监督数据，无监督的学习的价值。那还只是语言。
	但是这个客观世界有太多不只是语言可以去归纳的内容，还有很多客观知识，它不是语言。那怎么么样去学？比如说这个乐坤世界模型？我们让image in bedding也能加入进来去学图像，图像和这个语言能够联合在一起去学习。
	那怎么样去评价一个表示学习学的好不好呢？通常有三个维度，一个叫可分性，就是我们找的这个特征好不好？比如说我们找的这个特征是颜色，而不是红色、黄色、绿色，对吧？那红色、黄色、绿色最终都可以被归纳成颜色，这个就是一个颜色，就是一个好的维度。并且它还降低了最终的特征空间的维度。
	第二个就是说他的可解释性。就如果我们一个表示学习把这个特征空间建得好的话，理论上我们是学到了一些语义级别的信息的，这个在语言模型上我们也看出来了，这些语义的信息可以通过我们在向量空间当中的一些运算表达出来。这个运算就比如说我们这里的推理、类比和关联，都可以通过一些特定的运算服务，去把人脑中的这种语义的东西逐渐的去通过这个高维的一个觉得好的特征空间，去用计算机数学化的语言去表示出来。
	那这里就引申出来了一个比较大的问题，就我们已经从这个形而上也好，从这个理论的角度我们整明白了计算机逐步的能够存储，然后能够把我们的这个世界存下来了。我们通过刚刚提到的各种表达方式，但是存下来之后，我们现在有一个很大的麻烦。或者说我们为了达到通用人工智能要解决的问题就是我们到底要怎么样去学能够很好的自动的从这个原始数据里面去学出来。这里就提第一个也是我们最近十年最火热的一种表示学习的方法，就是深度神经网络。这里又提到了很久，深度神经网络其实是一种高效学习数据抽象特征的这种表示的一种方法。并且也就是最近十多年我们看到了深度学习。在各种各样的表示学习的内容上，不管是我们刚刚提到的图像语言等等，都体现出它巨大的价值和它的学习能力。
	在我们聊这个表述，学习这个词，很重要的一个点就是这里。我们看到2012年有一篇综述性的论文，叫做representation learning a review and new perspectives。This is a penuel一座发表的一篇论文。而这篇论文它比较全面的回顾了无监督的这种特征学习。
	我们刚才提到了无监督就是没有标注，我们能够自动的从原始数据当中去学。那那这种学习方式，我们刚刚都知道他他这个表述学习只是一个学习方式，对吧？它的核心就是要能够自动的学。所以它就涵盖了很多不只是我们的深度神经网络，包括像我们的一些概率模型、自编码器，包括一些在什么航空领域的流行崛起，深度网络当然也在这个里面，都是表示学习的一种。只不过我们最近十多年主要在用深度学习，这个好处也不言而喻。因为它能使用好我们的这个算力和我们的数据，深度神经网络的算法和框架层面都有一些进步。这个是很重要的一个概念。
	大家知道深度学习其实是表示学习的一种，而深度学习本身也没有什么特别的表示，学习是很重要的一个概念。他下面除了深度学习以外，还有以前的统计机器学习，包括我们的流行学习，都是属于我们希望让计算机能够学会人对于这个事界的认知方法。这里再提两个很重要的点，也跟我们强相关。第一个就是说在1998年的时候，这个乐坤，刚刚我们也看到了这个手写体数字，这个数字叫m list。这个数据集是1998年的时候乐坤的一篇论文提到的那这篇论文1998年为深度学习打下了一个很好的基础，因为在这篇论文当中第一次提出了怎么样基于这个反向传播。
	对我们深度学习很重要的一个学习方法是反向传播。就我们有前馈网络对吧？我们讲transformer的时候，他抛弃了其他的这个RN的架构，用一个多头注意力机制再加上一个前馈网络这样的一个方式。前馈网络在深度学习里面是最早被引入的那这个前馈网络要去学它，就需要能够反向传播迭代这个学习的参权重的参数。然后怎么样去迭代它呢？通过梯度下降这样的一个优化方法。
	在1998年的时候，乐坤提出了一个多从感知机的多层感知器的这么一个模型，叫MMLP，用来识别手写体数字。所以其实1998年的时候，我们整个人类计算机科学才开始把手写体数字这个识别的事儿干好。其实最近二十多年计算机的这个识别表示学习进步的非常快。大家可以想象二十多年前我们刚刚能认识这个手写体数字。到现在我们基本上让我们的计算机可以认识客观世界上的这种有形的物质，基本就都能认识。
	然后06年的时候，平城提出了一个叫做TSNE的一个降维方法。这个降维方法也是被广泛应用的一种降维算法。它能够很好的把高维空间的数据映射到一个低维空间，并且还能够保持这个数据点之间的相对关系，就它的语义信息尽可能保留住了。这是一个非常成功的基于概率学概率统计的一种方法。待会儿我们的实力实战当中也会去讲到这个方法。
	从我们刚刚的定义就知道，TSE其实是属于一种embedding的方法，对吧？他能够去做这个降维我。这里还提一个很有意思的点，就是我们的深度学习作为这种很很厉害的，最近十多年一直在被大家不断的在这个学术方向上不断去研究。
	深入的一个开篇就是我们的alex snet。2012年alex snet在作者其实是hinton的一个学生。他在2012年的时候发布的这个神经网络，第一次把我们关于这个识别图像给我们在image net上面去做分类的识别。给我们识别猫、狗这种东西。
	Alex net是把当时最好的这个方法直接提升了十几个点。这个就跟GPT出来之后，GPT这个3.5出来之后，大家感觉很多以前的自然语言处理的难题好像就没有那么难一样。那那anything net是起到了一个非常重要的里程碑的作用，而这篇论文就是alex net这篇论文。我们这里看到这个圆圈特别大对吧？因为他被引用了将近14万次这篇论文。
	然后这个小哥这个alex其实他还后续发表了一篇很有名的论文叫drop out。这个也是深度神经网络能够被不断的去叠加，然后去学的更深。我们让深度神经网络能够去容纳这么多知识。很重要的一篇成果做个out，也是一个引用四万多篇的一篇论文。这里还要提到一个人，为什么我们专门要提到alex net这里alee snet这个文章的二作，这是一个三个作者写出来的一篇里程碑作品。
	在深度学习领域。这位小哥也不叫小哥，这位大佬对他是当时跟hinton还有alex一起写的这篇文章，他现在是OpenAI的联合创始人和首席科学家。我们前面那节课就讲GPT的这个家族的变化的时候，我不知道有多少人读过这个论文，我相信大家都看到了他列在这个GPT3篇论文的通讯作者这个位置。他其实是很厉害，他本身的学术贡献也很多。我们之前第一节课学到的这个sequence to sequence这种很好的学习架构，是他的一篇学术论文。然后他也参与了dropout和worth west这两篇重要里程碑的文章的贡献。所以大家如果要去在embedding和深度学习相关的领域想去深入研究的话，找到这几位是可以很好的快速的去做学习的。
	然后他还参与推进了两个很重要的事情。第一个就是他在google，就他之前在OpenAI之前是在google在google的时候，他跟jeff dean，就是google非常厉害的大佬级别的人物一起合作参与了TENFFFW这个项目。他其实把这两波都赶上，他参与了tensor or flow这个项目，并且TECF flow这篇论文他也在当中。然后同时他后来去OpenAI之后也顶住了压力，对吧？我们上节课有讲到一副。这个大模型的家族在GPT3.5轰爆这个世界之前，其实他是研究的重点。但它顶住了很多压力，包括来自于资金的压力，各种学术圈的压力，支持GPT系列的研究。所以很厉害的一个人，大家可以去稍微去了解。
	我们话说回来，还是在研究表示学习，他表示学习这个世界上的知识这么多，那就会有他们三个的一个很有名的一个名场面的照片。当年深度学习刚刚出现的时候，也就十年前，hinton指导这两位在alex net这个期间的一些作品，包括做part。我们知道这么多的信息，我们要学，我们首先先不去想怎么学得会，深度学习可以帮助我们学得会对吧？只不过这个学会之后就会有一个很大的问题是说学出来了维度太高了，对吧？
	就有一个很有名的名场面，就是张无忌去学这个太极拳，对吧？学太极拳就问你学会了吗？好像学会了，记了好多东西。后来再问他，你还记得吗？我都忘完了。
	这是一个很有名的场面，对吧？这个过程其实是希望能够一开始学的很多知识全部收进来。比如我们以前说读书，要把书越读越厚，后面再越读越薄。这个越读越薄其实就是一个提炼的过程。但这个提炼的过程，embedding是其中很很有名的一个技术。能干这事。
	但是我们直观的来讲就是这么高的维度，我们怎么样知道我们学没学会如何评价我们这个东西学没学会呢？有没有什么直观的方式？就比如说一个最简单的道理，就是我们都背过那个四六级的单词表，但是你都记下来了吗？你怎么知道你记下来多少单词？你是不是真的学会了你是只记下来这个单词的一个意思呢？还是说你其实把英语学会了？
	这个是一个很有名的词，就是我们所有人都只记得第一个单词，可能其他的都记不住了。所以记顺序本身并不重要，记下来多少单词也不重要。最关键的事情是真的把英语学会了，对吧？
	那什么叫真的把英语学会了？我们肯定不能像最开始聊的，我们搞这个一千一维来学。我们需要用既要有表示学习的模型，同时我们还希望这个模型学出来的特征空间尽可能的小。所以这个时候我们就会发现，表示学习的这些学他们的这个模型和embedding开始有关联了。
	就因为我们认为学下来非常好，但是我们还希望学下来能够直观，能够以比较低的成本去直观的告诉我们学会了。这个时候我们看到这个TSNE是一个非常好的工具，一个降维的工具。他能把我们这个高维空间的学到的内容，在不影响它的分布的情况下，尽可能不影响它分布的情况下，降到低维空间。这个其实就是一种典型的embedding的手段。我们不管是图像的、文本的还是互联网的这种高维的，只要到了这个高维空间，就是一些特征的抽象了，对吧？就已经脱离了我们的我们看到的这个形象了。他通过这样的一些embedding的方式，让我们能够很好的去展示这个学的效果。所以embedding的价值就体现出来了。
	最大的第一个价值就是降维。因为在刚刚提到的几十万维的这个维度上，我们人是没法去看好坏的。我们也不可能穷举我们这个特征空间当中，我们每一个数据它的向量到底好不好，也没法去做直观的查看，降维通过embedding。第二个就是说它也能够去捕捉一定的语义信息。就比如说我们刚刚举到的这个关联关系，包括它的适应性，它的泛化能力和可解释性。这些都是表示学习本身就有的特点。那么embedding当然也有我他们的侧重点不太一样，但这里要记住的一个点就是说我们嵌入非常重要的一个能力在于降维那怎么样认为这个embedding是一个好的embedding，这里我们可以看到有几类比较典型的。
	第一类就是这个word to pt提出来的这个word invading的概念，就是我们怎么样能够把我们有这么多世界的知识，有这么多特征空间。在我们大一统之前，我们先分开学，然后有不同的学习的模型。我们刚刚提到的图像识别的模型，语言识别的模型，然后文本的模型。那这个语言的模型怎么样我们认为他学得好呢？Word invading是一种直观的方式。这个直观的方式首先我们能看到这里几个示例，就他们这个相似的语义之间做差也能得到一些相似的关系。
	就比如说我们用这个king减去这个，就用国王减去男人加上女人啊，得出来最近的这个距离的向量居然是这个皇后。这个其实是一个很直观的去表达。我们学好，大家可以我们再来理解一下word embedding。首先world embedding本身它没有去改变那个表示学习的模型。我们之前看到两大块对吧？Word embedding只是为了让我们理解这个模型学的还不错，他用一种embedding的方式让你理解了类似的像这个单词里面的时态，包括国家和首都的关系，都可以通过word embedding的方式，以一个很低的维度，人能够直接看得到的这样的一个方式去直观的呈现。
	那还有什么呢？比如说我们在单词维度，我们在书这个程度，这个是不同类型的书我们都会去做这个分类，大家看图书馆里都会有分类。不同维度的书，它其实也能去通过imagining的方式去把它降维到一个二维的空间里面，让我们理解。大家看到不同的颜色，其实不同的类型小说什么的，包括这个科技的一些刊物。通过这样的方式，我们能知道我们这个模型学的好不好，包括这个手写体数字的识别，这个是一个很典型的一个TSNE效果很厉害的点，这个是来自于之前我pencil flow那个课里面的一个图。然后也是用pencil boy的这么一个可视化的去查看我们的这个embedding的一个web端的一个软件，也是TENSOLO的这个家族系列当中的一个软件。那它其实干了一个什么事？我们刚刚知道手写体数字它的表达方式相对来说比较简单，就是一个矩阵。
	这个矩阵里面的每一个数字是一个灰度值。那这个玩意儿我们需要用复杂的深度学习去学吗？1998年的时候，乐坤用一个MLP这个多层感知器能够去做这个分类，对吧？但TSNE自己其实也能去做这个分类，就说明TSNE这个evading真的非常好用。就hinting的这篇文章真的是这个大佬的这个功能工作学术成果真的是能够被长期的去使用的。所以接生意能够帮助我们去很直观的去看啊，这个是一个在三维空间去展示不同手写体数字。
	这里我们就会再总结一下，evading就embedding其实是表示学习的一种特定的形式，对吧？为了降维embedding也不只是word embedding，我们还能看到刚刚的imaginary，就是图的这种embedding。然后除了这个以外，其实还会有一个非常重要的概念，就是我们的大家知道社交网络？七度空间理论，我们在全世界只需要最多六个人，就能找到任何两个陌生人之间的关联关系。Graph embedding也是一种很重要的这种invading方式去学习图的结构。包括现在我们看到一些图神经网络等等，都可以通过graph in bedding去捕捉节点间的结构和关联关系。
	所以总结一下，embedding是一种表示学习的特殊的方式，他的重点工作是把高维的数据映射到低维空间。然后embedding也让我们去理解一个模型到底学的好不好，是一种很很直观的一种方式。然后好不好可以用二维三维去表达。如果二维三维不够了，那我们可以用刚刚说的这种wording baiting向量之间去做这个操作的方式，比如说加减等等，去表示出我们到底学的好不好。
	我们现在聚焦到这个warning baiting，因为大语言模型更多的是在文本这个层面上。Word in binding刚刚有提到的，他能去表达出语义，就是我们能学到这个语义，然后这个语义的相似度也能够通过这个word invading展示出来。然后他还能够去做一些什么事情呢？就是上下文的推理，对吧？就是我们能够通过这个学习的过程当中去了解到他的一些上下文。然后包括这个文本的分类和情感的分析，这个我们在待会儿的实战里面也会有。
	然后机器翻译和生成模型在这里大家就会有一个直观感觉，就好像word invading就已经很强了，对吧？那word invading都这么厉害了，那我们还需不需要语言模型？他们之间有什么样的关联？首先这里有一个，当然所有的这些概念它都不是一个定死的概念，这也是我希望通过这一节的分享传导给大家的。就是大家不要被一些新的这些AI的自媒体文章，或者一些新的这些东西去去框定一个死的概念。
	就跟我们学单词一样，就是大家知道最开始学单词最笨的方式就是老师告诉你这个单词它的中文直译是什么意思，你就死记硬背对吧？最开始的warning mAiling其实是这样的，它是一个静态的，然后它就是捕获一个单词本身的一个含义，然后都还不太能理解语义。我们学单词也是，后来我们会学这个例句。几句事例的句子，通过这个句子我们能逐步的去了解它的一些语义语法。
	这个是最早期的一个world embedding。这也是大部分的embedding算法就在大语言模型出现之前的一个实际的状况。所以它通常是一个静态的东西，比如说TSNE，他学会了就学会了。
	你不管怎么样去重新用同样的数据，通过一次embedding的方法，它都是一样的向量出来的那语言模型其实不是的，语言模型，尤其是我们讲这个GPT系列的语言模型，它更多的是一个预测词序列的一个概率模型。就跟我们之前讲GPT对吧？它重在生成，并且它是动态的。就比如说我们现在看的这一千本书学出来的一个语言模型，你再给我另一千本书，他可能学出来的这个模型就不一样了。它的权重会去做调整。但是invading相对来说是比较静态的，并且他更多的能够理解和生成文本，就他能理解的更多，比如说就跟我们现在用TIGBT，他好像真的在逐渐理解我们的意思，但是embedding做不到这一点。
	这个也能从今天和周日的课程里面能看得出来，就embedding的适用场景，它的开发场景和这个语言模型的开发场景是不一样的。怎么样去区分他们？怎么样去理解他们，以及我们今天讲到这么多概念，我画了一个简单的维恩图。首先我们刚刚谈的所有这些东西，都可以算到表示学习的范畴里面。我们再回顾一下，今天希望通过这2个小时时间，让大家真的把这个事儿整明白。因为你要把表示学习整明白之后，其实所有的人工智能的东西都在这个大的框架里面了。
	表示学习其实跟我们目前学到的所有知识都有关联。深度神经网络是一种高效的去学我们这个数据的方式，对吧？Embedding也是。比如说我们的TSNE，他没有去搞这个机器学习，然后他直接就能把我们的手写体数字在三维里面区分出来，这个很有意思。然后我们的语言模型语言模型其实也是在学我们的文字语言对吧？我们的文本，我们各种类型的text，他们之间的交汇形成了什么呢？形成了我们的大语言模型。这一点其实非常有意思，怎么样去理解？
	首先我们今天学的这个embedding历史上有很多的发展阶段。我们倒着往前看的话，现在的embedding包括我们今天要用的以这种大语言模型的embedding为主。我们再回忆一下，包括大家课后可以去想一想，我们的GPT模型里面embedding放在哪的对吧？有没有embedding？然后它跟我们的之前的word weft有什么区别？这两个箭头的位置是很有意思的。
	首先我们的大语言模型，因为它充分的理解了我们的这个训练语料，我们给了他非常多的所有的书籍，维基百科、redit, 包括互联网的爬虫。他几乎上把我们的所有人类的这些文字类的东西都学完了。学完了之后，他当然天然的能产生一个向量去表达出来我们的一段特定的话。所以这个大语言模型的embedding的这种模型，就是用大语言模型去生成一个embedding模型是非常主流的，并且成本也比较低。
	这个是目前我们用大语言模型开发应用的时候最常用的一种方式，就一定的方式。那它也能解决问题，它能解决什么问题？第一个问题就是我们知道大语言模型很贵，对吧？它的token很贵。那么我们有没有一些简单问题可以不去大语言模型里面走一圈，那其实是可以的，通过embedding的方式，我们第一能把自然语言变短，token数变少。第二有些问题其实压根都不需要大元模型去做那些复杂的推理，他可能只是去比较一些相似性的那这种时候我们用大语言模型的embedding方法，embedding模型就能够去解决了。能算出你这句话跟剩下的那一千句话里面哪句话最相似，或者说他的情感是比较类似的。他都是表达的积极的或者说消极的那这个是大于基于大语言模型的这种embedding，就比如说OpenAI开放的这个embedding的API，就text embedding ADA第二代的这个模型，这就是我们今天也会用的一个模型。
	再往前倒，没有大语言模型的时候，我们通过这个word vest，通过这个global vector去做这种skip NN scrap。你可以理解成就是学的没那么多，他就在一篇文章里面，然后可能能够反复的去学之类的。这个其实是一个比较偏静态的，你学完之后它就是一个静态比我们刚刚提到的，他不太能够随着大语言模型的语义理解没有那么强。
	简单来说大家可以这么理解，这幅图里面还有两个小的圆圈，一个是这个LSTM，他是我们之前讲过的一种RNN的一个进化的版本，它能够去学习这个序列数据里面的一些知识。还有这个TSNE，它其实没有跟这个语言模型和我们的深度神经网络发生太多的联系，而且就是一种统计学习的方法。不过把它研究的非常好，所以保留了高维到低维降维当中的这个分布。这幅图如果大家真的整明白之后，我相信对于人工智能就会有一个比较全方位的理解了。
	我们再提一个小问题，也是一个课后小作业。就是大家都在问这个乐坤的这个世界模型，就是世界模型在这幅图里面应该怎么画呢？大家其实可以去思考一下，这个世界模型在这个维恩图里面，这个二维的维恩图能不能够充分的去表达它。好吧？
	好，最后再说一下，说了这么多好处，那它有什么坏处？就是现在的大语言模型也好，我们的大基于大语言模型的这个嵌入模型也好，都不能解决一个偏见的问题。我相信大家去看过OpenAI的文档，或者去看过这个GPT，或者说这个GPT4也好，3.5也好，他们都会有大量的说一些什么样的东西呢？
	说我们现在这个语言模型其实也有缺点，然后他会出现一些偏见的问题，然后我们再尝试解决。然后语言模型也迭代的很快，它会有前处理后处理去解决一些问题。但是它不管怎么样去尝试解决，它的效果都不算很好。但尤其是这种embedding的模型，它相对来说静态？固定了，他不太会长期迭代。像我们用的这个tax embedding ADAA，它迭代的频率远远低于这个语言模型。那么这种embedding的模型它为什么会有偏见？其实通过我们刚刚那个分享，我相信大家去细想一下也比较简单。首先说到头都是去统计的这个词的概率分布，他是说这个词跟这个词经常一起出现，那么我就认为他们的关联性好，他们是相似的那你要知道人是有偏见的。
	我们回到这个最开头的哲学问题，我们人造成了我们的人生产出来的这些客观知识的世界，对吧？这些有形的物质，这些艺术品也好，文章也好，这些音视频内容，电影电视剧，这都是我们生产的那人就是有偏见的，而且人的偏见也是根深蒂固的，并且他还会加深这个偏见。这些偏见产生的这些死的知识，就死的客观世界的知识，他就摆在那儿。那你让我们的一个AI去学它，学出来就是这样的对吧？有一个很有名的词叫近朱者赤，近墨者黑，对吧？那AI是没有我们这种世界2？没有这种主观世界，没有这种吾日三省吾身的能力的那他就是一个概率模型。
	说到头那你有偏见，你喂给我的数据就有偏见。我如果学出这个偏见来了，某种层面上说明我学的好啊，我你就是这样教我的，对吧？但是这个不是我们要的。
	所以像乐坤当时也提到，就是有一个很有意思的点。就是说有些事情，比如说有些真理也好，有一些最新的科研进展也好，他不一定服从这个概率分布。换句话说就是人类的知识是有权重的对吧？
	就比如说爱因斯坦他发明了这个相对论的时候，可能大家都不懂，大家都不认可，但是他是对的。那你不能因为认可他的人少，你就觉得他不对的，他只是有个认识的过程。然后慢慢我们就会发现其实是这样的，就是知识是有权重的，但这个权重人都没有把它捋明白。因为在我们真的理解这个东西之前，我们也不能去判断它的对错，我们也就无法在我们生成的这个资料上去把它的权重修正过来。这个是一个很难的问题。就是我相信如果通用人工智能真的出现的话，这个一定是一个需要被解决的问题。因为我们让AI去学东西，就不能简单的是这样的一个概率分布的学习方式了。而是像乐坤说的，比如说我还能结合这个图像，未来也许我们还能结合这个视频，这个逻辑也很简单，首先图像和视频它包含的信息量更大，然后我们现在有了AIGC，我相信未来我们生成图像视频的这个内容也会越来越多。
	自然的我们因为文字的内容已经学的差不多了，没什么可学的了。那这个时候自然就要去学多多模态的多媒体的这种不同承载方式的客观知识。这个时候可能我们只学文字就显得有点不足。当我们有多模态的不同的人类的客观知识一起学的时候，大家想一想其实就是在消除偏见。
	就是我们刚刚讲的苹果的例子，也是它有一个直观的图像的时候，我就知道你聊的那个苹果是水果，还是你聊的是那个手机。因为它多了一个信息渠道，它表达的信息量更大。这个其实就是一种自然而然的发展方式。所以想象得到就是世界模型一定是未来的一个发展方向，这个也是毋庸置疑的。我们一定但它不一定叫世界模型，但多模快的学习一定是一个发展方向。
	Ok这个是我们的这个理论的部分，我们停下来五分钟大家提问，正好我也喝点水，看看大家有什么问题。我们关于目前讲的这个部分，我们接下来就要开始上手了，然后大家也需要去准备一下。如果想要跟着这个直播的节奏来学的话，要准备一下这个电脑。对，我们要开始动手实际跑跑代码，干点实际的工作。
	有的同学写说的很有意思，问牛病，肖申克救赎的主角是谁？小帅。为什么不直接叫张伟？我看到有人在提这个问题，因为他不只是降维，对吧？我们再再想一下，embedding是一种表示学习方法对吧？但它有一个很重要的功能是能降维对。
	还有一个同学提到OpenAI的embedding，对于同样一句话貌似也不是一个定值。Embedding为啥便宜？因为它不用跑大模型的那么多的算力对。
	有一个同学讲的很好，就是他把这个例子跟嵌入式的这个开发举了一个例子。对他其实很重要的一个点就是它不是降维，它是用一个低维去表达高维的内容。什么叫降维打击呢？就是我我的打击水平没变。对，只是你这个维度太低了，所以我能找到你的弱点对吧？这个我想想三体大家看过对吧？
	最后这个歌者甩出了一张二相框，把这个太阳系变成了一个二维平面，这就是典型的降维打击，为什么？因为它在更高的维度，首先这个世界就是这个世界，我们为什么今天一来讲哲学命题？那这个世界就是这个世界，我们现在努力在把世界三给表达出来，但也许歌者已经把世界一表达出来了。
	这是为什么马斯克要去成立X点AI这家公司，他想证明他他已经想说我们整个世界为什么就他当时在这个采访里面，他原话就是为什么我们遇见外星人？因为可能我们整体就是一个simulation，就我们整个世界就是一个电脑游戏而已。我们找不到边界，找不到其他的同类，是上面那个游戏玩家有意而为之的，这个实验环境就找不到。歌者其实也是一样的。就如果我们整个世界，我们现在人类认知的这个世界，一世界、二世界三都已经被一个更高维度的生物也好。你说叫神也好，叫这个佛也好，他能够把世界一世界二世界三都用一套特征表示出来了。我们不是说存下来了，是他表示出来了，那他就能进行操作，那这个就很吓人，对吧？歌者甩二向步就是这么一个操作，这个是真正的降维打击对。
	所以它不是一个降维，对，降维是它的一个特征，一个特点。我们在9点20分的时候正式开始，还有两三分钟。
	有一个同学问他能不能理解成一个东西可以从多个维度来表示。比如说苹果的颜色大小等等，维度越多越能清晰的表示一个事物。其实不是的，就是维度越多不是就不是维度越多越好的。这个我们刚才讲词汇表现one hot的时候就已经说明了，就是维就比如说我们都在聊水果，你非要把所有的水果，不同类型的水果我都甩到不同的维度上去，那他们不就没有关联关系了，对吧？
	你大家学过这个笛卡尔坐标系，对吧？XY轴你XY轴里面X轴上的东西怎么样也甩不到Y轴上去，你怎么甩呢？如果你的X值不一样的话，那你你这样是有问题的那他们之间的关联关系就没有。我们其实我们要学的是水果，我们学的不是苹果。当我们要学水果的时候，我们就不会把苹果当成一个维度了。我们找到了颜色大小，但这个过程是人类的表示学习的能力，对吧？而这个也只是说我们把水果研究明白了？
	为什么生物学整的这个界门纲目科属种？大家想象一下为什么界门纲目科属种？是我们生物学这个学科尝试把生物整了几个维度来研究，这个其实是本身人就是在自己就是一个表示学习的一个机器。然后每个人都在抽象这个世界，尝试用不同的维度去做解答。然后你解答的越牛逼，你越能够用很少的分叉的低维的表示。但是这个语义信息又没丢掉，你表示的这个世界本身又没丢掉，那么你就是厉害的对吧？因为你的内涵更丰富，你找到了更巧妙的位置。
	有人说我们碰到一个问题，问一个大问题，embedding对不上，是不是需要把问题变成一个一个的小问题？这个是一个很好的思路，可以去尝试一下。因为有可能那个大的问题本身它的语义提取的不好，所以直接去问就不行。就比如说我们刚刚看的word in betting。你要用这个king减去man加上woman得到这个queen。那你如果来了一个什么哪个地区的什么king，然后他减去了比如说中国的，不是叫中国这个欧洲，这个英国的king减去了美国的男人，加上了这个葡萄牙的女人，等于哪里的女皇？这个表达不出来，对吧？因为切的不是很细的，不好对。
	老师我们需要学习算法内部的公式或者机制吗？这个看你的诉求。对，就看你的诉求。就是你要跑我们整个课程里面的所有的代码，然后能够去做应用开发，去理解这些代码，是不需要理解这个内部的公式或者机制的。但是理解了这个公式和机制，能够让你举一反三，对吧？能够让你不只是跑这个代码，你还能把这个代码改吧改吧，用到你的场景里去，这个是一个点。
	就比如说刚刚那个例子，就大问题猜小问题。你举一个刚刚我说那个例子你不就明白了吗，对不对？就是你猜的不就你那个大问题太大了，他学不到那个语义的，他那个embedding表示出来的那个向量是是是没学到的对。
	那么我们这个提问环节就到这儿，我们待会儿开发上手完了之后也能提问。对，我们先继续。首先开发环境，我看这个群里各个同学都在问，我相信这个课上完也还有人没有把开发环境搭好。但是没关系，第一最重点的先说清楚，我们会用pythons来调用这个OpenAI的API和调用nature。
	第一是因为pythons确实是一个很好学的语言，我也建议大家就稍微通过GPT来学pythons是可以的。我也看到有朋友发的通过GPT去学这个Cooper natives这个容器的管理平台，这个云原生的一个容器管理平台，这么复杂的一个项目，对吧？那学个pythons，你去问GPT是可以的。这个版本是3.10以上都行。就是现在它的迭代很快，你就用3.10它更高的版本也ok然后第二个就是说我们会用一个交互式的开发环境，尤其是在OpenAI的这个PI就最近这两周的课程里面，包括我们用人券去调用OpenAI的这个GPT的时候，也会用这样的一个环境。
	为什么呢？第一个是因为to patter lab是一个非常好的形式，它是一个交互式的开发环境。交互式开发环境的意思就是说这个环境本身它不是说像我们以前运行完一个代码它就退出了。而是说它把这个东西一直维护在那儿的。就它有什么样的变量，你定义好的什么样的结构，都跑在内存里的。那么你想要继续在这儿去做测试什么的，它是一个很好的互动开发，然后帮助你去持续去把这些数据存在然后去做这个测试研究的一个环境。
	这个环境很好，包括像现在的这个google的co lab，也是在模仿那个猪排特lab，只不过他做了一些其他的功能。然后还有一块是ID，就我们的集成开发环境，这个是二选一。像我自己的话，我用的是这个VS code的这个visual studio code，排charm也行。
	然后这个IDE有一个很重要的功能是什么呢？就是有一些小的bug。比如说我去传一些特别大文件的时候，我从因为我是用的一个在海外的服务器，所以我会通过远程的方式去连接这个服务器。那to path lab，因为你可以在那服务器上起一个网页，那你就可以通过网页直接访问，就不用走SSH这种远程登录的方式，而是可以直接通过网页访问，对吧？那这个是wide的好处。但是有些文件你还是会去传，那这个时候VS code就变成了一个比较稳定的方式，那VS code也是一个非常好的IDE今天还有人在问这个VS code的什么插件需要安装，我觉得最重要的一个插件就是它的远端的这个explore，就它的这个removal explore这个插件它是用来管理。
	就假设你是一个天天都在写代码的人，然后你有很多台开发服务器，那VS code自己有一个小插件叫remote explore。它可以帮助你去连接各种各样的服务器。并且其实他就是把你的本地的SSH config这个文件解析了一下。然后你存的密钥也好，密码也好，他就帮你能够建立连接，并且这个链接还能存下来。你上次关掉了什么窗口什么的，他都还留着。对，那你要传文件的话，他可以走SCP这个协议直接传过去。这个是一个比较好的方式。
	当然他们也ok比如说你本来就是一台科学上网的环境，对吧？那你用paton ok的，你不需要用远端的服务器，base code也是同理。但是我们还是推荐用这种drupal lab的交互式开发环境。然后我们的这个demo也都是这个tript lab的代码，你可以直接可能我们的项目。
	然后第四个就是需要有一个OpenAI的APIP，待会我们也会有这个网页去跟大家讲是什么样的一个东西。这个APIK就是用来调用我们的OpenAI的embedding模型和我们的OpenAI的GPT大模型的对，那就这四个东西是必要的那除此以外，比如说pythons还有很多依赖包，对吧？那就是你跑起来你发现它提示你没装这个包，你装一下就完了，就在jpy lab里面就能装。对，这个是开发环境的部分。
	第二个就是说我们今天刚刚给群里面各位都发了一个项目，叫OpenAI的quick start。就是我们去学习这个AI大模型的一个开源项目。这个项目也是协议很友好的，用的这个阿帕奇的2.0的license。然后我们课程里面的代码，包括我后面会整理一些。因为我看上节课上完好多人在提这个prompt，怎么没有讲，怎么用？讲的都是一些理论的东西，其实会有会把我觉得一些我自己用的还不错的这个prompt，我会把它分享到这个项目里面来。我今天还没来得及传，包括这张图也不是最新的。
	大家去访问的话会发现，我们其实已经把这节课的相关的代码数据都已经commit到这个项目里了。大家可以通过这个链接去访问，欢迎去这个watch和star，包括hook，可以去做改造，这个是我们课程的一个项目，里面会放我们的这些，包括这个论文的索引链接。然后我们的这个代码，我们的这个prompt都会放到里面去。包括后面的南茜，我们也会在这个项目里面去做这个作为一个统一的入口，这就是我说的其实已经传了。我们最近这两周都是基于OpenAI的I来做开发。这个里面有一个OpenAI的API这么一个目录，然后这个目录下面准备了中文和英文的这个手册。
	然后如果大家有问题，我觉得是一个提的多的问题，也会把它同步到这个项目里面来，大家就能在这个项目里面找到那个问题答案，我也欢迎大家肯定有会用github的对吧？也欢迎大家给这个项目去做贡献，不管是文档类的还是这个代码类的。你发现大家经常问的问题就提一个这个pulled，我就会把它，如果我们觉得这个合适，我们就会把合到这个项目里面来。这样大家就可以在这个八周的时间当中一起去参与，也像是在参与一个开源项目对吧？实际上就是一个开源项目。
	然后我们在快速的几分钟时间跟大家讲一下OpenAI的embedding。刚才讲了什么是表示学习，什么是embedding，对吧？那open I的embedding是什么呢？首先OpenAI在他的这个官方文档里面有写，就是在这个models下面有很多的这个模型对吧？包括我们语言模型，包括它的这个纹身图的模型，语音识别的模型，这里有这个embedding的模型，这个embedding的模型，他自己推荐使用了第二代的embedding的模型。这个也是好几个月，应该半年前更新的一个版本了。
	然后这个模型有几个重要的指标。第一个就是你的模型名字一定得搞对了，对吧？你别搞错了。然后第二个就是他用了这样的一个token ier。在我们具体的代码里面，就是它的一个encoding的方式。那它输入是有一个上限的，就是就算他是embedding，你也不能把什么一本红楼梦丢给他，让他去帮你去做这个向量的计算，对吧？不太可能。有一个token的上限，token上线通过这个token的zer能算出来8191个。
	然后我们刚刚学了这个基础的部分，我们知道embedding高维的降到低维的空间里面来，对吧？那降到多少维度呢？降到1536维那，这个是第二代的embedding干的几个关键的指标。然后他给了一个初算的数字，就是embedding相对我们的GPT4这样的模型还是比较便宜的，他应该是这个我看这边有写，差不多3000页，就一美金能够3000页的这个文档，还是比较便宜的。跑了两次我们待会儿要看的这个美食评论集，跑了两次完整的1000条评论，也就是0.03美金，也就是两两毛钱两三毛钱人民币。对，这个是它的一个价格。
	对，大家在用的过程当中，对，这里还有一个点，就是我可能这里没有截图，待会我们在网页上可以看看，就是整个open EI他自己其实是能看到你的用量的，并且它能设置你的一个APIK的一个总消费。就怕你一行代码写错了，对吧？调了一下GBT4的这个API1下花了几百美金。但其实是它可以设置这个上限的。就你一个APIK可以用多少钱，可以设置一个上限。
	然后这个是他的AIP，这里有截图，我们看到这里有一个APIK，然后你可以设置不同的P然后每一个P可以去设置刚刚说到的这个使用费用的一个上限。对，好，那我们接着就开始动手这个代码的部分。这个是。
	我看大家看到这个窗口是代码吗？
	我怎么进来，还在pt稍等一下。现在是这个网页吗？是OpenAI quick star的网页吗？
	好是的，切过来了过，对，我们看这个，我看看这个字大小怎么样，有点小。这个尺寸应该差不多。然后我们看到这里有几个点，第一个就是说我们这个项目叫OpenAI的这个quick star。大家觉得需要给那个read me加一个中文版本吗？需要的话在这个评论区说一下，我们可以对家庭作业第一个？
	大家发现在这个项目里面，任何你觉得需要有中文的部分，启用GPT4，然后用一个好的这个prompt把它翻译成英文，然后把它提交上来，用这个full request，这个大家可以去学一学。然后形式首先我们一定会read me，还会有各种各样的其他的文档。这个文档有英文的版本，通常就叫默认的。然后我们会有这个中文的，大家可以加一个杠CN的这个文件名，那么我们就可以把这个事儿给搞定了，对吧？然后我们看到左边这个其实有点类似于我们本地的这个项目浏览器去展示这个目录下面有什么内容，你还可以在这比较方便的去做这个展示。然后我们这个中文的版本这里有写在最后简单唠叨一下，就是我们要看现在这个拍摄lab，就我们现在这个项目，你看这就是一个不好，就是我用GPT4翻的，翻译成记事本的对吧？缺少上下文，一比一之后就应该还是要保留这个notebook。
	然后我们有四个步骤，刚刚有提。第一个就是说你需要如果你是一个像我一样用的是max或者是你远端的这个服务器，是一个linus x或者是unix的系统的话，在你的这个环境变量里面需要去设置这个OpenAI的这个APIP。我们看一下。
	包括这个费用设置，这个还蛮关键的，我稍微花点时间给大家说一下在哪。就这里我们有这个P对吧？然后我们有这个feilding，这里有这个付费方式，这里有一个用量的一个限制。这个用量这的限制，比如说你看看我给现在这个organization，对我刚才说错了，是在organization下面去做设置，而不是一个具体的APIP。然后你一个organization下面可以有，就你一个组织下面你可以有多个这个key，然后可以设置一个上限，这个上限是可以改的，并且它有一个硬的上限和一个软的上限。对，然后这个软的上限你可以设置更低一点。比如说你设置个五美金，如果你超过这个，他会给你发这个信息提醒，然后这个是按每个月来的，还有这么一个设置还蛮重要的，然后你也能看到你的这个不是你sorry。usage. 
	对，就比如说我们昨天跑了两次美食评论的这个embedding，也就消耗了这个0.04，其实也不多，我们看一下。现在昨晚。对，你看跑了49602个token，也没多少。然后又跑了一遍，整个加起来也就四枚分，对吧？这个比起大家每天喝的咖啡的这个钱其实不多了。
	对，然后我们把这个项目里面的这个APIT刚刚拿到之后，我们通过get clone的方式，我们克隆下来这个项目吧？至于github怎么访问，怎么样能够让他克隆下来，我们在群里或者各种地方大家可以去分享怎么样能够比较方便的使用github。然后接着我们可以安装这个就拍的lab，然后安装就拍的lab。其实我看有的同学，其实这个安装有pipe对吧？这是最常见的这个pythons的这个包的安装工具，也有同学说用安娜康达、mini康达之类的都可以，包括virtual ENV，这些都是可以的。
	如果你有多个环境需要去管理，然后这个环境之间你是一个资深的开发者，对吧？你有多个环境，不同的环境是用来开发不同的应用的那他们之间可能有依赖包的冲突，通过这种环境管理的工具是很好的。然后把它弄好之后，我们可以把刚刚clone下来的项目进入到里面，然后去启动我们的这个jupiter lab。那他就会变成一个根目录，就是我们的OpenAIr que star这个项目的。比如说你现在这个jica lab，你自己有一些本地的代码的更新，再去pt lab里面也能看到它的变化。然后如果你在主pt lab上面改了，然后你对应的那台服务器或者启动它的这个命令行也都能看到。客人都挂了，我们重启一下参考。
	比如说我是在他的上一级目录，因为我还有别的一些项目。大家如果在这个open eco e star的启动的话，就能看到这个进来是这样的一个目录。然后有OpenAI的API，然后目前我们在这个项目里面放了一个jupiter ter notebook文件，给我们的invading看一下，这个大家能看得清楚吗？应该能看得到，这个字这个代码的字体大小大家能看得见吗？我可以小一号吗？这个有点太大了。
	好啊，那在这个里面我们也再次强调了，就是确保设置了这个APIK。如果咱们要去跑这个embedding生成，然后这个read me，就是我们刚才这里看到的，它会跳转过去，大家安装这个依赖包，依赖包是这样的，就是我们可以我用的这个mini康达来管理的。然后在一个能欠的虚拟环境里，大家可以不用这样的方式，就这个pipe跟这个康达是能混用的，对吧？
	然后这个命令是干嘛的呢？就是我我考虑到有的同学这个不一定是之前有拍成开发经验的，我这里稍微唠叨一下。然后我也看到大家反馈后面决定我讲这些代码的这个深度，我对大家各位有了解。
	首先这个就拍的lab可以执行直接执行pythons的代码，也能执行命令行的这个指令。那执行命令行的指令加一个感叹号就好了。然后比如说我们这里要去安装一些依赖包，对吧？我们要装OpenAI的包，要装pandas，要装这个meta pt lab，然后ploy包括这个SK2，还要装一个男派对，这些包都可以在这直接装。稍等，这网络。
	关掉，重新开一下。好，然后它就可以执行了对吧？这个其实大家能看得到，它其实就是一个他这个是80，他没有前期不重要。对，那这里其实就是一个管理，我们到底开了多少个notebook的一个界面在这，然后能管理。然后我们回到这儿，它安装了所有的依赖对吧？所有的都已经满足了，这里有输出，我们这边就跳过，不保证我们都已经装好了，这里我们简单附了一些图跟大家做解释。
	第一个就是说embedding，我们现在主要聊这个world embedding，这个text embedding。他其实就是把我们的一段三语言，一段文本通过这个embedding model变向量，对吧？因为我们刚才讲了很多了，是这样的一个过程。然后使用的是这个text embedding ADA002这么一个模型。然后我们整个这个notebook的示例就是用的这样的一个示例。
	亚马逊的这个美食评论的数据集也是OpenAI的官方的使用的样例。不过我发现他的他官方博客里面的这些示例代码，有一些小的版本不一致，各类的问题。大家可以主要参考这个代码，然后我也补充了很多的代码注释。
	好，那么回到这儿这个美食评论的数据集你点这个索引能看到到这个页面来，就是去讲这个是一个什么样的美食评论数据集。简单理解就是跟我们大众点评美美团一样，它其实是一个大家对各种美食发表评论的这么一个数据集。然后这个数据集的文件格式是一个CSV这样的一个格式，这个格式能够被我们拿来很方便的去做读取。好，包括他的一些信息，我这边就不展开聊了。
	然后具体我们要干什么呢？第一，我们需要去引入一些比较重要的包，我们是马上就要用到的。第一个就是这个pandas的包。第二个是这个tacking token的这个库，是OpenAI的一个开发的一个库。第三个也是这个open I开发的，用来做embedding的这么一个函数，不用整体导入进来，我们执行一下。
	然后我们要去加载这个数据集对吧？在这个目录下有这个数据集，稍微拉过来一点，有美食的这个数据集，里面是1000条美食的评论。然后这个一embedding EK是OpenAI提供的一个官方的embedding的结果。这个一K的这个demo是我跑出来的一个结果。对，就是在这份notebook里面可以跑出来一个结果，大家他也能去跑部分的评论。或者你把这个代码里面的美食评论换成你自己的这个数据，也能够同样的去跑出这个embedding的结果来。
	这个我们就直接往下看，首先这个代码把我们这个数据集读进来，读进来之后做了一个什么样的事儿呢？把我们的这个内容做了一个拼接，把我们的这个相当于组成了一篇小短文，小短文就这个combined字段，把这个相当于把这个有个标题，然后有一个正文，组成了这么一个新的字段叫combined。执行一下，可以通过这个data frame的这个head方法去去直接在这个去拍的notebook里面去看我们的这个data frame，给我们读进来这数据集是长什么样的。CSV就是类似于一个excel对吧？那我们通过data frame能把它存下来，可以能看到这个内容。然后我们也能直接打印特定的一列，就是我们combined出来的这个结果，通过这样的一个方式能看到有1000条的评论，这里就直接取得1000条的评论，大家也可以去这个数据集这个网站上去下载完整的。那就更大一点。
	然后我们讲了一些关键的参数。第一个就是说我们开始有看到这个模型是有一个模型的名称的类型的。就这个类型，然后他有一个token nizar，是这样的一个token ized名称。然后这个都在我们的这个官方文档里有啊，再来给大家看一下官方的文档也找不到，在这里跳到invading，然后。
	Sorry, and the apes. 
	应该是在PI。搞错了。
	Guys, 对，在这个里面，我们在这儿能看到他有提到，就最关键的几个参数在这儿你也能看到，就这个model name和这个cocozza，包括它的这个输入项也都能查到。然后如果我们要去玩一些其他的模型，也都类似的，可以在这儿去查对。然后我们设置好这几个参数，然后刚刚有看到最大的是这个8191，对吧？我们给它设置一个八千，实际上如果我们用完整的数据也是需要去做过滤的。然后输出的这个向量维度是1536。就我们这个embedding之后的这个向量，我们这里看到的这幅图的图示，这个向量的维度是1536。如果我们使用这个模型的话，下面我们也会去验证它。
	好，然后我们把这个段代码留不在这儿，是为了考虑大家来如果用别的这个数据集集，或者用全量的这个美食评论数据集，可以通过这段代码去过滤出超长的这些，就我们刚刚有提到，他有一个上限8191对吧？那如果这个太长了，我们就要过滤掉，这个没法直接invading，你要把它拆成两段评论也行，就是你要再去做一些处理，然后这里就是去过滤掉他的，然后我们只筛出来前1000条，然后剩余就1000条这个DF对吧？然后接着就是我们去生成，那这里就是会去实际去跑一下这个，比如说我们这里我们就跑一跑呗。对，一个这个是2146，就这个时间点的。
	然后怎么样去做呢？其实比较简单。第一个就是说他要干个啥事儿。这行代码其实就是去调我们的OpenAI的这个模型，就这个embedding的模型，把它存到这个有一列，叫做这个embedding这一列。我们看到现在他没有这一列，对吧？他现在是先从原始的这个CSV里面造了新的一列叫combined，就是我们即将要被做嵌入的这么一个原文。然后再新加一列，新加一列存到这个embedding里面。然后通过去调用我们的这个OpenAI的这个是in text embedding的这个模型，然后把它放到这一列里面来，然后输出到哪能输出到这样的一个文件里，然后跑一跑可能要耗时，我们就先不管它，待会儿它生成好了我们再看。
	那么往下看，就是如果我们不想生成，而是直接去用的话，那这里也有我刚刚有提到，这里有一个with embedding 1K或者说这个EK demo是我这边跑出来的。然后we chinese这个地方也就类似于EK demo对吧？他会刚刚跑出来一个新的准备，一定的结果，然后你也可以直接去读。然后读的话你在这儿指定一下这个结果。比如说我们就去读这个官方给的这个embedding EK也可以读到一个新的DF里面，不要跟他搞重了对吧？专门取了一个别的名字，这里最好是两个D，应该是之前的一个拼写的笔误。
	那我们今天也不改了，因为它涉及到引用比较多，大家可以把它改正过来。很好的一个家庭作业对吧？找一个同学是一个PR把所有的DF的embedded的加个D，保证这个还能运行好吧？然后因为这个就wider notebook有一个特点是什么呢？它其实是一个串行执行的，就是我们这在跑内容的话就是不能执行的，他会等他执行完再执行。所以我们先继续读代码，就是我们从官方给的这个embedding 4SV里面可以读出来一些一个新的加了embedding这一列的内容。我们也可以去查看这一列的内容，这一列的内容我们还记得刚刚说的这个embedding的向量应该是一多少来着，我们看一下。
	1536对吧？应该是这个维度才对。为什么官方给的这个会是一个34000维度的向量呢？这个为什么？这个我不知道有没有人同学遇到过类似的问题，这其实是一个很常见的问题，就是我们用pandas去读这个CSV文件的时候，它的默认会去把读成这个字符串的类型。所以其实他把这里的每一个包括这个括号、数字，都变成了一个字符串，所以我们去输出它的这个类型也是一个字符串。那这里我们可以通过这个转换的方式，把它重新转换成一个向量类型。转换为一个向量类型之后，我们输出这一行都能看到是一个1536维的一个embedding的向量了，那这一步骤会反复的使用在下面对，还没执行完，那我们继续看代码。
	然后接着我们假设我们已经处理完了这个embedding的向量了，包括我们也可以从这个文件里面直接读到这个invading的向量，那这个embedding能拿来干什么事情？对我开始有提到。第一个就是说我们可以可视化的去看一看这embedding模型到底效果怎么样。然后通过TSNE这样的一个方式，或者说这样的一个方法，我们能够去把它可视化到这个很低维的空间。因为1536位还是太高了。我们假设直接通过TACE把它变成一个两维空间的一个结果，会长什么样呢？首先这个TSE的输入不是我们的自然语言，对吧？
	输入的是我们的embedding之后的1536位的一个向量。然后他原文是这些美食评论，就我们刚刚看到的那些有标题有正文的自然语言，变成了1536维的向量。这个1536维的向量又可以降维到两维，对吧？
	好，通过刚刚我们的这个转换之后，我们可以看得到整个一列数据，就是我们的这里的转换之后的一列数据，因为它是有1000个，对吧？这一列数据它是一个什么数据类型呢？是一个pandas的这个序列类型。
	对，然后我为了保证大家不出错，给了一些断言。就比如说我们确保embedding的这个VAC，就是这个嵌入向量都是等长的，也出现了你给我的一堆embedding向量，结果大家维度都不一样。那这段断言是为了检查这个事情的，我们需要把这个，因为TSNE接受的是一个二维的一个南派的数组，南派是一个很有用的库，上面有写一个开源的数字计算库，把它转成一个狼派的格式，变成了一个metrics。然后我们去新建一个TSNE的一个模型，它用非线性降维的方式能降维，然后后面可以去把可视化出来，这里我也明确的写了一下降维后的维度在这里设置，然后进行的数量，包括初始化的方式，这个学习率等等。然后我们把这个像维护的数据聊到这个TSNE的这个新的2D空间的坐标里。那这里其实又进行了一次invading，对吧？
	然后回忆一下这个学习的内容，embedding是把高维的降到低维，并且尽可能保留它原始的这个分布。那么我们把刚刚那个1536维的降到这个二维里面来，这个是它降维后的一个结果，这里有详细的注释我就不念了。然后通过massacre lab这么一个很方便的库，我们能够在japie lab里面去显示出或者说去可视化很多数值结果。
	那这里我们把这个TSNE的这个二维的结果展示出来了。那他为什么会有这些坐标点？就是因为我们通过TCE把1536位变成了二维。然后每一个二维的数字它都可以有XY的坐标，对吧？那就可以把它平铺到这里来，然后平铺到这里来不同的颜色进，你可以这样看，首先这个是一个鲜艳知识，就是整个TSNE差不多就有三大类。大家看有三大块，我鼠标这里三大块，然后其中有一个大类的评论大多是负面的，这个是从就是我先看看这些数据知道的那我们后面也会看实际上怎么样去了解这些信息。
	跑完了吗？还没有对，我们继续通过两次的embedding，我们能看得到这个评论其实是分成了三个大的类型。然后有没有办法通过一些聚类的方式去更？就大家知道TSNE是没有通过这个learning的方式，对吧？是一个纯统计的方法。这个前面我们已经讲过了，它变成了两维。那有没有可能通过一些聚类的方法？
	聚类是一个很很有用的机器学习的一个算法分支。它把不同的特征通过这个特定的一些聚类算法，找到他们的一些共性的特征，然后把它合在一起。但这个k mess是一个需要人手动去指定到底有几类的那我们这里用了这个SK learn跑出来了，这里刚刚新增加了一条。
	好了，大家看见没有？5秒钟前，我们可以去执行一下下面的这些代码了，确保它都是可用的。我们重新把ek的文件加载进来，然后去看一看是这样的一个结果吧？然后三万多位，是一个embedding的这个字符串的一个结果。我们这里可以打印一下，看看什么概念来看一下。它其实是把它变成了一个字符串，它其实不是一个向量，就不是一个矩阵。对，是一个一个字符串。Ok这样也能看到，那么我们把它转换一下，转换成一个真正的矩阵。
	1536位的一个矩阵。转换之后，我们把它放到了一列新的这一列里面。我们可以通过刚刚的一个方法，不知道大家还记得吗？这个he的方法。这个就是我们完整的结果对吧？还有这个combined，我们有这个embedding，这个是字符串的数据类型。我们也有把它转换成这个向量的一个是排成类似的数据类型，在这个invade的vector里面embedding the vector里面。
	好，然后我们可以通过TSNE把1536位再进行降维。它都是1536位的，没有问题。我们把它转到这个metrics里面去，新建一个切身一这么多的注释，就是为了方便大家去反复的去修改去测试，这个是给大家留够了这个空间。我们去跑一下这个接生意，出来的这个结果是一个相对稳定的一个统计决策方法。
	好，我们继续看这个k means的这个聚类的方法，这里我们的这个类别数可以去调整，然后我们其实是之前知道了它大概有几个类别，那我们后面可以去去再详细解释这个过程。K miss其实是很快就能跑完，这个方法很快。这个方法跑出来的结果，因为我们一一开始就指定的是四个类别，所以它相当于就把它sorry把它聚类到了四个不同的类别里面，并且新增加了一列。
	在我们的TM in baby的这个data stream里面，我们可以给大家再看一下。比如说我们现在在看他的，我们会发现这有一个类别。那么第就第零条这个美食评论是在类别三里面，那第297条是在这个类别二里面对吧？它是在不同的类别里面，那么怎么样把它画出来？我们同样的可以通过这个借生意的方式去把它画出来。
	这个就是它的四个不同的类别，然后我们之前这个OpenAI的官方文档里面也有写，就是这个美食评论这些欠条，他们这四个不同的类别是有不同的类别特征的。其中一个是专注于这个所谓的狗粮，就宠物粮食，一个是负面评论，还有两个是正面评论。大家可以去调整这个类别数，我们可以看到不同的这个结果。时间也我们这边就不再展开了，大家可以自己下来玩。
	然后我们现在有一个很大的困扰是什么呢？我们知道它有四个类别了。那我们能不能通过某一个评论，就这里的原文这里的combine的原文找到相似的评论呢？那这个其实是一个很重要的，我们讲到的embedding的便利之处，它有一定的相似性的推断语义的能力。
	OKAI官方推荐的是通过这个cosine的相似度来度量不同的向量之间的相似性。就我们两个向量之间相不相似本身就有很多方式。我们这里用这个cosine的相似度，我们来做度量。我们看到这个embedding里是一个list，对吧？它不是一个screen。刚刚我们有讲到这个embedding是一个screen，那embedding the vectors in the list，然后我们定义一个简单的函数，这个函数用来干什么的？
	首先我们其实是要在我们的这一千条评论里面去做搜索，对吧？这1000条评论就是我们的一个总的知识库。这些知识库这个知识库它里面的这些内容有原文，就是我们的这些原文同时也有embedding的结果。
	就我们通过这个1536位的这个向量，我们怎么样去找到相似的呢？其实就是把这个1536位的这个向量，这个embedding vector这一列去跟我们想要搜的，这个是我们输入的，就跟我们搜索引擎用google用百度一样，对吧？我会一段话，这段话会变成一个embedding，然后再去这一千条里面找跟它相似的。然后怎么样算相似呢？那它就是cosine指相似度算出来的这个值，那我们用它来作为它是不是相似的一个标准。
	Ok然后这里就定义了几个输入参数。第一个就是这个DF就是我们这里这个DF in baby的我们这个pandas的这个data frame。大家可以想象成就是有一张表，对，就这个是我们的知识库，然后这样的一种定义方式，在后面的很多开发里面是一种比较常见的输入项，这个是我们的知识库。对data frame，但它很小，就1000条，我们如果多了的话，可以存到向量数据库里。
	然后第二个，就是我们想要搜的这个内容，我们把它定义成这个product description。我尽量保留了跟官方文档一样的这个参数名称，免得大家造成歧义。这有个N是指我们返回几个候选值，就比如说算出来有一个按照这个cosine的这个值能有一个大小，对吧？那我们可以选择返回几个，一个、三个、五个、十个都可以。然后这个key print是指一种打印方式。对，这个是pythons的一个方法，我们这边就不再展开了。
	然后假设我们现在要收这个跟这个beans比较近的这个内容有哪些？这个是官方给的第一个例子，我们看一下能不能运行。这里会有很多种不同的，包括官方文档，OpenAI的这个官方的github项目上，这里都会有一些I的出入。然后我把它调试之后，给了一个可以直接运行的，可以在这上面直接改。如果你发现跟官方文档的这个代码片段有些不同，是因为他们没有更新。对，所以或者说他们的版本没有匹配上，跟我们现在这里用的，大家不用担心用这个就ok对，这里也有用，这个之前我们已经定义过的一些代码，我们这边就不再去展开了，这里是算这个相似度的。
	然后这个就是去把我们的输入的这段话去用test embedding ADA这个embedding模型变成一个向量对吧？这里就把我们的输入也变成了1536位的向量，这样才能让他去算。那他就会简单来说，这里其实就把我们输入的这个向量跟这里的1000条向量挨个去做了比较。挨个做完比较的结果就存在了新的这一列里面，对吧？那存到新的这一列列里面之后，我们会把这个结果打印出来，这里我们找这个delicious steam，他这边就会有一些跟他相关联的。
	然后我们再再看看刚刚聚类的这个例子，看大家还记不记得，就聚类的这个例子里面，我们新增加了一类，然后有这样的4种不同的类别。这个很直观了，就是相同的类别在一起。但这一个点其实是一个高维向量，然后也是一段自然语言的评论。那我们能不能从这个点里面找到实际上跟他相似的原来的自然语言呢？肯定是可以的对吧？
	首先大家稍微研究一下就能知道这幅图里面是可以给这一个一个的点去把他的这个编号写上去的。你可以不用打印1000个点，比如说就打印50个点，这样你就能看到哪个点是具体的哪一行。就这里的每一行有个ID，然后我们现在就把其中的这个第零条，就这个这里面第零条是一个类别三的评论，对吧？我们把它展示一下，对类别三的原文是什么呢？是说wanting to Steve to bring to chicago family vary一大堆，然后这是他的原文自然语言，然后我们可不可以把这个类别三相似的找出来呢？
	我看看。这样也是ok的。首先一定是把它自己类似的找得到。因为他跟他自己相似度这就是同一个点，所以他一定排的是自己的这个。所以大家在玩这个事例的时候，如果你N等于一，那那你一定会找到你自己这条原文，对吧？所以你就是输入的这个类别3，就是他你输入的这个带货选项就在这个库里面。那这个是能找到跟他类别相似的这个点的，然后我们可以去还可以去改一下这个的内容。比如说我们找这个狗粮，那狗粮其实它能找到一些狗粮，比如说dog hold，然后类似的像这个dog snacks，love name，这些东西都ok然后你可以找一些负面评论，比如说offer很糟糕的，也能找到类似的信息。
	这个其实是什么意思呢？这几个例子想要跟大家表达就是说首先输入的这个就跟我们用搜索引擎一样，这个词是可以大家随便调的，这个东西会变成一个新的高维向量，就这么一个词变成一个一千多维的向量。然后他会去跟那1000条向量去做比较。然后比较之后通过我们embedding的，我们知道embedding有这个表达一定语义的能力，他就会找到跟它相近的，这个是ok的。大家也可以在这儿去做各种各样的尝试。然后同时希望再留一个小的家庭作业，我们会在优秀的学员里面去选择比较好的去送一些汽车时间的周边。我们现在这个data里面只有一个美食评论集。
	如果大家有一些比较不错的协议友好的数据集，也可以提交到我们的这个OpenAI的quick star这个项目里面来。我们用别的数据集也可以跑一跑类似的事例，尤其是中混的。好，这个就是我们这一节课的embedding的一个实战。
	我们相对来说第一节课准备的不算特别深，但是把整个embedding的功能，包括怎么样去生成它，怎么样去读取它当中的一些细节，然后怎么样去再次embedding，然后可视化出来，然后怎么样用文本相似性去做搜索，都在这展示出来了。然后未来很多的功能，其实都是可以通过这些东西去生长，有往大的去做扩展。好，最后留五分钟给大家做这个提问。看大家有什么问题。
	大家先说一下，现在这个代码就embedding的这个notebook是难还是简单。大家就二选一，打到评论区就是难就觉得难，简单就觉得简单。对。
	三万多不能输入，三万多那个例子是为了告诉大家，pandas这个库去读CSV文件的时候，它默认会用这个字符串。所以你直接往里丢会报错的，所以你要把它转成一个pythons的向量。还差不多，有难有简单的对。
	有的同学说找不到这个包，这个token对吧？装一下，对，这里没写装一下，对。头疼。对，在这儿直接装装完之后就有了。好吧，我们执行一下这段代码就可以了。这是一个非常基础的操作，我没有依赖包，在这装一下。对，安装一下。
	验证正确性。好问题，其实我们刚刚这几个事例都是很关键的。就是offer找出来的是不是跟FFO相关的对吧？狗粮是不是跟狗粮相关的。然后还有一个比较好的方式，就是我刚才提到的，希望大家找一些中文的数据集，我们也能去做试验。然后我们下节课也会去找中文的数据集，但这节课想发挥一下大家的主观能动性，顺便看看各位的对于目前这个代码示例的难易度的感觉。
	对我再回答一个很多人问的问题，就是用什么样的电脑配置，然后GPU啥的。大家会发现到目前为止，不需要什么电脑配置，就是所有的算力都在OpenAI那边。他去做了这个text的embedding model，包括我们下面两节课要去调这个GPT的这个模型也是他那边。对，包括用南迁也是他那边。一直到我们生态片片去真正要自己去部署一个大模型的时候，才会需要比较大的算力。在那之前大家其实都可以不用担心自己的算力情况。
	我这台服务器远端的也就是一个两核4G的很很低配的对，不会影响什么的。怎么知道是1500多维度的？我们刚刚有讲，我说我讲那么慢，那么细节，就是大家看这儿是1536维的对吧？这有啊我们找到这儿对吧？这不是1536位的吗？对。
	还有一个同学问看不懂pythons怎么办？好问题，看不懂pythons，我们应该让GPT教我们，对吧？我们举个实际例子，我们这就来先说一下。
	不能开一个。开一个新的。收起来，比如说这是应该是非常常用的一些prompt的用法了。
	这都已经有了，但他应该也会有一些这个。
	主要是因为这个注释已经是用77式生成的了，给他给出来的结论不会有太多的新意。但是我们可以找一个什么样的场景呢？如果大家不太懂，就是我们可以深入去问这个，比如说这个函数对。
	对我看有的同学是有很好的pythons和机器学习的基础，包括做这个data mining的一些基础。数据挖掘这个专业跟我们表示学习其实是紧密相关的对吧？那你看他对这一行有了更多的解释，包括他说这个是用来进行数据的降维的对吧？那你再问问他这个TSNE这个函数还有什么参数建议的取值是什么？这些东西都是我们怎么用GPT来教我们理解代码的，好吧？你看他也会去讲这个n components巴拉巴拉这里就会讲的更细一些。然后其他的代码大家如果有不懂的，都可以通过类似的方式去学习，让PPT4来教我们。好，那么我们就今天的这个直播就到这儿。回头有什么问题我们还可以在群里继续交流，好吧，就先到这儿了，各位早点休息。