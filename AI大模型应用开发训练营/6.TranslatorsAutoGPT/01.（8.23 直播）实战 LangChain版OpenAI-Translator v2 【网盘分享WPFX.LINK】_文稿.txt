	今天的课程，然后也感谢大家这一周耐心的等待。但我看评论区和这个群里面的发言，大家好像正好趁着这一周赶了赶之前的进度。今天其实我们就进入南茜的这个实战环节，总共我们课程准备了三个南线的实战项目。第一个就是我们在基础篇的时候，已经有同学非常厉害，也拿了我们的一等奖，500元的现金，我们做的这个OpenAI的translator，我们今天讲怎么样用人券来重写一部分open I translator。这个项目实现V2.0当中的一些规划。
	我们今天的这个课程，其实本来这个项目应该会用三个多小时的时间。我们周三只有2个小时，所以我这边做了一个拆分。今天我们会把前面两部分讲清楚，就是怎么样使用这个chat model和我们的chat from template。我们之前在连线的基础模块里面已经学过了，它属于我们的model IO模块当中的一部分。然后我们当时用了大量的都是使用的LLM这个语言模型，聊天模型我们用的很少。今天我们正好借这个南茜版本的OpenAI translator，向大家再深入的去学习理解我们chat model。
	第二就是我们要用能欠来优化一下我们OpenAI translator这个项目的一些设计，包括我们既然已经使用了能欠了，那是不是大模型管理这一部分能够用人券来接管过来。同时我们之前的open a translator v1.0，就是我们用GPT4生成的这个版本。Model部分我们是做了一个model的鸡肋，然后有这个OpenAI model和GLM model，然后里面还会去构造这个prompt。我们看看如果用南茜来实现open I translator这部分会怎么做。当然我们既然用的南茜，我们定义了一个station chain，实现了一个统一的翻译接口。这样我们未来换了不同的基础模型之后，translation chain也不需要做改变，因为能欠把模型这块接管过去了。我们只需要用不同的底层模型传入不同的model name和对应的T就好了。
	然后包括配置管理，我看今天还有同学在问，怎么启动这个项目的时候要传很多的这个参数，我们还有一个config e mail文件，这两个是怎么回事？其实这俩是为了适用于不同的启动和部署方式的那我们在今天的课程里面，我们会去讲怎么样用一个更简洁统一的一个配置管理。这个也是为后面的这个图形化界面包我们的web server去做一些铺垫。我们用一个统一的配置管理，一个单例模式来管理我们整个项目的一些配置。当然这个配置管理还可以再去做丰富和扩展，好，我们今天就正式开始今天的这个课程。
	第一个就是我们的这个chat model和chat prom tempt这两部分，我们需要再深入的跟大家讲一讲，温故。我们其实在讲年线的基础模块的时候讲过这个check model，这是我们当时用的一个示例。我不知道大家有没有印象，我们是在这个chat models这个模块里面使用这个chat OpenAI，它其实就是对OpenAI的这个chat o OpenAI的这个chat API做了一个封装，默认其实是这个GPT3.5 turbo这个模型，我把显示的写出来让大家能够理解。他会把我们的这个给模型的这个提示词，按角色去做区分，比如说我们有这个AI的这个message，那对应着OpenAI的API里面就是它的assistant有human message。这个其实就是我们人，就是我们的用户他的一些内容，还会有这个system message。这个倒是跟我们的这个OpenAI的API定义是一样的。
	我们当时做了一个这样的事例，就是我们构造了一个聊天记录message，然后里面把我们的这个信息存下来。他做了一个抽象，叫这个schema下面有这个message，我们能看到这里的这个message是有这个系统的，有这个用户的，然后有AI的返回结果。然后用户再来问一个新的问题。经过这一个message的构造，我们把它全部传递给了OpenAI的GPT3.5这个模型的chat OAPI。
	然后传过去之后，我们把它的结果打印出来，可以看到chat model message这一行，就我这个鼠标这一行，它其实就是去调用我们的OpenAI的API，返回了一个结果。这个结果是存在了AI message里面，他写的这个结果是可以去对我们很很方便的提取出来。这个是我们之前讲的用chat model的方法，但它有一些不太好的地方。首先第一我们如果用这种方式，怎么跟我们的prom tem结合起来？这里是手动构造了一些message，就跟我们裸着去使用OKAI的API是一样的。但年前最重要的一个部分是它出现了model IO，他把我们的这个prom temple ate的这个抽象做的很好。
	那对应check model有没有这个prom template呢？显然是有的。我们今天就会用一个notebook先让大家去理解我们的聊天模型和它对应的提示词模板应该怎么做。我们看到这里其实是构造了不同的template。比如说我们有system，有AI有human 3种不同的角色，那他自然都会有自己的prom complete。同时我们还需要构造一个聊天记录，这个聊天记录本身也可以有一个模板，叫做prom tempt，就这四个大家可以理解。Chat就是把他们仨凑在一起包了一个模板的列表，这么简单理解就好了。
	那我们把这个翻译任务由system这个角色注入进去。然后把我们带翻译的文本，用我们的q man用户这个角色输入进去。待会我们会在那里面详细讲，这就不再展开讲了。
	所有的prom template都有两种方法去构造。其中用的最多的也就是我们这里看到的方法，叫做from template。就是把一个s string也好，或者我们之前学过的，还可以用其他的格式化字符串的方式传给他，传到from template里面。那我们把它打印出来能看到这是一个标准的南茜的一个prom template的一个抽象。那里面会有input variable，就把我们这个from template当成一个函数，对吧？我们一直在讲用连线来用自然语言编程，整个抽象其实跟我们用LLM是一样的，这个system也是这样，我们的这个human的也是这样。这就往后走，大家能看到这个chat的这个prom tempt，其实是一样的。它其实是把我们刚刚造出来的这个system和我们的human这两个prompt组成的一个列表。
	大家可以理解，我们要维护一个消息记录，我们消息记录本身还可以用模板来构造。这就是chat from comment要干的事儿。它from message对吧？因为它本身是要构造message，所以有一个对称的方法叫from message，刚刚是from template。好，我们用这样的一个方式构造了一个可以给聊天模型，就是给类似于这个GPT4GPT3.5的API这样的一个大模型的API的构造的一个模板。这个模板大家能看到仍然是符合南券的这个抽象。它里面有我们的输入的变量input variable al，有它的output puzzle，这边我们没有定义，有message，之前这里是一个prompt。
	这里聊天模型需要构造的是一个message。Message里面其实就是把我们刚刚在前一页构造的系统的和用户的拼在了一起，变成了这样的一个模板。然后我们可以把这个聊天记录的提示词模板给他format。这也是我们之前学过的内容。如果大家之前有落下的课程的话，需要去补课。
	Format prom，就是我们看到这下面有一个生成真正的prom的时候，要用这样的一个方法，其实就是把我们的刚刚说的把它当成一个函数的话，就是要去运行这个函数。其实就是把它真正要给我们大语言模型的这个prompt给构造出来。这里就有一个变量是我们的，其实就是我们待翻译的文本要传进来。比如我爱编程对吧？I love programing, 把它传进来。传进来之后，就会生成一个真实的给聊天模型用的那个字符串，它就是一个没有这个变量的就是说传进来了。然后这个时候，它就构造出了一个这样的一个真实的一个message。大家能看到它的这个prompt values，它的类型，你待会儿去实际操作一下就好了。
	这里有一个点就是说chat prom complete，就是我们构造这个聊天记录的模板很复杂。刚刚我们看到左边这一部分其实是输出的地方，看着非常的难看。也给大家提供了一个小工具，一个在线的工具。
	怎么样把这种message给他做一个规范化，变成右边这种形式。这样大家在第bug的时候就能够很方便的去看到每一个prom template到底里面是什么样的。它的这个input是什么？Output color是什么？它的它是什么？它的format的这个类型是f string还是均价对吧？然后它的检验的这个模板里面的参数有没有？有的话是什么？然后类似的human的这个message也是一样的。
	然后我们刚刚做了这些做法之后，我们要实际的去调这个模型，那怎么调呢？我们刚刚看到用这个format prom。变成了这个真实可以给我们的聊天模型用的这个message，就如我们下面看到的这个chat from的一样。这个是要传给我们的大模型才能真正去执行的那所以我们仍然使用chat open API，chat OpenAI的这个API。这个API它会去调用我们的GPT3.5这个模型，我们把这个temperature设置为零。因为是一个翻译，我们把这个多样性降低，尽可能使它的翻译结果是一致的那这里我们构造了一个chat OpenAI的实例，它是使用GPT3.5来作为这个大模型的那这个跟我们去使用我们的LLM是完全一样的，然后这个形式上也是完全保持一致的，构造出这个模型之后把需要用的这个prompt传给我们这个模型，就会得到这个大语言模型的生成结果。
	那这里其实就是它的AI message，那我们能看到这个translation result其实就是一个AI的message，叫AI message的content这个成员就是它里面具体的值，那它自己就把它翻译成了中文。因为我们的system message里面有写，是需要把它的这个内容从英文translate english to chinese，那么他就把他这个输入的英文i love programing变成了这个我喜欢编程，就是以这样的一个方式去执行的。但是我们只用这个也是没有完整的走完我们之前学的这个语言模型的路子，因为我们有model IO，我们还学了欠。那chain就是把prompt text ate和model整合到一起。
	我们学了一个最简单的一个整合方式，就是这个LLM欠。并且LLM券其实也支持它的内部去使用LLM或者是chat model两种它都支持。所以我们也会构造一个LLM欠，就在这里，它的构造方法里面，我们会传入两个，一个是这个聊天模型，一个是这个聊天模型使用的提示模板。因为它是一个聊天模型？是一个chat model，所以它需要传的是一个prom tempt，就传的是一个消息记录。然后这个消息记录里面的每一个都可以用一个对应角色的提示成模板来做构造。我们实际跑一跑，大家跑一跑，这个代码就会有概念了。听起来还有点绕，是因为他把多角色和消息记录也引入进来了。
	但是这个用法和我们基础篇调OpenAI的这个API也是一样的，跟我们连欠的这个LLM的用法也是一样的，就是有这个from complete，然后要把它变成一个真实的字符串，就要传它的变量进去，然后构造出真实的字符串，这个字符串再变成一个消息记录，就是一个python的一个列表，里面按照这个顺序构造了不同角色的提示词。然后这个提示词再给到我们的这个大元模型，只不过这个大语言模型也是一个聊天的模型，就是我们的chat model。这个chat model和这个提示是模板一起又归并在了一个LLM茜里面，最终我们把这个LM券命名为叫做我们的翻译的这个链translation券。这样就不需要每一次都把这个chat contempt传一个变量，然后再一个format再去to message再传进去，这个流程就很复杂。因为我们用LM茜把它封装起来了，这个LM茜本身就能帮我们去做这个提示词的生成。我们看到最终就会使用这个欠的run方法。
	这个大家应该已经很熟了。我们任何一个run方法就是传入一个我们的模板需要的这个变量名和它对应的真实值传递。对这个KBI6传进去之后，它的结果是一致的那这样我们就只需要每一次都调用这个券的run方法去传我们需要翻译的文本就够了。
	这个是用我们的LM chain来简化构造这个chat from。因为我们时间紧任务重，我们就先把这个代码跑起来，大家看一看有什么问题接着再提问。这个代码目录，我们首先这部分代码我们都已经传到这个项目里了，我们给大家看一看。对的。
	在我们的这个项目里面，我们更新了南茜的这个目录。这个目录是我们之前学基础模块的时候已经有学到的。在这个目录下面，我们之前有一个叫做去拍摄的这么一个目录。这个目录在上一节课的时候，我们已经把它整合了，有这个不同的模块的基础模块。我们学了这个model IO，我们学习了这个memory，学习了chain，学习了agent，学习了data connection，然后我们有一些测试用例，接着我们把这个用南茜版本实现的OpenAI translator放在了这个里面，外面有一个是基础篇的，里面这个是用连线版本重新实现过一部分的。大家可以点开它之后，能看到这里是这个2.0版本的OpenAI translator，里面也有一个jupiter，这个jupiter里面有这个translation券，包括我们周日会讲的用flash实现的这个web server和这个radio的图形化界面也都已经传上来了。那我们现在就先到这个translation券，这个translation券就是我们刚刚看到的这个PPT课件里面的，截取到的这个代码就取自于这里。
	那我们现在就来实际看一看这个项目，这个notebook这里，这个是这个translation chain，其实既是一个让大家理解怎么样用南茜去重新实现open a translator，同这也是通过这个让大家更深入的去了解这个核心的模块，就chat model，这个chat model的意义很大。第一不是因为他有这个多角色的能力，对吧？我们之前的那个LLM其实是不支持是这个多角色能力的那通过这个chat model，我们可以去使用之前学习的多种角色了。这个多种角色在我们的系统里面，除了刚刚提到的system AI和human以外，它其实也支持我们我们在基础篇学到的这个function calling，也能通过chat model来调用，逻辑也是一样的，大家可以看看文档就知道怎么样去调用它了。类似的这个chat model它也需要自己的提示词模板，那他就要使用这个chat from complete。比如说我们最早看到的这个model IO的下面这一条线，就是他能使用LLM，也能使用这个chat model。具体来看的话，我们之前的这个调用也在这里，让大家有一个直观感受。
	最要理解这个过程，其实最直接的一个理解方式就是以终为始。就是我最终要用的是一个有聊天能力的大模型。这个聊天能力的大模型需要输入的是一个消息记录，这个消息记录在在南倩里面的抽象，就是在他的这里南茜的这个schema下面有各种各样的message。这些message其实就是我们这里看到的不同的角色加上它的内容，我们可以通过构造这个message去调用对应的这个大模型，然后它最终输出的就是我们的这个结果。我们可以看看这个message被打印出来的话，其实就是长这样的一个message，跟这儿定义是完全一样的。然后我们去调用这个GPT3.5 terble之后，它会给我们一个GPT3.5的返回结果，是一个AI的message。这个是一定是一个AI的message。会把我们的这个生成结果用AM message这样的一个抽象承载起来，然后里面的内容就在它的AI message的content这个成员变量里面，这个是最直接的一个理解方式，就是我们构造message给GPT3.5或者给GPT4，它就能给你一个结果。
	但我们实际的使用会更高级一点。因为我们在构造message的过程当中，我们需要给他一些变量，那这个时候就要开始学习怎么样用我们的这个蓝券构造我们的chat的from tempt。那构造它的方式和我们普通的LLM1样，只不过它需要有一个新的方法。因为它是要构造message，所以它不是from template，而是from messages，类似维护这个message的模板和维护message一样。只不过它的这个大家可以简单理解维护这个message的模板和维护message最大的区别就是message的模板里面还有一些变量，是要等我们真正去调用的时候动态传进去的，就这么一个区别。
	这个chat prom plate就是维护的这个message的模板，我们能看到，这里我们去首先导入了在这个proms的chat这个模块下面的四个不同的提示词模板，它是带有角色的，我们首先要构造的就是这个system的这个模板，说是模板。其实我们在为了让大家理解这个过程，这个里面我们没有加入变量，就直接告诉他你是一个翻译专家对吧？你精通各种语言，然后我们现在需要你做的事情就是把英文翻译成中文。然后这样的一个template，其实就是一个python的一个字符串。这个字符串使用这个from template就可以构造出一个prom template，这么一个所有大模型都可以用的。最底层的抽象就是我们的提示词模板。所以大家可以理解成这三个，其实也就是三个基础的prom tempt。唯一不同就是它针对我们不同的角色做了一些预定。
	这个我们在讲年线的基础模块的时候，也有跟大家说过，代码是怎么样去逐步设计的，然后怎么样定义的，大家可以复习就能够理解这个逻辑了。我们通过这个from template的方法，我们能够看到定义好了一个system message的一个prompt，就是我们给系统这个角色的一个prom。它其实是一个complete对吧？它其实是一个提示词模板，然后这里不需要任何的输入参数，因为它就是一个静态的一个字符串。只不过他可能每一次去调这个GPT3.5的时候，都需要传入这个模板。然后他的这个format方法用的是f string。对。
	然后类似的我们把这个待翻译的文本，就我们要翻译什么，我们用human有人人这个角色来传，有一个变量text。好，同样的我们把它输出出来。这个时候我们能看到它有一个input variable，需要在format这个prom的时候去生成真正的需要翻译的文本。
	然后这里最关键的一步是构造这个聊天模型需要的提示词模板。使用这个from message的方法，把刚刚这两个模板放在这儿。然后如果我们实际是什么样的顺序，就按照这个顺序去整合就好了，我们做其他的应用的时候也是一样。然后我们把它这个输出出来很乱，这就是我刚刚说的你在实际构造的时候，可能甚至都不止构造这两个角色的这个prom template，你可能还会构造更多的这个template。
	那这儿就更不好看了，对吧？就这个聊天的这个提示词模板就更复杂更难看。所以这里就是给大家提供了一个小工具，这样的工具很多。我这边只是随便找了一个广告比较少的，然后还比较常用的这么一个工具，就把这里的这个message是最核心的，前面这些都是输入的变量和这个output power对吧？就它的I和O的部分，但是真正的最核心的模型的是message，那怎么把这个message能够清楚的做一个查看呢？就通过这个地方，我们能看得到有一个python的formatter这么一个在线工具。就我刚刚打开的这个工具，可以把这个message拷贝过来，然后点一下这个format，他会在线的把它转成一个这样的形式。如果你这儿构造了很多个，那它也能够这样平铺下来，让咱们能够比较清晰的去了解我的这个message的模板，它是长什么样的。
	这就是我刚刚从这边拷贝回来的一个结果，大家能看得出来这里system是长这样，然后它没有这个需要的变量。然后我们的human是长这样是长这样？他们需要有一个在真正format。这个聊天模型其实是模板的时候才传入的一个参数，就我们的text，这里真正的这个chat prompt是长这样的那能看到这里有prompt value，其实就是一个给聊天模型用的真实的聊天记录，它里面有这个message，那message里面有这个system的message和我们的human message就已经传进来了。那这个相当于就是把这部分内容丢给我们的聊天模型，就可以真实的去做，生成了。
	那这里，还有一个很重要的点是在于我们的普通的语言模型，因为它就是一个字符串，丢给大语言模型就好。但是聊天的这个模型，它需要的是一个聊天记录，并且聊聊天记录是有角色的，所以它内部做了各种角色的封装。为了能把这个信息变成一个我们普通原生态的去使用OpenAI的这个API构造的这个python的列表，然后里面都存的字符串。
	如果是要构造这种类似的形式，有一个叫two messages的方法，就是把我们刚刚就这儿生成的这个内容，它是一个chat prom value，这个是一个能劝自己的抽象。但是我们如果要把它变成一个message，可以调用这样的一个two message的方法。这个也很好理解。构造这个模板的时候，我们有一个from message的方法，里面传的全是模板。那真正我们要用的时候有一个to messages的方法，就from to？对称的。然后使用这个to messages的方法，我们能看到就构造出了真正可以给聊天模型用的这么一个message的一个列表。这个列表和我们最开始看到的这个例子其实是完全一样的。
	就我们手动构造一个不使用这个from ten place的时候，我们手动构造也是这样构造的，维护一个messages的列表，里面有不同的角色和它的内容，以这样的方式在使用。我们通过模板最终也是为了达到这样的一个效果，其实就是长这样的to messages。到这儿其实就完成了我们从普通直接手动构造这个message，到使用chat from tempt来动态的构造这个message，不同之不同的方法。但是其实他们最终的这个结果是一样的，都是要构造一个messages，这个message是南茜的schema里面定义的一种抽象，它能直接丢到我们南茜的chat model里，那就是丢到我们的这个chat open API类似这样的一个chat model里面。
	我们看看到其实这里，我们把这个GPT3.5 turbo作为了translation model的基础的大语言模型。然后这个大语言模型，把这个chat from丢进去，也就是这里我们刚刚看到的这个message丢进去。然后进行一下对应之后，我们我们其实已经知道这个地方的translation results就是一个AI的message，对吧？我们可以再看一眼。其实就是一个AI的message。这个AI的message调用它的content方法，这个就是拿到了他真实的翻译的结果。
	整个流程其实就是向大家展示了我们在一开始讲模块的这个基础能力的时候，可以使用这个聊天模型手动构造。但是其实南线也提供了去动态的构造提示词，用其实用这个聊天的其实是模板的方式动态的去构造聊天模型。用的这个提示词的这个消息记录，就是我们刚刚看到这一套方法其实也很简洁。通过这样的方式，我们可以不断的动态的去生成聊天模型需要的这个聊天提示词。并且通过这个聊天的这个模型，拿到了AM message和AM message里面的具体内容。
	好，然后我们再接着讲一讲怎么样把聊天模型构造成一个LLM券。这里我们看到还是同样的方式，从这个能欠的欠里面去引入最简单的这种欠的使用，就是用的这个LMT。它就相当于把我们的prompt，每一次用我们的传入的这个提示词模板的方法，加上我们每一次传入的这个q value，他们俩一起构造出这里的chat from就我们这里这个值，每次都可以通过传入的这个key value动态的去生成这个chat prom。然后把这个结果再传给我们的这个translation model，就传给我们的这个聊天模型，其实就把上面这一部分的内容，就是我们这儿再加上这儿这两部分的内容，通过一个LM券能够管理起来，我们可以看一下，定义好一个translation chain，这个chain result可以通过这个run方法拿到，然后这打印出来。那类似的比如说我们想要再翻译一个别的内容，我们可以怎么做呢？为了直接输出，我们就直接这样运行，比如说我们把这个改成。
	应该给英文。
	你看这个地方，这个LM其实被翻译错误，变成了法学硕士，但这个不重点。我们能看到的是要做翻译的话，就能够通过这样的一个方式去快速的把我们的要翻译的文本反复的去执行这个软方法，这个统一的接口就能够完成这个翻译了。类似的比如说我们要翻译之前，我们的这个样例里面有一个表格。他也能够把它很快速的去做这个翻译。
	然后除了这些以外，其实我们刚刚为了简化chat这个聊天模型里面的这个from tempt，我们没有把系统的这个提示词去做参数化。但其实要去支持多语言对的翻译，在这样的一个设计下面就非常简单了。因为我们要翻译什么内容，其实是通过system的这个角色去注入的那我们只需要把翻译的这个语言变成这个系统的模板里面的两个变量就好了。比如说这个source language，我们的这个target language。那这个时候我们再去构造这个系统的这个tempt的时候，它自然就会带上两个参数，对吧？我们可以看看。好，我们输出一下。
	我们在run的时候增加了两个参数，一个是我们的这个语言，从音译中我们可以改成中译英，对吧？那么这个中译英就只需要调整这两个参数就好了，比如说你现在输入的就是这个中文。他会帮你把这个英文给输出出来。类似的比如说我要把它翻译成，比如说日语，对吧？
	就可以直接翻译成这个日语。所以通过这个方式我们能体会到年轻的价值，这是第一个。把大语言模型和prompt，它能够以一个很巧妙的方式封装放在一起。同时我为了实现很多复杂的功能，不同的语言。然后我为了有一个统一的接口，能够方便我管理和调用。那通过这个translation券，它就能够实现实现出来非常简单，在这里留了一个小的家庭作业给到大家。第一，我们刚刚其实已经在改这个system的from complete，我们把它扩展了两个新的参数，一个是这个source language，一个是这个target language，就能够实现多语言。对了。
	但是我们在实际的翻译场景里面，通常为了信雅达，我们翻译的这个词具体的选词会有一些不同。就比如说一个很典型的例子就是LLMS这个说法其实它是有歧义的。如果你没有给清楚它属于什么样的一个语境，因为法学硕士显然是一个在GPT3.5的大模型本身的学习语料里面的权重更高的。LLM其实是一个后来的词汇，就是我们在训练GPT3.5的时候还很少有LLM的说法，对吧？但我们如果把它变成一个large language model，它可能就会翻译对了，对吧？因为这个就很明确。
	那类似的这个意思是什么呢？就是说为了更好的实现翻译效果，如果大家真的要把它落地的话，这个system的prompt肯定可以再做更细的设计。现在的这个problem在这个system这个角色里就只是强调了它是一个翻译的专家，然后精通各种语言。但如果我们现在要翻译小说，那你是不是可以给一些更精确的一些prompt给这个system。
	这个也是我们在基础篇教过的，就是我们为了让这个大模型生成的内容更精准，效果更好，我们的system这个角色是非常关键的。要尽可能的去精准的去描述你的任务，这个是第一个我们的homework，就是我们能不能让大家自己去把这个system prom再改，然后让它的这个效果做更好，甚至可以去对比？怎么样对比也很简单了？你就像简单想象成你做了两个欠两个券是由不同的这个system prompt构造的，然后你每次就去同时跑这两个券输入都可以是一样的。跑完之后，你可以把结果再去做一个对比。不管是用表格的形式，还是用这个panda的这个panda as的这个data frame的形式都可以。你可以自己看一看不同的这个system的prom，会带来什么样的一个效果。
	第二个就是说这里还有一个点，我们的翻译任务的使用场景。通常来说你要翻译的内容比较多的时候，你是不可能一次性丢给run方法的，你要把它拆成不同的段落，然后就为这个大模型去做翻译。这个时候你的source language和这个target language其实是定下来的对吧？你不用每一次都传，那这个是一个不算是很难的一个技巧，就需要大家在写这个代码的时候，可以去看看有没有一些什么样的方式。它其实是一个引子，是一个抛砖引玉。它就决定了其实跟我们之前讲某一节课的时候很像，就讲这个提示词模板的时候，提示词模板本身也可以有嵌套，有模板的模板，对吧？
	那同样的在这个模板里面，有些变量是每一次都需要调整的，有些变量不是每一次都需要调整的那我们在设计这个大语言模型的这个应用程序的时候，怎么样去做这一层的设计，哪些变量是可以有默认值的，或者说有一个固定值的当它传进去之后，我如果应用的场景没有变，我都可以不去变它。那哪些又是一直会去变的，这些其实是很值得大家去玩味的。就是在做这个AI大语言模型应用开发过程当中，prom是怎么调整，然后prompt的变量要怎么样去做巧妙的设计。
	好，通过这一部分的介绍，我相信大部分的同学应该对我们的聊天模型和聊天模型的这个提示词模板有了一个更深的理解。然后我们也通过这个check model和我们的check from template，完成了我们做多语言对翻译的一个设计，并且把他们俩整合成了一个translation chain。那么到这儿为止，看看大家有没有什么问题，我们用十分钟的时间回答问题，尤其是有很多同学可能没有紧跟着我们的这个学习进度，可能会有很多问题。那我们来密集的回答一波，我们跳到最后这个部分对。
	大家有什么问题？这个怎么保留上下文的内容？咱们是想保留什么上下文的内容？这个其实在memory的时候已经讲过了。这个同学如果是想说我们的这个聊天模型的这个结果怎么存的话，可以翻到memory那一节，然后和对应的notebook的demo里面会有讲我们的conversation的这种memory应该怎么存，都有这个对应的方法的。对，你看有同学这个就学习的很好的，就知道memory模块讲过的我们的conversation相关的memory。
	有的同学问有的欠是运行转方法，有的是直接嵌输入，有什么区别？欠输入是指什么？是直接我没见过欠输入。欠输入这个同学可以具体举个例子吗？还没看过。你如果是指的这样的一个方式的话，这个是它的构造函数。它的构造函数通常是直接用来定义一个特定的欠他要用什么基础模型和用什么提示词模板。但也许他他自己写了一个什么方法，这个方法就是去复写了这个直接调用它。
	这是家庭作业，然后这个什么只编写一次，不是只编写一次的意思。大家可以再细看一下，是指只传入text的这个变量，不是每一次都需要传入三个变量。这个是跟咱们应用开发具体实现有关，待会我们在下半节课的时候也会去讲。对。看大家有什么问题，感觉大家没有提这个太多问题的话，我们就往后走了。
	对翻译这个场景确实不用保留太多上下文，最多保留需要翻译的source language和targets language就好了。
	对，这个同学说的很对。如果我们用python自己的pya native，就python这个语言特性来用这个扩方法是可以的。但我不建议这样做，因为直接用run方法可能相对来说可读性更高，大家也能一目了然的去理解对。
	大家还有什么问题吗？这个mr store推荐的更新策略，跟我们这会儿讲的没啥强相关的这个关联性，这儿我就不再解释了，而且相对来说这个问题比较泛。
	第二个homework work没看懂，这个应该我再解释一下。第二个homework是什么场景？这个homework就是说我们现在要翻译了，然后这个翻译你想象一下你的这个同事正在给你提需求，他的需求是我需要翻译，接下来我需要把中文翻译成英文，然后我要开始说我要翻译的内容了，然后说了一大堆话，接着他又说了一大堆话，但是他说第二段话的时候，他不需要再告诉你，我需要把中文翻译成英文，对吧？所以有些变量是一次性在初始化的时候就可以设置清楚的那有的是每一次调用的时候都需要传入的那这个场景是很常见的那对于我们来说，我们要怎么样去考虑这样的一个设定。有的，这里就有很多种方式了，把prom ten place是不是可以拆出来，一部分是传语言的，一部分是传内容的等等等等。这就不再赘述了。而且我们还故意做了区分，这两个language是system的这个变量，text是我们的用户测的这个变量，这反正可以玩法很多，是为了让大家去熟练我们的年限的使用和你自己的场景抛的一个专。
	扩展多语言翻译的时候，是不是可以使用root chain来选择从某种语言翻译到某种语言？通常来说感觉我理解一下这个多语言翻译，您是指我需要把语言，就是我需要输入一段话同时翻译成英文和日语吗？如果是这样的话，不建议用复杂的这种设计。就loser，这也没必要，你就直接写清楚，你要翻译成两种语言在system里面就好了。除非你是说你还不确定你要翻译成几种语言。比如说你现在的需求是用户告诉你我这会儿要翻译成三门语言，下一次要翻译成4种语言。那这个场景就可能还比较适合用root茜。如果是他已经确定了要翻译成几种语言的话，那这个时候一次性把它写清楚比较好。
	对我我的建议也是就翻译这个场景来说的话，不建议自动识别。为什么这么讲？给大家看一个最简单的例子，就是不是不是一段话一定就是通过输入，一段话并不总能确定你要翻译什么语言。我们看一个最典型的应用。比如说谷歌翻译，这是一个非常经典的应用了。就算是谷歌翻译，它也会告诉你，我是可以帮你自动检测你要翻译什么语言。比如说。
	他可能把你检测出来了，这是一个中国的这个应该叫什么简简体中文，不是这个繁体中文版。但是在一些西方语言，比如说一些小语种里面，其实它一样用的是英文字母，但它就不一定能够检测出来它是一个什么样的小语种因为我们实际支持的这个翻译有这么多不同的语言，你很难区分出一些小语种之间的这个语言它到底用的是什么。那我们还是建议把这个输入是什么语言作为一个显示的变量传进去，这样是比较稳定的，当然你能检测那也行。
	好，我们十分钟的时间回答问题，这会儿大家应该也没有太多别的问题了，我们就进到第二部分。就是我们怎么样用蓝券来优化这个OPS translator这个架构设计，其实核心就是我们刚刚体会到了聊天模型挺好的。然后我们的这个聊天模型的提示词模板也还不错，他们能够通过一个券组织起来，然后用一个统一的接口来做翻译。
	那么具体来看我们的OpenAI translator会怎么样去做调整？熟悉这个open a translator代码的同学应该能看懂这个UML内图，就我们在GPT4生成的这个OPI translator v1.0这个项目里做了一些抽象。第一，为了能够去支持不同的大语言模型，我当时给做了一个抽象叫model。这个其实跟人的想法很相似，蓝线其实底层是一个python的一个抽象方法抽象类。这个抽象类衍生出了不同的model，包括我们的这个chat model和我们的LLM。
	这个model里面有一些很重要的方法。第一个就是我们的一个大元模型，最重要的是要能够给他输入一些prompt，让它生成这个对应的结果。我们做了一个叫什么方法呢？叫translate的这个from方法，就是给所有的，因为我们做的这个项目比较简单，没有做复杂的抽象。我们的这个基础积累模型里面就要实现一个translate prom的方法。这个方法接受的输入是一个我们定义的抽象，叫做内容content。
	这个content里面就会分普通的文本类的和我们的这个表格table content。大家如果有印象的话，同时他还接受一个target language，就是他要翻译成什么。因为我们1.0的版本里面，我们接受的其实是一个英译中这么一个设定。甚至我们在第一个版本里面，这个target language都没有开放出来，他就默认翻译成这个中文，输入是英文。
	然后为了实现这个translate prompt，我们没有去用这个大语言模型的能力来处理这个文字和表格。因为当时用了这个ChatGLM不算特别稳定。这个chat GM6B如果不去处理这个表格的话，它会格式混乱，回来的这个结果就不太稳定。但是我们当时在讲这个1.0的这个架构可以做的一些拓展的时候就提过，表格类的这个文本的处理，其实是可以交给大元模型来处理的。今天我们在扩展2.0的时候，我们也会这样做，我们会把一堆表格，就怎么样去处理这个table content交给大语言模型来弄。那他会给你一个相对还比较不错的结果，因为GPT3.5的能力还是很强的。
	好，有一个抽象的方法，或者说需要子类去实现的方法，这个方法叫make request。其实就是去调这个模型，就跟我们的用连线的时候这个model传一个具体的prom，它就应该能去调用。那么这个基础类其实它没有办法具体去调用，要看子类是什么再去调用。OpenAI的model，the make request其实当时就用的OpenAI的API。那么GLM其实就是直接去调用一个GLM的私有化部署的模型，有一个1L输入的其实就是这个prompt返回。
	别的就是这个translation，就是它生成的这个结果。就跟我们刚刚用刚刚的上半节课讲的，就是我们构造了一个message。那其实就在这point让我们得到的是一个AI的message，里面是我们的翻译结果。我们的translation是一个这样的一个类型，然后还有一个status用来标志我们的翻译是成功了还是失败了。失败了这个translation可能就是原文没翻译，这个类图是非常简洁明了的，那我们可以看一看对应的这个代码是怎么实现的，给大家回忆一下。
	在我们的这个1.0里面，我们的这个model。IT had to sleep a model. 为什么还有个球拍的？这当时给大家讲这个PDF的解析的时候，生成的这个大小大家能看见吗？我们再放大一点。对。
	大家能看见吗？我看一下大家的评论。这个代码应该能看清楚，对吧？
	大家能看清楚的话写一下这个一，我去吧OK那么我们能看到刚刚聊的这个model的抽象，这个model的抽象最重要的就是这个translate prompt。这个方法就是用来构造我们的这个提示词的，因为我们自己没有去构造一个提示词模板，这样的抽象在我们的这个V1.0里面，所以我们而且我们也只是干这个翻译，我们就简单写了一个方法，translate prom的方法。这个方法根据我们输入的内容不同，我们会去调用具体的方法。比如说普通的文本翻译就直接写了一个翻译为对吧？后面就跟着这个内容，这个是还有个语言，这个语言默认其实就是中文，这个也其实是啊现在在看这个函数的这个写法，就跟我们用南茜的complete很像，这个抽象这俩就是传进去的变量，所以这个其实就是我们的prom temple te，只有一个f stream，对吧？那么整个底层的设计，其实你只要玩大语言模型，基本都这样的一个设计。
	在这儿我们的表格翻译略有不同，比如说保持间距，空格分隔符以表格形式返回不了。这个是为了针对不同的大语言模型，就比如说我们用chat GM6B它可能能力没有3.5强。如果你不去保留这些prompt，他可能就没法给你返回一个表格了。用人券来实现的时候，就可以根据不同的单元模型，我们去设置不同的prom template，这个是可以去做的。然后在子类里面我们要去实现这个make request的方法这个如果没有实现的话，会抛出这个错误，对吧？
	那我们看一下OpenAI的model是怎么定义的那OpenAI的model这里导入了这个OpenAI，这个就是open I的python的这个SDK。通过它我们能够访问open I这里open I的这个key其实是从这个环境变量里面读取出来的。然后如果我们使用的是GPT3.5的这个turbo我们会去构造对应的给GPT3.5的这个prompt。这其实就跟我们刚刚很像了，是一个message对吧？那open I的API也是支持不同的角色，有不同的内容，跟我们刚刚看的是一样的。然后我们没有把这个版本，我们没有把多角色用起来，我们直接把刚刚说的这段话放给了这个user。这个用户没有去剥离system和这个user。这一部分其实大家在做这个家庭，做这个基础篇结业的这个作业的时候，有些同学也做了改造，非常好。
	然后最终我们通过调这个OpenAI的API，这个check completion的API，拿到了结果。然后从结果里面根据他的这个结构取出了这个content，然后做了一些处理。如果我们是调的这个分歧003这样的模型，它是用completion的这个API对吧？然后直接去构造就好了，然后拿到这个结果，这个是它的make request的方法。
	但是通过这样的一个实现，我们就会发现包括在做了三次尝试，如果我们有这个rate limit error的话，这个三次尝试是用来解决这些一些问题，然后呃那这样的一个实现会带来好处就是它足够灵活自由，尤其是我们不会用连线的话，它已经能让我们支持不同的大语言模型了。但是它还有一些坏处，坏处是什么呢？我们可以看到它它有一些问题和挑战。我们我简单列了几个比较关键的。
	第一就是说如果我现在要去接别的大语言模型，比如说我接GLM model，我需要实现一个GLM model的子类，他继承了这个model。比如说我要去接这个apple OpenAI，它也要去实现这样的一个子类，甚至我的这个text prompt和这个table prompt还得改写，就不能用这个鸡类的这个方法，要自己再去重新写重整重写一下这个方法。这个是因为不同的机电模型，它适应的这个prompt的不一样。那整个这部分的工作其实能欠都做过对吧？
	然后第二就是说我们能看得到我们的OpenAI model要去make request的时候，这其实用make request已经是为了尽可能去抽象一个类似于券里面的这个run方法，对吧？它其实就是去发请求，跟这个run是很像的。但是具体的每一个make request你都要去写一遍。OpenAI model你要写一个GLM的model，你要写一个edge OpenAI的model，你需要写一个类似的其他的nama这样的模型，你都要去写这样的一大堆不必要的实现。你需要去实现make request，你需要自己去管理这个prompt。然后这个prom和model还是耦合在一起的，因为这个prompt其实在model里面定义的。
	然后完了之后，我们刚刚看到的opi model里面，你还要去区分你用的是这个GPT3.5，还是用的这个达芬奇003，你需要去做if的判断。但是其实这些工作南迁大部分都已经实现过了，对吧？就比如说我们刚刚看到的这个重复的大模型扩展的工作，门券的框架其实已经实现了很多模型的对接了。这个在官方文档里面也有，之前的课程里面也都讲过。只要是南线框架支持的这些大模型，你就直接用就好了，你不需要自己再去实现一遍这个模型的子类。
	第二个就是说这个接口，因为model IO的这个抽象我们已经讲过了，所以就不再赘述了。这个model IO就是覆盖了它支持的这些大模型，然后有标准的接口，你可以通过这个标准的接口去实现它的prom tempt，去对接它的LLM或者是chat model。然后完了之后，你可以用券。比如说我们刚刚的示例里面，我们通过这个AIM chain可以用来管理prompt和model，就把这俩作为欠的这个构造函数里面的输入就好了。那么用嵌来统一管理prompt和model，你就不用再手动管理。现在这个实现是我有一个model，model有这个prompt的一些方法，用model来管这个problem，然后以及这个model的请求。但这些都可以通过change来管理起来。
	区分不管是LLM还是chat model，这是南迁里面的抽象对吧？在大语言模型里面，抽象就是一类是生成模型，一类是聊天模型，这两类框架也原生支持并且区分出来了，那么我们就不需要自己再去做这个区分了，这是比较典型的几个问题。就是如果用券来实现的话，我们能得到的一些收益也很直观。比如说我们要去构造一个translation券，其实就是把我们刚刚都book里面的一些关键的内容丢进来，这个translation券接受的一个输入是我们的这个model name。然后我们因为是一个翻译任务，所以这个基本上已经确定了，我们这儿就不再把from tempt丢出去了。
	但是如果我们这儿想要大家想要去根据不同的这个语言模型再去做抽象的话，可以把这里的prompt这个template这一部分的内容抽出来，变成translation chain的一个构造的一个参数。那就可以根据不同的model name，我们去构造不同的这个chat from content。然后这里我们直接使用了OpenAI的这个模型，也是为了简化。后面大家想要去支持不同的这个模型的话，就去改这个model name就好了，这里也需要去做对应的改造就好了。这基本上就通过一个LM券，就把我们的刚刚看到的model的抽象子类的实现，就通过这个transportion券就可以覆盖了。然后我们的make request其实就跟下面的run方法是一致的，我们的所有的子类的make request都可以通过这个run方法来做替代。然后他能接受三个参数，分别是我们的text，就是我们要翻译的内容，以及我们的原语言和目标的语言。通过这样的一个方式就可以传进来最终的反馈结果也跟之前V1.0保持的一样的。
	就是我们的翻译内容和我们的是否是翻译成功。那我们解耦这个model和prompt，还有一个点就是在于这个接口的一个统一。我们通过这个run方法就可以在之前这个主逻辑里面，我们需要去逐个content，就我们自己定义的这个content逐个的去做翻译。这个翻译可以通过这样的一个统一的接口去做了。我们也对这个content做了一个小的抽象，也用到了一个魔法方法。就跟刚刚有同学问这个call方法，其实python的机制还是很值得学习的，有很多很好用的方法，这个待会我们在代码的时候会细讲。
	那之前这里需要去构造一个prompt，这个prompt里面需要根据你的子类。大家看这个self其实是一个具体的子类，对吧？然后你要去在子类里面实现自己的这个prompt，然后再把它翻译过来构造一个prom，再丢给这个model，去实际去做请求。这样就通过一个统一的接口去做构造就好了。
	然后同时我们为了更好的去管理这个配置，尤其是为了后面图形化界面这个web server，我们把这个配置上也做了一些升级，我们做了一个translation的conf，这是一个单例的一个设定，就我们整个全局就设置这么一个conflict，这config怎么来的呢？我们首先这个程序要启动，我们会运行一个python AI translator斜线main点PY这么一个启动这个项目的一个pyo文件面点PUI。然后我们通过拍摄去启动它，给一些参数。以前的这个大家会觉得这个要给很多的参数，对吧？但其实你只需要在之前1.0的时候，你只需要去确定你要给什么样的模型类型就好了。大部分其实config里面都能直接取，它有默认值。那现在也是一样的，我们通过这样的一个代码逻辑的实现，把这个逻辑捋的更更顺畅，更简洁一点。
	第一，我们还是会用这个argument part这样的一个类，在我们的ut utilities这个模块下面，仍然会有这个没有变化，只是说它的参数有一些调整。然后我们仍然是用它来解析命令行里面的结果，并且存在这个August里面。这个是一个很标准的python去解析这个命令行参数的一个做法，这就不再展开了。然后把它作为我们全局的单例配置的一个输入，把这个config的初始化传进来。他要做的事情其实就是很简单。
	简单一点来说就是我们这个逻辑就是想说有一个config压mail文件，我们推荐使用这样的一个配置文件的方式来传配置。这个配置文件的路径我们需要对。那这个配置文件的路径是我们去启动这个面，启动这个程序的时候，最重要的一个配置项，或者说最重要的一个启动参数。那这个启动参数如果没有的话，那就用我们系统指定的那个config e mail，它有一个默认的一个路径。但如果你指定了一个新的config email文件，那你就写清楚那个路径是什么就好了。那他就会去读那个config email文件，把它加载到这里面来。
	这个时候假设你在运行的时候，你会有一些有经验的同学，他会在生产环境用这个配置文件。然后在这个开发测试过程，可能会传一些临时修改的参数。那这个时候我们可以通过传入这个命令行的参数去覆盖，就它优先级更高。比如说我现在在在做这个开发测试在第八，然后我的生产配置文件我不想动，它就保持那个状态不动。然后同时我在开发测试的时候，我用一些本地的一些参数去覆盖它的时候，可以通过我们的这个命令行。这儿它会去从命令行解析？各种各样的参数，命令行解析的这个参数的优先级比配置文件更高。这样我们可以实现一个不修改生产文件，生产环境配置文件的前提下，为了开发测试去更好的解析这个参数。
	但是又不用去做代码层面上的调整的这么一个设定，通过这个配置的单例来实现这么一个设定，然后这个就是我们的translation conflict，大家可以看到他用了一个单例的一个设置。这个单例模式是很经典的一种设计模式，我这边就不再赘述了。简单来说就是我们通过initialize的方法，可以去初始化一个config的实例。然后我们去初始化的这个实例里面，最关键的就是要去加载这个配置文件。大家看到这里有一个配置文件，那这个配置文件会被加载存到这个config里面，这一部分跟之前的加载配置文件是一样的。然后这里有一个新增的逻辑，就是我们的这个over right value，就是我们复写的这个逻辑。就我刚才讲的，我们会从这个参数解析，参数解析器就这个argues里面去解析。这个解析出来的这个值，如果它的P和我们配置文件里面一样，那它优先级会更高，它会去更新。那么这个里面会去更新这个命令行传进来的值。然后最终把我们的全局的单例的配置就设置为我们的这个country。
	之后如果我们要再去改它，那我们是就直接去用这个translation config就好了。然后如果我们要多次加载，它已经存在了，我们是不会让他去做这个设定的。然后如果我们要去取这个对应的key的时候，我们也不用每增加一个变量我们都去写一个key。通过这个给get attribute方法，其实就可以很方便的把我们我们的这个config变成一个成员变量，就通过这个方式就可以去获取这个值了。待会我们看代码的时候应该有很直观的一个体会，我们实际来看一看这个代码是怎么实现的。
	那这儿就到这个南茜下面，我们有一个OpenAI的translator，那这个OPI的这个translator下面已经没有model模块了。在这儿看看已经没有这个model模块了，会有这个translator的模块，这个是主逻辑，然后这个book我们没有变过。然后在这个utilities里面，我们把这个config loader和这个translation config去做了一个合并。那后面的这个配置管理，就由这个translation config来统一做这个处理，然后我们把这个收起来，能看到这个面点PUI文件我们也做了一些调整。我们之前这个地方的加载很复杂，我们再做一个对比，在老版本的实现里面。
	这个是我们之前的一个V1.0的实现，我们会在这儿加载这个命令行的参数解析器，然后我们也会加载一个配置文件的一个config loader，对吧？然后这里其实就指定了这个配置文件的地址，然后这会去加载这个配置文件里面的配置，然后我们会去做判断。这其实也是一个优先级的复写，就我刚刚说的这个all right这个value对吧？就如果我们在命令行里面传了这个open I的model，他会去选用这个命令行里面的这个model，不然的话他就会去取这个配置文件里面的，类似的在这儿去做这个模型的初始化，然后去娶这个要翻译的文件，然后取它的翻译的这个格式是，然后再把它传进来，这个是之前的一个实践方式。
	现在，我们把改成了什么样？第一个，配置这一块，我们通过translation的config来做一个全局的初始化的一个设计。然后在这儿，我们可以通过这个initial ize去把这个命令行里需要解析的这个参数给取出来。最重要的就是取这个。配置文件的地址。
	除了配置文件的地址以外，我们也做了一些调整。我们可以看一看，第一配置文件的地址是我们最重要的这样的一个命令行参数，默认的这个值最后它是有默认值的。就假设我们现在什么都不传的话，那么他就会默认直接去取这个样例的配置里面的结果，甚至这个路径都是直接使用的样例的这个配置路径。剩下的这些参数其实都是如果你不传的话，那么他就不会去做这个复写。但是如果你去在启动它的时候，通过这个命令行参数去传入的话，那它的优先级会更高。
	然后有一些参数，比如说我们传入的这个model name，这个model name就对应着我们蓝券里面的这个模型名称，还有这个input file，output file format，这里我们把一些参数名也做了规范命名。首先输入的这个文件就是我们需要去翻译的这个文件，我们把它重命名叫P这个input file，输入的这个文件。我们输出的这个格式，我们之前默认是用的这个markdown。那这儿也是一样，我们通过这个output file format这个参数来做约束。然后我们输入的这个文件所使用的这个语言，我们使用的这个source language，然后要翻译成什么语言，用他get language。这个和我们的更新后的这个config压面是能够一一对应起来的，大家可以看到这里就是它的这个默认值。那么简单来说，如果我们现在什么都不做，直接去执行这个面诊PI文件是可以的。我们可以看一看。
	他这边打印出来一些日志，就包括我们的这个row text。这个是之前V1.0就保留下来的日志，我们的这个相当于解析PDF这个环节的日志。我们使用了一个translation券，这个translation券把它的LM券内部的结果打印出来，待会我们会去解释这一部分。
	之前的课程你也学过，我们这里再加强一下这个使用方法。这个是它的LMT内部的这个输出结果，大模型的prompt system是这一段内容。我们要翻译的具体的这个呃待翻译文本，在这个human里面有对应的结果，这个是第一段。第二段也是一样，我们把这个system和这个human丢进去之后，翻译出来这个对应的结果，然后这个是我们翻译之后，再把这个翻译结果写到这个table里面的时候的两个日志输出。一个是直接获取到的翻译结果和它转换成这个pandas data frame的结果。然后这里有一个第八个的日志输出是我们最终保存的是什么格式。然后在保存的时候，开始导出到这样的一个文件，翻译完成，文件已经导出成功了，保存到这里，我们也优化了一些简单的日志输出。我们接着看一下这个配置管理更新之后，我们的主流程有一些什么样的调整，到了这个PDF translator之后，我们在这个部分也做了一些优化，以前我们这是实现了一个self点model，大家有印象的话，那现在我们就直接通过这个LM就translation券把它直接整体接管过来。
	大家应该在刚刚的这个描述里面，和这个实践里面能感受到，以前是通过大模型加上大模型自己的子类去管理这个prompt。那现在我们只需要去实例化一个translation券，它输入的这个构造函数的这个变这个构造函数的这个变量就只有一个，就是我们的模型名称。那这可以去传GPT3.5的turbo，也可以去传我们的GPT4。对吧？那在这儿其实我们能看到它的默认的这个变量是一个GP3.5。当然我们在config页面里面也可以去复写，它这就会被真正的传进来的model name给覆盖掉。
	然后刚刚我们看到打印出来有很多的日志，对吧？这里有一个小细节我们再强调一下。首先这个verbs是用来控制N里面的大语言模型、聊天模型和我们的嵌的一个日志输出的一个详细程度的。它广泛存在于男性的各种抽象里面。在这个verbals里面，在这个verbs我们能看到，我们没有让这个外部去传入，直接在这儿去做的这个设定，也是为了让不熟悉这个代码同学能够始终看到这个日志输出。但如果你不想看到这些中间结果，你把这里设置为这个false就好了，或者说直接把这个注释掉。因为它默认是force。
	然后在这儿有两个地方的verbs的这个值，我们都设置为了true。一个是这个大语言模型，就我们的这个聊天模型，chat OpenAI这个聊天模型。一个是这个LM券，就我们自己定义好这个translation券，这俩都需要设计。如果我们没有设置这个大元模型的话，你最终看到的日志其实是他能告诉你你进入了这个AM券，但是这里的prompt你是看不到的。这个我们之前的课程也讲过了，这就不再赘述。然后这个run方法也是一样的，我们通过这个self chain的这个run方法就能够去执行我们对应的真正要翻译的内容。好，然后到这儿为止，其实我们把这个translation券基本上就给大家分享完了，跟前面也都是一样的一个设定。
	然后唯一需要在稍微讲一讲，就是说我们为了实现不同的聊天模型的对接，这里可以做的一个展。就是我们在这个地方我们能看到chat OpenAI。比如说我们现在想要去支持GPT3.5和GT4，这里基本上是不需要做任何变化的，因为它相对来说比较稳定。但假设我们现在想要去支持这个ChatGLM，它是一个self posted，就是我们的在人群里面，它针对不同的聊天模型需要导入不同的抽象。我们这儿看到这个抽象，在学习model这个模块的时候也讲过，chat models下面有很多不同的抽象。这个抽象他要import的这个内容会不一样，就相当于之前我们自己要去实现这个类似于chat OpenAI或者说这个lama。现在你不用，你可以直接导入，因为权限实现了，但是你还是需要导入进来的。
	然后你在使用上面，根据这个model name，你需要去调不同的这个chat models。这一部分是如果我们要去支持不同的单元模型需要去做的一些修整。然后第二就是说对应的这个chat prom template。如果我们在不同的大语言模型上，首先这个任务足够简单，如果你不换这个temp plate，理论上应该也是可行的。但如果你要做一些别的任务的时候，那这个chat from flt也是可以被你拎出来变成translation chain的一个另一个成员方法再去做处理，这样是可以的。
	这个是translation chain的一个变化，第二个就是说在处理这个过程当中，我们可以看到，还有一个变化在于我们的content。之前我们这儿需要先构造一个prom，再去构造一个make request的方法，然后再实现实际的翻译。现在我们其实统一的用这个run方法之后，这个content之前之所以处理成两类，就是因为有文本有表格。那现在的表格我们可以直接丢进去做处理了。
	我们看到这个日志输出这里丢进去的是什么？就这里是我们丢给大语言模型的prompt after formatting对吧？这个其实是真正丢给大语言模型的内容。这个内容里面，我们传给大语言模型的这个待翻译文本是这样的一个结构。然后这个结构返回的一个结果是长这样的，其实是很稳定的，他没有去做任何的变化。之前的这些中括号，包括这个括号也都保留下来了。
	我们在用V1.0的时候，因为我们用的是chat GM6B就我自己在测试的时候，包括我们用的这个达芬奇003，他的这个翻译结果不是很稳定。当时我们就在说，为了弥补大语言模型的不稳定的这种输出，我们需要去自己处理这个返回的结果。甚至还需要去把这个返回结果去做一些比如说这个行距的处理等等。这个我们可以再回忆一下之前的那个部分，我给大家看一下。
	在我们的这个content里面，这个是我们V1.0的时候，为了支持这个table的翻译，我们对结果进行了一定的处理。我们拿到了这个结果，这个translation的这这个结果，然后我们当时做了一些打印，然后为了获取这个数据，大家可以看一下，我们去做了strip的处理，包括把每一行都区分出来。因为它没法稳定的保留这个行距，它会变成一个很多很多行的这样的一个数据，甚至里面还会有一些乱七八糟的逗号，或者说这个中控会被干掉了。所以当时做了一些这样的处理，为了通过这个方式，把这个stream变成了一个嵌套的一个数组，嵌套的一个链表，然后嵌套这个列表里面存的是真实的数据。然后这个真实的数据，我们希望把它转成一个pandas的一个data frame。然后其中第一行我们认为它是表头，所以把这个第一行设置为这个表头。
	然后第二行，开始的才是真实的数据，然后最终把这个结果存进去了。但是在实际去做做这个数据存取的时候，我们新的这个版本其实就做了一些简化处理。我们现在再看到这个2.0的版本。
	好，那在2.0里面，其实我们这个做了什么呢？第一，我们去取了这个黑的，是我们的去取这个表头。然后这个表头其实直接就是我们认为这个格式是不会变的，并且这个格式是我们在plus PDF里面去生成的。
	我不清楚大家还记不记得我们讲这个PDF的这个buber这个库。首先我们丢给大语言模型翻译的时候就一定是长这样的。这个是因为我们的PDF解析的这个库，它生成的这个table就是这样的一个格式。所以只要我们成功的解析出来的这个数据，就是我们在这儿看到的，这个是我们pass PDF的一个结果。
	这个结构其实是由这个解析PDF的这个库固定生成的那现在我们就维持这个结构，我们也不再去做更多的调整了，我们之前的那个代码里面会去做一些调整，那么我们也就维持这个结构。并且我们认为第一行的这个就是我们的这个表头。并且我们通过GPT3.5和GPT4的这个翻译之后，这个结构仍然保留下来，它它会识别出哪些是待翻译的文本，哪些是分隔符，这些分隔符仍然很稳定的保留下来了。所以我们通过这样的一个方式可以取这个表头。Split就相当于通过这个来做这个换行符的分隔，相当于把这个表头取出来，相当于简单来说就是把这样的一个结构，通过这个中括号的回避的这个中括号，那就是回close的这个中括号，就拆成了多行。
	在这个多行里面，我们取出来了这个表头，剩下的就是我们剩下的这些就是我们要的，真实的数据，所以我们可以看到它翻译的这个结果，这里，仍然是保留了这样的一个结构，那我们就可以直接复用这个结构。这个结构取出来之后，我们构造了这样的一个panda的pandas的这么一个data frame。这个data rose就是我们剩下的这些内容，然后我们的这个heder其实就是这里的这个内容，把它构造成了一个我们翻译后的一个结果。然后把这个panda as的这个data frame变成了table这种内容。Table content它里面的这个翻译结果，然后为了我们去取这个翻译结果更简单，我们还做了一个改动。那这个改动是什么呢？就是在这里我们之前为了去取这个content的时候，我们没有给普通的文本类型增加这个方法。
	这个也是一个python的语言特性，大家可以注意就是当我们使用这个的时候，就跟刚刚有一个同学问，有的时候用的是欠点乱方法，有的时候是直接嵌然后一个括号。这里其实是类似的，当我们使用排成类似的两个下划线所装饰的这个string方法的时候，简单来说就是我直接去访问这个content的实例，它就会返回这个东西。所以我们在PDF translator里面可以直接输入一个content。当我们直接去输入这个content的时候，它它相当于就会直接返回它的original，就是我这个玩意儿返回它。然后类似的如果我们是一个table的类型，那它就会返回的是这个original two，因为这里的original其实是一个表格，这个table的content，它的original其实是一个表格。然后我们把它to spring，其实就变成了这样的一个结构，就这样的一个结构。
	那通过这样的一个方式，我们用一个python的语言的一个小技巧，让我们去构造给门券的这个prompt，就更统一。那这样的话我们就可以通过统一的一个run方法，然后统一的一个content。这个content不管它是文本还是表格，因为用了类似的这个方法，它都会返回一个文本类的带翻译的文本，带翻译的一个文本是一个字符串的类型。然后如果它是文本类的，就直接给这个文本本身，就我们这儿看到的test data巴拉巴拉这一大串。然后如果它是一个table的话，它本身应该是一个嵌套的list，对吧？我们看到了在plus PDF这个方法里面，table解析出来之后是一个嵌套的列表。但是我们通过这个string方法，其实给他的是一个我们的这个string是一个文本。这个是我们人生需要的，然后我们可以通过一个统一的rn接口，可以去调用它。
	到这儿为止，其实我们整个的改动就改就介绍完了。我们可以总结一下。第一，我们通过这个translation chain去替换了我们自定义的一个model。我们就可以通过不同的model name去访问我们想要的这个大元模型。如果只是这份代码原封不动的话，至少你可以去替换用GPT3.5或者是用GPT4，以及用他们的不同系列的版本。因为GPT3.5和GPT4本身还有不同的版本，来直接去体验不同的这个模型，只需要去更改一个model name。同时我们的prompt temple ate也做了抽象，直接使用南茜的这个chat prompt tempt，完成了这个聊天模型的提示模板的一个设计和它对应的这个构造，用软方法来构造真实的聊天记录。就我们的这个message，通过这样的一个方式可以直接构造出来。
	我们的content里面，使用了这个string这样的一个python内置的一个小的语法堂，包括我们在这个普通的content里面，那就能够通过一个我们的PDF translator里面的一个统一的run方法和统一的带翻译的文本字符串的格式，实现了这样的一个看起来很干净的，非常干净的一个翻译的一个接口。这个翻译的接口就是传入一个待翻译内容，源语言目标语言，并且使用的是我们的translate chain的这个run方法。返回的是我们的翻译的结果和我们的这个翻译的状态，然后可以把它设置到你这个safe translation，通过我们的这个content的这个safe，然后设置回去。这个里面刚才也有讲了，怎么样通过这个稳定的一个输出，把这个表格里面的结果又重新摘回来，然后变成一个data frame，然后传给我们的这个table的content。那到这儿看看大家有没有什么问题。
	看大家有什么问题吗？没有的话，我今天这个时间紧任务重了。我们把这个radio和这个flash可以讲一讲，这样下节课我们就有3个小时的时间讲这个offer GPT了。
	PDF到文本的时候，如何保证表格都是这个结尾？这个我刚刚果然有的同学可能没有去注意。这儿我们再看一下，pass PDF，这个是我们第一次讲OpenAI translate这个项目的时候重点提过的。然后对于表格类的表格类的这里大家还有印象，这里我们输出了一个debug日志，debug日志里面大概可以对应的这这一行日志，这个pass PDF54对吧？这个地方是它的文件和它的函数，以及它的第几行代码，那就对应的这一行，那它输出了这个table，那这个table怎么来的呢？
	这个table怎么来的，首先它是tables对吧？它有多个table，这个table是从extract tables方法来的。我们在open a translator这一节课的时候，花了很多功夫跟大家讲这个库怎么用，到时候大家再回去复习一下。对，然后有这个PDF配置，这是一个PDF baber，就这个P点club这个第三方库提供的这个方法。然后它可以extract text，可以extract tables。这个extract tables的这个结构就是一个嵌套的列表就是一个嵌套的列表。然后这个就能够以这样的一个形式给到我们，然后我们在真正去翻译它的时候，我们刚刚看到了我们重写了这个python的这个类的string方法，就可以把这个玩意儿是一个list，变成一个stream，这儿变成一个string。因为我们知道这些prompt都是字符串，所以它就可以变成一个stream，就我们这里。
	这里就变成string了对吧？这个self original就是我们刚刚看到的pass PDF里面的那个table，它为什么是一个to string方法有效呢？是因为table content里面我们有这样的一行，我们输入的这个data就是我们的这儿解析出来的这个data，他可以直接用这个pandas data frame去构造，构造完了之后，我们用了这个content的这个基础类型，就直接继承这个基础类型，我们把它传进去，对吧？这是那个data frame我们构造出来的。他就会用这个鸡肋的方法，这个鸡类的方法，这不就是用的这个方法，他super不就继承这个方法吗？
	那这里，就是table content了，对吧，传进去的是table content type。然后传进去的第二个参数是什么呢？是original，对吧？所以它这儿就会复用这个基础类型的方法，把它的这个表格，就table的这个content传到这个content type里，然后把这个data frame变成这个original。所以现在我们的table content，它的original和它的这个translation都是data frame了，这就统一了。然后他的直接去获取这个string方法，就是拿着这个字符串，那这个应该是啊这里有一些小的语法弹劾技巧，是用来处理表格和这个字符串的转换的对。
	如何切换成ChatGLM模型首先这个部分在生态篇会去讲。第二可以讲一下实现的思路，思路就是这儿就我们这个南茜里面会有不同的模型，这个之前在模型篇的时候已经讲过了。那么ChatGLM的这种模型，私有化部署的模型，在南线里面的抽象叫self posted，就是自己相当于自己部署的一个模型，你把它导进来，就跟我们今天花时间去讲之前的实现，我们要自己去实现子类，但它现在已经帮你实现了。那你就看一下这个self posted的这个model导进来之后，你要传什么参数，你肯定要传这个GLM的这个UIL，这个是GIGM官方的最重要的参数。然后完了之你给他传这个prom，他可能不一定支持这个角色的设定，那你就直接传这个prom就好了。如果他支持角色的设定，你就可以这个方式去给他传。把他的角色的这些抽象在男性里面都已经帮你抽象好了。你只需要把这个chat OpenAI这个部分，改成我们的self posted的需要的model name和它具体的一些参数。
	好，那我们还有时间，我们就继续往后讲一讲。
	我们有提OpenAI. 
	的范斯列特V2.0，其实刚刚那个事例里面，他肯定能支持多语言了，对吧？这个多语言怎么支持的，大家应该也都懂了，对吧？我就不再赘述了。在config的这个压mail你可以去改，在启动它的时候，你也可以去改这个source的这个target。
	好，那我们接着讲讲怎么样去给他一个图形化的界面。这图形化的界面有一个很很有名的图形化界面的一个库，这个库现在也广泛的被使用了两万多颗星了，叫radio。这个库其实在stable diffusion的这个社区应该更火。你看他这其实做的这个图也是跟纹身图相关的，然后他自己的事例里面你可以看到他有这个stable to fusion，也放在比较高的位置。他其实除了做纹身图以外，这个库已经有好几年的时间了，他也能够去对接。比如说我们的持续预测，time serious的forecasting就是持续预测，比如说这个股票明天涨不涨，对吧？或者说这个物联网里面你有一堆的时序数据，用过graphite a的同学应该知道，时序数据的可视化现在已经很成熟了。就普罗米修斯加这个加加这个graph，这一套从采集exporter到visualization这一套是很很成熟的。
	但是这个实际预测也有一些同学把它放到radio上面来做除了以外这些以外，还有像机器学习3d post也可以在这上面做。包括这个sketch的recognition，就是在这个深度学习领域里面，我们去做这个识别什么的，也可以在这做。因为它的核心其实就是帮你提供一个界面，这个界面里面预定义了一些设计。
	比如说聊天的框，这种可以设置就像这个温度值一样的这种框，包括这个下拉框、文本框，包括它这里面还可以渲染这个down，还有这个聊天框等等。这些都是他预定义的一些内容。所以你看他自己的这个定位就是为了方便去做machine learning的这个应用，机器学习的应用，包括这个nama two，他现在也支持了，可以一个比较方便的他预定义的样式去做渲染。
	那我们做的这个样式也很简单，就是我们就基于现在刚刚看的那份代码，我们做了一个简单的图形化界面的设计。他需要哪些输入参数？第一个我们需要有一个PDF的文件，这是我们需要去翻译的，第二个我们需要指定source language和target language，对吧？那我们有默认值是english和我们一些提示词。比如说原语言默认是英文，告诉他万一这个同学英语真的很差，吧？他不知道english是英语的意思，那那默认是英文，告诉他目标语言默认是中文，写一下。然后你真正要用它的时候，其实你就把PDF文件拖过去，然后或者解一下他就能上传这些东西待会儿我们会去看看代码怎么实现的。很简单然后你把这个文件传上去之后，你这儿都不用改，因为它有默认值，你点一个submit，它这就会开始翻译了，它就还会去显示它用多长时间。
	要翻译好之后，会给你生成一个文件，你就荡下来就完了，所以它的代码实现也很简单，然后这些代码我们也都上传到这个课程项目里了，大家待会也能拿下来看。它最核心的就是我们不是有一个面点PY作为我们的启动文件，对吧？然后同时我们也有一个radio的server，点PY文件为另一个启动入口。然后我们去启动这个radio server的时候，他会去执行这个文件里面的内容。
	让我们看看他其实就把这个server作为他的启动的这个文件，拍摄的很基础的知识，在这儿我们稍微花点时间讲一讲这个地方的gradual的一些这个库应该怎么用。首先它有两种模式去定义。刚刚我们看到那个框框，一种叫做GR，就是radio的这个库，那它一种叫interface，就是你直接针对这个TOI，针对这个数据化界面，都可以用一个interface去描述里面的这个内容。还有一个叫做它的blog，那就它有一个很多个一个界面，有很多个block组成。我们这个应用比较简单，我们使用interface，这个interface里面具体最重要几个参数是什么呢？其实就是我们这看到这些参数，可以认为是它要实现一个应用来说最重要的参数了。
	第一个就是title，就是我们刚刚看到的这个界面里面有这么一段话，这个抬头是可以用字符串来描述的。第二个就是我们的inputs，就我们左边看到的有三个，一个是GR到fire，这个是指我可以上传文件，在inputs里面可以上传文件。你这个label的这个意思就是说我显示一个label，相当于说明，text box就是我们可以去填的这个输入框，这输入框一样有label对吧？然后有这个play holder，有这个value，分别是什么意思呢？就是就我们看到这儿有它的这个占位符，然后也有它的实际值。
	这里大家需要有一个比较重要的参数上的区别。大家看到这里显示了一个english，那这个地方显示english这件事是通过place holder传进来的。我有个占位符，相当于提示你这个地方可以写english，对吧？但是facebook更多的很多用法是指，我举个简单的场景，就是你你现在这个电脑你启动的时候，你需要输入密码才能进到这个桌面。但是我们通常来说，你的密码如果很复杂的时候，它不会有一些提示词，对吧？就是那个地方显示什么什么。但是他不他不代表着那个提示词，那个占位符就是最终传进去的。它可能这个地方face holder并不是你真正要传进去的，也可能他这儿写的是请输入源语言，请输入巴拉巴拉，对吧？
	这也是很常见的一种用法，这个叫place folder。真正我们说有一个默认值，是通过value这个参数传进去的。所以大家这个一定要整明白。对，就是简单来说，我如果不设置place folder，我只设置一个value，他那儿是不显示english的。但是它也是能提交的，因为它默认有一个默认值值，你不知道默认值是什么。那这样的一个方式可能更友好，你的站位服务和你的这个默认值是一样的，你如果不传的话，那就这样的一个方式是为了让大家理解这个radio的几个重要参数，我们专门把这个两个值都放在这儿了。
	Output就是我们右边这一列，是一个也是一个file，我们会把这个生成好的这个文件，用这个outputs右边这个框框，就我们这里有一个下载翻译文件。当这边真正的运行完之后，这个下载翻译文件就会生成一个文件在这儿，你可以当load，然后最后还有一个叫什么呢？叫做flagging，allow flag ing这个地方可以传这个lever auto，就是自动的以及手动的menu。我没记错的话，应该是有这三个字可选。
	然后我们也可以看到，什么意思呢？就是说咱们其实这个应用作为服务端，其实有一个图形化的界面是给大家用的。这个界面甚至也是可以给很多，就你现在提供了一个服务了，还有界面，你可以给你的同事去用，对吧？但是你要知道你的同事是把就不只是你自己用了。你的同事或你的用户是把他的这个要翻译的文件传给你了。那这个时候这个文件要不要存下来呢？你可以简单理解。Flagged就是对于我们这个应用运行过程当中，有很多应用的中间结果也好，最终结果也好，上传结果也好，用来做存储的那never就是一种选择，不去把这个按钮展示出来。
	这儿本来应该有一个按钮，就跟这儿有个player一样，这里本来应该有一个flag的按钮。如果我不设置成never，我用这个menu或者auto的话，这里会显现一个按钮。当我设置成时候它就没有了，然后这个按钮的作用就是用来保存这个结果的。就是数据安全的考虑，我们包括各种大家会在课后还会去用这个项目，我就我们never表示我不会去存这些结果，当然大家也可以去调整这个地方的参数。
	然后这个东西设置好之后，他们都统一叫统一到这个i face里面。然后这个i face里面有一个叫FN的东西，我们刚才没讲对吧？这个function这个function是一个变量名，或者说叫一个函数名，就function name。那么这个function name其实这个translation就是指的这里的这个translation，它其实就是一个主逻辑，什么意思呢？就是当我们这儿有一个这个submit点提交的时候，它会有一个主逻辑来处理这些inputs，然后并且给回这个outputs。
	所以你能看到这个translation的这个函数，它输入的这三个参数，就是这里的这三个inputs，你可以理解成这个input file就等于这儿，然后我们的source language等于这我们的target language等于这那这个默认值。English chinese就是传给了他们，然后这个上传的PDF文件就传给了这个input file，然后这就是我们的这个界面的完整定义。然后当我们去点这个提交submit的时候，它就会去实际执行这个translation的逻辑。然后我们还加了一行这个日志去打开这个。
	在这个命令行里，我们服务端的命令行里能看到现在有一个新的翻译任务原文件。不好意思，原文件是什么？原语言是什么？目标语言是什么？但这一行就是一个我们刚刚已经很熟悉的一段调用了，对吧？
	我们translator的这个translate，然后里面要传这个文件的路径，source language，他get language，这个就可以复用。之前我们open s translator里面已经实现了主逻辑，最终返回一个output的file time。这个是我们的radio可以接受的一个格式。就相当于我们翻译完了，翻译完了之后还生成了一个文件，这个文件有一个文件路径，在我们的这个radio的服务端，把这个文件路径直接丢给outputs，它是能下载下来的。Radio会去这个路径里面找这个文件，然后再把它丢给我们的这个图形化界面上。你点当load的时候，video就会去这个文件路径下来，还有一些细节就是我们这个video它是一个也是一个服务端，它最终要提起来的时候还会有一些参数。
	比如说我们为了让其他的用户也能使用，我们设置了两个参数。第一个叫这个share，这个share是指假设我们现在是在这个本地，你自己有一个笔记本电脑，或者是你本地有一台电脑，然后你要去运行这台电脑上的这个radio的应用的时候，你这两个参数都不设置。你都不设置这个里面什么都不传的时候，你也是能访问的。因为你就在本地，你通过你本地的端口就能访问有一个local的这个UIL。但假设你现在想要给很多其他的用户去访问的话，那你把这个share设置为true，它会给你生成一个URL，就动态的一个URL。待会我们都能看到，这个URL是可以直接对外部访问的。他这个UIL是一个很长一串，就是基本不可读的里面还有一些哈希。
	但如果你想要通过一个固定IP，甚至是把它变成一个服务。域名解析到一个固定IP的话可以把这个server name再设置为一个就相当于任何IP都能访问的一个设定。那这样的话，你就可以直接通过IP加端口的方式去访问这个服务了，整体来看，我们要用这个其实掌握我刚刚讲的这些内容就够了，尤其是gradual，就主要是这一部分。Title inputs out foots function name够了，然后启动它，要使用这个length的这个方法去启动。这里有两个参数是用来，如果你要对外提供服务的话，需要设置好，我们看下代码。
	在我们的这个OpenAI . 
	translator里面，AI translator下面有一个radio server，the radio server其实就是我们刚刚看到的代码就在这里了launch radio和我们的这个translation，就这两个。然后还需要略微微讲一讲，就是我们把这个启动，如果我们把它作为启动的PY文件，它会去执行这段代码，对吧？然后它会初始化一个translator，然后会启动这个radio的服务，启动这radio服务就我们刚刚讲的那我们再简单讲讲这个translator的这个初始化。这其实这一部分跟我们面点PI是一模一样的，你把其他的给关掉。
	这儿是跟我们之前一模一样的对吧？包括下面这个地方，我们实例化了一个PDF translator这么一个类，把它放到旁边。
	来实例化了一个translator . 
	relay在面点PUI里。这儿我们因为radio的这个图形化界面，使得我们变成了一个服务，整个会启动变成一个服务，变成一个web server一样的东西。我们不希望实例化那么多PDF translator，其实只需要一个就够了。简单一点，我们这儿做一个全局变量全局变量为了这个区分出来，我们我这个首字母大写translator，然后在这儿一次性实例化一个PDF translator的实例，传入这个model name。Model name就我们的这一部分这个单配置单例就用上了，我们可以这个单例模式设置的这个config就能去获取这个model name。这个model name我们默认用的是这个GPT3.5对吧？这个地方就有这个translator了。那在每一次我们上传新的要翻译的文件的时候，它都可以去复用这个translator。好，然后我们来实际执行一下。
	我们首先把这个。把这个参数给它干掉，这个是最敏感的变化。
	大家看到这个地方，就我刚才讲的，如果我们是在本地的笔记本上面去运行的话，这样就可以了。他会告诉你running on local UIL，然后你的127.0.0.1对吧？你本地的一个IP值，7860这个端口是规定有默认的一个端口。然后它还会提示你就是如果你要创建一个public link，可以把它设置为true，就是我们这儿的这个设置为true。然后因为我这个是一台在美国的服务器，所以我如果不使用这个CL等于处我是访问不到这个图形化界面的，所以我们把它打开。先关掉了。
	大家看见没有？多了一个链接对吧？Running on public UL, 然后这个就是我说的前面会有串哈希，这个是可以直接访问的，我们可以打开一个浏览器。这是它生成的一个动态的一个链接。
	就我们的这个review的这个页面，我们可以给他传一个PDF试一试找一个GDF。
	通了服务器。
	下载。
	一个。
	我们下载一个测试文件。发到桌面上。
	这边正在写当loading。
	不会这么久，已经下载下来了，他没更新。对，好，那我们现在在穿一个。大家能看到这有一个变化，对吧？那写的uploading，然后那应该是没下载下来，那个0.0B肯定不对。还没下下来，网络有问题。
	我回了这么久。
	这边其实有写，他背后是在搞这个bell copy，就从服务器下载下来这个动作。这也太慢了，我们去get hub上面直接下一个。呃。
	对，您下来了。好，这个是我们刚刚从github上面下来的这个测试文件。然后我们把这个传到radio这边来，这肯定就不对了，插一个重新传一个。
	好，他正在上传，我们可以看一下后台的这个日志，在这里会接到一些结果，我把这两个。绑在一起。
	看一下这个日志。
	首先我们。
	看这个地方。
	我们有默认值是english和这个chinese，我们就不先就先不写了。我们点一下这个提交，这里会看到有一个时间，这个时间是实际运行时间，然后这边已经开始工作了。我们右边是我们的这个radio的这个服务端，对吧？
	打印出了我们刚刚说的这个翻译任务这一行的这个日志是在我们的这个radio server 12号，这里会有一个翻译任务原文件，这个是它radio自己生成的一个临时文件目录，放在term radio下面，自己有一个临时目录test。这个应该是这个括号没有被成功的打印出来，但这个没关系。然后原语言，目标语言，接下来这一部分就是大家熟悉的日志内容了，就是我们的这个串就是我们的这个translator这个实例里面的内容，包括它的pass，然后到它的这个具体去掉大模型，然后到大模型的这个结果设置到这个book里面去，最终导出到一个文件，这个文件就是我们的这个，也是在这个radio的一个临时目录下面，然后这个是生成的这个文件，对吧？Test translate ted but down把它下载下来。OK下载下来之后看一下。
	这个就是我们从刚刚通过这个radio所展示的这么一个完整流程，很简单，其实就可以做一个图形化的界面，代码量也很少。大家再回顾一下，这个是它的一个生成结果，对吧？没有问题。然后也把这些内容都甚至出来，跟我们这儿显示的是一致的。
	然后我们回顾一下这个代码，其实确实是很少，就只需要去处理好我们的这个interface和对应的这个translation的逻辑就好了。然后唯一需要注意的就是说，假设我们现在每次这个都不一样了，就我我我还有一个小细节需要跟大家讲一下这个参数的事儿。对，我们看到这个地方它实际翻译完了对吧？我们把这个V6关掉，这还在下载，我们可以让它停掉了。全部管。然后假设我们再次启动它，他这边有写kill你，把刚才那个关掉了，所以这儿刷新应该已经就访问不了了。
	这个其实是radio这个中转，他会去帮你去做的这样一个。大家可以简单理解成这其实规定的这个服务商是有参与的。网络有点卡，现在服务器。
	我可能得重新开一个，稍等。
	不知道为什么今天这个网特别慢。
	我们重新起一个VS的。
	可能这个就一时半会这个网好不了了。
	好了，切到这个别的站点转过来了。
	稍等，正在加载python的这个环境。在这里其实想跟大家讲的故想跟大家讲这个点，就是说会每一次都生成不同的UIL，那要解决这个问题的话，其实主要就是把这个参数给打开，但是带来的一个风险是什么呢？如果咱们是自己内部去使用的话，还是自己做做这个开发测试的话，是可以把这个设置成这样的一个参数。然后你就可以直接通过这个IP和端口的方式去访问了。但是你要知道的是，这样的一个访问方式它很有风险。就IP加端口并且没有健全这样的一个访问方式是很危险的。所以这gradual自己也提供了一些健全的方法，大家可以查阅一下文档，但这个不是我们最主要的这个课程部分，我就没有再去花太多时间给大家讲这部分的内容了。但是通过这一部分的interface这些定义，咱们是能够很方便的去做一个图形化界面的，我们可以终于有加载。
	进来了。
	我们把这个改成通过IP端口的方式来访问。对。
	需要进到蓝。
	券的。
	好，我们看一下它这个启动，然后在local 12L是7860，然后从127.0.0.1变成0.0对吧？然后当然他仍然会使用这个gradual，通过这个gradual给你提供一个UIL，但其实就是在这台jupiter的这个7860端口，我们也可以通过公网访问了。当然前提是我把这个防火墙也打开了，可以支持这个端口转发。这样就可以通过一个IP加端口的方式在服务器上访问。然后你也可以配置一个域名，然后通过这个域名的方式去做访问，整个这个效果其实是完全一样的，就这样的一个模式就可以去做访问。然后类似的你也可以在这儿去传一些不同的语言，对吧？
	可以再试一次这个。760。怎么会只有760，可能是一个破损文件。这个应该是完整文件1.5兆。对。正好这个破损文件触发了另一个异常路径，大家可以看到这个760的那个版本应该是我们开始登录的，只当录到一半的，他这边会在plus PDF的时候出问题，在这里test PDF，它的报错处于我们在解析PDF阶段就出问题了。是因为他没下下载完就这个UF这个应该是大家处理过文件，编程就处理过文件的话，应该很多同学都见过这个UF的全称应该就是n file，他没有把这个文件下载完，应该是之前的下载的时候出问题了。
	那我们可以当登录的这里调整的一个参数，对吧？这里就变成了别的语言。类似的假设我们要把这个参数再去做新增，那么只需要去把这个input里面的这个text box，把这的translation对齐就好啊，这样就可以实现一个很简单的图形化的界面。
	好，我们今天就讲到这个radio这部分。Flask就通过这个到这儿为止，我们其实教大家怎么把图形化界面做出来。那下一步，我们在周日会把这个尾巴再给它清扫掉，就是我们能不能提供一个web server，这其实已经是一个web server了，就是我们现在这已经是一个web server了，只不过这个web server它没法通过API调用的方式。就假设我们现在这个变成一个服务了，它也提供一个API出来，然后我们可以通过客户端来调用。因为翻译图形化界面是一种，API调用的是另一种。那么怎么样通过一个API服务对外提供，然后你有一个客户端，你通过replace来获取这个服务，那我们在周日的时候把它讲完，到这儿看看大家还有没有什么问题。
	先把这个服务关掉了，毕竟现在大家都可以访问。
	大家有什么问题，我们再留五分钟时间这个回答。因为我看刚刚这个OKR translator大家都整明白了，现在就gradual，可能这剩下半个小时，不知道还有没有同学不太懂。
	没看懂几行代码产生的界面，界面的内容怎么传入的这还怎么传入？他就从这儿获取了，这就是这个库要干的事儿。他变成他其实自己是一个就规定自己是一个简单的web server。然后这个web server能从这个界面上获取数据，然后怎么获取呢？Inputs要定义好，他就知道怎么获取了，对吧？
	然后获取好之后，要传给谁呢？传给这个function，这个function怎么接呢？定义好名字，跟这个顺序是一致的，然后它返回什么呢？那就返回这个，那返回的这个要符合什么要求呢？跟outputs一样，结束了。
	会负责域名的IP的解析吗？会负责的有成本。所以你会注意他写的是只有72个小时，如果你使用它的那个public POL，只有72个小时的有效期，是有成本的。这个同学很敏锐，大家可以看到这this share link experience in seventy two hours，对吧。然后他也有写，如果你想要免费的去post，然后还有一个radio deploy的这个方法，而不是去启动这么一个gradual。
	这个算是回答了，翻译出来找我看还有什么问题。翻译出来的结果可以通过南茜转换成其他的格式吗？你是指文件格式吗？南茜，首先你问了一个很好的问题，就南线能不能写文件？这个其实是通过tools来实现的。但我们这儿没有通过toss，我们这是自己实现了一个writer，就我们没有用门前来改造writer，因为writer又是很多业务侧的东西，这个同学问的还挺细，挺好的。
	我们看看这个地方，我们在实现这个translator的时候，有一个PDF translator，它有一个成员叫做writer。这个writer是我们自己实现的，这个writer之前应该在第一次讲的时候有讲过，它支持两种格式，一种是PDF，一个一种是mark down，分别写成两种格式。然后我们现在输出的这个markdown格式是调用的下面这段逻辑。
	如果你想让他用这个我们的能劝自己的toll来实现的话，首先它是有这样的tool的，就是write file tool，就写文件的toll。这个托儿我们在下节课讲奥特GPT的时候还会用，但是它有没有那么好用，这个就另说了。对，就写文件是一个蛮蛮常见的操作。我建议是说如果你想要高度可控的话，还是自己来实现。如果你是想省事的话，你可以用它的。
	还有同学问这个顺序的问题，这个顺序的问题目前就是按照这个顺序来的对。
	国内能访问吗？不知道，你试试应该可以的。我印象当中。
	南茜可以加速大语言模型在生成式问答中生成的速度吗？如果你用的是向量数据库的话，这一整套下来，这套组合拳是可以的。但你如果问的是他能不能直接去对这种大语言模型的调用的效率提升，那是没有的。大语言模型它就是一个外部的一个服务。大语言模型本身就是一个外部的服务，所以他没法让人先去给他提速的。但是你作为开发的效率和复用一些这个prom的效率，那是可以的。
	有的同学问三个实战，除了out GPT还有哪一个？我们的这个课程应该是有一个课程详情页的，还有一个就是我们的这个销售机器人，就我们的这个question answer over dogs的一个典型的应用，就针对一个我们私有的数据，然后我们通过向量数据库embedding，然后我们的用户去提问的时候，他能针对我之前的私有数据去回答。还有一个这样的实战，对，一共三个。还有三分钟，看大家有什么问题。就我们的这个。我给大家看看这个实战，我们现在正在这个位置对吧？我们第六周的第一次课，然后我们还会有这个auto GPT和我们的基于知识库的一个销售顾问我没放大，对，我们还会有这两个。
	好看大家有什么。
	问题？
	GPT3的翻译phone我们不太会翻译tone，GPT3的翻译tone太贵了，而且很快要下线了。今天GPT3.5的find tone出来之后，GPT3的find phone很快就会下线了。在课程当中我们也讲过这个GPT3的bone，我记得是在明年1月份就会正式下线。然后GPT3.5的因为涉及到信息安全问题，其实你相当于把你的数据交给了OpenAI，我们暂时也不会去碰这个红线，但是后面的ChatGLM的反应中我们会去讲。
	有同学问怎么把ChatGPT输出弄成一个固定格式？这个我们之前讲蓝线的基础模块的时候教过output power，就是我们在抽象大模型输出的时候，有一个东西叫output power，可以用它来做。看大家还有什么问题？我们还有一分钟。好，那那我们今天大家还没没有什么问题的话，我们今天的课程就到这儿。然后回头大家有问题可以在群里面再提问。好，那就我们周日再见，大家可以把今天的课程再消化，尤其是这个chat model，chat from template the radio很值得大家去深挖，它有很多可以灵活调整的部分。