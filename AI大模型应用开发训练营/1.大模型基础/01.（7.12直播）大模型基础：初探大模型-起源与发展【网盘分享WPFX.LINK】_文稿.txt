	大家好，我们准时八点钟开始我们今天的直播第一课。
	跟大家说一下，其实我也在这个直播间里面，所以我是能够在评论区看到大家的。我是能够在评论区看到大家的这个评论的，这样很好能够跟大家互动。我不知道现在刚刚看到有人说黑屏了，我不知道现在我们的这个屏幕正常了吗？如果正常的话，请同学们扣个一，让我知道这个推流和声音是正常的。Ok好，我看到有同学回一了，那我们就正式开始。
	非常高兴通过了最近二十多天的这么一个时间，我们的整个AI大模型应用开发的这个实战营的课程也迎来来了很多人的关注。然后我看大家的学习积极性也非常高，所以我们今天这个课程正式开始也是一个非常好的事情。接着整个这个课程，我们因为昨天其实有过一个开源直播，但是我从整个教研组的这个团队这边了解到的情况是，昨天有六百多个人来听，说明还是有一小部分的同学昨天因为各种原因没有参加。我们还是花一点点的时间先整体性的讲一下AI大模型应用开发这门课程的一个安排设计思路、学习路径以及我们的一些热点问题。
	首先整个这门课程叫AI大模型的应用开发实战营。那这门课程的一个设计思路是怎么样的呢？我简单跟大家分享一下，就是从我们的课程表里面大家也有看到，我们一开始其实讲的这这个内容叫做理论基础。然后接着会有开发基础，然后应用实战，最终我们有一个生态篇。我们先抛开这个开源生态最外面的这个大圈，单看里面的这三环，我们的理论开发基础和应用实战。其实大家现在可以再去看看手上的课表。其实我们不管是从这个大模型本身，还是到我们的OpenAI的API的开发，到我们的OpenAI的这个OpenAI translator这么第一个实战，以及后面的南券，我们都遵循了这么一个理论开发应用实战的一个模式。而最终的开源生态，是在我们学到这个课程的最后一两周的时候，我们会跟大家去讲一讲，我们现在都已经把大模型应用开发的这个技能基本掌握了，我们也能自己独立的去开发一些应用了。
	这个时候我们可以把我们的眼光再放远一点。我们能看到其实大模型有很多个大模型，除了OpenAI的大模型以外，国内也有大模型，国外也有一些其他的公司在做大模型。那我们能不能从这个横向的视角，从开源的，从生态的视角去看看还能做什么事情。
	所以在生态片里面我们准备了这个CHATGM，会去介绍这个hugging face，并且非常好的一点是因为这个课程整个时间有两个月的时间，让我们其实每天都能看到我们的AI大模型这个领域在有新的内容出来。就比如说我们看到昨天这个code interpreter，就我们的GPT4开放的这个能力，就我们所谓的代码解释器的能力。这个是一个很新的内容，我们之前的课表里面没有，没有关系，我们会把它加到课程的内容里面来。比如说在准备课程的过程当中，CHATGLM发布了第二代GLM26B。可能我们在课程里面也会用更新的内容去替换掉之前准备的这些材料，所以整个课程我们会会与时俱进，在有新的内容出现的时候，我一定会把这些新的东西加进来，不会让大家错过这些最新的技术，这个是第一个点。那为什么要花一两周的时间去跟大家讲理论基础？其实昨天的开营直播有跟大家介绍过，就是当我们去学习这么一个新技术的时候，要学的内容非常多。那有没有可能通过老师的一定时间的积累，我可能花了十多年的时间去积累，然后又花了几十篇论文的阅读，去跟大家把整个大模型理论的脉络给梳理出来。
	如果能够梳理出来的话，大家其实会发现整个大模型它所应用的很多设计的思想，它的技术底层的一些内涵，其实有很多相通之处的那这对于我们去理解大模型，让我们能够未来即使这门课学完之后，又有新的技术出现的时候，咱们依然能有一个抓手去找到这个技术的发展脉络。那这个是，我们去讲理论基础非常重要的一个动机。也是希望通过理论基础的讲解，能够授人予以。使我们的所有的同学都能够真正的去理解大模型到底是怎么一回。没事，不是一个黑魔法，太阳下面没有什么新鲜的事情。这个是我们的一个学习路径。
	昨天在开营直播里面也有提，就是大模型的应用开发的技术栈，就今天来看的话可以分成三层，然后第三层里面的中间件不是必要的。但是我们做一些复杂的大模型应用的开发，AIGC的应用开发的时候，中间件是一个必不可少的。我相信昨天的这个提问也能看得出来，有一些同学是java后端，有的同学是前端，有的同学是项目经理，他们都有不同的know how和背景。
	从我们之前了解的中间件的定位来看，中间件其实更多的是一个帮助我们的的应用去解决复杂场景的问题。比如说我们的数据存储，我们的热点数据和data的缓存。比如说这里有两个向量数据库，chroma和pythons，这是目前来看最热门的两个向量数据库。这两项的数据库也会在我们的课程里面去讲到。作为这个中间件，我们把它高亮出来的这个男券是我们课程当中的中间花了最多内容安排的一个部分。
	南倩现在也可以说是我们大模型应用开发的一个事实标准了。这么一个开发框架，我们一定会把它拆开来揉碎了跟大家去讲到底这玩意儿是怎么样去设计出来的，它有什么样的好处？为什么需要这样的一个南线作为中间件？在我们的基础模型层和AIGC的应用层之间去工作。它能够帮助我们带来什么样的一些好处？那就是中间这一层，中间键非常关键，它能让我们更多的去给我们的应用赋能。
	最下面的基础模型层，其实在我们的理论基础部分，也就是今天和周日的这个课程里面，我们会去重点来讲。我们讲基础模型不是为了让大家成为一个基础模型的开发专家。我相信来来参加我们这个训练营的同学没有绝大部分同学应该不是说我要搞一个GPT室出来，我要搞一个CADGLM出来。而是说我们已经有这样的大模型的基础了，有这样的一些基础服务提供给我们了。那我们如何能够利用他们，把它作为一个杠杆，把它作为一个我们生产力的工具，然后再叠加上我们的中间件去开发各种各样的应用。
	在众多AIGC的应用里面，我相信现在最成功的是CATGPT。它不管是从用户量，从访问量，包括这个code interpreter or出现之后，我看今天群里面也有这个同学访问这个ChatGPT出现了问题，就是因为它又热度又起来了。大家去看谷歌的搜索指数，发现这两天它的热度又再一次串起来了。就是因为open NI这家公司一直在很有节奏的去给大家释放它它的新东西。它它每一次的新技术都会使得大家对AICC的这个情行又有着新的关注和高涨。
	这个是我们在IDC应用层方面发现的第一个非常成功的那像微软的bing搜索引擎也集成了我们的GPT的能力，包括像github自己有它的这个代码编程助手co pilot。然后notion AI是我们很多产品经理和我们的文字内容工作者的一个生产力工具。包括像the journey，我们看到最新的这个GPT的这个code interpret or也能够跟mid journey一起去生成图像，甚至生成短视频了。这些都是一些非常新的技术，我们也会在后面的课程里面，及时的把这个内容加进来，来让大家享受到最新的技术红利。
	好，这个应用开发技术站其实我们今天会着重去讲这个基础理论的部分。然后我相信大部分可能没有这个AI大模型的经验。我尽量尝试用一个白话这个大白话的方式让大家理解这个模型的工作原理。并且这个模型里面有几条主要的技术脉络，我会像这个庖丁解牛一样，让大家非常清楚这几个主要脉络。然后大家如果有什么问题，我们会在我估计会在九点半左右的时候，留下五分钟集中让大家去提问。然后我会看一看这个问题，然后我会跟大家这个热点问题再去做一些回应然后我们也会有实战的内容。
	实战我这边举了一个简单的例子，这个例子就叫open a的translator。在课本里面也有提到，就是我们用大模型开发一个简单的应用。这个简单的应用它就是没有我们中间这一层的，他甚至没有用券那我相信之前参加过直播的也知道，这里也有去介绍整个项目。现在大家能在github上面看到的这个版本，0.1的版本是完全使用GPT4生成的。就整个这个项目没有一行代码是我写的，是完全用GPT4写的那这个也是很有意思的一件事情，包括现在我们看到的这个read ME，就是介绍这个项目的文档也是由GPT4生成的那我相信在这个群里面有很多同学想用GPT4。去生成代码生成这种帮自己这个工作的一个生产力工具，那这个事情是可做的。并且我们也会在这个课程里面跟大家介绍怎么样用GPT4去逐步的一环接一环的去生成一个可用的开源项目。然后接着我们会把这个开源项目一个输入PDF或者其他格式的这个书籍，我们把它翻译成其他语言这么一个开源项目进一步去扩展。
	大家可以看到这个课表里面会有讲，我们的第一个助手叫做OpenAI translator。我们刚刚看到这个项目，会在我们的第三周的时候开始去讲这个项目到底是怎么样去设想的。为什么需要做这么样的一个开源项目，它是有实实在在的市场需求，第二个就是说怎么样去设计它，包括它的功能应该怎么样去做定义和规划，它的技术上面应该有哪些方案和架构的考量。GPT4它目前还不能非常好的去帮你把代码的技术架构给设计出来。但是通过我们自己的一些自己的开发经验，我们其实是知道设计一个好的代码项目应该有哪些主要的要素的那这些东西你可以教GPT4，它就能够学习。他的学习过程当中其实是我看有的同学说没有声音，只是一些部分的场景，我继续这个过程当中其实是我们把如何设计一个项目的这个思路告诉GBT4，他就能够按照这个思路去逐步的生成。这个过程其实非常像什么呢？
	就像我们接下来会讲的这个prompt engineering，我们注意看的话，这里会有这个呃在我们的这个内容里面会有一个prompt的部分。就是在我们星期日，就是这一周的星几天，会有一个提示学习的一个小的篇章。这个地方就是教我们怎么样用我们的这个提示词。因为提示词现在大家都叫他做这个黑魔法，就像看这个哈利波特会念的这个咒语，他其实也没有那么难。这个提示词本身其实就是一个教我们的大模型去很好的生成内容的一种手段。
	那他为什么需要这个提示词？然后这个提示词为什么要？我们看到提示词工程里面有做模板的，有去精挑细选改造这个提示词里面的一些关键词的，包括去组织这个提示这个结构。比如说告诉他你是要什么样的的一个身份，然后你要做什么样的一个任务，然后你甚至可以给他一个示例。那这些东西为什么要这么去做？这个跟我们的理论基础分不开。
	就是我们的这里简单讲一下，就我们的整个GPT其实是一个自回归的模型。今天我们最后的讲这个大模型的这个篇章的时候，今天最后一部分讲bert跟GPT的区别的时候，我们也会提，其实GPT是一个自回归的模型。那他在学习的过程当中是没有做很多有标注的学习的。
	按照我们以前说法叫监督学习，他的这个学习过程，就大模型的这个训练过程，跟我们最后去用的这个prom pad的这个设计过程，其实是很有关联的。因为它的训练其实就是用prom pad在进行训练，所以如果我们了解了这个理论基础，知道他是这么训练出来的那是不是对于我们去造这个prom pad有帮助呢？答案显然是不言而喻的。Ok除了简单的去做prompt以外，其实现在也有很多的像train or prompt，包括我们的trail of prompt。我们prompt本身是可以去给它做设计的，做成一个复杂的链，做成一个树状结构都可以。这些都是我们在用大模型的过程当中总结出来的经验，以及了解理论基础之后可以进一步深化的部分。
	在进阶的篇章里面大家可以看到，其实我们有差不多3周到4周的时间都在学这个南券的开发框架，以及基于他去做实战三个实战项目以及最后落地。我们怎么考虑这个安全的问题，隐私的问题。这些其实是非常重要的中间件的部分。我相信搞懂基础片的同学，基本上你基于OpenAI的API去做一个简单的应用没有问题了。甚至我们刚刚说到这个OpenAIr的translator or可以搞的，它的插件也可以搞。学会了年轻人的好处是说，我们可以做更多的应用开发。
	就包括这里提到的，我们能把这个智能翻译助手，智能翻译助手有一个最大的缺点在哪？就是说翻译一定是我们希望他翻的内容越多越好。但我们如果用了OpenAI的模型，大家都知道token很贵。那么我们同样翻译过的内容是不是可以让它存下来？那这个时候南茜这一个框架它可以去对接向量数据库，可以存下来对吧？那么类似的一些翻译内容是不是可以让他不用再去重新跑一遍大模型，可以的，包括去对接这个post Green circle这样的传统的关系型数据库，我们的南迁也有对应的一些抽象。第二个，比如说我们的auto GPT，刚刚有讲到人可以去造复杂的prompt，auto GPT其实就是代替人。就是我问AI我问这个AI应用也好，问大模型也好，一个复杂的问题，这个问题可能一步解决不出来。
	那通过auto GPT他能够自己的去自己去问他把这个造prompt这个思维链的过程在这个auto GPT内部去完成。就比如说我昨天在直播里取了一举个例子，我让这个AIGC的一个应用软件，可能我正在跟他聊天，我给大家提了一个问题是说，请给我一个明天我的出行计划安排。这个问题其实比较难，难就难在我并没有告诉他我明天出行的目的地在哪，然后我是一个几天的出行计划，这些都没有然后to GPT的好处是说，第一它可以更多的去思考，去利用上下文。也许之前的前几轮对话我也告诉了他这个问题的答案，或者说刚刚提到的这个目的地和几天的这个问题的答案，他可以去那儿搜。如果实在没有的话，他可以在之前的聊天内容里面去理解里面的语义。有可能我没有直接告诉他，但是可能我告诉他我对哪些地方感兴趣。通过这样的方式auto GBT这么样一个应用就能够自己去完成这个复杂的思维链条。
	最后给我一个我想要的出行的方案。如果这个出行方案我还不满意，我可以继续问他他甚至可以去联网去搜索，这个是auto GPT非常重要的一个项目，也是现在很重要的一个研究方向，这是最新的一个技术。还有一类场景，就是我们基于知识库的一些聊天机器人。就比如说我们现在在淘宝上买东西，在京东上买东西，在极客时间上面定课程。包括我们这一门课程的这个助手。大家能发现它其实我们的这个大模型课程的助手，本身就是是一个基于知识库的销售顾问，对吧？他能回答你一些AIGC大模型相关的最新的问题，同时他还能回答你的一些课程的问题。这么样的一个销售顾问，这么样的一个机器人怎么样去做？我们的这门课程也会教大家怎么样去做这样的一个机器人。在你自己的行业问题里面，这是一个非常好的一个应用落地的场景。
	紧接着就会带来一个问题，我们的数据要给到大模型去学，它才能变成一个基于知识库特定知识。酷的顾问也好，机器人也好。这个数据安全，我们在大模型时代，我们的开源代码，我们的数据有什么样的协议？如何保护它？有没有一些相关的法律问题？这些我们会在8月23号的这一这一周去花两三个小时的时间给大家做一个全球化的一个视野的一个分享。就看看中国、美国、欧洲这样的一些发达国家和地区，他们又是怎么样来处理这样的问题。
	最后我们会去讲这个生态篇的内容，就我们横向对比大模型底层的这个基础模型层有这么多模型该怎么样选择，国内的大模型CHATGIM我们要怎么样去用CHATGIM其实除了我们的这个大语言模型本身以外，我们知道现在的视觉和大模型的结合是一个非常火的方向，李飞飞老师时也在做这个方向。聚生智能的跟大模型的结合是这样的一个方向。这些东西我们也会在生态篇当中去跟大家做分享。所以整个生态篇我们后续会把很多新的技术都放在这个篇章里面。在最后的一两周时间跟大家去分享最新的技术。并且用私有化的这种国产的大模型来做这个训练。它能一定程度上解决数据安全问题，并且节约的我们的这个开销和合规的问题，因为它是私有化的。然后有几个热点问题这边再做一个回应，大家待会儿有什么热点问题也可以在评论区留言，通过我们教研组和助教的同学把这些问题收集起来。
	首先我看到这两天这几个群当中的热点问题。第一个就是说我们这门课程当中有知道要用OpenAI那么用OpenAI的GBT模型的时候，是不是必须要使用GPT4？因为GPT4的API相对来说比较贵一点，是这个GPT3的十倍。我印象当中这里的答案。这是不是就是我们在整个用OpenAI的GPT模型开发的过程当中，我们可以用O这个GPT3，也可以用更简单的模型，这个都不是问题。所以说大家不用担心它。
	但是用GPT4的好处是说，它的答案会让你更满意一点，因为本身它更复杂。而且从成本的角度来说，OpenAI去运行GPT4的成本，其实本身也是GPT3的将近十倍。所以它的价格也就这么来的。
	那么除了GPT会不会讲别的大模型？这个是肯定会的。我们今天就会讲到这个bert对吧？所以大模型不止GPT，基础模型成当中除了GPT以外，我们还有很多的选择。甚至我也在考虑在生态圈里面是不是有机会跟大家分享。比如说像其他的国内大模型，最近商汤发布的这个日日新，华为的盘古这个文心一言等等，我们都可以去做这个分享。看大家到时候有什么问题，我们可以在群里收集。
	是不是需要科学上网？这个问题大家都明白对吧？我就不再展开了。
	是不是会讲基于私有数据的开发？这个会的我刚刚有讲到，在我们的实战里面会基于一个知识库，这个知识库就是一这个私有数据，我们会基于私有数据去做开发的这个呃实战。然后学完之后是不是能够独立开发大模型的应用，我相信这个一定可以，只要咱们认真的去学习，然后每次把我们的家庭作业，然后把我们的整个课程附带这些代码，包括理论基础的这些内容，我们会做一些选择题给到大家看。大家是否对一些关键概念有了比较好的理解。这些东西大家都学会之后，我相信在开发篇，在开这个进阶天的这个过程当中，每个人应该都能够做一些自己的小应用。
	它可能不一定是说代码量特别大，这个也是现在大模型开发非常有意思的一个特点。它的代码量不一定非常大，但是他的这些关键的技术点，你需要掌握，掌握之后你就会发现用最小的投入，原来可以获得这么大的收益。就比如说以前开发一个翻译软件会觉得很复杂。那现在大模型有语义理解的能力，有总结的能力，有翻译的能力，那就不需要那么多的代码就能把它开发出来。甚至大模型本身生成了oppen AI translator这个项目对吧？
	然后这个内容安排其实跟我之前讲的这个我对于新大模型这个时代的研发人员，开发者的一个技术能力分布是比较像的。大家也可以去看看，就是我们整个课程的内容安排上，我们实战为绝大部分的内容一定要多动手，不管是prompt还是南线的开发。第二部分就是我们的mate note，我们的原知识昨天有讲这个meta learning，对吧？就我们要学会如何高效的学习，如何去掌握新知识的主要脉络。这个也在理论基础部分，我准备了非常多的PPT记的内容，包括这个知识的体系，以及我们后面助教也会把我们的这个相关的论文我都会发给大家，大家有兴趣也可以去深入的去了解和读。论文里面我也做了一些批注，然后包括如何跟AI去做交互。
	这一部分其实跟南茜也有关系，因为我们的南茜其实就是大模型的第二大脑。它是跟我们大模型直接交互的一个很重要的手段，包括我们跟OpenAI的交互，这些都是在这个课程里面会占到一定比例的那最后这个创新思维其实就是我在每一次上课的时候都尝试能够一点点的这个方式。让大家能够get到这个创新思维也好，或者说怎么样去把我们的这个方法论做调整也好。同时我们的课程内容安排上，本来也会把很多最新的技术，这两个月的技术加到课程的安排里面来。这个也是在践行我们要关注新东西，并且我们这些新东西能不能启发我们在学习过程当中之前的一些章节的内容，在学到后面看到一些新东西出来的时候，有一些不同的改观。就比如说我们的OpenAIr translator。
	一开始GPT4和我们一起去做了这个项目，后来我们用南茜可以去把它改造一遍，甚至我们可以让他去对接CATGPEChatGPT的这个pluggin，这些都是一些不断的去创新，然后去跟踪新技术，然后去对我们之前的应用做迭代的一个手段。好，这个是整个的一个课程安排，可能花大概20分钟的时间。我们现在就正式进入到课程的内容里面。今天我们首先讲大模型的这个理论基础，就是我们的这个理论和技术的演进。我们今天会讲这里看到的这个初探大模型起源与发展，以及它的发展脉络。
	它在这个大模型的起源的过程当中，首先AI有好多人的发展，那么前面几轮是怎么回事？怎么走到这儿来的？然后走到这儿来之后，我们这一轮的大模型的预热，就是我们通过注意力机制的这么一个发现也好，发明也好，我们发现大模型这件事儿是有可能的，然后通过注意力机制我们引申学习到了有transformer。Transformer它的价值就在于它带来了一个AI的范式的改变，所以我们把它叫做变革的里程碑。通过transformer这么一种自注意力机制的模型架构的崛起，使得我们现在看到了GPT和bert这两个不同方向的大模型的出现以及它的迭代。
	简单来说就是我们捋一下这个思路，大模型是第四轮AI对吧？那么这个过程当中attention是第一个预热的，这个大家可以理解，就是我们这个火刚刚开始加速的时候，通过attention来的。然后同时在这个attention出现没没几年，transformer出现了。Transformer使得我们这个attention的机制在这个开枝散叶的过程当中，有一个大道至简的这么一个东西，就transformer。后面你会发现不管是GPT也好，bert也好，包括现在国内的很多大模型都在用transformer。它有一个里程碑的意义。但是只有transformer也不够，在transformer上面我们又会叠加很多东西。这些东西在我们以前的AI发展过程当中也都有它的身影出现。通过整个今天的分享，我们把大模型的起源与发展跟大家做一个拆解，整个过程会有比较难的部分，但我会用尽量简单的语言跟大家讲一讲。
	好好，正式看一下我们的AI历史，其实AI的历史已经有六十多年了。我不知道这个事情大家知道吗？AI这个词叫人工智能。人工智能这个词其实是1956年的时候第一次提出来。在一个达特茅斯学院的会议上，我们以前会讲AI有人工智能，有机器学习，有深度学习，现在叫大语言模型对吧？那这四个阶段其实是有不同的里程碑事件的那比如说第一个1950年代到1980年代，其实那个时候的AI做一些非常简单的工作，就大家现在看来都是某一个输入法能做的工作。大家其实那个时候搞AI的人并不是一个很很新奇或者说很时髦的说法，但甚至人工智能这个词也就只是提出来而已。
	人工智能真正开始影响人类的生活，其实是在第二轮的出现。就是我们现在都还会有感知的。就是你会用email，在email里面会有这个垃圾邮件。那垃圾邮件是怎么样被识别成为它是一个垃圾邮件的呢？
	Asia learning的出现在1980年代到2010年代，有一家很有名的公司叫IBM。IBM他做了一个很有名的系统叫沃森专家系统，对吧？并且在第二轮的这个发展过程当中，还有一些标志性的事件。比如说深蓝这样的一个系统在国际象棋这个领域打败了人，人类用的就是专家系统的这种手段。专家系统本身也是这个符号学派的一个很重要的研究堡垒，是他们的一个研究成果。从2010年到2020年，这十年时间我们知道深度学习非常火，包括我们国内的AI4小龙，像国外有很多深度学习的公司，比如说这个deep mind，他们用围棋这个所谓人类智慧的结晶，然后用阿尔法go这么一个人工智能的系统，深度学习加强化学习的系统，把我们人类所谓历史上的结晶，这个最难的，包括我自己也学过一点围棋，真的很难。这么样的一个游戏也好，博弈也好，把人给干趴下了。
	同时，深度学习在十年之前有巨大的资源上的投入。不管是我们看到学术界开始搞这个image的数据集，还是我们在工业界有很多的创业公司，然后大公司事业投入，然后我们看到了signet的发展，这些都是一个火速的发展。然后这十年你也造就了很多优秀的公司，优秀的算法。一直到我们看到这个GPT3出现，然后一直到去年底的PPT的出现，大大语言模型时代就接过了很重要的接力棒，就是我们的AI的接力棒。因为在疫情这几年，大家都会发现好像AI的公司也就平平无奇了。然后我们的AI发展也遇到了一些瓶颈，但实际上不是的。
	从这个GPT3的发布，整个学术界也其实有了不一样的看法。甚至我们会发现这个OpenAI很聪明，他做这个ChatGPT过程当中有很多不只是AI技术上的发展，也有一些产品上的优化。使得这个大语言模型的成功变成了一个全行业，包括今天来的可能也不只是这个AI背景的人，也会有其他行业的人，全行业的人都开始关注AI了。这四次发展可以说是一浪高过一浪。
	然后我们这一轮的这个AI大模型，其实是它的影响可以看得到，一定是从2020年到2030年，甚至还有更多的一个影响。因为它不仅是把以前深度学习，是我们人来造数据，我们造的一个image net。但是大语言模型最大的一个特点就是我不再需要那么多的监督数据了。
	我把全世界所有互联网上的数据都吸收进来。那这个时候我们可以看到他的潜力其实是远远还没有被挖掘出来的。包括我们这些私域数据的这些数据也都还没有把它挖掘出来。然后他还可以去跟我们的视觉系统，跟我们的机器人学去做大量的结合。所以我在想，如果要做一个类比的话，现在我们其实大模型还没有到resize net，还没有到打败李世石这个状态。大模型还有很多的潜力值得我们去挖掘。刚刚我们其实有讲在第二轮的这个时候，我们的这个符号主义就是是这个人工智能的两大学派对。这个符号主义是我们的第二轮专家系统的代表。
	然后我们的连接主义其实是从人工智能这个词发明之前，就一直在研究的。因为它是从神经网络，是从生物学里面得到的一些启发。然后我们又看到深度学习在第三轮里面得到了更多的关注。并且这一轮我们看到其实仍然是连接主义。就深度学习这一帮科学家们，或者说人工智能的科学家们，得到了非常长远的发展。但现在有一个非常好的趋势是说，这两个大的山头终于开始逐渐的合二为一。大家再找一些新的方法，能不能让我们的大模型真的具有这个推理能力，因为我们的符号主义的代表就是有推理能力，我们有知识图谱，我们有专家系统。
	我们的连接主义的优势是说我可以吸收足够多的数据，而这些数据通过类似于GPT这样的大模型，我甚至不用做标注，把这两个能力如果能够吸收进来，相当于你有一个非常强的吸收知识的能力。同时这个知识还能够被你拿来做推理。这个事情是非常具有想象力，并且也是值得研究的一个方影响。
	这两个大的山头里面也会有一些重要的人物，比如说我们的杨丽坤老师，比如说我们的这个hinton，以及发明反向传播的romance还有我们的这个hot fell，它是发明这个神经网络人工神经网络的代表。包括明斯基是符号主义的一个很重要的人物。大家如果感兴趣，这是一个可以让你去找到这两大人工智能学派的很重要的一个入口。
	刚刚其实我们有提到深度学习是因为imagine net数据集以及这个竞赛的发布，使得有越来越多人去关注它。然后我们的人工智能本身也有足够多的数据让大家去学习，我们的算力也得到了足够多的成长。英伟达这家公司跟着整个人工智能的发展走了起来，最近也突破了1万亿美行的这个市值。就简单来说，其实深度学习是通过数据和算力的增长来得到了一个繁荣的发展。并且我们会看到，因为数据算力的增长，使得我们的神经网络的架构也可以做的越来越深，越来越宽。甚至这两者还能够结合起来。这是一个我们能看得到的一个现状。
	那我们再往后面走会怎么样去发展呢？其实整个大模型的发展跟深度学习的发展之间是完全并行的两条线。但现在又开始逐渐合拢。比如说我们看到左下角这根轴线是我们这个大语言模型，或者说NLP这个领域的一个发展图谱。其实1956年的时候，就我们刚刚发明人工智能的这个词的时候，其实深度学习也好，这个NLP也好，一直都在发展。但是他是在什么时候有一个质的突破的呢？我们可以看到右边这根线，右边这根线里面把我们上一轮深度学习，比如说这个m list，比如说这个imaginary，刚刚提到的几个词对吧？
	这是关于计算机视觉，就我们深度学习识别各种能力，包括识别人脸识别物体，识别动物。这个是我们能看到在2010年开始有一个长足的发展，并且到了2015年的时候，resonant出现了。然后我们有一个基准测试，这个基准测试就是指AI和人到底谁厉害，AI有没有干掉人。
	历史上其实AI一直在找各种各样的具体的任务来跟人做比较。比如说我们前面提到的国际象棋深蓝，比如说我们提到的围棋阿尔法狗。关于识别这件事情，就我们人找这个质检员也好，找这个检察员也好，在网络上认各种各样的东西做质检，让AI也来做这个事情。在1516年的时候，我们的reconnect的出现直接促成了这件事儿，就使得AI能够比人更会去解决识别类的问题。就这种判别性的问题。然后到了2018年的时候，是GBT这个模型发布了。
	当时就是open I发布这个GPT，包括19年发布了bert之后，其实在一些自然语言，包括我们这儿看到其实最关键的一个叫language understanding。就我们理解语点和我们的阅读这两个很重要的人类的能力的这两个方面，其实我们的AI也超过了人。后面我们也会讲，现在当然是远远超过人。在这两个具体任务上，整个大模型其实我们能看到，我们先总结一下，1956年开始人工智能这个词出现了。然后我们的大模型的研究其实是从二零一几年的时候开始的对吧？具体来看其实有一个里程碑事件，就是我们的注意力机制出现了，然后接着出现了transformer，然后有GPT，GBT的出现在这根轴线上，使得我们的这个A在一些具体的人自然语言处理的问题上超过了人类。所以我们可以把18年当成是普通广播普通人或者说非这个领域的研究人员开始关注他的一个时间节点。就我们的2018年。
	类似的像这个深度学习，在识别问题上，我们通常把这个alex net和这个imagine net两个。一个是ND flat，是一个神经网络。他是刚刚看到这个hinton的团队做的，他让我们的这个识别类的问题有10%几个点的一个跃升，然后在15年的时候超过了人类。所以这两个时间节点使得我们的这个人终于发现，原来AI可以在这么复杂的问题上开始超过人类了。
	接着OpenAI这家公司，google的一家公司，包括海外的一些其他的公司，持续不断的在18年的成果上叠加他们的这个研究资源。我们后来发现GPT1、GPT2一直到GPT3，开始有这个代码生成的能力，各种能力。一直到22年底的ChatGPT，整个大模型终于得到了全社会全球化的资源的关注。
	我们今天才会在这儿讲这个AI大模型。Ok现在我们就来顺着这个思路来看一看。首先讲一讲这个注意力机制，我们注意力机制到底什么叫注意力机制，我们研究这个问题之前，因为刚刚讲了很多跟AI历史，包括发展相关的问题。我们先轻松一下，差不多有30 10秒的一个视频。这个视频也许有的同学看过，那在看这个视频之前我只有一个问题。这个问题就是假设我现在这个提问的对方就是咱们这些同学，你是一个大模型，那我现在的问题是就我的prompt是什么呢？是你帮我总结一下接下来这段视频里面的发言，到底他想讲一个什么样的内容，对吧？帮我总结一下这段发言。
	我们说当务之急是找到关键的问题，那么关键的问题是什么呢？是我们要找到问题的关键。那么一个如果在关键的问题，关键的领域，关键的这个环节上，我们找不到那个关键。我们把握抓手不在关键上，那么我们等于就是说无法解决找不到关键的问题，找不到关键问题，解决不到问题的关键。那么这个但是关键在于什么？关键就在于大家。
	能不能准。
	能不能狠，能不能快。现在是这个。
	是啊大家能不能告诉我这个领导他主要是想讲什么内容，你可以在评论区里发一发，作为理解。这个问题很难对不对？我相信大家看完这个都会会心一笑。但这个问题领导的发言是经常有的，对吧？然后你也经常会遇到类似的场景，你能不能总结出来？我们常常说人和大语言模型，现在BK？大语言模型在抢我们的工作。那我相信就今天来说，其实我们很难找到一个大模型能把这个总结的很好。
	然后注意力机制为什么靠这个来引出？其实就很有意思。其实注意力机制它的发明之初就想解决一个很重要的问题。重点就是我们会发现自然语言处理里面有很多经典问题。比如说翻译问题，比如说总结问题，阅读的问题，语言理解的问题。我们刚刚可以理解为它是一个language understanding。就我们在理解语言理解这个语言之后，我们要去做这个summary，我们去做这个总结对吧？那这也是OpenAI的API提供的能力。
	注意力机制其实一开始有一部分的设定就是希望说给你一堆的文字。因为我们人读文字也需要找重点，对吧？那AI也是一样的，AI要去学这个数据，学这个语言，那他也要找重点。不然的话学不到重点，其实就找不到这个重点的问题，对吧？那么那我们接着再看一个稍微简单一点的问题，我们现在仍然，我们脑子里面假设我们是一个AI，我们是一个语言模型，我们要去怎么样解决这样的一个问题。问题是问我到底去了几次咖啡店对吧？那这个可能这个是我们的问题，然后我就把这段书给他，我相信现在GPT应该能解决出来。但是我们先看看在十年前，在这个attention机制出现之前，这个问题怎么解，这个问题是我去了几次咖啡店，然后描述是这样的，就是我们昨天我在一个繁忙的一天结束之后，就跟我现在一样，对吧？
	工作了一天，先决定给大家讲一下这个AI大模型，他是决定去最喜欢的咖啡店放松一下。我走进咖啡店点了一杯拿铁，找了一个靠窗的位置坐下来。我喝着咖啡看着窗外的人们匆匆忙忙，非常惬意。然后我从咖啡店出来，回到了家中。
	这个问题比刚刚好总结一点对吧，他好歹言之还是有些内容的，并且你也能从里面找出一下我到底去了几次咖啡店。大家现在先思考一下，我们人是怎么样解决这个问题的。其实我们一看完这段内容，我们就知道他只去了一次，对吧？但我们怎么知道他只去了一次呢？
	这个问题其实值得大家去去想一想好，那A会怎么做呢？首先AI是没有刚刚大家的很复杂的逻辑推理能力的那AI首先要学的是我能看懂这些中文，对吧？因为AI也不是一个黑魔法，它也是一个技术的累积过程。他第一件事情是要看懂这段中文。那看懂这段中文，我们知道自然语言，英文很好，英文它是天然就是一个词一个空，它不需要再去做分词了。但中文需要去做分词。像我十年前搞NLP的时候，第一个任务就是使用这个结类似的这样的一些库去做这个分词。这里我们首先通过，比如说早期通过词库的方式来做分词。
	我们问题是咖啡店。我们可能能看出来在这段话里面一共出现了三次咖啡店。那这三次咖啡店是不是就表明我去了三次咖啡店呢？显然不是的。肯定不是说出现了几次就去了几次，对吧？那这样也太傻了。怎么样能够让我们的AI去理解他，我到底去了几次咖啡店呢？
	那问题就来了，就是虽然咖啡店出现了三次，但是它并不是每一次都是我们的关键信息。即使我们的问题问的是我到底去了几次咖啡店。这就告诉我们一个很重要的事情，就是高频不代表是重点，对吧？就比如说我们最开始看的这个视频，这个领导说了什么问题？领导说了关键对吧？因为讲的都是关键，但是关键本身并不是一个很关键的内容。
	那么到底去了几次咖啡店呢？我们人来理解的话，其实首先我走进咖啡店？走进咖啡店是一个最关键的工作。然后点拿铁也是一个关键动作，但是它可以记录用来做别的信息。比如说假设我后面又问我点了几杯咖啡，对吧？那这一次点了一杯拿铁就是很关键的一个信息。
	那么咖啡店的信息其实已知了，然后后面就是我从咖啡店出来回到家里面，从咖啡店出来是一个非常关键的信息，然后回到了家里面，其实我就从一个空间到了另一个空间，这两个是关键信息。但是因为我一开始已经知道，我走近的时候我就已经在咖啡店了。所以从哪儿出来这件事情只是一个状态变化。
	我其实知道我只去过一次，因为你要去一个地方要先进去再出来，对吧？这个是一个我们来尝试理解AI怎么去回答刚刚那个问题。然后我们会发现最前面还讲过一次，就是决定去我最喜欢的咖啡店放松一下。但这个时候其实我们还没进去。
	通过这一页，其实我是希望大家在脑海当中去想象一下，就是AI到底是怎么样去理解我们的问题，然后回答我们的问题。第一就是高频一定不是重点的那第二我们找到了一些关键的词，但这个关键的词它本身并不能回答我们的问题，它还需要跟一些逻辑联系在一起。比如说走进，比如说出来。那么要又要让我们的AI去理解走进和出来怎么理解？这两个也是非常关键的词对吧？然后点咖啡回家也是关键的词，然后决定去哪里其实还没有去吧？那这个信息他能不能理解到位？这些都是我们在十年之前让语言模型想要去回答刚刚那个问题是非常困难的。
	首先我想告诉大家，十年之前是非常困难的。就回答刚刚去了几次咖啡店非常困难。因为这是一个语义理解问题。大家如果在十年前搞过NLP就会理解了。但是现在我们可以做到这一点了，希望同从这个attention former到后面我们的GPT，今天到这个bird这个理论基础的分享，让大家能逐渐理解他为什么就能回答这个语义理解问题了，对吧？
	那现在我们回过头来看，要回答这个问题，首先要抓住我们注意力，我们的关键点。最关键点就是我刚咖啡店，然后我去了，然后我出来了这三个点。那这三个点要怎么样被捕捉到呢？我们看看怎么样去做对吧？
	首先讲一个比较典型的机器学记的架构，或者说我们的这个机器翻译、机器学习用来做翻译的架构机器翻译的架构。这个机器翻译的架构也是attention mechanism，是注意力机制这篇论文做这个注意力机制介绍的一个架构。这个架构是一个什么样的含义？这里简单做一个说明，就是首先这个架构分为上下两张图。上面这张图是没有注意意力机制的。就是我们假设我们没有注意力机制，我们以前人这个人工智能是怎么样做翻译的。下面是引入注意力机制之后，我们怎么样去做翻译的一个图，这个架构图，网络架构图。
	从上面这个图里面，我们重点看几个。第一个就是说我们这里发现这是一个翻译问题。我们首先要让AI能够去做的第一个自然语言比较难的任务就是翻译，就把一个语言翻译成另一个语言。然后其次我们才能让他去做内容的理解。
	我们先假设一个更简单的问题，是要让他做翻译。翻译其实在很早以前，就十几年前我们就在解决这个问题了。在解决这个问题过程当中，有一个很经典的架构其实叫sequence to sequence就是序列到序列。然后这个序列里面，第一个序列我们叫encoder，我们的编码器。然后我们的后一个序列叫解码器。Decoder, 用的这个看着像是一个串行的链路，这个叫做RNN的一个结构，就我们的这个循环神经网络。这个循环神经网络里面，其实我们看这里还有一个中间的一个词，就我们的每一个encoder RN，我们的编码器中间都会有一个隐藏的状态，叫隐藏层的状态。
	这个hidden state 1，hidden state 2，hidden state 3，我们可以简单理解成什么呢？就是我们有一个神经网络，这个神经网络就是在学习这个词，它到底应该怎么样被AI去理解，变成一个向量。然后我们的这个词通过了这个神经网络变成了一个向量，这个词通过了一个神经网络变成了一个向量，那这个词也变成了一个向量。然后解码的过程就跟它类似，就是我们把它解码成这个一就我们最终会输出各种各样的向量，通过这个网络，这些向量又能够被解码成一个一个的单词，不过是不同的语言，这个过程就跟我们做视频编解码的逻辑也挺像的。
	就我们以前看小时候看电视叫这个模拟信号，对吧？后来变成了数字信号，那也是就像我现在在这说话，你们为什么能听见？因为这个麦克风把我说话的这个内容变成了电信号。电信号是被这个DSP去编码的。然后你们在听的过程当中，你们的这个终端设备上会有一个解码器，再把这个信号给变成一个人说话的这个信号，然后就出来了。这个编解码的逻辑其实是类似的，这也是为什么叫编码跟解码。
	但是大家会发现有一点不同在哪里呢？就是这里一个单词一个状态最终输送到了我们的这个解码过程当中里面，对吧？那那这里如果这句话特别长，那会不会有一些内容其实我们在编码的过程当中就丢失了。就我们昨天有提就是再好的这个信息的传达的媒体，他在这个媒介，他在传递过程当中也会有信息的丢失。那么我们的INN如果遇到一些长句子的时候，他会把这个信息可能因为RN它也是要学出来的，他学的不够好的话，信息就丢了。那在这里它的效果就不会答的很好。
	我们的attention network跟这张图最大的区别，我相信也是一目了然的。我们把这儿的每一个单词的状态放到了这儿，就是相当于把它放到了一个要输入给我们解码器的这个序列里面。那那这个事儿就比较简单了，就是我不再是说我像这个击鼓传花一样的，大家就相当于同学A告诉腾讯同学B今天发生了什么，同学B告诉同学C今天发生了什么，这个时候越传越变形对吧？这个话我们直接让这个同学A告诉同学B告诉同学C然后最后告诉老师好，我们现在不这么干了，同学A同学B同学C还是传这个话。
	但是我们把他们三个人传的话一起告诉老师，这个事儿就很有趣了。老师就能够去判断你们之间说的这些内容谁说的比较重点对吧？谁说的重点是什么？可能同学记得这个重点是我想去咖啡店，同学B记得这个重点是他点了一杯拿铁。同学CAG的这个重点可能是说我今天很累。最终老师去整合这个信息，然后老师去做判断，去判断到底我们这一完整的一段话里面重点是什么。不过他已经不再是原来的这个自然语言了，而是我们编码的这个状态。我不知道大家理解到这个重点没有。
	好接下来会有几篇去讲这个attention network它是怎么样去就就我们的这个注意力网络它是怎么样去实现的。我们可以看到右这个右下角是这个截图来源的这篇论文，后面我们会直播之后也会跟大家去讲，这是一篇综述论文，2021年发表的一篇论文，是去讲attention的。就我们的注意力的这个模型到底是怎么样发展过来的。我这边尽可能让大家理解为什么这个注意力机制有用。以及它其实跟我们的向量数据库，包括跟我们的大语言模型的开发都有关联。我们接着把刚刚那个图我们再看一看，最大的区别是我们把这个注意力网络把所有的输入都接进来了。它其实就对应着我们这里的这幅图。
	左边这个就是我们没有注意力网络的，同学A给同学B传话，给同学C传话，然后他又传给老师，对吧？老师也是一个一个去传话的。那么注意力网络就是说好了，我要听你们每个人都给我说一遍。然后同时我还要去找出他的重点，对，这个其实就注意力网络干的事儿。那重点在哪儿呢？其实注意力网络引入一个很重要的机制，就是我刚刚说的，老师要听你们每个人的内容，并且我还要找到你们谁说的重点。他说的是这个咖啡店对吧？他说的是点咖啡。好，那通过什么样的一个方式。你知道通过这么一个新的概念叫做context vector，就是我们的上下文的向量，这里这个公式其实是比较好理解的。
	首先我们看这个H1、H2、H3，就是指同学间传过来的话，就是我输入的这个语言。它会通过我们的一个神经网络变成了一个隐藏层的状态，然后这个是一个加权的平均。我想这个应该大家看得明白，这个context vector其实就是让我们老师在听老师之间也要强化，老师能够在每一个节点上都能够去知道每一个同学的输入。通过这个context的vector，好，这个context其实有一个很重要的功能，它就是把我们的这个输入，就我们输入的这个序列完整的那段话和我们这个位置关系，就是我输入里面我我我这个自然语言表达了很多内容。
	那我到底哪个词最关键？我要去学一个权重。这个权重大家能看到就是这个阿尔法对吧？H就是一个一个的词，它是向量化的那这个阿尔法是我们要学习的重点。然后这个C就是把每一个词的权重的重点丢出来，然后加到一起加权平均就变成了我们这个context vector。而这个context vector就是我我们跟没有attention之间最大的一个区别。
	大家注意看这个部分，我们的这个没有attention，是直接把这个老师间的传话和这个上一个词，就我们上一个解码器作为这个上下文，就是完形填空一样，也会有上文对吧？然后你还会有之前的一个隐变量。现在我们不仅有之前那些内容，还有输入端的所有的内容，并且它是有权重的。
	就是我是知道输入里面有很多废话，那我要去学出来输入的这些词里面哪些是重点的话。就比如说我们刚刚看到的那段，这个咖啡店里面有很多词是不必要的那那些词的权重就会比较低，这个是context vector最重要的一个作用。通过这样的一个方式，让我们的这个解码器，让我们的老师能直接跟所有的学生接触。并且在所有的学生里面，我们还知道谁是好学生，谁是坏学生。因为我们要学这个权重。
	好，这个是S2，我们接着就看这个权重怎么学的对吧？我们要怎么找出这个好学生和坏学生呢？其实就通过一个词叫做注意力权重。就我们刚刚在讲的这个阿法，这个阿尔法其实就在找关联关系，就是在找我输入的有一个词，输出的有一个词。
	大家还记得我们现在解决的是一个翻译问题，对吧？那么翻译问题里面我们中文说这个男孩子？男生在英文里面可能叫boy。那我怎么知道那一段英文里面的哪个词跟我中文里面的这个男生是有关系的呢？那这个时候可能我的训练集就是中文英文这样的一对一对的中英文的这个句子。并且我可能会去标注出来。标注出来之后，我就知道哪个词跟哪个词关联比较强。这个标注出来这个事情其实就是类似于一个attention with its，它其实会自动的去找这个关联关系。
	所以整个attention with想要表达的就是说我在自然语言处理里面，我的输入的这个和我想要得到的这个答案之间的关系。这个重要性关联性通过attention with来表达。因为本身我们输入会全部丢给输出，所以这两个query和答案之间的注意力就由这个attention为此来学，它是一个矩阵。我们其实看到这里有一个公式，这个SI和这个SJ减1和HI就分别代表我们这H就是我们的输入，这个S就是我们的输出。然后我们通过这个A这么一个函数来学所有的这个矩阵里面的状态，然后把它丢到P这个函数里面，然后才变成了最终的这个阿尔法，对吧？
	那个听起来很复杂，然后我们这里简单跟大家解释一下什么意思。首先我们刚刚提到这个注意力是为了识别出每一个哪个话对于我的这个答案是有价值的。因为你刚刚说的这个博弈可能是对应的南海，但是可能这个student对应的是学生，对吧？这两个词在中英文里都有出现，那我针对不同的这个答案，比如说我输入是英文，输出是中文。那假设S一是我们的不会是我们的这个男孩，S2是我们的这个学生。那S一可能更关注的也许是H1，那S2可能更关注的是H3。那么他们对应的这个注意力权重肯定是不一样样的。
	那么怎么样去用一个什么样的方法来找这个注意力呢？就是我们刚刚提到用A这个函数来找，A这个函数是一个有名字的函数，我们在这篇论文里面也有明确的定义。而且这个词我相信大家看这个AI大模型的文章经常提到叫对齐。就我们看这里鼠标有写这个alignment function叫对齐，这个词也比较取得非常好，就把问题和答案对齐。这个对齐其实是一个让我们去找到这两个关系的一个函数。这个函数其实有很多的这个对齐函数，我待会儿有也会去讲。
	假设我们现在知道有个函数，它能学这两个关联关系。那学出来之后，其实我就找我就能把这个丢很多的中英文对子之后，我就能学出这个阿尔法了。这个阿尔法可能是一个很大的值，比如说它是五点几、六点几，那我们搞这个加权平均都知道最好，我们说希望它最终加起来的权重是一对吧？所以这个P是干什么的呢？P是一个叫做distribution function，它是把我们的这个学出来的值，又把它映射到我们的这个0到1之间，是啊。刚忘了按这个了，不好意思。其实整个这个逻辑我们刚刚讲完，再再根据这个答案我们再看一看，就是我们的这个alignment function，我们的对齐函数是为了很好的去学出每一个权重，然后每一个权重就这个打法？然后能够学出来。
	然后学出来的值，我们希望它变成一个0到1之间的值，这样才能去做加权平均。那最后才能造成这样一个contest vector，就相当于才能把这个处理过后带了优先级，带了注意力权重的这么一个所有的输入给到每一个输出的单词，然后让他们去再学习。这个过程当中，我们可以把它这个公式抽象出来，再简单的去做一个书写。
	就这么多公式，其实最终我们可以把它变成一个非常简洁的输入，这个输入是怎么样来的？我们看一下关键词，第一个是这个query。我们开始大家回想一下我们开始那个问题就是我去我很忙，我很繁忙的一天工作之后要去一个咖啡店，然后点咖啡对吧？然后出了咖啡店回家这个东西我们称之为query，就是我们的输入的这个问题。我们的这个输入那这个输入它会被拆成一个一个的单词。然后一个一个的单词经过了一个神经网络，变成了一个一个的向量，对吧？单词是我们的X然后这个神经网络之后变成了一个一个的向量，就是我们的这个是query。
	但是我们注意力要干什么事儿？我其实要找的是你这一堆单词里面那么多废话，我要把有价值的信息提取出来，提取出来信息我们叫这个K对吧？就是你说了那么多废话，到底你想说什么？我给你总结一下，就是怎么总结呢？那就是把那些不重要的话它的权重降低，对吧？就是我在回答你的这个问题的时候，我其实会知道我这个单词是学生，这个单词是那个男孩。这个男孩对于他来说可能只有H一重要，那剩下的他的这个注意力权重就都是可能零了，或者0.00001就不重要。对于S2来说的话，可能H1H2的这个阿尔法，它可能就变成了接近于零的一个值。
	然后整个这个过程就组成了一个新的输入给到我们的解码器，让我们的这个老师能关注到重点。这个过程当中，我们希望把这个阿尔法的注意力权重再回到0到1之间。那这个C就变成一个真正的加权平均值，那就不影响我们的整个公式，是这么样一个逻辑。
	那这里还会有一个V其实是对应着我们的输出，我们这里就不再展开，大家可以理解成整个A代表的是我们的注意力机制。它要解决的问题就是找到我们的输入，就输入的这一堆废话里面的关键词是什么。然后找到这个关键词，我们就能够提取出最关键的信息，有效的信息。其实有效的信息才是真正对我们的答案有帮助的对吧？然后整个注意力机制就可以表达成我们右边的这样的一个形式。这个形式就是我们有一个对齐的函数，它是用来找关键词和输入之间的关系的。然后我们通过这个P专门一个分布函数，把它变成了0到1之间的一个值。
	然后0到1之间的一个值会跟输入我们的这个V就是我们的输入，也就是相当于这个H结合在一起，然后变成了我们的注意力。其实这个注意力的机制，这个A然后QKV这么一个经典的结构，其实是更通用的一个注意力机制的表达方法。我们看这个函数，还是刚刚这个函数是谁发明的呢？其实是来自于14年的这篇论文，我们的这个banner和我们的这个本就他们在14年的这个文章里面讲到到了这个attention mechanism。我们的注意力机制，其实就我们刚刚看到的这样的一个结构。我们刚刚其实有提到，就我们的这个对齐函数，它是用来学这个query和key的那除了刚刚看看到这个结构以外，我们把这个对齐函数展开来看，其实会有很多种不同的对齐的方法可以去学，并且它的功能还不一样。但是大家抓住一个本质，就是说我们现在回过头来看注意力机制。
	通用的注意力机制有几个关键要素。第一个关键要素就是说相对于没有注意力机制的网络，我把所有的原始的输入都给到了我的输出，让他能去直接学而不是说我这个像击鼓传花一样的，同学之间传了小话，然后再告诉老师老师之间再去传话，不是这样的，而是直接把所有同学的话都告诉老师。但是直接告诉老师，老师的这个脑子也装不下。那我们需要有一个注意力机制来学习他。
	要学习的关键就是说这些同学的话针对每一个老师都有关注重点。其实就相当于我们刚刚的这个机器翻译里面，每一个词都有关注的重点。这个重点要用一个函数去学，对吧？那学就有这么多函数可以作为候选，当然这也不是全部的要去学的时候，可能我们放在最前面的就是这个相似性，对吧？我们去找不同的问题，他们可能对应的key都是一样的。
	这个也是我们现在向数据库去应用的很常见的一种方式，通过这个对齐函数我们能学学完之后，我们把所有的输入词和注意力权重，再用一个分布函数变成一个0到1之间的，然后就能变成一个上下文的向量context vector，然后给到我们的老师，这个是一个巨大的不同，能让所有的老师直接去从所有的输入里面去学内容。然后14年的时候发明了这么一个东西。但其实这个思想是一九六几年的时候就已经有的了。
	我们今天再来，现在我们再回过头来，再把这个脑子再清醒一下。在这个过程当中，我们知道有个很关键的问题是这个阿尔法要去学，对吧？然后这个是我们搞深度学习，搞机器学习，搞叉叉学习，都知道的能学。但是一九六几年的时候，其实机器翻译也好，我们发现平均也好，其实就有一篇论文在讲这个思想，不过他没有用学的思想。
	大家可以想象，就是我们机器学习找早期的一些文章就是非参数化的方法。所谓的非参数化的方法就是不用像机器学习，像神经网络一样，我要学一个神经网络出来。而是我有一些基于统计学的方法，比如说各种分布和方。那样的一个一九六几年的成果，其实当时就很有意思的一个成果。就是说我们这不是有一个context vector，我们的这个上下文向量，然后他们需要学这个注意力权重。假设我们现在不需要学了，我们直接套一个和方法，比如说套一个高斯分布，然后这个方法本身的参数是不用学的那也能做到这样的一个效果，但是它的适用性就非常短，对吧？那我们能看得到的是，但是这个思路跟我们搞深度学习，搞这个大家都知道。
	以前我们怎么做识别问题的，要通过各种专家造各种滤波器，对吧？造这个filter。后来我们是怎么做的呢？我们直接通过CNN去学这个feature map，这个就造出来了各种各样的卷积核。
	你可以把这个卷积核和这里的这个上下文向量做一个对比。在很早以前，这个RNN这种序列问题，我是击鼓传花式的给到后面的人。后来我们发现其实很早以前就有人想过了。你不用击鼓传花，你把所有的这些输入套到一个核方法里面，就像这个专家系统或者说这个滤波器一样，丢给他一个固定的套路，不用学的东西，它也能达到一些还不错的效果。就比如说在图像里面，我们有一些经典的去检测这个边缘，去检测这个对比度。这些东西其实都是有一些经典的方法和方法去学的，去去让我们值得学习的。但它本身不需要通过数据去学习。
	所以这两个大家可以对比着来看，就我们的计算机视觉和我们的自然语言处理都是有这个非参数化的方法到参数化的方法就是因为我们前面有讲到数据变多了，我们的算力变强了。然后大家就可以去学，这也是数据和算力直接影响了我们现在去搞研究的一个思路。这样大模型未来也会影响大家怎么样去设计神经网络架构。
	好，说回来就是说这个有这么一个很重要的一个注意力机制，那它的实现我们用这幅图可以更细的把刚刚那个矩阵拆开来看，就是怎么样去学的我们的整个对齐的函数，这个alignment这个function，是为了让我们去学出这个attention with？这个阿尔法，那阿尔法是需要学的那怎么学呢？一开始可能我们通过这个对齐的函数，然后它把这个输入这个H和我们S之间的关联关系就关注对齐了。对齐之后，我们再套一个这样的soft t max，就是我们刚刚的这个P这个distribution function，我们这个分布函数。那那让它把变成了一个0到1之间的这么一这个词。
	针对我们的每一个，我们这里要拿到这个解码器里的输出，它会拿一个上context vector，我们的这个上下文向量它就都能够得到一个加权平均的结果。那针对这里的我们的每一个输入和输出之间都能有这样的一个上下文向量。那这个上下文向量就能使我们找到这种输入输出之间的重要关系。那我们也就能针对特定的问题或者说特定的语言去找到。简单理解成机器翻译。你就是单词和单词之间的关系，就我们刚刚提到的boy和男生，student和这个学生之间的关系，就通过这个注意力来搞定的。那么为什么需要这个注意力呢？
	其实我们可以再再来想一想，其实我们刚刚花很长的时间一直在讲，它有助于克服这个循环神经网络RN的一些挑战。比如说长度增加的时候性能会下降，然后我们的顺序处理会导致计算效率降低，因为它是串行。然后同时最低机制在刚刚说的这些优势以外，其实他在当时出现的时候，已经在很多的这个任务里面，就我们刚刚提到有各种各样的任务需要去解决，对吧？那很多任务里面，它都取得了非常好的性能，包括我们现在看到的GPT，它的根儿上其实也是注意力机制。除了这些以外，还有一个优势是说注意力机制除了能够提高我们主要的任务。就比如说我们刚刚在搞机器翻译，我们也可以让他去做总结这样主要任务的性能之外，它还能够提升我们神经网络的一个可解释性，让我们去解释这个模型的决策过程。并且它是同时体现在了我们的计算机视觉和自然语言这两个领域，都能提升它的可解释性。
	所以把刚刚三个总结来看，第一个就是说在传统的编码器解码器模型当中，我们的信息是容易损失的。这个已经刚刚说了很多遍了，对吧？击鼓传花我们同学A之间的互传，再告诉老师信息一定是有损失的，然后模型没法对齐了。因为你你你击鼓传花的过程当中，老师拿到的信息是最后一个学生说的信息。他都不知道A传给B什么，B传给C什么，这个序列的信息丢了。这两个问题通过我们的注意力机制解决了。因为老师能直接拿到所有同学的信息，然后再由老师由这个对齐函数去决定我到底关注哪个同学的内容。
	然后第二个就是说它能够让我们访问整个编码的这个输入序列，然后通过这个注意力权重来选对吧？就我们学对齐的这个关系，并且它是自动去学这个注意力权重的这个相关性是学出来的，是通过丢大量的数据，然后我们加这个学习的时间学出来的那这个跟60年代的那个核方法其实有本质的不同。核方法就是我跟数据没关系，我就套这么一个方法。然后我可能套一个特定的任务，或者数据分布刚好适合我，那我能解决，不然我可能就没用。除了这个以外，它构建了这个上下文向量，就是我们刚刚反复在讲的这个context vector。这个上下文向量构造了这么一个东西，使我们的解码器让每个老师都能够全面访问。
	最后它提升了模型的性能，改善了输出的质量，并且提升了提供了更好的模型的解释性。Ok那这是什么呢？这是GPT4参加了很多专业领域考试的一个结果，对吧？GPT4是典型的注意力机制的一个受益者，因为他用了transformers transformer就是注意力机制，大家可以直接理解，后面我们会去讲。所以在各种考试里面，注意力机制就是比没有用注意力机制获得了很多的优势。包括我们在像美国生物竞赛，像这个SAT的数学比赛，包括现在我们还看到各种各样的比赛，用了注意力机制都得到了很好的结果。然后可解释性对吧？这个是在计算机视觉领域里面，我们不知道我们以前都不知道这个神经网络到底它在识别上面是怎么学的那现在我们看这幅图其实可以看到这里有很多的编号，这个有编号的数字是我们输入给神经网络的原图像，就跟我们刚刚在学习注意力机制的时候输入的X原画。
	然后旁边的这个图，是你可以理解成是神经网络学完之后，就是在我学了很多东西，然后他关注的重点是什么？是右边这个图，这个其实就是很很简单的一个逻辑。就是在图像这个领域里面，其实神经网络也是能去学这个关注重点的，就跟我们刚刚学这个废话的这个过滤一样。在这幅图像里面，其实这些像素就相当于一个一个的输入的X对吧？那针对我们一个特定的问题些，我们去掉那些不重要的X把这个重要的像素抽象出来，这个是比较浅层的。这个feature map的激活，你可以理解成就是他还没有去做大量的卷积。然后这里就已经能体现出我们的计算机视觉里面的这个注意力机制的可解执行。
	然后同样的在我们的自然语言里面其实也是一样的。大家看这个机器翻译问题，中间这个颜色比较深的，其实你注意去看的话，我们看这边的这有纵轴和这边的横轴，其实是不同的语言。但是颜色比较亮就说明它捕捉到了这两个词的相关性比较大。那就比如说这个1992跟这个1992，这俩其实都跨语言了，对吧？所以他们的相关性最高。
	这个混淆矩阵那么颜色越亮就说明这个词和这个词它的关联性越强。所以有一些明显是直译的。因为有些语言之间是可以直译的，有的可能是找不到对用的词，所以它会有好几个地方都被点亮了。针对这种自然语言的问题，其实通过这个混淆矩阵，它也增加了可解释性。
	就是我们这个模型，我们的注意力的这个网络，它到底有没有学到不同语言之间的相关的表达。就我们所谓的这个language understanding，就我们的H跟S输入跟输出不同语言相同的单词关联性到底学没学到？学到了。通过这种混淆矩阵的方式我们也能看到，ok这个是我们的注意力机制，很重要。在1415年的时候，这篇论文使得我们神经网络在自然语言这个事情上有了一个飞速的进展。
	我们能够刚刚这篇论文大家我不知道有没有记得这篇文章的名字，我们再回顾一下这篇文章的名字叫做neural machine translate ning translation by jointly learning to a line and translate。我们会看这篇文章的标题，就是这个图灵奖获得者和他的这个学生banana一起把这个机器翻译这个任务，用神经网络来学习机器翻译的任务。但是在这个过程当中，他还把a line对齐和翻译这两件事情通过注意力一起做了。后面我们会看到transformer就马上要讲transformer把这件事儿给发扬到了极致，让我们的注意力机制能够同时学更多的东西，不只是对齐，不只是翻译，还能学很多其他的工作。那接着我们就讲这个transformer，这个transformer到底怎么厉害了，对吧。刚刚我们有提到14年的attention的mechanism注意力机制，和17年的这篇文章隔了三年的时间。这个attention is oiled的就是transformer这篇论文。这篇论文其实进一步的去展示了这个注意力机制。
	我们继续去看一下这个注意力机制本身是怎么样的。就是我我们简单的一个总结。其实刚刚我们讲注意力机制那部分，主要就是把这个注意力机制用在了一个叫机器翻译的特定任务上，然后使用的神经网络架构是这个encoder。Decoder就是编码器解码器的这个架构，所以我们通过这张图我们能看到我们在聊什么是注意力机制的时候，通常可以把它分成三个部分。
	第一个部分就是我们用注意力机制解决一个什么样的特定问题。那你要解决这个特定问题，其实就是你你问他什么，他告诉你什么，对吧？就是这么一个问答的场景。这个问答的场景里面，你告诉他什么是我们的输入数据，我们的input data。那那他告诉你什么是这个output data。所以其实这个范式是非常广泛的，它是一个非常通用的一个研优范式。
	我们的注意力机制不仅能够解决序列的问题，也能解决图像的问题。包括我们的计算，类似于计算几何里面的这些图结构的问题，cabinet的问题。然后这个是我们从应用的角度来看，因为支持这么多应用，那它必然有这么多不同的数据类型它都能表达。然后要解决这些问题，除了输入输出我们整明白以外，它需要一个什么这样的神经网络架构呢？这里我们有很多种方式，比如说我们看到的编解码这种序列，序列到序列的架构，transformer也是其中的一个架构。包括我们的记忆网络，我们的这个graph。就我们知道图神经网络很火，但是图神经网络和注意力机制其实也有结合。这是从神经网络架构的角度来看。
	我们刚刚还有提到一个很重点的事情是说，注意力学的关键就是学输入输出之间的这个关系，对吧？那输入输出之间这个关系有这个对齐的方法，对齐的函数，这个对齐的函数也可以再去做扩展。这里大家就会看到我们这里有什么各种各样的对齐方法。包括基于序列的、基于层次的、基于位置的，包括不同表达方式的那整个transformer其实就是把我们刚刚聊的这个三大块，就我们把这个问题拆成把注意力机制，注意力模型这个技术栈拆成三部分的话。
	第一，他在注意力的这个机制的类型上，他用的是一个self attention，就是注意力力的机制。然后他使用的这个神经网络架构是他自己发明的，我们待会会去解读叫transformer。然后这个架构后面被很多的大语言模型无数的去借鉴和使用，这个其实就是transformer。一点都不新鲜，对吧？那他的论文是怎么讲的呢？对他论文的摘要，对他论文里面也说得很清楚，就是attention is oil内的那这篇论文在他的摘要里面非常清楚的讲出了我们开始花了很长时间去解答的这个encoder decoder对吧？The best performing models also connect the encoder and decoder through an attention mechanical。提到了我们通过这个连接用注意力机制来连接encode decode我们的编解码器，取得了非常好的效果。
	这个是一个非常好的肯定。对于三年前本九的工做对吧？那他们做了什么事儿呢？他提出了一个新的架构叫transformer，这个新的架构非常的简洁，这是它的重点。并且这边我们还看到绿色标注出来了一个细节，叫做in symbols。
	那什么叫ensembles呢？其实in symbols我们我不知道大家有没有关注过这个nama和我们最近GPT4这两个大语言模型。拿玛就是那个羊驼，大家会看到这个叫facebook这家公司，现在叫meta他们做的那个然后GPT4，其实大家最近昨天那篇文章有很多人在讲，它是八个不同的专家模型，然后像智囊团一样的给大家去做群策群力，对吧？其实用的也是这个simple的思想，并且就在transformer这个模型架构里面也同样用了这个思想。并且用了这个技术手段之后，他还在BLEU这个基准测试上又提升了新的效果。所以大家可以简单理解成这个群策群力一定是一个好方法。所以大家在这个学习过程当中，在有不懂的问题在群里互相交流也一定是一个很好的方法。
	其实整个transformer在摘要里就用了这个方法，所以后面的大语言模型也不断在用这个方法。然后我们会看到其实这个方法就那么多种，套路也没有那么多。左边是我们刚刚看到的这个encoder，decoder就我们的编辑编解码的这这个架构，然后用注意力网络之后是这么样的样的一个架构，对吧？那右边是我们的这个transformer，来自于论文当中的一个架构。这个是一个直观的对比。
	这个对比我们重点要看哪呢？其实重点在于，这里的这个架构，我们刚刚有提有H有S对吧？因为有一个NN的序列，它需要把我们的输入编解码成一个隐藏层的状态。这个隐藏层的状态是我们循环神经网络必须要用的。
	那么self attention一个很重要的点就是大家会发现，其实他都没有在用RNN了，就他没有再用这个循环神经网络了。这就是这篇论文为什么叫这个名字，attention is all you need，就你只需要attention了，这个事儿是挺神奇挺crazy的对吧？我们以前知道大家学深度学习，我要么是RNN要么是CNN对吧？我要么搞卷积的，我要么搞这个循环神经网网络。Transformer告诉我们他们都不需要了，你只需要attention就够了。
	这个是一个很有跨时代的一个论文，而且它也非常的简洁。其实整个网络我们待会儿会把它拆开来讲，就是什么叫transformer？大家其实可以看到这里有一个小细节叫做N就是我们的这个N在论文里面这里取了这个六层，就是他把这个encoder和这个decoder各叠加了6层，并且它们之间还可以直连。然后他自己取的这个自注意力，其实就是这里的这个muti head attention，再加上他这边有一些不同的encoding的方法，这个我们待会再细讲，然后整个transformer attention is all you need。大家可以记住它最重要的一个点就是我不再依赖RNN了。不再依赖RNN就会有蛮多的好处，不好意思，就会有蛮多的好处。
	我们先从效果的角度来看一看，就是左边这幅图是我们开始有看到的，这个经典的注意力机制他学的是什么呢？他是在用注意力机制同时去学对齐和翻译，对吧？所以对于不同的两个语言，这个方块就这个混淆矩阵里面这个方块越亮就说明他们的关联度越高，学的是两个不同的语言。那什么叫self attention？叫自注意力。
	这个右边是transformer这篇论文当中的一幅图。大家可以看到什么叫自注意力呢？就是我的输入跟输出。我们刚刚看到这个encode，decode是两种语言，对吧？之所以其实我的输入输出其实就是同样一种语言，那我在学什么呢？
	通过这样一幅图我们能看到，首先这里有一根线条，这根线条是什么含义呢？这个线条的含义就是说我们看这段话，the law will never be perfect, 就法律永远不是完美的，but is the application should be just我们到这为止，这里有一个is它是一个代词，对吧？我们知道我们人去理解语言的过程当中，代词是最重要的。就是因为他没有直接去说明一词是什么，那这个its有可能只代很多东西，对吧？比如说他可能指代的就是人类，就是这个法律。而且我们人其实也理解这个人来理解这段话的话，这个意思其实本来就是指的这个法律。但是这个代词它其实指的一般都是名词，那有没有可能指的是这个application呢？那也有可能，所以它这里还有一条线，然后去指代application。
	其实这个自注意力他首先要解决的问题是说，我没有说我要去把这个什么跨语言，对齐啥的整明白。我首先整明白的一件事情是说，对于这个语言内部，我能不能把这个语言先整明白，就我去做语言理解这件事儿，语言理解最重要的一件事情就是我每一个词我都理解了，对吧？我不再是说我要去找重点了，而是说我理解我的这段话，我真的听懂你说什大家可以简单这么理解，transformer是想说我真的要理解语言。所以为什么很多语言模型都把transformer这个架构接进去了？就是因为他是开始去学习这个语言本身，这个语言当中的每一个单词，它的上下文到底是怎么指代关系的。Ok所以我们看右边这个完整的图就会看到the law这个the the跟这个law是有强关联关系，对吧？那这个its也是指的这个law，never就是指的never，perfect就指的perfect，没有什么这个线条就说明他们之间没啥关联关系。这个just像这种词就自己指的自己，那这种自己指的自己的这种词其实也很容易被摘出来，那是不是就可以通过另外一种方式去关注到我们的重点。
	因为我们人去问问题的时候，我们现在再回到最开始的那个问题，咖啡店。这个咖啡店是不是就是类似于一个its和long这样的一个关系？如果我们把这个语言理解了，我们就逐渐能够理姐我决定想去咖啡店的时候，我其实没去。因为决定可能是一个我们能学出来的东西，它会让我们知道这个背后的这个语义，其实是没有去的。然后甚至这个咖啡店我可能没有用这个咖啡店本身，而是用的那里就我去了那里，就跟我们一开始指的这个我要去这个明天要去旅行，对吧？然后我没有告诉我这个大语言模型，但我可能用了一个之前的上下文的对话。然后这个对话里面我也没有说具体去哪里。但他能够通过这样的一个方式去找到我的一些特定的代词也好，我的一段描述也好，最终是指向了一个特定的名词，有语义的这个名词。那这个名词就可以继续去下游做各种各样的任务了。这个是self attention和我们的一开始bengel和这个bad amal提出来的最开始的注意力机制的一个区别。
	Ok我们又回到这个自注意力机制了，这个自注意机制还是这样的一个经典的注意力的范式，QQV吧？我们的query key和我们的inputs这个vector和我们的inputs attention机制其实整体就是说把他们仨关系整明白，然后self attention其实是这样的一个范式。Soft max像不像我们刚刚看到那个P函数对吧？就是P然后这边是这个V把QK再除以一个我们的这个模型的长度就能做规划。
	好，我们简单来讲一讲这个自注意力机制，我们时间关系我尽可能讲快一点。首先这个自注意力机制是用了一个什么样的方式来解决这个学习的过程呢？就是它仍然是用的这个注意力机制的方法。
	这个skilled dot production product attention，这个我不知道大家有没有概念，其实在刚刚我们讲注意力机制的时候，我们有各种各样的对齐函数。还记得吗？这个A函数alignment function。其实这个scale的dot product attention就是一种对齐函数。大家这么理解，其实他就还是没有跳出我们的注意力机制的范式，对吧？整个中间这一大块其实就是一个AA函数。
	他在学学的是这个一段话里面我词跟词之间自己的关系。那就变成了这个问题就变成了一个不再是翻译的问题，变成有有十个人互相在传话。然后我们的任务就是这段话我们要理解明白，从A同学传到B同学的时候再传给C同学。C同学要能够知道在传话过程当中，某一个代词指代的是什么意思，不要因为强化过程当中我们把意思搞丢了。
	好，那那整个这个架构，其实我们能看到就左边这幅图就是一个完整的注意力网络了，注意力结构了。但是我们看右边这幅图其实这里的每一个小框框，这个scale的product attention是个什么呢？就是我们右边这个结构整体就叫做这么一个小框框对吧？那这么一个小框框它又叠在了一起，你大家能看到这有个H对吧？有我们假设看这幅图就三个，但其实有H个？那这个是什么意思呢？这就是我们现在在很多公众号，自媒体文章里面都会看到这个东西频繁出现，叫muti head，就是多头注意力机制，muti head attention. 
	那它是个什么东西呢？其实大家简单理解就是我们开始有讲到这个呃群策群力对吧？那我们通过这个muti head，就是我搞H个不同的注意力机制。那就相当于举个简单例子，不恰当的例子。但是希望大家能理解，我们现在这个传话是A传到BB传到CC传到D是这么传的。然后大家会会记住，然后会学出一个注意力。
	好，现在我们不传话了，我让A写个小纸条，A写个纸条给B但是B不能直接把纸条给C那B看完这个纸条，又把他的理解写成一个小纸条给到C然后传纸条的方式又来一遍。我们再通过录音的方式来一遍，录像的方式来一遍。那每一种方式大家都会知道方式的不同，他学的内容可能重点又不一样，对吧？到最后我们可以把这N种方式一种方式contact到一起，就加到一起，然后再输给我们的下游。
	这个过程很有意思对吧？就像像不像我们刚刚的这个context，我们的这个中文应该叫什么？叫上下文的这个向量环境向量。上下文的这个向量它其实就是把所有的输入全部给到这个输出，对吧？然后我不要有遗漏。现在我不是把所有的输入给你这么简单了，我甚至变着法了将漏的回有4种解法，我变着法的把这个输入告诉你，希望你能够有不同的关注重点，而且是诊断内容。对，我们看到下面是把诊断内容QKV的拼接起来输给他。
	这个其实就叫muti head attention，跟我们最终的这个GPT4在搞，就是这个搞完之后我再搞八个大语言模型。他们学的又不一样，对吧？学的重点不同，我对的数据也不同。其实这是一个非常常见的办法，就是我我走量？我现在算力够，我数据够，我就通过这种叠加的方式去学不同的重点。然后整个刚刚看到这一大块multi head attention，其实就是我们整个transformer架构当中的这一小块，这一个小块其实是我们的muti head attention。
	这个Martin head attention后面又接了一个全连接网络，一个faded forward的一个前馈网络，这个前馈网络大家可以理解，全连接网络就是全部连在一起的所有神经元，就简单这么理解。这个全连接网络会把这个输入的内容就直接输出给我们的解码器。所以大家把这个网络如果倒过来看，就是我们转个90度，其实跟那个序列网络很像，对吧？就把我们的输入项全部给到这个输出项。
	然后这个是我们的编码器，这个是我们的解码器。让我们再注意解码器里面，我们不看这个下面这一部分，就看最后这两块，跟我们的编码器其实是完全一样的。但是解码器其实解码器跟这个就我们能看到这个input output对吧？这个鼠标我们的编码器输入的跟我们的解码器输入的其实是同一段，对吧？自注意力输入的是我们刚刚提到了传纸条写的内容是一样的。
	最终那个解码器最后那个同学要告诉我们A说了什么，就跟这个王牌对王牌，我不知道大家看过这个综艺没有，里面就会有大家把耳朵道上什么的在在那手舞足蹈的在那表演。最后你要告诉我A说了什么，差不多就这个意思。但是解码器还做了一件事情，就是它模拟了一个很好的逻辑，就是这里有一个不红点对吧？Mask的就是我是掩码的，什么意思呢？就简单来说，就是我在传话的过程当中，通常我是从左往右一个字一个字往外蹦的。我如果前面没说完，我是看不到后面的那内容的。
	你可以这么理解，那么这个masked muti head attention做了一件什么事儿呢？就是他在编码的时候，首先我们说话仍然是一个序列，对吧？那这个序列我可能是十个字，那我现在输入的是第五个字，那我就把后面那五个字都遮起来，不让他看到，不让他学。因为他要学这个顺序，对吧？一点一点把这个序列也写下来，所以他做了所谓的这个叫position的encoding。其实后面也会去讲他怎么样把这个顺序也学学过来。然后同时在这个过程当中在输出这个就是在我们的解码器的输出的这个数据的训练里面，是从左到右的做了这个遮挡的。这个其实就是transformer整体架构，没有那么复杂，对吧？
	前馈网络其实就是一个简单的一个全连接网络，这边就不展开了。然后刚听下来有点麻了对吧？我们再再通过一个更简单的投递让大家理解什么是transformer。我们再看这个图，这个self attention其实就不神奇了，对吧？就是我们这儿的这个muti head attention，就是我们刚刚叠加了叠加了好几个对齐函数，然后又通过这个垒在一起，有多个对齐函数连在一起，变成了这个muti head tension。这个muti head attention后面接了一个全连接网络，然后这是一个典型的。然后这个过程当中我们的decoder除了刚刚看到这个以外，我们还加了一个encode decode的推荐。就输入输出的不要被这个名字迷惑了，对吧？
	这个encode decode attention是不是就我们上一节讲的，学的是输入跟输出之间的关系。但它比较吊诡的是输入跟输出一样，而不是两种语言。他学的就是我们的同样一段话里面，哪些词跟词之间是有一些关联关系的对吧？就刚才说的is和这个law是有关系的，然后这个是C和腾讯讯员的输入，跟这里会略有不同。因为他做了掩码，他当前输入的只能看到我当前位置，从开始到当前位置的后面的我给它遮住了。Ok我们刚刚有提到整个网络架构，N在google的原文里面是六个，所以它这其实六个，所以大家能看到整个。
	这样来看，是不是就很像一个我们的这个序列到序列了，就是跟本就跟14年那篇论文很像，就是只有一个decoder，然后。每一个encode其实都会连接这里的这个decode这个图画的略微有一点点小问题，其实这里应该也要连过去。然后这样的一个中间的这个部分就是这个transform架构，然后这个架构也能接过去做这个机器翻译的问题，那这我们就不再展开了。然后这个架构，我们也能看到就是encoder之间是这样连接的。Encoder跟这个decoder我们编解码之间的关系，是通过这样的一个方式去连接的。
	然后内部，我们其实也做了一个展开，但是不再细讲了，刚刚有提到这个position，the encoding就是我们的输入的这个词，就是我们讲说一段话是有序列的。我这儿巴拉巴拉讲了一堆的话，我把这个语序也要就是这个词之间的位置，这个语序也要做一个简单的编码。然后这个编码就给我们的网络，他才能学到这个位置信息。不然的话他其实是不知道我们的这个顺序的。然后整体这篇论文，其实有很多的这个公式什么的，我这边就不再去细讲了。大家可以回头去看一下这个PPT里面其实是有讲这个公式里面的这个重点的。像输入的这个单词原文，它的embedding，然后它的QQV最重要的注意力机制三大项对吧？然后它这个分数怎么算的？
	然后为什么要出一个DK让它变成一个最后变成一个soft max。学到这个权重，然后再去乘上这个输入项，到这儿为止是不是就很像我们的这个context vector，我们的上下文向量，最后把它加起来就变成了我们最终学的这个Z就是我们最终学到的这个架构，这个transformer。然后我们能看到这个有muti head，就是我们有多个对吧？我们刚刚有提到回向度的回有4种写法。我可以传纸条，我可以录音，我也可以跳舞来表达我的含义。
	这里我们通过不同的颜色，就是相当于用不同的传达方式，就我们不同的head attention。我们多个头，每一个头就是每一个颜色他们学到了哪些重点呢？大家就能看出一些区起来，就比如说像这里我们这个the law还比较确定，他可能学的距离就不远。像这样的左边这个红色的这个the law他就没有学到那个一直，但是像这个右边，其实它就有一定的关联关系了，但它还不够深。大家能看到吗？这个是不同的层数，我们刚刚不是看到有六层，就有六层的encode，六层的decoder。那不同的层数，层数越深，他能够学到的这个就越大家可以理解他那能力就越强，简单这么理解。Ok右边是不同的颜色代表不同的markey hand。
	然后我们能看到transformer当时发布出来之后，他打败了很多之前很厉害的研究成果，取得了非常好的效果。我们能看到下面在BLEU这个基准测试上，取得了很好的进步，然后最后我们再来回顾一下，这个transformer我们刚刚花了很多的时间去讲了attention和transformer？他们是怎么一脉相承的。这个transformer就是我们的self attention。这个self attention就是换了一个对齐函数，这个对齐函数换成了一个点击，这个点击又叠加了变成多个头，这个多个头又在这个深度上叠加了，就我们有六层的encoder，然后这个六层的encoder，六层的的搞的都是这个结构。所以包括在这个原文当中，作者也说了，你还可以去调这个层数，用不用六个，用12层，用几十层都可以，当然最后发现6层可能是最好的一个实验科学。
	然后整个这个架构告诉我们，其实RNN是可以被取代掉的那当我们取代掉RNN的时候，只用attention的时候，其实就带来了另一个好处。就是我们看到RN是一个序列模型，它是一个串行的模型。但是我们都知道GPU做并行计算非常强对吧？他有扩大的这个核心。所以因为他做了这样的一个架构之后，所以他在输入这一侧的时候，他可以去并行的去计算。
	然后这个是对于GPU，包括像谷歌的GPU非常友好的一种计算架构，所以它是一个面向GPU友好的一个并行的一个模型架构。然后同时它做了我们叫sentence level，就是一段话这个级别的表达。就是我们知道语义的这里面最重要的事情就是我们今天也花了很多时间，自然语言要变成一个向量，但是向量和向量之间的关系我其实不太能理解，对吧？那我们要去学，那那慢慢我们能学可能不同语言之间相同的两个词，男生和boy。他也许是一个含义，但是我能不能理解这一段话跟另一段话都是一个含义呢？那transformer开始研究这个东西了，因为我用这个self来分选，其实我是慢慢的理解the law跟那个is其实是一回事儿，慢慢我就能做到在一句话这个级别的表达，这个向量的表达，这个是对于他去捕捉它叫non range，就是长距离的依赖的时候非常重要的一个进步。
	然后这个是从这篇论文本身的价值来讲，但是我们承上启下，现在很多的基础模型？我们有提到技术站基础模型层基础模型都在使用transformer。所以我们可以甚至这样讲，就是transformer开启了这个基础模型的时代，它使得我们通过transformer可以学到一个基础的。
	我学到一个比较就是学到一个对语言理解非常好的一种模型。这个模型相当于他真的有点他有点开始理解你在说什么了，就是我们通过各种各样的输入，就我文本的、图像、语音的。大家还记得这个注意力机制的三件套，对吧？其中关于我们的应用，大家都手上都有课件，回头可以好好复习。我相信今天的信息量比较大，输入的这个内容就是input data type和我们的output data type就是这里，对吧？它都可以向量化，通过我们的向量空间来表达我们的自然界当中的很多内容。但是我向量化它这只是一个形式，我到底要什么样的向量才能准确表达，这就是要去学的那transformer开始学，并且学的还不错。
	他当他学的不错之后，其实他就可以去给下游做非常多的任务了。比如说我们去做这种问答的机器人，去做语义的理解，去做信息抽取，包括去做识别类的问题，去做我们现在说的做这种总结，做写日报之类的问题都可以做。并且中间这个东西不用怎么变了，这个是非常重要的。
	所以第四波的这个AI的大发展就在这，我们基础模型起来了。基础模型起来了就意味着AI开始理写人类可以理解的东西了。我们人就是我之前看这个大模型的这个潜力，我说一个最简单的体会就是有之前有段话很有名，就是人类的本质是一个复读机，对吧？大家非常喜欢这个点赞的时候，加一加一谁也评论了一句很有意思的话，一直说这个其实是非常本质和很客观的一个现象。
	人其实就是在不断的去学习这个很牛逼的人学习很牛逼的知识，在不断的再去延展。就跟我们看这个注意力机制，他把这个所有的输入叠加到一起输给解码器，然后我们的transformer把所有的输入丢给解码器，甚至觉得一种方式还不够，我换着花样吧？我用这个markey head，完了之后还不够，我还要叠加吧？我给你多说几遍，我说一次不行，我给你多跳几次。对，就是我们当然你画我猜的时候，我给你换着花样，然后换着花样不够记忆不深刻，然后我再给你跳一遍。这个其实就是transformer大白话理解，你就这么理解是非常好的一个很形象的一个解释。这个过程就是要让人人工智能去理解人到底是怎么学习的，非常像这个模式。通过这样的一个方式，人工智能开始理解我们生活当中的很多不同的类型的数据了。
	举一个再举一个非常简单的例子，让大家理解深度学习和人工智能这一波区别在哪儿。之前我在讲tensor or flow的时候，我经常会说我们这种判别类问题是识别问题。就是我们说这个地方是一只猫，那个地方是一只狗，这像一个什么问题呢？像是一个照猫画虎的一个过程。就是我让我的人工智能天天看这个猫，看了好多猫，它就慢慢会画画，甚至会画虎了，对吧？
	就是这个成语的含义大家理解，那么大语言模型像什么过程呢？像鹉学舌。就是你买来了一只鹦鹉回家当宠物，你天天在那儿念念念就说你好啊你好你好，鹦鹉也会说你好你好你好。然后当有一个客人来你家的时候，他就会说你好，尤其是是怎么触发这个鹦鹉的，一定是你天天在念叨，给鹦鹉念到的那个prom pat念的多的时候，那鹦鹉就被触发了。其实整个大语言模型非常像，因为这个智回归包括transformer，其实就是这么一个不断强化人工智能，强化语言模型的一个过程。好，这个是四次大发展的一个最后两次我们深度学习和大语言的一个大白话的解释，希望大家能get到这个点。
	我们最后再花一点时间去讲讲第三部分，就是我们的GPT跟bert对吧？就是现在大家都只聊GPT，其实GPT1。GPT1出现的时候，就是我们的OpenAI发布GPT1的时候是2018年，就是transformer发布的会。我们大家看到这个地方有这个attention is all you need，对吧？在这儿是google发布的。然后OKI紧接着在这个基础在这个工作的基础上去做了这个GPT1。然后又隔了一年19年的时候，谷歌发布了这个bert，这是几个主要作者，大家也都可以去了解。我还发现这个很有意思是我跟这个作者居然只有这个二度关系，就是也follow了，不知道他有没有回关我。整个过程当中，现在大家都在聊GPT，因为其他GPT火了。但其实回到2019年2020年的时候，bert其实当时是更受到学术圈的待见的。
	因为它比TBT1要牛逼，效果要好。所以我们会花一点时间跟大家讲一讲bert。当然大家看这幅图里面是这个引用关系。其实我们能看到，其实他们都引用了这个注意力机制的源头上的文章。就我们今天花了很多时间一直讲的这个注意力机制。然后注意力机制其实除了我们刚刚提到的这个transformer以外，还有一些相关的技术也是很重要的。就比如说这个LSTM是这种循环神经网络，最后实战当中在学术界里面用的更多的一种结构。它能更好的去学这个词之间的序列关系和含义，就是长短期的non short term的记忆，这个是一个很有名的一个网络，大家也可以去了解。
	然后这里的这个world to react和这个global vector其实是用来学习这个词嵌入的。就我们讲embedding，那embedding的这条技术线的发展，从13年的word vest到14年的global vector，有一个很长远的发展。所以我们从第三波的人工智能到第四波的人工智能，从神经网络到注意力机制到大模型，其实是有这么一个发展脉络的。然后我们今天已经讲了attention mechanism和这个transformer他们之间的一个关系，我们接着会去讲bert，然后我们会在下一次去讲GPT的这个发展。
	然后整个横向来看，其实注意力机制里面我们提到三件套。其中一个是我们的neural network，我们的神经网络结构对吧？这篇文章就是bengel一开始发的这篇文章，用了这个sequence to sequence，就是我们的encoder decoder的神经网络结构，transformer自己提了一种神经网络结构，叫attention is all you need，对吧？把LSTM把RNN给抛开了。但是他仍然需要embedding，只不过他没有用word wept自己去造了一个in这个embedding的方法，它叫position encoding对吧？然后embedding本身是用来把我们输入的各种数据变成一个向量的方法，是一种方法论，是一种技术。
	从word ret到global vector，其实都是在不断的去学习。像transformer它就是在不只是研究word，它是研究sentence之间的这个关系表达了吧？那么这个是一个发展脉络。
	好，我们再快速看一下bert这篇文章，bert是google发布的一篇很有影响力的文章。然后这篇文章的标题我们再解读一下什么叫bert。首先bert的前提还是预训练，就是我们看到预训练是一个现在所有人都知道的关键技术，预训练非常有用。这个bert的单词的含义是by directional双向的transformer，是双向的transformer用来干嘛的呢？用来做这个language understanding，就是用来做我们的语言理解的。其实我们transformer这件事情，让我们开始学习这一段话级别的表达，其实就是在理解语言，但是能不能理解的更好？可以。所以这篇文章一来，这个摘要一般摘要就是这篇论文最大的价值所在，它它非常重要的点明了我这篇论文到底带来了什么样的贡献。
	Bert当时发布出来一个最直接的就是又刷新了很多state of the art。大家经常会看到sofa SOTA对吧？简单理解就是sota的意思，就是当前最好的，当前最牛逼的。它在11个自然语言处理的任务上面都拿到了最好的结果。这个结果导向？我在11个自然语言处理的这个任务上，我都干到最牛逼了。我是不是值得你们大家学习一下。
	Bert其实核心就是by directional encoder representation from transformer。这里我们看到的双向的编码就是双向的，就从左到右从右到左的编码的一个表达方式，就是学习自然语言。那怎么怎么来学呢？从transformer里面来学。所以bert仍然是基于transformer的，不过它有双向的一个编码方法。
	我们刚刚有提到11个任务，包括这个glue，然后course，然后我不知道大家还记不记得前面我们有提到自然语言处理，这个跟计算机视觉一样，超过了人类有一个human performance。就是这两个比赛包括这个glue，其实就是bert当时立下了汗马功劳。你可以在某种意义上理解他就是自然语言处理当时的这个raise net第一次超过人类。
	但是比较遗憾的是bert现在不是最好的。但是我们也不知道，因为这个语言模型还远远没到终点，对吧？还在快速发展。还有很多的未来的事情值得我们去考量。Bert整体提出来了一种新的范式，一种架构。这个架构跟GBT不一样，它的范式右下角是这篇论文的原文，回头我们会把这些相关的论文整体打包上传到这个课件里面，大家可以到时候去看看这些论文。
	他提出了一种新的架构，就是我先去pre training一个bert，就是先training一个的模型。这个模型本身就bert这个模型本身设计之初就是为了能够去快速的给下游很多任务去做适应的。就比如说大家可以理解成，今天我是一个校招毕业生，我在学校里面一直在苦苦的学习，我本身就是一个bird。然后现在我开始找工作了，我现在找工作我可能投的简历是一个前端后端或者说算法。
	就是针对这里的不同的任务，大家可以看到这里有三个不同的任务。MMNLINER court的命名实体识别就是一些特定的任务。针对这些特定的任务我可以去find tuning。就比如说我现在正在上这个训练营的课，对吧？假设我们现在还有一个训练营是教大家前端的，还有一个训练营是教大家后端的。那我现在通过这个快速的fine tuning，不需要太多的数据，但是需要标注这个question answer appear，然后我就可以去干活了。这其实就是bert提出来的一个学习的范式和架构。我前期预训练，然后不需要打太多的标记，因为我要学的是语言本身的这个表达能力，对吧？
	我学会了，我知道一些基础的能力了，我能跟人聊天了，我能看懂文章了。然后我针对特定的任务，下游的任务我去做微调，我就能工作。这个是bert提出来的一个非常核心的点。这个点其实跟以前的模型这句话是相对于2019年来说的。因为那会儿刚刚发布了GPT1，它有很多独特的价值。
	我们再来解读一下，第一，它能够双向理解上下文，就是能从右往左的理解也能从右从从左往右理解，也能从右往左理解。为什么呢？因为他就是为了理解语言，所以这个做双向也很make sense，很合理。第二个就是提出了一个新的范式，预训练加微调的范式这样的一个策略。第三个就是它能有各种各样的泛化能力，就是我基础打得牢对吧？就跟我们现在在上的这周上的这两个课一样，我基础打的牢，后面我的课程这个课程里面跟大家讲各种各样概念。
	你有个抓手你需要微调，你有个抓手你能理解我说的话，我说了很多单词对吧？做了很多这个名词。我给你讲注意力机制，我给你讲自注意力机制，我给你讲embedding，跟你们讲这个对齐函数，你能理解我的这个词是什么含义。即使这个词我们刚刚讲到对齐函数有很多种，今天我用的是相似度，明天我用的是点击。但是你只需要微调一下，你就能理解到位了。
	这个就是它的设计的巧妙之处，然后能做多语言的支持，然后性能很好啊。同时它是开源的，然后使得很多人都能够去在bert上面再去做很多的成果，这个是非常有意义的。比如说我们国内还有一家公司叫西湖新城，这个南振中这个哥们儿也认识。然后他其实AL bert的作者就是把这个bert轻量化的这么一个论文。所以bert当时在19年发布这篇论文之后，取得了非常多的影响，非常好。
	然后我们再稍微花一点时间讲这个embedding是什么概念？就是embedding就是把一个词变成了一个向量，这个我刚才已经提过很多了，对吧？Embedding的方法有word vector，有global vector，他们都是用来去把这个词变成向量的，然后弄到一个向量空间里。不同的词之间的关联关系就可以通过数学的方法去算，对吧？我算两个向量进不进，可以有很多种方法去算，这个是embedding干的事儿。然后这个阶段我们我们通常在店里面去学习这个就我们刚刚有看到这个架构，第一步是这个pre train，然后不需要去做标记，我就丢很多的这个数。
	自然语言进去就跟transformer一样，对吧？有很多自然语言进去，我可能就知道king和queen之间是有比较近的关系的。因为他们都表示比较牛逼的人，对吧？那这个是第一个点，我们用embedding这样的一个方式去表达我们的自然语言。但是在表达过程当中，我们就会想只表达一个词是不是好的？但其实会有很多问题，就比如说这里有很多在bert之前的成果，都有一个问题，就是它通常只是从左往右的去学这个序列。
	但是其实人理解语言，就我们以前经常看到就是给你一段中文，那个语序错乱了，你好像也能理解意思。就是因为人其实是跳出一个一个的词和一个词紧密的顺序来理解内容的。所以因为你的输入的这个语料里面有可能就是有错的对吧？那你如果你能够双向去学，能学到更多的内容，这个是当时的一个很重要的启发，所以做了这么样的一个设定。
	然后因为可以你从左往右，又可以从右往左。其实他当时在论文里提出了一个很很有意思的点，就是叫see themselves。就是我能看见你了，对吧？因为以前我看不见你，只有一个方向去看，对吧？大家都军训过，向右看齐，我就只能看到右边的人，永远没有向左看齐。那么右边的人看不到左边的人。
	当我能够双向去学习的时候，这个by directional的context？这个contexts又来了。这个上下文我们在注意力机制第一篇文章里面，他再怎么样去学，他也学的是一个方向的。但是bert让我们双向来学，这个时候我能向左看齐了，我后面的词能看到前面的词。不管是说人说话的时候就会有语序错乱，或者说输入语料里面就有语序错乱，还是别的原因，反正我能学学到更多内容了。这个是bert一个很重要的一个设定。
	然后他怎么样具体去学的呢？怎么样去做训练呢？其实就是在训练过程当中，我有很多完整的话，然后我把其中的一些词给抛掉。就有点类似于大家中学的时候搞这个英语的完形填空。就是我把一些词给抛掉之后，然后我换成了一个特定的一个掩码。你可能是一个空格，或者就像这里写的一个框框，然后里面用一个特定的单词来代替，然后用这样的方式去训练，那成本就很低，对吧？
	为什么成本低呢？因为它具体来做的时候，他是把80%的这个训练语料都是不需要标注的。训练语料里面的单词换成了这个掩码，换成了一个特定的掩码。然后这个页要换的这个词本身也是可以随机的去取的。所以整个80%的训练数据就随便的就是很随意的去找出我要替换某一个单词，然后替换成一个特定的掩码，一个特定的占位符。而10%是换成一个随机的单词。因为它在模拟我们人人怎么理解的，就是他有可能有错误或者巴拉巴拉的。那这个时候我就把store换成一个随机的单词，还有10%的这个是不变，就我的输入输出是不变的，我就保持原样。
	通过这样的一个方式在训练这个bert这么一个函数，然后怎么要去打标记的下游。比如说去学这个，比如说有个特定的任务去学这个，我的第一句话跟下一句话之间是不是上下文紧挨着的，通过这个label就很明显，比如说sentence是A就是第一句话，sentence b是第二句话。The man went to the store. He bought a gallon of milk. 
	男这男人进到这个商店里面买了一加仑的牛奶，他这个标签就打的是对的。因为我们这个特定问题，大家还记得这个范式对吧？Pre train加fine tune，他现在已经pre train完了，我现在要微好，这个微调的任务就是判断两句话之间是不是上下文关系。通过这个标签就可以去做标注，这就是一个上下文关系。所以它的标签打的就是他是他的下一句话。那什么情况下不是呢？是。The man went to the store. 
	Penguins are lightness，有企鹅不会飞的那这肯定就是没有什么关联的一句话，那就打成一个not next sentence。就是这样的一个方式就能去做下游任务的学习。其他的也是类似的，就根据我们具体任务可以去打对应的标签去做微调。这个就是bert的一个核心思想。
	我们这一小节叫走向不同，对吧？Bert跟GPT走向不同，哪里不同？首先bert是一个自编码的一个训练方式。我们刚刚看到了他没有去做，就他在预训练阶段没有去做什么太多的研究自编码对吧？跟这个transformer当时干的事类似的。他他的预测目标是他给一个上下文，我能预测其中的一个或多个缺失的单词，就是这么训练的对吧？完形填空我掏掉一些词，你来帮我把填上，这是它的训练方式，又决定了它的使用方式。
	预测目标，然后在输入的时候，它是双向的，然后适合干什么呢？适合理解上下文，然后能够帮助你去做一些信息提取，问答系统、情感分析。所以bert这个论文这个成果一开头它就是为了让你更好的理解文章，理解语言的。然后它是基于transformer的这个编码器的这个架构去做设计的。但这个语言模型，大家会说它是一个判别式的语言模型。然后它的优点就是对上下文的理解能力很强。但是它生成文本的能力比较弱，因为它就不是拿来干这事儿的对吧？生成问题是一个另一个问题，所以大家没有把它直接标注成一个深层式的语言模型，但是他的理解能力很强。
	那么GPT是什么呢？GPT就跟它不一样，GPT叫自回归的。然后他的预测目标是什么呢？是我给定你一个前面的单词，你帮我预测下一个单词对吧？就是我就跟我刚刚讲的这个鹦鹉很像，对吧？
	你现在在训练GPT，就是在家里养了一只小鹦鹉，你天天跟他讲讲讲，然后他就学，有有样学样的在那学。然后下一次来了一个其他的朋友，说了一句跟你刚好类似的话。就比如说你天天在家问你吃了吗？我吃了，然后你就自己在这念叨，然后有一天你你你说了一句你吃了吗？因为我突然来了一句我吃了。这个是很常见的一种鹦鹉学舌的现象。有可能下一次是一个朋友过来来你家，你他突然问你，你吃了吗？鹦鹉先回答了，我吃了。
	其实GPT是这么一个逻辑自回归，然后那那是这样的一个逻辑。同时他也能学习的时候，从左到右，从右往左这都可以学，这都不是什么难的事儿。后面我们会专门讲GBT的这个家族迭代，然后它适合生成式的任务。
	鹦鹉大家想一下鹦鹉是干嘛的？就学对吧？学你这个说话要基于的是解码器，因为解码器就是干这活的，而是深层次的。然后他的预测连贯性比较强，因为他就是这么学的，但是上下文理解能力比较弱，这个也是GPT1的弱点。后面我们会发现后面的几代GBT在解决这个问题，他用包括用各种各样的方式，这我们就不再展开了。
	这是他们的差异走向不同，但是他们有哪些共识，我们会发现这些共识是优点。第一，都用了transformer对吧？只不过一个是基于encoder一个是基于decoder的这个优势。训练的时候都用了大量的无标签的数据进行预训练，这个现在几乎是为成为了一种共识。这个模型训练就是用无标签的数据去训练。这种pre train的方式是大家都在玩的。
	然后可以都可以通过这个fine tuning，或者像后面的GPT的版本。现在GPT4，GPT3.5可以通过这个p prompt来进行学习，对吧？Prompt learning或prompt tuning来进行学习，这是我们后面会去再展开的。
	今天跟大家抛砖一下，大家可以预习，任务迁移的方式不同，但是GPT1是可以通过翻译憧憬进行下游任务的。然后训练目标都是通过预训练去理解这个语言，这已经开始有这个大语言模型的雏形了，对吧？就我们训练目标都是希望我们能不能搞一个模型，就我们讲这个范式的迁移，我们能不能搞一个模型。这个模型真的理解了人类的说话，那他就可以干很多的活儿。这个是一个很重要的一个思维方式的改观。然后最后他们都支持多语言模型的训练，现在我们能知道CIGBT早就支持N多语言了，对吧？但是在1819年的时候，其多语言模型训练还是比较难的对所以我们再回到这张图，这就是我们今天看大模型发展，就是今天我们讲的时间2个小时，能把这个讲玩已经很不错了。
	就是我们花2个小时时间跟大家讲了什么呢？讲了这个大语言模型的起源和发展。这个起源和发展到了1819年的GPT和bert，大家相当于就看见黎明了，transformer是这个黎明的曙光，GPT跟bert基本上就是黎明已经出现了。然后我们这门课因为没有大量的设计，bert的这个相。光研发它现在也不是这个重点，可能是GPT为主。所以后面我们会把GPT再给大家庖丁解牛。但是这个黎明往后走，其实我们就看见不仅有黎明，现在这个CHAGBT就像朝阳一样，辐射大家。所以这个新的范式就是现在我们研究各种各样AI大模型的一个研究范式。
	这个foundation model你把它直接替换成ChatGPT，替换成GPT4大家发现也是合理的对吧？然后我们现在其实已经在研究怎么用大模型来开发应用了。然后我们会直接通过这个基础模型层跟应用层去做连接。然后为了让这个应用更加的合理，我们甚至可以去搞这个南倩搞向量数据库。所以其实这就是一个新的基于基础模型开发的年代。
	以前我们讲面向对象编程，未来可能是面向基础模型编程。面向基础模型编程我们需要什么？面向对象？我们需要各种各样不同的面向对象的语言，对吧？我们需要java，我们需要C加加，我们还需要一些其他的面向对象的语言。面向基础模型，我们现在是在怎么玩的呢？我们现在在用prom pact，prompt本身还是比较弱的，所以基于prompt我们还有很多的工作可以开展。就基于prompt未来它会不会发展成编程语言的一个下一代的形式，这些都是值得大家去探讨。
	好，今天我们2个小时的时间干货满满，然后信息量也很大。希望大家能够回头也能够去反复的去看这个材料和听我的一些说明。我希望今天的解释还是让大部分的同学能够理解，我们是怎么样一步一步的从我们的这个注意力机制到我们的transformer，然后又到了我们的这个bert，这是我们的这个GPT，今天我们主要的授课的内容就是到这里。然后周日那天我们会从这个GPT一直讲到我们的如何用这个提示词来玩我们的这个大模型。然后这个提示词本身的一些最新的进展也会跟大家做一个分享。好。