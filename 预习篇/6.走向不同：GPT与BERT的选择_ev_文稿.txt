	好，我们通过前两节课向大家分享了attention机制、注意力机制和transformer各自的技术原理，底层设计以及它的关联关系和它的发展脉络。接下来我们进入到正式的大模型，就是我们的GPT和bert。这节课我们先向大家讲一讲bert这个大模型，它是怎么样一回事儿，什么时候发布的，有什么样的一些优势，它和我们的GPT家族有什么样的一些不同。首先我们看到这个经典的论文检索的代表，我们看得到在attention发布之后，17年的时候发布了这个transformer。然后18年的时候，其实OpenAI的这个团队发布了这个GPT e，当时叫做generative pre training，这其实就GPT的含义。但是这篇论文他也开宗明义的讲了这篇论文讲的这个标题就很清楚，叫improving language understanding。就像我们刚刚讲transformer一样，现在我们的基础语言模型的发展已经从解决特定任务转向了提升语言理解能力。
	到了19年，其实是18年底，发布了google发布了这个bert。Bert其实它也是叫pre training of deep by directional transformers for language understanding。这里很有意思，虽然GPT1没有提transformer，但其实就是用的transformer。然后19年google因为transformer也是google团队发布的，他自己就讲了bert是什么意思？就是一个预训练的，深度的双向的transformer用来干什么的呢？用来提升语言理解能力的。
	所以其实在18年，我们就能看得到大语言模型的发展方向。就已经从一个具体的深度学习任务，到我们的这个attention is oil need to transformer的发明，再到现在的这个大语言模型，就是为了着重用预训练模型去提升语言理解能力的这么一个新的时代。所以18年很关键。
	在这18年的时候，我们看得到这个文章，它的主要作者，比如说18年的这个GPT1发布的这个作者很有意思，这个叫alex red ford。他现在在一家初创公司，当时其实是我们的GPT e这篇论文的一作。还有很重要的一个作者，我们的bert的译作叫jacket book。David叫这么一个名这么一个研究学者。他现在在google做软件开发的工程师。然后还有一个很重要的一个角色，他其实是我们的整个GPT能够出现到世人眼中，能够在大众眼中去，最终呈现出来一个很重要的一个作者，就是我们的SARS。我们不太会念这个丽亚，就他其实是OpenAI的联合创始人和他的首席科学家。那他其实是整个在google期间和在OpenAI期间都做了大量贡献的一个科学家，我们可以具体来看看，这里是列出了14年的论文attention mechanical这两位就是我们的班banner和我们的bangle。
	在这个过程当中，其实整个大语言模型它的发展是有很多的基础技术的进步，它的迭代发展过来的。第一个就是我们看到RNN，它自己有这个LSTM，就我们的这个RNN的变体，叫做长短期记忆的这个网络，叫non short这么一个概念。它其实要解决的问题就是我们经典的RNN，这是1997年的一篇很早期的论文。经典的RNN其实提出来之后，没有办法被广泛的使用，我们先抛开性能各方面不讲，他虽然能够处理时序问题，但是就会出现一个捡了芝麻丢了西瓜，就是聊了这句忘了上句的情况。LSTM就是用来解决这个问题的，并且bert里面也用了LSTM。
	然后我们还能看到在这里有关于embedding的技术。就我们的13年的时候，google发布了这个world vest，非常有名的一个embedding的工具，做词嵌入的一个工具。然后接下来14年的时候，发布了更大范围内的embedding的方法，叫global vector，就全局的一个vector方法。然后17年的时候发布了这个transformer。
	所以整体来看，其实大语言模型的发展跟这些基础技术的发展，其实就像我刚才说这个DNA的双螺旋一样，大家都是在并行的往后去迭代。只不过在18年的时候，大家突然发现研究方向可以变成语义理解语言理解的这个研究方向了。并且这个研究方向是值得去投入，值得去长期去深耕的一个方向。为什么呢？是因为它有几个很重要的发展的条件都具备成熟了。
	然后我们再回过头来看神经网络的一个发展，就是神经网络其实它有我们上节课讲transformer时候给大家庖丁解牛过attention模型，就是我们的注意力模型，有三大部件。其实神经网络也有很多发展方向。那一个是我们这种像LSTM这样的RNN系列的，包括像CNN系列的，它其实都是在网络结构层面上做过一些不同的迭代，去解决一些特定的问题，INN解决这个序列问题、持续问题，CNN解决很多像图像，计算机视觉方面的问题。除了这一类就是网络结构基础的调整以外，embedding也是一个很重要的发展方向，就比如说我们的这个word vest global vector。
	除了这些以外还有一个新的分支，就是14年的时候，由本就和banana做的这个注意力机制。注意力机制的发展，从14年到17年的transformer就self attention，就各种各样的对齐函数的出现，一直到我们的transformer后续又引进了不同的大语言模型，就比如说我们的GPT和我们的bert。所以整个神GPT和bert其实也是属于连接主义下面的一个大的分支。他们的来源就是从我们的transformer，然后再往前倒可以看到我们的注意力机制。但是他们都可以放在这个连接主义大的这个框架下面去理解他们，甚至我们的这个transformer本身也用了encoder decoder这样的一种结构。
	那我们再看看bird这篇论文重点在讲什么，我们已经知道它使用了深度的神经网络，对吧？那这个深度的神经网深度的这个transformer这个深度的transformer其实还是一个双向的transformer，用来干什么呢？用来做语义理解。然后这个语义理解通过什么样的实现呢？一个预训练的transformer，很多的前缀，这个定语，具体来看，它的结果很好。首先他在11个自然语言处理的任务上获得了新的最好的成绩，叫IT obs new state of the art就是sota对吧？这个results eleven natural language processing task，就是它在11个非常有名的有代表性的自然语言处理任务上获得了最好的成绩。其中一些比较有代表性的相对glue，我们之前在前面几节课有提过。
	这个自然语言和我们的计算机视觉都有一个人类的表现叫human performance。不知道大家还记得吗？这个human performance当时在18年前后，自然语言处理领域都超过了这个human performance，超过人类。那谁干的呢？其实就是bert干的对吧？
	Bert当时做了很重要的一个事情，就是大家能看到叫这这个叫start question answering dataset。其实就这个score d它有两个版本，1.1和这个2.0的版本。这两个版本是为了解决这个阅读理解问答任务的这样的一个基准测试。这两个也是在我们之前那幅图里面都有出现的。大家如果还影响的话，还有一个叫glue，这个glue包含了九个自然语言理解任务的基准测试。比如说这个文本蕴含的识别，语义角色的标注，情感的分析。这个也是bert在上面取得了非常好的成绩，包括像下面提到的这个swag和这个coral都是很基础的自然语言处理的任务，bert都取得了非常好的成绩，超过了以往的所有的模型。
	所以首先从这个结果的导向来看。的很成功，他取得了这么多成绩，所以他是一个我们值得学习的一个大语言模型。那他到底这个成功在哪儿？他跟我们的GPT有什么不同，我们接下来看一看。
	首先bert提出了一种新的范式，这个范式叫什么呢？就是用free training加fine tuning的方式来做大多数的下游任务。简单来说就是我们刚刚有看到一个，上节课有看到一幅图，就是一个叫做基础模型的时代。基础模型时代就是有一个基础模型针对下游任务去做这个适配。然后上面这个上游给大量的数据。这种模式其实或者说这种训练的范式，其实就是bert提出来的。
	然后大家能看到这三个是不同的这个任务，命令命名实体的识别，包括stanford这个question answer的这个dataset，还有其他的任务。这些任务都是要做下游任务，下游任务只需要去做微调，用很少的数据，比如说这里我们这边都是一些没有打过标注的这个语句，右边是打过标注的这个问答。然后我们只需要很少量的问答，就能够去把这个bert变成一个特定领域的模型，这个是第一个点，是这样的一个范式。
	第二点带来的好处是什么呢？我们都知道要提升它的语义理解能力，我们要训练一个很有用的基础模型。但是训练一个基础模型在计算机视觉领域，我们需要造一个image。这样一个巨大的成本，然后巨大的数据带有标注的这个训练集。
	但是在自然语言领域，其实做这样的一个训练集很难。因为它不像这个图像里面天然有一个好处，就是中国人、美国人、英国人看到这只鸟，大家都。知道它叫鸟，但只是说它的这个单词不一样。我们叫鸟可能他们都叫bird，倍儿的。
	然后这个图像天然有一个好处，就是它的内涵很丰富。但自然语言不同，就是一只鸟一只鸟的图片你标注一次就够了，你只需要换那个标记就好了。但你说一段话，你怎么去给他打标记呢？他一定是根据下游任务来的，你没法直接去打标记。然后像中文还要去做分词的处理，就有很多很多挑战。
	那么bert的一个最关键的点就在于这一句。大家能看到on label我的sentence a and b pay，我不用去做标注了。我就把人类历史上的这么多数据，这么多自然语言，我拿过来直接做预训练。那怎么做预训练对吧？这个很关键，但它的好处大家应该能听明白。第一，这样的话有大量的训练的语料，就是可以拿来为我所用。我能通过他们训练出一个语义理解能力很强的模型。
	这个模型只需要少量的标注数据就能解决下游的具体任务。这个就是一个free training加find trading的新的范式。我们接下来看看它的这个价值除了刚刚讲的以外还有什么样的好处。第一就是我们这个新的范式提出来之后，我们刚刚提到他能用大规模的无标签的文本进行预训练。学习语言就提升与language understanding，提升这个语言的理解能力。
	然后第二个就是说bert除了这样的一个直观的在数据依赖和下游任务训练上面的优势以外，最重要的事情它还有一个词叫做by directional双向的。我们包括学attention，学这个transformer都知道，包括transformer的这个decoder还是用了掩码，就是不能看见右边的对吧？不能看见后面的。
	那么bert充分的利用了这个双向，这个双向其实也是在我们刚刚有提到神经网络有三大三大方向，三种不同的知识方向、知识脉络。其中有提到这个神经网网络结构本身，比如说这个LSTM，其实LSTM自己在这个迭代过程当中也演变出来这个by LSTM就双向的LSTM。那bert就把这个优势给它结合过来，双向的去学习。这样就可以不只是像GPT单向的去理解它。其实也可以倒着从这个一段话的末尾往前面去读去学习。
	这个是他第二个优势。因为这样的一个方式使得它你理解能力更强了，这个也比较好去有一个直观概念你可以理解，就是人看一段文字的时候，我们中国中文经常会发现，有的时候他语序乱了，但是你还是能读懂，这就是因为这个语序本身的理解能力，其实人是具备的，但怎么具备的我们很难知道。By directional其实就是尝试去用这个方式。就我的语序本来训练语调里面有可能就是乱的有有部分，在结果上面也有可能是乱的那我在学的时候就去理解这种乱的语序，对于我来说其实我能学到更多，甚至这个乱的语序在不同语言里面还会不一样。比如说像日语他把动词放后面，那中文可能就把动词放前面，大部分语言都把动词放前面，我学了好多种不同的语言，我的理解能力就会有更多的提升。
	第三个就是说它的这个跨任务的泛化能力很强。因为它的语言理解能力变强了之后，下游的这个微调就变得很简单了。相当于你有一个特别强的本科毕业生，他要去特定的工作岗位上。他如果本身足够优秀，那他只需要简单的一些学习，就能处理具体岗位上的工作。
	包括刚才提到多语言的支持，它的性能也很好啊，刚刚说到刷新了11个基准测试，它是开源的，因为它开源，所以基于bert有大量的衍生的模型的整推动了整个大语言模型相关研究。当时应该叫预训练的这个模型，叫pre training的这个transformer，就GPT或者说叫做预训练的模型，现在都叫大语言模型了。那具体怎么做的，我们讲到这个预训练，这个预训练其实不是一个非常复杂的过程。我们已经讲到了怎么transformer是什么。现在无非就是说要预训练一个transformer，就提前拿着一堆未标注的文本去训练出一个transformer，其实就这么简单一个逻辑。
	首先我们要理解这个word in bedding是什么？就是word in bedding其实是NLP领域里面一个很基础的概念。13年的时候，world to wake的发布出来，它要解决的一个什么问题呢？就是首先我们自语言本身是没法去做复杂的推理运算的。就比如说我给你讲了一段话，这段话其实是自然语言，但是我的计算机是理解不了这个自然语言的。我是计算机只会去做，你可以理解成做一些逻辑运算，或者说概率的这个分布的一些运算。
	那么我要能够把它变成一个数学问题。第一步就是我希望把一个单词变成一个向量，变成一个可以用来做运算的向量，具体来说是这样，那简单看一看，就是比如说这里有一个king这样的一个单词，我会变成一个向量，我有一个queen的它也会变成一个向量，那这个它变成一个向量，多少维度很重要，是一个参数，对吧？第二个就是说每个维度里面的值是什么也很重要。Word to vector，global vector其实就是用来解决这个事情的。比如说world to vector，它就决定我现在把它变成一个五百多维的向量，可以，这就是参数。这个word vest和global vector其实都是用来学怎么样把一个词变成一个什么样的向量，这样的一些算法和方法。
	当时word web的这个发布有一个很好的一个事例的举例。就是指就是我把这个king和queen学出来之后，把这个king和queen去做一个差值和我另一门语言，比如说我叫这个国王和这个皇后去做这个差值。其他他算出来invading的这个简单的算术运算符的结果是相似的。所以当时world way推出之后，大家就觉得学习语义是一种有可能实现的事情。只不过work是在单词这个层面上去学。那有没有可能是在一句话或者一个短语一段话上面去学？这个是word embedding和预训练模型的embedding ing。就我们这种比如说gbt 3.5，它的embedding最大的一个区别。
	好，那么in bedding要解决的问题其实就是我们希望把自然语言变成一个可以用的单词，把这个自然语言的单词变成一个可以用的高维的向量。然后这个向量我们就能去做运算。做运算就可以为下游的任务和我们的本身的学习去做各种各样的操作。所以morning building是第一步，也是我们深度学习的一个基础。只不过后来我们就从单词变成短语，变成一句话，甚至更长的段落。
	那么怎么样去解决我们在之前方法当中的一些问题，这里有一些很很有点子。第一个就是说我们刚刚有提到transformer也好，GPT e也好，它一直都是一个像RNN满足这个RN顺序一样，就是我是从左到右去看的。所以那个时候的语言模型，但这个是站在bert的视角去，就是18年底的这个视角去回溯这些问题。
	但很多问题现在可能大家都觉得不是问题了。但是站在当时的那个视角，大家去理解bert发布的时候，他要解决的问题。第一就是说这些从左到右看的这些语言模型，天然就缺失了双向的信息。这个是当时的问，因为没人去做双向的语言模型或者说预训练模型的学习。
	第二如果是有这么一个问题存在的话，那bert的这个团队就在想，为什么我们的语言模型一定要是单向的？虽然这个单向更符合人的直觉，就是我说话我是我只会正对吧？我不会真的倒背如流，我不会倒过来背一本书，我就是正着去讲话的。但是这个也是大部分的人的直觉，但是作为一个语言模型来说，是不是只能这样去学东西？如果我不只是单向的去学，而是双向的去学会不会带来一些更多的价值，然后它的这个概率分布会不会更好、更健壮一些。这个是当时bert思考的一个问题，并且他也给出来了一个很好的答卷，就是把bert给做出来了。
	所以我们能看到这里有两个分析。第一个分析是说我们其实是需要一个更好形式的这样的一个概率分布。这个是我们要的一个结果，就是我们在模型里面最终要生陈述的这些单词，这概率越高，那个单词可能就是我们要的这是我们希望要的一个概率分布。第二就是说我们的这些单词本身，就我们输入的这些单词本身在输入的这个单词本身的invading里面，我们刚刚看到就是这个king，queen这些这段话里面，其实这些一个一个的单词他们都是孤立的，就是他们本身是没有什么关联的。我们通过网络模型，通过这个神经网络的这个架构，encoder decoder的架构或者transformer架构，使得他们能够从左看到右。就是我因为我是从左到右，包括我用编码的方式能把他们的关联关系这个顺序给做出来，包括用这个位置编码。但是我始终是无法做到从右看到左的那是不是当我能够让他能够双向去看，就是我的这个。学习的过程当中，我的这个顺序是两边都能去看到他们的时候，我就能得到一个更好的一个概率分布的一个函数了。
	其实有了这两点思考之后，bert就做了一个大胆的尝试。第一个尝试就是说我们把这个顺序，大家能看到这之前的，比如说我们要开一个银行，open a bank, 它是这样的去构造我的神经网络的这就是transformer的一个典型的一个结构。从左边开始，然后这样按顺序从左到右的获取。但是我们可以尝试让他们能从右往左的去看，所以你看现在这幅图它就变了。我们不只是有一条线是从左往右，在layer two这边。我其实还有一条，同时有一个从右往左，就从文末往文起文开头去学的这么一个顺序。这个是第一个点就我有双向的一个上下文。这个信息对于我来说是以前从来没有的，但我能够用一个神经网络去整体去学。
	这里需要点出的一点是以前在bert发布之前，也有一些研究学者做了一个什么事儿呢？先弄一个模型去从右从左往右的学，再弄一个模型去从右往左的学，把他们两个模型的结果再去做一个拼接。是这样的，有人去做，但bert是说我就在模型内部同时让他去学，然后组成一个新的向量。新的向量是包含了两个方向的，这个是bert的一个很重要的一个创新，虽然这个创新你会觉得在by LSTM里面有人做，但是没有人把它做到这个场景里面来。然后你读了很多论文之后，你会发现其实创新的思路就那么几条。但是需要你要有快速去做实验拿结果的能力，才能看到很多的好的成果发布出来。就比如说这个双向的ASTM，双向的学习，之前也是有前人去做过的，但是他们没有再更进一步。学术的发展其实就这样的一个过程。
	我们知道了这个思路，就第一步知道这个思路，第二步怎么样去学习它，怎么样去训练它。这里其实我们在讲transformer的时候，其实有讲到一个很重要的思路，就是它这个mask掩码是从左往右的，然后我后面的看不见。那么对于我们的这个bert来说，其实他要的核心是什么？他要的是预训练吧？就是他跟transformer最大的区别是包括GPT也是要预训练的。Bert要预训练，并且也是在没有标注的文本上面去做预训练。
	那他到底怎么预训练呢？他用的都是一堆自然语言，然后这对自然语言没有任何标注。他要训练的时候，其实他要做的事情就跟我们小时候学英语去做完形填空一模一样。第一他没有标注，他就需要去掏一些空空出来，让but这个模型，这个预训练的模型能学会。
	人到底是在怎么理解这段话的？这段话里面有一些是关键的词，它需要去理解语义，理解语义就是通过完形填空的方式。这里我们简单来给它做一个拆解。
	其实是这样的，第一我们输入的都是自然语言，但是我们在训练的时候把一些词给掏掉，就像我们完形填空掏了一个空，那这个mask就是被掏掉的一个空，然后我们训练它的时候，我们把这个训练的文本这样一段话可以掏出一个空，然后去替换一些特定的词。就比如说我可以，我当然可以替换成一个原来它这是正确的这个词。比如说第一个空空，这个mask应该填store，就是这个人他存钱，然后这个人走到这个商店想要去买这个一加仑的牛，对吧？然后这个store它就把这个填回去，这个是一种。还有一种方式就是说我就保持这个没有，就是我这儿就留一个空，就像我们人来学这个完形填空一样。还有一种方式，其实就是我填一个错误的值，相当于造一些反应，这都是一些方式。
	但是这些方式各自有一些特点，就比如说我们来分析它，假设我们现在给它填空，如果我们给它的这个mask非常少的话，其实会造成一个问题就是我训练的成本非常高。比如说我的这里这个mask本身就只有一段话里有十几个单词，但是我就只掏两个空。那这个时候可能我就得把这一句话分成100句话，因为你你做排列组合，对吧？
	那这个时候变成一个什么问题？就是原来可能只有一个GB的数据你需要去，但因为你只掏两个空，你现在其实实际上训练100个GB的数据，这是有可能的，那么这个训练成本是非常高的。这个是往一个极端去想，就是如果我们的mask涛的非常的少，那么它的训练数据就得造非常多，才能满每个空空。我说我关心的这个词都有这个mask那这个是一个思路。那那往另一个极端去想，就是如果我全部都给它套成mask比如说这段话就是我就只留下了这个the和这有什么to a之类的这些介词，我没有给它掏空，其他全部掏空了，那相当于没掏空对吧？因为所有的词都可以变成这样的一个形式，就是the空空空空空空空空的空空by拜都没有，by也耗掉了to mask然后mask然后mask mask那这样的一个东西会造成什么情况呢？就是它没有什么上下文去学了，因为全是空，就像你你面对一个完全填空的题里面90%都是你需要填空的内容。
	那你说这个完形填空还能不能做？就学不到什么东西，甚至怎么填都可以。那那这样其实也是不好的。所以很关键就是我们要把这个mask的数量保持多少。当时这个bert把这个数量留给留成了15%这么一个数量。
	所以整体来看，其实他去mask他去掩码这个语言模型的这个套路就是第一用15%的这个频率，就15%的词是有这个mask的，就是一段话里面有15%的词是有mask的。但是在这15%的mask里面，有80%是直接替换成mask，就相当于变成一个空，你需要去填。10%是随机的一个词，这有可能是大部分都错了，就是文不对题。然后还有10%的情况下是保持原样，这是正确答案。
	所以我们捋一下这个思路，就是给你一段话，其中有15%是完形填空的空，你需要去填的。但是这个空有80%的时间是一个空，有10%的时间是已经填好了，但是是一个随机的词，可能不错的。其实能造出这样的超参数来，也是因为可能他们对这个数据有大量的观测。因为人人造的这些没有标注的文本本身就会出现一些情况，就是我写错了，然后还有10%的时间，这个时间是保持原来的这个词。这个其实就是他训练的一个套路，就是他在pre training的时候，怎么样对他未标注的数据去做的一个套路。
	然后我们刚刚还看到一个很重要是一个基准测试，他他做的比较成功，就是去预测这个文本蕴含之类的一些任务。他在训练的时候还做了一个很重要的少数的事情，就是我做了大量的预训练的学习，然后我还想学习这个句子跟句子之间的关系。就比如说我判断我这两句话之间是不是紧挨着的，是不是挨在一起的。在在这个句子层面上，这里就有一个很好的事例让大家理解他怎么训练的。第一就是说我们看这句话。The the man went to the store. He bought a garden of milk. 
	就是这两句话，他会打上一个标签，这个是他去做下游任务的时候，我们一个典型的这种标注方式。就是我打了一个标签去标注这两队之间是有关联关系的，并且它就是它的下一句话。然后。那么service a和sentence b的另一个反应是什么呢？就比如说还是这个人去了这家商店。但是sentest b是讲的这个penguins，就是这个企鹅，它是不会飞的，企鹅不会飞这两句话显然就没有什么关联，所以他就把这个label标注成了note next sentence，它就不是它的下一句话。
	通过这样的一页事例是想告诉大家，其实我们学会了上面的这个语义理解能力之后，下游任务就可以通过这样的方式去做标注。很简单，这也是大部分预训练的语言模型的fine tuning的一个常见套路。就是我在用小量的文本去做这样的标注标注的微调，就可以让他完成一个特定的任务。比如说通过这样的方式去微调，我们的下游的bert就能解决这个句子之间是不是？当然你这个关联关系有很多种，这个关联关系特定的是指我是不是他的下一句话，也可以用别的一些关联关系来做标注。比如说这两句话是不同义词，我这两句话是不是有别的一些关联关系，都可以打标注。
	那么bert跟GPT，GPT我们还没有讲，会在下节课去讲。有什么样的差异呢？我们这里先做一个预览。
	首先bert它是一个自编码的，它不需要有太多的标注训练数。你自己就可以去直接去训练。然后他的预测目标是一个给定上下文预测其中的一个或多个缺失单词的这么一个预测目标。这个我们刚才也讲了，就花了很多时间跟大家分享这个mask的这个设。他就这么训练的，所以他就这么预测的。所以你给他的这个任务，通常就是你给了他一段话让他去做完形填空，那他肯定做的很好。包括你不是填一个空，你填一句话，那他也可以做的很好在输入处理上它是双向的，我不只是单向从左到右的去学习这个语序，我也能反过来学。
	然后它的适用场景，它非常适合用来理解上下文。理解上下文的意思就是大家以前读书的时候，会经常老师不管语文还是英语老师就教你读书的时候应该快速的怎么读。甚至可以先读第一段，再先读第一段，再读最后一段，最后再去中间读。
	这个其实跟这个bert也很像，他能通过各种顺序的学习，能够理解上下文，这样的方式就能有助于他去做什么呢？做信息提取、问答系统、情感分析。从我们看他的这个实验，基准测试这上面取得的好成绩也能够理解。架构上面它是基于一个transformer的这样的一个编码器的一个结构。
	然后适合这种判别式的语言模型。然后优点就是对上下文的理解你很强，缺点就是它的生成的这个文本的连贯性比较弱。那么跟GPT比起来，GPT其实它是一个自回归的模型，这个我们后面会去详细去讲。自回归的模型它就单向在训练，所以它就特别适合你给上文他学下文。
	到今天为止我们用ChatGPT你都会发现有很多比较好的prom的提示词的设计，都是你给出上文。比如给我写首诗，给你写首诗，然后最好还加个冒号，这样它能理解。因为它会有各种各样这种冒号、引号这种符号的一些训练，它也有。所以GPT是更适合做一个生成类的任务。
	然后包括刚刚提到的诗歌、文章，甚至代码的生成。然后他的预测的这个连贯性相对来说比较强。他学的时候就是给上文生成下文，他学的时候就按这个方式去学的。但是GPT的上下文理解能力就比较弱，所以我们这后面这个课程里面还会去讲怎么样用南茜来维护这个上下文。就是我们用GPT的这个chat API也好，用其他语言模型的这个聊天类型的API也好，你要维护一个上下文，这个上下文是每一次都要交给大语言模型的。让他去不断的去理解。那他们有什么样的共识呢？其实有很多的共识。
	第一种就是他们都用了这个transformer，这是最重要的共识，也是我们讲大模型的这个理论，都基于一个同样的transformer很重要的一点。第二，就是他们都需要对这个数据进行total ization，第三，就是他们都使用了无标签的数据进行预训练。这也是未来就是从18年开始到现在的一种典型的训练范式，就我们讲的无标签数据的预训练，加上有标签数据的boring就微调，这个有任务迁移就是这样做的。然后训练目标都是希望能够更好的理解这个语言本身，就是提升基础模型的语言理解能力。这两篇论文都这么干的，bert也是我们的这个GPT e也是都支持多语言模型的训练。这个是bert和GPT他们都共同采用的一些好的地方。
	我们再回看这张图就更有感触了。就是讲pre training加find tuning的这个范式，其实就是是一个基础模型时代，我们做AIGC，做这个生成式人工智能应用的一个典型范式。大量的无标注语言，人类的历史上的书籍，我们的redit知乎这样的一些问答社区。然后互联网数据无标注的，甚至不是结构化的，留给我们的基础模型去做预训练，或者说预训练之后得到了一个基础模型。基础模型在有有标注的数据上面去做一些下游具体任务的一个微调，然后解决我们的这个下游任务。这个其实是从注意力集体到transformer，到现在我们的bert或者说GPT这样的大语言模型的一个典型的一个架构。