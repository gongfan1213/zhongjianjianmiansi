	好，我们接下来看一看GPT这个模型家族，它是怎么样从始至今发展而来的。其实大家如果关注archive这样的一些论文网站的话会知道其实在archive上面有很多的免费的论文，大家是可以第一时间去学习的。在archive上面有一个论文，叫2023年发布的，叫大语言模型的综述，我印象中是一个复旦的团队发布的文章。A survey of large language models这篇文章其实介绍了什么呢？他介绍了在archive这个网站上关于语言模型和大语言模型，大家就这两件事儿来说的话，有多少的论文被发表出来了。
	我们能看到这个趋势增长很有意思，当我们去搜索语言模型的时候，GPTGPT2还有T5GPT3，这是我们今天会介绍的一些模型。它的增长其实是非常大的，并且它的这个数量也很大。大家如果去看这个纵轴，就是能看到，然后当我们去搜索这个大语言模型的时候，我们会发现其实是直接从这个T5，GBT3开始的。这里就有一个很细微的区别，到底什么可以称之为一个大语言模型？就我们知道语言模型的这个数量就能看出来语言模型已经发展了很多年了。
	但是其实从2019年10月开始，这个大语言模型才慢慢进入大家的视野。不过它的增长趋势非常的猛。大家能看到尤其是今年这个ChatGPT发布之后，这个大语言模型的研究一下就火起来了。在之前其实语言模型只是属于自然语言处理当中的一个小的类别，像我们去聊注意力机制，聊transformer，它都属于自然语言处理里面的一个类别。但大语言模型是独树一帜的，成为了一个新的研究方向，并且它的这个热度非常的高。这个其实是从语言模型NLP当中的一个分支到大语言模型的研究，其实跟ChatGPT的出现是非常有关联的。因为它使得很多的关注度都聚焦到了这个大语言模型，然后大家会发现大约模型的能力预期非常高。然后它确实在ChatGPT这个应用程序上给大家凸显了很多的能力，所以我们能看得到从这个ChatGPT发布之后，关于大语言模型的这这个文章，从一天只能发0.4篇，迅速增加到了每天都有89篇的文章发布出来。我相信现在可能这个数字还在增长，了解这个创投圈，各种资源的叠加，使得我们能看到GPT，尤其是ChatGPT点燃了大语言模型的研究。
	从LP这个语言或者说NLP这个自然语言处理关于语言的技术发展来说的话，其实我们做语言模型不是一个像孙悟空一样从石头里蹦出来的。人类其实一直在研究这个语言模型，这个技术分支也很直观，就是相当于是说我们这些计算机科学家，想要去帮我们的人的这个知识能够转换成计算机的知识。那这样的话计算机就能帮人做很多的事情，而不只是说我做了一个excel做了一个PPT的工具，去提升你的生产力。有没有可能我做一个语言模型，它能理解你的意图，它就能帮你做更多各样的事情。一个能听懂人说话，能理解你意图的这样的一个模型或者说算法，它的价值点是非常高的。就像我们上节课讲的bert，它的核心是为了提升语言理解的能力。其实这是一个长期以来计算机科学家一直在努力去研究和突破的方向。
	具体来看，其实我们可以把语言模型这个技术的发展分成五个大的阶段，在1950年代到1990年代这40年左右的时间，其实整个语言模型的发展技术，都还是以这种手工设计的这种规则系统为主。现在我们看起来就是各种规则集，就是各种if else说的这个再简单直白一点。然后这个规则集就决定了我们可以造很多的规则集。然后这些规则集就基于这些专家的知识和规则来设计这种系统。那这种系统，我们很显然能想到它的一些缺点。
	它的缺点第一在于制作这个规则集的成本很高，然后制作这个规则集的质量就受限于这个专家的水平。所以在1990年代以前，NLP是一个比较冷门的一个技术。因为没有一些突破性的手段去提升它的效率和质量。我们知道后面这些深度学习机器学习起来之后，都是用算力加数据的方式去提升了我们整个算法的模型的质量。所以在那个时代，其实人工规则这种做法没有取得特别好的成果，就只有一些规则性。这些规则系统也许能做一个email的垃圾邮件拦截，这是能做出来的。因为垃圾邮件有一些典型特征，但是你让他做更多的广泛通用性的任务做不到。
	然后到1920年，1990年代到20一二年这段期间，统计机器学习非常的火。像这个混合高斯分布，CTF、SVM值向量机这些代表性的成果都是在这个时期能够快速的进入大家的世界。而且在这个时期，其实已经有一些基于统计机器学习的算法而做的这些语言模型了。就我去统计持平，就是大家简单理解这个时期概率论很重要，因为大家都在研究概率分布。
	那什么叫概率分布呢？比如说我看了100篇小说，每篇小说其实他都会描述这个人物、地点、事件。他去描述它的时候是有一些套路的。比如说我讲这个晴空万里，接下来去讲这个天气，这个是有相关关联性的。然后我去讲这个很帅的一个男主角，他可能接着就会讲他的家事，讲他的身边的朋友之类的。就是他讲一个特定的主题或者特定的事件的时候，他相关的这些词，它是符合一些特定的概率分布的。如果你看的这个书足够多，这也就是为什么大家现在去看很多这个短视频，你都知道套路了，你都知道他接下来要讲什么事情。就是因为它符合一定的概率分布，统计机器学习阶段其实就干这么个事儿。
	但是这个概率分布，我得让计算机知道什么叫帅哥，什么叫好的天气。那是需要去打各种标注数据的那这个时候虽然我们可以开始去用统计机器学习的算法，用machine learning了，但是这个标注的成本依然很高。并且它因为受限于成本，所以它的标注有一个上限，到百万级的这个级别就上不去了。所以它只能解决一些特定，像我举的例子，某些小说或者说这个新闻之类的一些非常特定的小的这种场景里面，它能做一些比较不错的行。
	那么到了2013年到2018年这个期间，其实深度学习大放异彩。我们知道计算机视觉出现了突破性的一些进展。人脸识别，各种物体的识别，even gennet的出现，算力的出现，框架的出现。我们有tenn sor flow，有py touch，所以各种各样的计算机视觉的这个领域的进展出现了。但同时我们也知道学了上节课我们知道attention的这个注意力机制。
	Transformer其实也是这个期间的成果。但是我们很多人没有关注到，transformer其实是在118年前后的这个时间其实深度学习整个的发展，使得我们可以做更多的数据标注。然后更多的数据标注能带来更好的成果。这件事儿在计算机视觉这个方向上已经验证了。所以在NLP这个方向上，大家也开始对各种各样的标注数据，这个是一个在深度学习阶段的一个进展。所以像我们上节学科学的这个encoder这种编码解码的这种网络架构，包括word vest这种word embedding就是这种词向量的这种工具和算法的生成，包括注意力机制，都使得我们开始把深度学习，深度网络这些非常好的在计算机视觉里面已经落地的网络结构，包括我们的学习的方法借鉴到了语言模型这个领域里面来。
	在这个阶段，也有一些还不错的成果，但是都没有到我们预训练模型的这个阶段来的成果这么突出，为什么呢？我们知道深度学习阶段，在上节课讲注意力机制的时候，大家都还在研究一些具体的任务。比如说机器翻译，而且这个机器翻译是特定的机器翻译。比如说英语到法语，英语到德语，这里是两个模型，还不能是一个模型。
	到预训练阶段，其实开始逐渐有一个新的学习思路出现了。我们在学bert的时候也讲过，他可以用未标注的数据来进行训练。这个时候的想象力就非常大了。因为人类历史上产生的这个文字非常多，但是你要去给他全部标注一遍的成本，那不就相当于又要产生一遍人类历史上所有的数据吗？这个是很难去实现的一个现状。所以能不能直接用他们不去打标注。
	我们在学bert这一节的时候，也给大家做了bert的这个训练方法。类似的像我们这节课要讲的这个GPT，也是直接用微标注的数据来进行预训练模型。就相当于我们把GPT，把这个bert它都是一个预训练的模型。而这个预训练的模型直接使用未标注的数据，让它的语言理解能力直接提升到了一个新的高度。
	然后这个时候我相当于就训练出了一个我可能不会做具体工作，但是我语言理解能力非常强的一个。你可以理解成有一个中学生，这个中学生他读了很多的书，但他不会干具体的。各行各业的工作，但没关系，通过翻译to少量的标注数据，我来这个试用期两个月3个月。你跟着我试用期当中的这些工作，就相当于有个师傅带你。这个师傅告诉你遇到这样的情况应该这样做，遇到那样的情况应该另一种方式来处理。
	这个就叫斐托尼。它可以用一个预训练的模型去批量应对很多不同下游领域的任务。这个范式我们在上节课也讲过，叫retraining加的范式，是在这个阶段就预训练模型为主去产生的。
	然后到今天为止，2020年至今，就ChatGPT的最核心的这个数据集，其实就基本上在2021年的时候就已经训练完成了。其实GPT3，包括GPT3.5，他们的这个训练数据没有特别大的变化。因为GPT3.5更多的是训练技巧上的变化，我们待会会讲。
	GPT3.5和GPT4，我们现在更愿意把它称之为一个叫做大语言模型。这个大语言模型的核心是什么呢？首先他们仍然都是属于预训练模型的这种技术上的范畴。但不同点在于它的模型规模参数量到了一个新的高度。这个新的高度带来了很多好处，第一个就是说涌现能力的出现，包括我在去训练这个大大语言模型的时候，我的训练技巧变多了。我可以用这种指令微调instruction tuning，包括这个提示词微调，或者说这个提示词学习，包括像ChatGPT使用的这种基于人类反馈的这种强化学习的这种训练方式。然后在这个过程当中，我变成了一个线上应用，我还在拿用户数据。这用户数据又会反过来变成基于人类反馈的强化学习的训练数据。
	所以大语言模型跟预训练模型的一个本质区别就在于，第一它用户反馈变多了。就是在预训练模型GPT3出现之后，他的语言理解能力已经很好了。但他回答你的这个质量不够高就同样是我理解你的需求了。但是会说话的人和不会说话的人，你的感觉是很不一样的。会说话的人让感让你感觉是如沐春风，不会说话的人也许他说的这个内容是正确的，但你听着就是不舒服。这就是预训联合大语言模型一个很重要的一个区别，就是我们的IHF做的这个事情。