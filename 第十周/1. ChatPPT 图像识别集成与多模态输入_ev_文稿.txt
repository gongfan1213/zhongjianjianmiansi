	好的，我们的第一分钟，我们再进来开始。
	大家进来的打个一。然后上节课的域名配置，不会的打个2，我大概了解一下这个情况，要不要再额外讲一讲，这文档其实也传到项目里了。就上节课咱们讲的域名发布，SSL证书，反向代理什么的，还有不完全没整明白的，还需要再提一提的。
	有吗？
	我看群里竟然有人在问。
	就都搞定了，对吧？不需要再再讲一下这个事儿了。就是咱们的域名发布和反向代理等等，不会的可以打一个二。我说还需要再提水这个文档的，我们在项目里面今天新提交了一个说明文档。
	我看都写的一，行，那那咱们就先开始今天的内容。如果有不明白的，尤其是上节课，我看好同学反馈，这方面就是怎么去做域名的配置等等。还不是很熟的，待会儿的QA环节可以再聊一聊。
	好，我们就先开始今天的内容。大家好，今天是我们ChatGPT倒数第二节课。然后这节课我们主要是集成多模态的模型。而多模态的模型对我们的好处是可以帮助我们的ChatGPT去理解识别图像。同时因为整个现在PPT它面向的场景是企业办公的绩效，所以这节课我们会看到整个内容会分成三部分。第一部分主要是给大家做一个概述，尤其是如果咱们听课的是没有参加过我们的这个应用开发实战营或者微调训练营的同学的话，可能这方面的基础稍微薄弱一点，我们稍我讲一讲就图像识别算法它的一个发展过程，很很快速的一个带货，然后也会提到一些参考资料，你可以去了解。
	然后为什么要提到图像识别算法？其实它整体来看是有三个大的演进的阶段的。从深度学习之前深度学习加速图像识别领域的研究，到多模态的模型开启了图像识别的一个新方向。有了这样的一个初步的理解之后，其实接下来我们就要着重动手上。就是我们怎么样能够动手去把图像识别和多模态输入的功能集成、设计开发落到我们的chat PPT的应用里。
	图像识别的模型其实现在很多也是卷的最厉害的。在去年我做第一门大模型课程的时候，其实就提过对24年的一个判断。多模态肯定是一个非常重要的方向。有这么多的多模态模型如何去选择？我们这些PPT会选择什么样的一个模型？理由是什么？选择了这个模型又如何去使用它？
	在上节课我们引入了hugin face这个公司，以及它开源的一系列hugin face ecosystem open source projects，就是它的生态系统里面的一系列的开源项目。其中我们使用到了transformers的pipeline API，pipeline API可以快速的把我们的一个模型变成一个模型服务的流水线。同样的这节课我们还是会使用transformers，不过我们不再使用pipeline n的API了，因为多模态它的任务类型太多样化了，我们可以直接去使用它的technic er和model。这个我们待会会告诉大家如何去使用，然后怎么样去把这个模型用transformers加载起来之后，我们要去如何去验证它，然后最终把它集成到chat PPT的radio的这个界面里面来。第三部分是多模态输入的一个功能。这个多模态大家可以看到从产品设计和技术方案来说都很明确，这个部分我们主要是通过跟大家交互式的演示，让大家了解它面向的场景是一个在企业办公里非常常见的一个case。
	就是你可以想象你的老板，你的同事或者你的一些朋友们，他总是会有一些需求是跟powerpoint相关的那当他需要你去帮忙做一个powerful point的时候，他能给你什么呢？大概率是给你一个文档。在0.1版本的ChatGPT里，我们就已经实现了用markdown到powerpoint这样的一个流水线。那在实际的互联网和这个工作当中，其实word文档应该是在我们叫办公室白领，或者说在数字化文档里面更常见的一种格式。而word文档其实它跟power point的天然，它的内容组织结构都不太一样。那如何能够一键把word文档变成一个powerpoint，并且保留它的图像，然后把它的图像还能够合适的嵌入到我们的这个powerpoint里面。而且大家如果自己手动做过这个事情的话，你会发现它非常的繁复，而且没有什么成就感。
	在这个过程当中还有很多的隐藏的坑，就包括这个word文档里面的这些图。其实它是这个auto scale的，它是有自动的伸缩的，或者说它是经过一些简单处理的当它原封不动的到了PPT里面，你会发现你复制过去的时候，它得你手动的去调整，去adjust这个图。它在这个PPT的某一个slide里面，它是不是合适的，然后它应该是放大还是缩小，还是应该怎么去摆布，这些事情其实都非常的没有创造性，所以这些工作我们也会在0.5，也就是今天讲的这个版本里面一次性把它做好。
	那从技术方案的角度来说，其实就是三个部分组成。第一个就是我们要支持dox这种word文档的解析，第二个就是在dodos这种文档里面，其实是有很多的图的那这些图我们要从文档里成功的解析出来并且另存为，最终在生成PPT的时候还能够加入进去。并且在加入的过程当中还能够去做到图像尺寸的自适应，这个是很重要的。不然你会发现你插进去之后张牙舞爪的，无法使用。最后就是word文档和power point里面的内容肯定不是直接能考过去的，原封不动，所以这个过程当中我们还需要用大模型去抚平这里面的一些人工操作的繁复的内容。好，最后我们也会带大家去看这条流水线，这条word到powerpoint的流水线又怎么样在chat PPT里面去完成，以及它涉及的这个关键代码是如何的。
	好，那么这个是咱们这节课的内容，接下来我们就正式开始。首先图像识别算法，什么是图像识别算法？这个应该是我们这已经是倒数第二节课了，只要熟悉我课程的同学应该都能够理解。研究一个命题，研究一个概念、一个对象。5WYH是我经常使用的这个分析方法。那么什么是图像识别算法？图像识别算法首先它是一种技术，这个技术它属于AI也属于CV计算机视觉，并且是AI和CV的核心领域，这个图像识别的算法，它本质上就是让计算机能够去自动的理解分析和识别图像内容的技术。
	说起来感觉都是废话，但你去深究它的每一个词最终是如何去实现的时候，你就会发现不是那么简单了。一个计算机从存储图像开始就非常复杂，我们且不说那些怎么样让一张你或者说这个就照相机或者摄像机传感器那头的复杂的事儿。我们单说已经变成了一个图像文件之后，这个图像要怎么被我们解析，里面有那么多不同的通道，解析之后其实都是像素点，那这些像素又如何被我们的计算机去理解，并且识别到里面的一些主体。这些主体就包括物体、人物文字，甚至像我们现在做的更细了，有人脸、有情绪，当然也包括更大尺度的场景车、交通标志等等。
	要实现这些图像识别的算法，到今天我们来看，我们基本上已经不太会谈非深度学习的算法了。因为其他的或者说我们叫图像识别算法的早期阶段的那些的方法，就是用专家的经验去噪的一些算子其实已经不再适用了，就跟语言模型出来之后，NLP的很多tricks其实也不再使用一样。那到今天为止常见的仍然还活跃在各个GPU里面在运行的CN的网络是最典型的。上一轮的AI的热潮，深度学习主要也是CNN带起来的那RCN可能搞图像识别的这个领域的同学，搞CV的同学会听过，而且听的应该还是他的改进版本，比如说fast RCN或者faster RCNN。然后优路也是一个非常有名的检测网络，而且它一直在迭代，我有关注他好像都已经迭代到V9还是V十了。
	我曾经读过他的从V1到V4版本的论文，到V5开始我就关注的不多了。因为它已经转到了一个算是红海的这个感觉里面去了，然后VIT是一个连接深度学习和transformer，因为它本质上就是一个视觉的transformer或者说语言模型，跟是CV的一个结合，也是VIT。这个论文开始出现了一些研究方向和研究者的声音，说CV和NLP能不能大一统？那到底能不能呢？现在还没有答案，但至少不管是这个VIT本身，还是后续的多模态模型，以及我们今天要集成的这个模型，其实都是有去参考VIT的架构的那图像识别算法我们有了一个大概的理解，就是它是什么？有哪些典型的算法？为什么需要它？这个简单提一下，就是为什么需要图像识别算法，其实非常简单就能想得到。
	从应用的角度来说，不管是提升生产效率还是一些公共安全的这个角度，包括除了我们想象到的去检测人以外，其实医疗领域也有大量的图像识别的算法的科技公司，包括用户体验to c的，在siri在google pixel，在国内的各种手机里面的智能相册等等。但所有的这一切，其实最底层的一个原因就是我们人类的五感所能够接收到的数据。就我们能看到的、听到的，这些有感触的数据，其实有了互联网之后都被大量的沉淀下来了。我们可以说完成了一个信息化，或者有的地方叫数字化，就是把它变成了低级头，变成了一个的文件。但是这些存下来的数据都没有被好好的使用起来。
	第一个把互联网这个规模的数据使用起来的模型是大语言模型，是GPT3。后续大家都开始卷这个事情，所以文本类的数据在人类这二十多年来积累的互联网上，基本上都已经用的差不多了。甚至可以断言，其实最头部的几家公司已经。没有什么数据可用了。
	图像数据其实也积累了很多。但是这些图像数据并没有像大语言模型，像20年的GPT31样应用尽用。因为图像它相比于文本更复杂，它不像文本有上下文关系。有一个文章标题，有这个章节、有段落，有这个context可以去分析，有这个命名实体可以去reference。
	图像是一张独立的当视频除外，视频是一个持续加这个图像的一个处理，那个是另一个维度了，但它也是多模态的一个重要能力。所以首先从这个大的尺度和长远的角度来看。这些数据放在这儿了，如果没有用，那它就是一个负债，就是一个负担。但如果我们把它用起来，其实是可以挖出很多有意思的，或者说对人类有帮助的有价值的一些模型或者技术。
	所以从这个底层的这个视角来看，也一定是需要的那聪明的人就会想到，短视频现在又成了主流，不管是某手还是某音，包括海外的tiktok等等，twitter现在也开始疯狂的推这个视频流短视频。所以视频一定会越来越多。然后我们看到生成视频的公司也越来越多。然后，这么多不同的视频数据被放到了网上之后，那一定未来多模态或者说视频生成也是一个重要的研究方向。而且它跟图像之间的关联会更紧密一些。
	这个是从一个更宏观和底层的角度，我们其实是非常需要图像识别的算法的。它并不是一个简单的人脸识别，或者说打卡，或者说公安在这个公共场所里面去抓嫌疑人这样的一个尺度。所以从刚刚的分析来看，谁在用呢？其实所有的人都在用图像识别算法，几乎不管是企业还是医疗机构，这些垂类的医疗机构，公共安全部门，制造业等等都在用。
	那什么时候在用呢？我们其实对应过来，刚刚我们举的这四个例子贯穿了三个W，就是我们的这个y who和问，但他的这四个例子其实是很有代表性的，我这就不再赘述了。那么在哪儿使用也很显然，有的是在公共场所，有的是极其的数据隐私的一种场景。比如说医院，它是跟病例或者说跟病患签过协议。然后这个病例里面的各种影像数据，其实理论上都只有这个病患可以使用。然后如果你你要拿去做这个训练学习，你是需要去拿到特殊的授权，获取到一些脱敏的数据，这个基本上都需要卫健委及其他的下属单位才能够给你授权。
	我其实在14年，也就是十年前的时候，曾经处理过咱们国内脱敏过的上亿条的医疗电自病例，就EHR。然后也是因为我们公司的一些资质什么可以做这个事情。那这个公司现在也在为这个分诊。就是你可以想象有一个病人得了病，但他不知道自己是应该去什么科室，去三甲还是三乙或者二甲医院。AI的这个系统，其实就能够帮他去判断你大概可能得了什么样的疾病，去什么样的科室就好了，去缓解一些医疗资源紧张的问题。
	这些都是非常垂类的数据，隐私的数据，其他的领域也有这样的一些情况。智能设备更多的是面向to c这个领域。而工业场所这种产线上面的自动检测，我曾经也做过上汽，包括一些国内的在这各个工业场地里面的一些检测，这个就会更垂泪。
	在三四年前，五年前，1819年的时候，国内流行一个词叫产业互联网，在产业互联网里面就有大量这样的数据，有的是可能就是这种持续的，你去检测国家电网线上设备的各种维修情况，它像半衰期一样的，什么时候采购的那一批什么类型的，这个我们叫零配件或者产品，它大概可能出问题了。类似的图像有这样的一些需求，所以其实是无处不在的一个的需求和对应的算法。那怎么去做呢？其实我们有了上节课的铺垫，知道ASR怎么去做之后，我们再来看图像，其实是真的非常像一套标准的深度学习的做法，预处理这个预处理我们在ASR里面大家如果还有印象的话，在ASR里面我们是怎么进行预处理的。然后在CN在图像这个数据里面，其实要解决的很多问题是质量问题。就这个图像怎么样能够去去照，然后裁剪到一个统一的尺寸，因为搞过CN的同学都知道这里的门道有多深。你的输入是像素，像素如果分辨率都不一样，我们这个通俗讲叫分辨率你要是往往神经网络的输入端的网络结构来看，我们就知道其实输入的像素也是一个矩阵。而这个神经网络它不会变的，是你定下来的一个网络，要去对它进行训练的时候，它是不会变的那它的尺寸是固定的，所以你需要把你的图像统一的，你可以叫预处理成这个神经网络能够接受的一个输入的尺寸。
	比如说最经典的这个CNN里面的hello work，m list手写体数字的识别。那他可能就得处理成28乘28的那在七八年前的时候，可能大家的网络都还是256乘256的。后来能做到这个512，后来做到这个1080等等。
	更高的分辨率，你可以这样去简单的理解，所以图像的质量本身也是有要求的。如果你要训练一个对高分辨率去进行很好识别的网络，但你的训练数据都是分辨率很差的。其实它也很难做好，但里面有很多预处理的处理手段，包括神经网络的一些结构去做一些弥补，那不管无论怎么样去这个设定，或者说你的网络要怎么样去适应，但第一步一定是预处理，把你的图像数据变成一个你的模型可以接受的，你可以叫样式或者shape。然后第二步才是特征提取，这个跟SR是完全一致的，我们所有的深度学习的方法，基本上特征提取都是你可以认为是自动进行的。它跟上一代的机器学习也是一个最大的差异点。不需要人去手动的去设计各种各样的特征提取器。或者说在CN这个领域，上一代可能就是各种各样的算子，什么server算子等等，有的算子是专门检测边缘的，有的算子是检测特定角度的。但有了深度学习之后，这个事儿就由模型自己去训练，他训练出一些特征提取器，在CN里面就是一些我们叫卷积卷基层。
	有了这些特征提取的工作之后，逐步它能够开始学习识别到图像当中的一些特定的模式，边缘纹理都是一些典型的，而且它是偏底层的一些特定的特征。它的CN的这个层次不断加深之后，数据量变大之后，它其实还能学出一些更高维度的一些特征，就包括一些比如说特定类型的动物，鸟、猫，它们完全是不一样的。但是为什么能够知道它是鸟是鸟还是猫这种不同的这种生物类别呢？其实主要在第三阶段，因为所有的CNN的图像识别算法几乎都是监督学习的算法。就是我有一张图，假设我们用最简单的任务就是我有一张图，那我有一个标签，那这个标签可能就是这个图里是一张猫。假设这个图里只有一个主体，这张图里是一张猫，那张图里是一只小鸟，或者两只小鸟，然后另一张图里面可能是一只金毛，那么这些不同的图都带上了这个标签。然后我们的训练过程其实就是把我们的这个标签和图两两匹配，形成一个pair。然后让模型不断的去认知到所有的鸟，可以把所有的鸟的图都拎出来，就是长这样的那他就会去找这些鸟的共性，这个是通常通过全连接层以及最终的soft max多分类，去把这个分类和识别的任务给完成。
	其实检测和这个分割也是按照这套流程来处理，只不过它的三阶段会有不同，后处理会有不同。最终的后处理其实就是为了人更好的去看到和理解它，因为模型输出的是一堆这个人看不懂的向量以及它的logic。我们为了让这个视觉上一眼能看出来模型的效果如何，而不是只是算一个准确率或者某个matrix某个指标。所以他们会把这个图像里面的这个图的像素点和最终你能够获取到的一些，比如说bounding box像分割的话，可能就是一个一个点的集合，圈出来我的这个主体到底在哪。然后通过这种方式在原图上标注出结果类别或者一些特定的标签，让你看到我的算法做的还不错。这个其实就是图像识别算法的这个5WYH，希望大家听完这个之后有了一个粗浅的了解，便于你去进一步搜索。
	深度学习其实对于图像识别领域的研究是非常重要的。不客气的说，如果没有深度学习，其实图像识别还在一个非常迷茫，找不到未来的状态。为什么敢这样断言呢？其实很简单，我们可以看到2012年这个alex net这篇就是这样的一篇论文及其它的实现它的网络参加了2012年的image net挑战赛，然后这张图里展现的是top 5 error的一个情况。Top 5 err的意思就是我给你一个图片。你返回我它可能是什么？比如说返回的第一个top one是70%的概率这是一只猫，10%的概率它是一只老虎，5%概率它是一只小狗。一直这样把你的这个top 5的结果，只要你的top 5的结果里面有正确答案，那就不算错。所以这个是top 5 ever。如果你的top 5里都没有正确答案，那你就错了。
	所以我们可以看到今天来看，感觉这怎么会做不好，对吧？就是我怎么会有一个模型，连这样的一张图里是什么都搞不定，都猜不出来。但其实2012年12年前，这是一个世界难题，是没有人能够解决到的。你可以看到AX nt出来之前，这里的这个柱子，其实是当时的这个sota模型。就当时的用机器学习的方法，包括用各种各样的专家造的这个算子，去做特征提取等等。比alex net要差了10%。
	这个10%不是一个小数目，尤其是你叠加到top 5 error，并且又了解alex ne t的创作背景的话，它是一个非常简单的网络。后面它的发展速度也就是一二年之后，不到五年的时间。因为今天的挑战赛里的这些任务，AI就已经超过人了，就是人已经没有AI能够做得好了。在分类检测和分割相关的一些任务上，这是一个非常夸张的事情。所以大家如果了解到的话，图像识别本身做的就很早了。
	那他怎么做的呢？其实这里很关键的是CNN，CNN算是图像识别或者说深度学习在图像识别领域里面应用的一个最主要的网络结构。然后这个训练过程其实跟所有的神经网络几乎都是一样的，有反向传播来优化这个模型的参数，最小化预测误差。是因为它是一种监督学习的训练方法，应用领域也非常多，然后这个其实要额外提一句，就是关于深度学习，包括CNN各种各样的CV方向的一些网络。
	其实在应该在19年20年的时候的那个视频课里，我不确定这个视频课咱们的同学会不会有好像是有这个权限去访问的。但我不确定这个事情，大家可以去问一下自己的报名的老师或者班主任。这个特色flow 2项目进阶实战的这个视频课里，其实讲了蛮多这个小节里会提到的一些网络。这里我就简单截图，包括我们看到的internet，retinal t这都是像事实标准一样的典型的骨干网络。
	Rez NET这个名气太大了，应该是当下来说，在所有的CV领域里面你都绕不开的一个网络。Retina net它可能会有一些替代品，因为它解决的是检测问题，检测问题就是一张图里面可能有多个物体需要被检测出来，就不再是说一张照片里面就一只猫，可能一张照片里面有两只猫、三只狗，还有一只小鸟，你都需要找出来它们的位置在哪，它们的类别是什么。所以你可以想象它比识别要难一些。
	检测网络也有很多的典型模型，包括RCNN以及它的各种模型，优low各种它的演变retinal net，还有很多其他的这方面的都在这个课程里面会有讲。当时我给大家举的这个例子，也是我开源的一个代码和我们贡献给一个会议的一个开源的数据集。在上面去做货架商品的识别，那个会比识别一个猫一个狗要难得多。因为它一整个货架里面充满了各种各样的商品，我们要把它识别正确，位置要识别准确。我们这简单提一提，如果没有这方面基础的。
	大家可以看一下下面的网络结构这个图，这是一个典型的一个CNN的一个网络结构。然后CNN里面最重要的两种我们叫layer也好，叫单元也好，或者说网络结构也好，其实就是卷积和池化，那么卷积核，这个黄色的这个矩阵，我不知道这个叫据黄色的方块，这个黄色的方块它就是卷积层。这个卷积层它为什么是三维的？是因为这个卷积核本身也可以有多个通道，然后它的核心特点就是对我们的图像去进行特征的提取，然后不同的层次从左边开始，这是它的输入图像，我们通常会把这个地方叫做比较低的层，那越到最终的结果是越越叫比较高的层，就是top player。越低的层它抽象出来的特征或者说提取出来的特征越简单，越往这个top player这边，它抽象出来的特征越高维度，所以你也能看到这个网络的size，每一层的size在逐渐的减小，因为你可以想象，就是你一开始需要就像这个公司的一号位，对吧？他只管几个人。然后比如说他下面的人每个人要管十几个人，20个人，下面的人可能每个人有管几十个人，知道一个人可以管好多个人，比如说管1000个人。
	可能这个卡尔倒过来看，就像是最终的这个结果，也可以推导出任何的一你只要给一张图，然后这个网络是健康的。通过这个过程他就能够把特征提出来，最终通过训练找到我应该是一个什么样的类别。池化层的核心的作用其实就是下采样，去减少这个数据的维度。就我们刚刚提到的这个过程里面，大家想象在公司里面总有一些去减少这个沟通成本，然后去捋清楚这两个事儿做重了没必要一起做做这样的一个事情。然后应用场景其实前面也讲过了，这重点提到了在internet比赛当中，早期的这个alex net，也是图灵奖hinton的学生，alex和他的就这篇论文的二作伊利亚，和这个通讯作者hinton做的第一篇非常有影响力的论文。那VGG是牛津大学，当时应该是在14年还是14年还是13年底，我不确定，做的这个连续的convolution的一个贡献。就在这个经典的早期的CNN架构里面，是没有convolution，就是我们的卷积层之后再接卷积层的那这件事儿是由VGG开始的，包括后面的全卷积，也都是类似的一些思路的扩展。
	有了CDN之后，我们看到CNN它不太能解决检测的问题。所谓的检测的问题，就是我还要去锁定它的位置，有个候选区域。虽然是说整张图我能做分类网络来给你个类别，那RCN相比它它会去你可以理解成它可以在原有的整个输入image给出一个标签的基础上，还能够去预测我现在的主体它应该是什么样的一个bounding box。这个bounding box有两个坐标点就能够唯一的锁定。所以它的流程相比于CNN还需要先生成一个候选区域，然后这个候选区域再进行特征提取，再进行分类。
	所以你想你想象一下这个过程就是其实复杂度变非常高了。原来是整个输入给到CN网络再拿到一个分类结果，现在是整张输入。你要从你select一个子集，这个子集就是我们叫raft region，就是他的待他的这个候选区域要待检测的区域，这个区域再丢给我们的这个CNNN去拿到一个结果。我不知这个描述清不清楚。
	在这个过程当中，你可以想象复杂度非常高，就哪怕你是一个256乘256的像素，这个像素的位图，然后它还是多通道的。你可以筛选出来的候选框数量是一个天文数字，然后这些天文数字的候选框都要丢给模型，所以这是非常多的低效的部分，所以才会有后面的fast RCN和fast RCN他们的贡献点也各不一样了。像fast RCN主要是能够去共享一些计算特征，算重了的地方别算了。那fast RCN是提出了一个叫区域体育网络的一个方法，这儿我们就不展开了。它的核心概念它是什么？它怎么做的？它有什么样的缺陷，它有哪些改进版本，告诉大家，大家有兴趣可以去看一看。
	那优乐是什么呢？优先其实相比于刚刚的RCN来说，它day one第一天要做的事情就是快就是有一句这个小说里面的话就是那个天下武功唯快不破。是这样的，然后优乐其实就是想干这么一个事儿，它的第一代版本其实就主打了这一点，到后续其实一尤其是这个优乐V3，应该是在当年在2019年是具有统治力地位的。
	一些比较典型的现象就是在国内的一些嵌入式设备，轻量级设备。那会儿可能我记得invidia刚刚推出这个Jackson，就是它的边缘侧的这个设备。然后英特尔刚刚把这个芯片公司收购了，然后才开源open wino。所有这些NPO的这些制造商们都想要把这个视觉领域的这块蛋糕也吃到。
	视觉领域只做分类是不行的，至少也需要一个检测。你没有分割的话，像自动驾驶都已经是分割了。那那你需要算力的支持，但是你又不可能说装这么多个GPU的服务器到所有的应用场景里去，所以才会去燥NPU。而U它的好处就是又快，然后消耗的算力又少。需要的这个资源，比如说你要显存或者你要内存都还是很少的。
	它的实现方式相对来说就复杂一点了，这里面用到了各种各样的tricks，这儿我不再展开，大家对它有兴趣可以去跟我讲讲。但他这些技术，就是优乐这套技术，跟我们后面要讲的多模态和transformer又没有太直接的关联。但它确实是值得一提的，那就开始跟我们的这个主旋律有点接上了，就是视觉的transformer，vision transformer它是一个什么样的模型，我们从这个名字就能看出来，它跟transformer一定有点关系。具体的关系我们可以待会儿下一页是一张架构图，我们先说它的特点是什么，就是它为什么被提出来之后被大家关注，还会有人这么多做跟进的研究，并且投入了实实在在的资源去实践它、验证它、改进它。
	首先它有一个最大的特点就是图像的切块，它会把一个图像切成固定大小的非重叠的小块。从这个描述上来看，大家在结合优乐的这个图，你就会发现这完全就是两条路。但这个优乐也会切一些块，但它切的不像这个VIT这样，第一就VIT的固定大小的种类数类别数没有那么多，而yn o是可以适应非常多任意尺寸的大小的，所以优乐对优乐V3当年的小物体检测是非常牛的。你想象一个路边的摄像头很远，一辆车过来他都能数清楚这里到底几辆车。这就是他能够把一个比如说这个512乘512的像素里面，哪怕是4乘4的这样的一个小的像素块，它都能检测到。但我们看VIT它就做不到这一点，就比如说512乘512的，他可能16乘16，那16乘16成为了我的一个固定大小的输入块。然后如果这个16乘16的这个块里面还有好多个不同的元素，然后我本身对这个东西的数据足够多，那是可以检测好的。但如果你本身没有这种小小级别的这些标注，并且你的重点也不是干这些事情的话，那他可能就做不好了。
	尤其还会涉及到一些点，就是它是非重叠的。而yn o是滑就是滑动窗口的。所以当你的小块儿把一个你要检测的物体切成两半，甚至切成三半的时候，那这个时候会比较麻烦。这个VIT这一块，其实但它的优势就是计算量显著的降低了。
	然后小块最终你可以想象把一个一个二维的平面图切块，像你切蛋糕一样，切块之后再去做embedded ing编码，然后编成了一个序列。那这个就有点变成了transformer的输入的这种形式了。所以你就可以想象真正意义上实现了看图说话。就像我现在这张课件的背景有这个虚线的格子，我把这张图按照格子一个格子是一个编码，一个向量，然后把它拼成了，就像一段文字一样。然后再把它丢给transformer去直接进行处理。理论上来说就很诡异，对吧？就是我这个图你就丢给transformer去处理了，transformer处理的明明是文本，因为但是很神奇的是他跑通了，他这个事儿是能运行的，是能够很好的去在一些经典的数据集上达到一些很好的效果的。而且最令人感觉到神奇的是啊，我们开始提到了深度学习，去加速我们的图像识别。很重要的一点是卷积和池化。但我们都知道现在跳到下一页，我们的transformer的网络里面是没有卷积和池化的，哪怕是只学了这节课的这门课的同学，我们的预制的章节里也讲过transformer可以忘掉了。
	回头看一看，在former的结构里面只有多头注意力机制，Normalization这个就不用说，这本来就是做补光性，做数据的处理的那mlp其实本质上是全连接层，所以整个交通方面的结构里它没有卷积和池化，没有卷积和池化之后怎么办？我们理论上CN大家了解到的话就是它的位置无关性，空间无关性等等，这些特征其实都是由卷积和池化去提供的那在transformer怎么去弥补这个缺失，主要就是通过位置编码，在相当于给每一个小方块编上一个位置。那这个位置有了编码之后，我就能通过这个编码去算出来不同两个方块之间它是一个什么样的相对关系，是这样的一个逻辑。显然这个位置编码不仅在VIT里被应用了，在后续的所有的大语言模型里其实都用了。因为我们需要把我们输入的文字或者图像块，就所有的这些输入向量，只要它是有关联的，它的位置信息就是一个有效的信息。而位置编码能够把这个信息用起来。它相比于原始的这个transformer结构里的绝对位置编码会好很多。然后多头的这个自助注意力机制，这个就不用说了，这个本来就有。
	而且我们在第一节课注意力机制里就讲过了。注意力机制不仅能够去解决这个自然语言里面的一些问题，它在图像理解包括可解释性方面本身就有一些它的独特的价值，只不过一直没有被发扬出来，在VIT里终于得到了一个展现。但它还是有一个最大的缺点是什么？就是它的训练消耗的数据非常大，这个我就不展开了。那看它在高分辨率的图像上表现很好啊，这个跟它的图像切换和他的具体的训练过程是有很大的关系的那VIT的架构相比于一个标准的transformer的架构，其实主要我们看这幅图就能看得出来一个完整。也有点像朋友圈那个感觉，就现在他发朋友圈，我看很多人一张照片，他非要拆成九个，然后把这九个照片发到朋友圈里凑成一个九宫格。然后这个九宫格刚好它会有这个微信的有一个很窄的一个线去做分割。这个动作其实跟VIT的训练过程非常像，就是把一张完整的图给拆成了固定大小的方块，并且是毫不重叠的，然后把它们编码。我们这看到每一个小方块都有一个唯一的embedding，然后把它们变成遗传，就像我们输入给transformer的一句话一样，丢到transformer encoder里面，这个mlp head就是全连接层，就是这里的MLP。当然这个没有展开，就是我们的transformer encoder展开之后是右边这个形式。
	大家如果去细看，就是我鼠标在这儿，然后最后这个transformer外面还有一个mlp这个是干嘛的呢？这个其实跟encoder里面的mlp是不搭嘎的，是不相关的。这个mlp它更更应该等价于我们在。
	在这幅图，在经典的CNN网络里面的这个mlp，它是把VIT当做一个特征提取的一个前置的layer了。你可以想象成VIT替代了这里的卷积加池化，然后抽取出来了最终的输出的高维特征给到全连接层。全年阶层最终再去套不同的下游任务，是分类、是分割、是检测，然后拿到一个对应的输出。我们的VIT也同样是如此，最后接了一个MLPK，然后类别这儿如果是做图像识别，那它最终输出的就是一个多类别的标就多类别标签的一个概率，就不同的这个class不同的类别有一个logical，我们前面说的top 5 error就是logic。最大的前五个都错了，那他top whatever就不行。现在我们都知道这个top wharever已经没啥好比的，大家都快接近满分了，可能比的就是top one ero。
	这个是VIT transformer跟视觉任务的一个结合，极其的好用。我们刚刚提到的VIT这个transformer，其实本质上他们也都是深度神经网络，是深度学习。所以我们今天回看来就是这个往回来看的话，深度学习确实是能够下这样的一个断言，极大的推动了图像识别领域的发展。它主要带来的价值，一个就是我刚刚在反复提到的自动特征的提取，然后高效的训练，因为你能够自动特提取特征，你不需要请一堆老专们去给你像中医一样开药方，对吧？他不需要这些人去做特征工程了，直接从数据当中去学，所有的这个表示学习都有这样的特点，第二个就是它的非线性映射和这个多层网络。这里主要就是提到了internet，我们知道raz NET最大的一个贡献点就是残差网络，可以跳层连接，有这个短连接提升表达了这个模型的或者说提升了模型的表达能力，让它在一些更复杂的数据集或者数据分布上仍然能够学习到它的特征。
	第三个就是它的适用性非常好。就深度神经网络在计算机视觉这个领域上面的推动。不仅是说我可以做图像识别，我的物体识别、语义分割检测，包括其他的一些任务都可以做。因为它的核心特点就是所有的CV任务最终都是一个二阶段任务。当然也有一些端到端的训练方法，但绝大部分都是二阶段的一个任务，特征提取加下游任务的一个监督学习。
	那特征提取，深度学习的这个老本行非常好。下游的监督学习，我这些不同的适用性的这些任务。我只要有自己的监督学习的数据集，然后它的数据集质量还不错，我都可以快速的去做。这个叫泛化也好，叫扩展也好，所以它适用性非常好。然后它的应用领域当然也就扩展开了，因为你可以处理各种各样的数据。
	有了这样的一个介绍，我相信大家对这个图像识别以及对于深度学习在图像识别这个领域里面的一些关键的贡献和角色做了一个比较好的了解了。那接下来，我们再来看一下多模态的模型，怎么样叫做开启图像识别的新方向，这多模态他又做了什么？感觉图像识别好像就卷到头了，我们的renel、UNO都不错，为什么还需要多模态的模型？
	这儿就要提到多模态模型，我特指他去处理图像的时候，我们今天探讨这个任务，那多模态模型在处理图像的时候有哪些优势？首先多模态的模态这个意思更多的我们落到不去深究字眼的话，其实就是它能一个模型处理多种类型的数据，图像、文本、语音。他在处理图像的时候，比传统的图像识别方法，包括深度学习的方法都有一个额外的优势。就是哪怕是深度学习的这些方法，哪怕是VIT，他仍然不能处理文本。
	的简单一点就是前面这些所有的模型它的输入都是一个图一个像素点的集合。但是多模态的模型是可以做到你给我一张图，你再给我一句话，或者你给我一个文本，然后我能够把这两个结合起来，更全面的来理解这个图像的内容。我不知道这个描述大家能不能get到，那我们待会儿去看这个实践的样式就知道了。所以第一个点就是它融合了两个模态，两个维度的信息，既有图像本身的这些像素点，这个你有的我都有啊。第二点就是我还能够去把一些图像的，你叫原数据也好，叫图像的描述也好，作为我的有效的输入信息，然后我联合着来进行处理。
	这个里面的代表就是clip，这个language image repo training，这个对比学习的方法来进行特征对齐，所有的这些所谓现在今年24年的叫法就叫非原生多态的模型，几乎都是用clip这种类似的思路来进行跨模态的表示学习。这里提到的表示学习，因为本质上就是经过刚刚的前面的铺垫，我相信大家已经理解了二阶段的流程。那么图像第一阶段变成了一堆特征，所谓的特征就是一堆高维的向量。文本我们已经学过大语言模型了，就至少有微调训练营基础的，应该非常能理解我的这个意思。整个这个过程我们前置课程就是前面这个章节，有一个预制的课程，讲这个bird GPT怎么一代一代过来的，应该也有一些影子。
	就是所有的文本最终也是通过语言模型变成一个一个的它的我们叫高维的向量空间当中的这些特征。这两个特征它原本是完全不我们的两个特征空间，需要把它们能够对齐。这俩特征同样的一个苹果，它到底是一张就我说苹果这个词的时候，它到底是我的这台macbook，还是那个iphone，还是那个ipad，还是红富士苹果，取决于你给我一张什么样的图像。
	当你说帮我解释一下这个图里的苹果的时候，我的文本是没有变的，但是图变了，那出来的结果肯定就变了。假设现在图没有变，但是你的文本变了，那他可能能够理解的点也不一样。你只给一张图，你说解释一下我怎么知道你要什么。但你给了一张图，它是一个macbook。你可以说对比一下它跟别的什么区别，或者告诉我它的一些什么有效信息等等。甚至除了文本以外，这个图像本身的一些原数据也都能够被用起来。
	这是它最大的一个优势，就是融合了视觉和语言的信息第二个就是你衍生出来的新的能力了，包括这个多模态的问答和生成任务，这个不是每一个多模态的模型都有的是要去专门做训练，这里的所有的能力都是额外去做训练所获得的。所以大家不要认为谁天然就有一个大模型。甚至很多公司的大模型是把别人公司或者开源的两个模型或者三个模型合一起去训练出来一个新的。然后图像生成和编辑能力，这个也都大家平常去体会过的了。
	这里往更进一步延伸，就是前面也提过的视频视频的理解和时序分析。在多模态里面，其实现在越来越多的模型的开发者和机构转到了视频了，就怎么样让我的多模态的模型去理解视频，这个视频的理解其实比图像的理解要复杂很多。下面这个图来自于这个flamingo一个开源模型的架构的一个总览overview。
	那在这个视频的理解里面，其实更多的是你可以想象什么是视频。视频其实是一堆的图堆的图像。这一堆的图不是乱序的，它是有序的，是按什么序呢？按时序在分布的。所以在传统的这个视频处理，大家去想象一下这个技术的演进，我们最早讲图像识别，最早的图像识别就是一些基于规则，然后各种各样的一些人的设计巧思去做出来的一些处理方法。后来才有了自动学习的方法，learning的方法，机器学习、深度学习，包括这个VIT。我们的视频其实也是一样的，最早期的处理方法有一个关键词就叫关键帧。
	大家知道视频是按帧数来算的，就比如说你是24帧的，是30帧的，你是60帧的，就每秒有多少帧图像，就一秒有多少张图。你可以想象有一个画面就是这个叫什么呢？那你想象你在你家里放了一个监控摄像头，然后你家里没有小猫，什么都没有。那这一天可能这个视频摄像头拍的这些内容，它是没啥变化的。也许会有日光的变化，光照的变化，但那个也是缓慢的。所以所有的视频摄像头的厂家其实都会做这样的优化，就是让你的这个存储的视频肯定不是真实录的那么长，而且视频压缩算法各种也都能做。
	因为你没变化，所以我不需要完整的存下来那么多张图片，我只是需要存下它的变化就好了，这个是非常重要的一种思路，最终像罗拉这样的这个微调方法，其本质也是干这个事儿。我们没有必要记录所有的这个with在我的训练过程当中，因为最终变化的可能只有1‱，那我就只需要那1‱的德耳塔，1‱的德耳塔用更小的两个矩阵去。表达就可以了。
	那时序也是一样的。从最开始的我们的这个时序处理视频的这个关键帧到后面因为关键帧是基于像素的，就是我们开始聊的这些所有的视频处理关键帧的方法都是基于像素的。就这个像素里面的这个所谓的像素的就是这个图到底有没有变化。但有了我们前面提到的图像识别理解的能力之后，其实它可以在更高层次的就不是像素这个级别了，而是更高层次的特征上去完成。
	这样的叫关键事件也好，关键的对象和动作也好，去做这样的分析，我不知道这个描述大家能不能get到，因为你可以去理解那个图里面发生了什么了，所以你可以在图像的语义层面上去做打引号的关键帧。好，那么我们了解了从图像识别是什么，为什么，谁在用？怎么用到深度学习加速了图像识别这个领域，一直到多模态。在这个图像识别的任务上又开启了一个新的研究方向。并且它有一些天然的优势是单纯使用深度神经网络所不具备的那问题就来了，哪个多模态模型好用，哪个多模态模型我能用？
	有很多的榜单，现在这个大模型这个赛道这么热，做榜单肯定是也是一个很热门的生意。这儿我们就挑了一个，国内很多公司，他可能都会去在意的一个榜。而且这个榜单也有一定公信力，叫open campus。
	它的特点就是更新的很及时。我们看到阿里最近更新的千问2VL72B的这个模型，是10月28号才刚更新的。然后它的参数规模是730亿734亿。它的整个语言模型是用的千万2 72B视觉模型是用的VIT。也就是我们前面讲过的这个VIT的一个6亿的版本，然后平均分average score 74.8这里就提到了各种各样的benchmark，这些benchmark相对于我们标准的语言模型的benchmark，可能大家就比较陌生了。但列在这里，对这个事儿想要深入研究的同学可以去进一步的探索。
	除了千问2以外，我们看到第三名的是8月6号的GT4o，GPT4O确实它的图像识别很久没有迭代了。然后再往下我们看到这个interval 2的纳马3，英特尔VIT跟纳马三的一个结合再往上有lava，这也是去年在多模态或者说图像，或者说多模态的图像识别这件事情上非常有名的一个模型，nova的variation 72B和下面的这个72BSI2个版本那么。字节开源的这个nova我们看到其实今年应该还有更多的一些迭代，而且不只是这个nova了这些。所以大家看这个榜，首先要去了解更新的屏幕频繁，第二就是了解哪些公司还真的在干这些事儿，这个榜单绝大部分都会去快速更新的那这些模型去哪儿获取？
	上节课我们讲过了，还跟face，我们在hunger face里面搞models模型这个主页面里面我们可以看到左边有一个筛选框，然后最顶层的边就是多模态march moda我们选image text to text，然后在右边它默认的是按趋势榜来进行排序的。在这儿我们把它重新选成按照最多下载次数，most downloads看到了这个千问2VL72b instruct，这个版本下载了两百多万次它还有一个小尺寸的70亿的版本千问2VL7b instruct，这个也下载了八十多万次，小小100万了。这些其实就是按照下载次数的一个排序，大家应该就大概有数了。结合榜单去看，哪些模型还不错，并且还在持续迭代。而不是说好几个月前我发了100取得了一个不错的分数，我就再没迭代了。
	第二个就是去hugging face上面去找这些模型的权重，最终要用的也是这些权重。这个就是千万二的这个VVL72B在hugin face上按下载次数的第一名。这个模型它自己也提到了，它是一个sota级别的图像理解的能力。然后可以理解长视频，20分钟以上的长视频等等，然后也支持多语言历史下的数量在右边。然后我们看到这个千万二的这个小尺寸，7B的这个多模态模型，还有图像视觉，还有其他的一些奔驰mark。
	在这里面我们会重点发现一个，第一我们看到了in turn VL28B，这个前面也提到了，然后有他自己前文二的VL7B，还有一个叫GPT4o mini，这是我们经常用的。第二个这个叫mini CPMV2.6，这个模型在视频理解里面，你看它对比的，首先也有这个intern VL，也有这个字节的nova，也有这个mini CPMV2.6，同样出现了这个量子硬件，分数也都还不错。那这个mini CPMV2.6到底是个什么模型呢？就我们这儿红框框的这个模型，其实我们不用卖太多关子，就是我们接下来要用的要在我们的项目里去集成的一个模型，它也支持中文，是国内的这个公司开发的一个多模态的模型。这儿我们就准备进入下一个主题，就是我们的把刚刚提到的多模态模型集成到现在PPT里面，要集成的就是我们刚刚预告的mini CPM。
	好，那到这儿为止，看看大家有没有什么问题，我们可以简单提两个问题。我刚刚休息一下好吗？大家有什么问题吗？就关于前面那一趴。
	我再去拿瓶水。
	我看到有个同学提问，为什么选择mini CPMV2.6千？问72B太大了，我不知道这个同学自己跑没跑过700亿的模型。首先这个问题是这样的，我们不知道每个同学的算力情况，所以我们整个三门课程应用开发的实战是没有任何算力要求的。你直接用所有的这个大模型相关的要求，都可以通过API的方式。不管你是调OpenAI的，是调其他家的模型的API服务都可以完成。微调训练营我们是有算力的，最低要求的是为了让我们的实验作业能够跑起来，让你能够通过实验去学习微调，包括指令微调等等各种各样的技术。
	当时我们提的最低的硬件要求其实就是T4，就是英伟达的特斯拉T4这种非常老的卡了。但它性价比很高，你可以在各种平台上面租到廉价的T4，包括我们也提供了华为云的7折的折扣这个应该用过券的同学都知道，还是蛮香的。一般一般可能只有消费到一定数量的数额的企业才会拿到7折的华为云的券。因此我们就一直按照这个最低要求，像咱们的这个叫做企业级AD的这个课程，我们也是按照这个资源来提的要求，就是T46GB的显存作为我们的最低的资源要求。所以我们选择的所有模型都得在这种卡上面能够跑起来，那么72B显然不行，可能A100才能跑起来，然后7B8B是能跑起来的。而且如果你去细研究一下，你会发现也只能跑起来量化的版本，你直接去跑一个非量化的版本，你也是跑不起来的。图像会比文本的输入消耗的显存更大。我不知道有没有回答这个同学的问题。那还有什么问题吗？我们可以再再回答一个。
	好，没有的话我们就先往后进行。好，接下来我们来看看如何把这个多模态的模型集成到ChatGPT里去完成图像识别的任务和功能。首先这个mini CPM的V2.6版本也在high face上面能够很方便的找到。我们看到它上个月的下载数量有13万次。应该是我没记错，是面壁智能这家公司开发的，他也写的很清楚，这个模型是由两个基础模型去训练出来的，分别是sag LIP4亿的模型和千万27B的模型。
	所以再回答一下前面的同学的问题，就是mini CPM本质上和千问系列的模态模型，他们用到都是千问的。你可以认为他们共同使用了千问二的这个基座模型。只不过他们使用了不同的视觉模型，然后用到了不同的训练的数据和技巧，我不知道大家能不能理解他的这个概念，结合前面这些前面这个小节的介绍，多模态是怎么来的？然后他自己的这个V2.6，其实是性价比非常高的一个模型了。
	怎么看？我们前面介绍了open campus，但其实open campus这样的多模态模型的榜并不只有他一个，open campus会有一个分数。其实最早这个图应该是在讲deep set的时候就介绍过，应该是在微调训练营讲过deep set介绍过这样的一个多模态的图包括像这个GLM，它的多模态模型cog VU是叫做也有画过这个雷达图。在这样的一个图里，其实能看得更清晰。这个图里既有图像的，也有视觉的，还有文本理解的能力，包括OCR的。不同颜色的雷达图的覆盖范围，表示不同的模型。
	这里进入这个比拼赛场的有GT4V，四月份的版本，基本的1.5 pro，然后interview two，mini CPMV2.6。那么我们看到最外围的这个紫色的区域，其实mini CPM这个模型还是有蛮大的优势的。而且即使在我们刚刚看的那个open campus的榜单里面，他也在13位还是十几位的一个位置。他最近更新的比较少了，那依然还是好用的。然后各个榜单的一个综合的评分，就是把刚刚咱们那个图里的一个结果倒过来。大家可以看到更多的模型，有商业化的议员的，也有开源的。
	还有一个能力，我们可能暂时还用不到的，叫做多图理解能力。它是一个非常重要的多模态的理解能力。像刚刚提到的质谱，它应该叫cop VLM。然后mini CPMV2.6在这个多图理解能力上面也仍然是取得了非常好的成绩。
	除了MIRB以外，基本上都是在这个榜单里面的sota一个简单的示例，就是用这个模型能干什么，就我们让我们的模型能理解图了。一个最实际的例子其实就是大家可能不一定会想到他能干这事儿。我们看到左边有一个这个图是拍了一个自行车，这个自行车拍完之后，他提了一个问题，叫做help me know my back set。我自行车坐垫太高了，能不能帮我调低一点？那下面的这个mini CPM就跟他提到需要有五个步骤。Locate the set post clam on the underside of the set。You should see a claim that hosts the set post in place. 
	他跟你说，你首先要找到关键的地方，然后把它放松一点。这个release the club就是这个像松紧一样的东西，然后把它松开，松开之后就可以把你的坐垫放下去了。Gentle你lower the set unit to reach the desired hat，就是你把它松开之后，你再把这个垫坐垫放下去，到合适的位置就差不多了，然后再secure the set，然后test set。其实你感觉这个case好像就很傻，怎么会有人问这种问题，怎么会有人不知道怎么调这个座椅？但它只是一个信息的，你可以理解是一个信息的错配，可能我们从小知道这玩意儿怎么用，但讲道理其实很多人是不知道的。就像我们看一些欧美的人，他们做很多事情很很简单，我们却觉得很难一样。
	类似的还有一些问题，比如说问这个he is my menu and two box。Do I have a right tour? 然后他就告诉你，i identify the tool required, check your tool box, uh, determine availability, uh, conclusion based on the information from the manual and the two box, you should have the. 
	说一大堆，通过这几些图，他就可以开始去跟模型问一些更复杂的事情了。这样的场景就是直接看这个图大家会觉得比较傻。但其实我真实的遇到过一些什么场景，我相信我遇到这个场景应该绝大部分人也不知道怎么办，就是我家里会养很多植物，比如说痒这个天堂鸟、虎皮兰、琴叶榕、龟背叶，然后我不知道说这些大家知不知道是啥，然后他们会有不同的状态，有的时候会长得特别好，有的时候他就发黄了，或者枯萎了，根烂了，各种各样的情况。那你说如果以前我们用搜索引擎这个事儿你怎么讲，你很难讲明白的。因为你不是专家，所以你用不出准确的词。就假设你现在就跟这个自行车一样，应该很多人不知道这个release a clamp这是什么，直到你看了这个图，你一下就知道他给你的这段英文是什么意思。
	同样的我刚刚提到的这种不同的植物也应该怎么处理？其实一样是一个很很其实会的人会觉得很简单，但不会的人就怎么都都不会。但有了图像的理解能力之后，其实模型就能帮我们处理更多的事情。你可以拍个照说，这个是我的虎皮兰，这个他怎么样了，他有可能就告诉你这玩意儿不是虎皮兰，对吧？他是天堂鸟。然后天堂鸟的处理方式是喜阳的，然后龟背叶是不能喜阳的。
	你现在把它丢到这个阳台上晒坏了，以前应该让它放到一个能看见阳光，但是不会暴晒的地方。这些东西都是可以通过多模态来进行理解和沟通的。我们真正去集成的这个版本，在16GB的显存上，能够跑的是int 4的这个版本。前面的那个版本是有一定概率，图像很大的时候或者一些风险，它会这个OM，那么英特斯的版本在我们的绝大部分的同学，因为有同学我了解到还只有12GB的显存，非常老的一些英伟达的消费级的卡，应该也都是能跑起来的，那怎么跑呢？
	我们待会儿会通过实际的代码演示，我们在0.5的分支里面新增加了一个PY文件，跟open IOS per类似，我们新增加了mini CPMV model这样的一个PY文件。在这个文件里面我们新增加了一个方法，叫做chat with image。它的输入参数其实非常简洁，就是一个image file。因为为了保证它能够方便去测试，所以我们看到也有一个默认值给到question。这个question就是我们要给这个mini CPM给他图的同时向他提什么问题？然后simply temperature stream，就是其他的一些用在流式输出的一些参数了，可以先不关注这个，再往下我们看到这个代码要实现的这个流程，其实就是上节课跟pipeline类似的一个流程。我们有这个pipeline里面之前我们讲过，大家可以再再回忆一下，尤其是没有用过的，它其实是三个大的步骤，token either model model的结果再进行或处理。所以to niza跟model非常关键，所以我们要去加载to neither和model，只需要把刚刚我们看到的这一页里面的这个，就像它的这个URL，就它的ID一样，把这个传给我们的tokenism和model的from free train的方法，那么它就被降下来了。
	然后最近因为transformer也在疯狂的迭代它的版本，所以大家经常会看到各种各样的warning。也不用着急，你最好的思路，最好的方式就是等到模型的这个服务提供方官方去理清自己的这个依赖关系。因为他们会依赖torch，依赖transformer，依赖很多生态里面的这个库，等他们去做迭代，不要去自己做适配，我特指你还是一个新人的情况下啊不要把时间花在这种地方，窝点就窝点不影响什么。
	然后我们把model跟这个token either加载进来之后，有一个方法很重要，叫e mail。Evaluation的evil不能叫义务意外。Evaluation evaluate这个方法其实就是让我们的模型只支持只去执行这个。你可以认为执行这个前项的计算了，不让他去做任何的反向传播相关的一些计算。然后在chat with image里面我们会用到这个model，就在这儿，我们几乎都是不太会用流式输出的。在我们的使用场景里，这个模型的交互。因为我们在ChatGPT那边会有别的一些处理，在本地测试你可以去测这个流式输出，它其实就不断的去刷新这个打印区，然后我们真正去调查的时候，只需要用model chat这个方法就够了，然后model chat里面传对应的这个值，我们接着往下看。好好，然后我们接着就来实际操作一下怎么样去处理这个新的模型。
	看一下。
	我们打开这个。放大一点。首先我们应该在hugging face上面能够找到这些模型，放大一点在。这个尺寸在model里面我们可以去看到左边还有这个mart moda，选中这边就是选出来的模型，然后我们按照当note出来，就能看到有这个千问二的模型了，包括前文2的7B的模型，微软的这个V3.5，nova google的这个帕里金马等等这些模型都可以试。并没有说它就是最好的，只是说它的中文知识还行，所以大家所有课程里面用到的模型，大家都不要认为这是最好的。就不要让我替你们做选择，我只是教你们怎么样去做选择。但不是说这个就是最佳的选择，因为没有最佳的选择，你看到了它是一个雷达图，没有谁是全方位领先的。
	好，那么这个mini CPMV2.6，这个V2.6其实在这儿就能够复制它它要加载的这个模型，但你可以自己去试试一下，你如果去加载这个全精度的，可能会这个OM我们这儿就不去试验它OOM，所以我们会直接去加载它的量化版本。这个量化版本怎么找呢？我们看到有两种方式，一种就是在这儿可以点到它的group，就发布这个模型的group。对，到这来。在这个group里面，我们看到这个connection是他发布的各种各样的模型的版本可以展开，有mini CPM的mart model，它去把它的模型做了一个归类。
	然后往下。
	我们看到这里有各种各样的版本，这都是open BNB发布的这个模型。那英特斯带这个int的其实就是量化的版本，这个英特斯就从这里可以找到，但同样的你去搜它的前缀也是一种很好的思路。我再给大家看一下怎么弄这个，比如说在这里搜，通常也都能搜到不同的这个尺寸。那你像这个GGUF，我们前面讲过GGUF，你就可以把它导到欧拉玛里面去运行，还有印象吗？这个GGOF的格式。
	好，我们看到了这个inter 4的这个版本，他明确提到了这个是mini CPMV2.6的英特四量化版本，大概需要消耗的这个GPU的显存最少是7个GB，这个是指裸的参数加载，需要使用额外的一些python的依赖的软件包。这些我们在requirements里面也都已经加上了，在这里大家需要去拉取一下最新的代码，应该就能看到新增加的这些依赖包好。他也给了怎么样去做的测试的一个示例代码，那我们回到我们的代码库，这个是在服务器上了，在这儿我们就不再能使用macbook来进行演示了。
	因为使用这个模型的加载需要GPU。所以我们首先到ChatGPT的这个github上面来看代码，这样直接一点，右边我们就当做它的执行环节，就把这个放大一点，然后我们找到south。切0点5分之。然后我们看到有这个。迷你CPMV model，在这个方法我们刚才已经讲过了，然后我们同时也为了方便测试它，可以支持把它作为主程序的入口。这个主程序的入口在这里也会打印提示，这就传入一个图像的UIL，可以这个question就是when what is in the image，response等等等等。这我们就找一张图。
	这里应该生成了一些图了。
	不过大家应该看不到这些在这这这这些是在本地的这image下面的二级目录的图，是我本地的一些结果有点久，我们故意加了一个no的。我的印象，应该是丘吉尔，我们直接问一问。
	需要一个。
	中文。
	这么卡，我们把这个八号下载下来。
	然后我们来执行一下，有点卡。
	什么情况？
	稍等一下，这个网有点卡。
	也太久了，挡路了。
	还是主色。
	的。
	哇。新建一个。
	不夸张。
	连接这个ChatGPT。
	靠，太卡了，我好，终于好了。应该终于路由到了台湾好了。我们先执行一下，force这个mini CPM images八点天气。
	这里会去加载模型，我们为了方便看GPU的消耗，开一个。加载进来了。65，这么快吗？
	这边已经输出了，大家可以看到这一段话放大。Uh, the image features a man wearing a shoot for tie and a head. He is holding a scar in his final response. Image driver输出的这两边分别对应着我们代码里面的这个。Final response在这里。
	应该能看到，这里有一个结果，然后我们看这个图被下来没有。对，其实就这张图，但这个图我不知道大家认识吗？这人其实是丘吉尔。但它的每次测试都要重新加载比较慢。一会儿我们把它集成到radio，我们再来试这个功能，那现在大家应该能get到，就这个东西要去用它，用arc base确实很简单，因为它这套接口做得很好，然后也很统一。
	现在我们本质上要做的事情是要把这个输入一张图，然后输入一个文本，然后问这个模型，我说让这个模型帮我们办事儿，把这个事儿变把这个能力变成一个函数，给其他的模块去调用，是哪个模块呢？其实我们的这个radio server的模块。就我们的这个主文件的模块。所以check with image还是用左边来看代码，需要让我们的gradual server可以去调用。
	第二个什么呢？他导入了很多，他现在是从我们的这儿，从我们的mini CPMV model导入了check with image，这个是定义在这里的，所以我们可以去。这段函数里面这儿使用了chat with image。我们的generate contents它本质上是咱们的这个页面里面的chatbot去响应这个chatbot message的一个hindle function，它的处去响应它的一个函数，那我们看他做了什么扩展，第一个就是说在上一个版本我们可以处理啥呢？可以处理音频文件。大家还有印象想法，可以把音频文件里面的说的话语音识别出来。识别出来之后就把它当成我们打的字一样，就可以进一步的去让这个check box生成PPT。
	那现在我们是说有这个图像，我们想象一个场景，就是这些图像其实我们是拿到就是可能是自己找的，有可能是别人给你的。但是你先要去把这个图像，其实就两件事儿，你搞不明白这个图像到底是什么。比如说一个最简单的场景就是你的领导给你发了一张图说你帮我调研一下，但可能你压根就不知道他是啥，对吧？就你怎么调研呢？然后最关键的事情是你也许搜了一些资料，找到一些网站有这么一个图了。但是他们也没有基于这个图再去给你讲各种各样的展开的内容。所以你可以想象，实际情况是我们把这个图像识别的能力嫁接进来之后，一个非常有意思的场景就可以被解决好了。
	就你拿到一张你的需求方给你的图像，然后这张图你再去我们的TIPPT上面去问他，这个是什么？能不能给我介绍。然后他给了你一些相关信息之后，这些信息再作为我们原来上一个版本的流程就已经支持的内容。你知道了它是个什么，你再让它生成一些相关的内容，然后这个内容再变成PPT，是这样的一个流程给大家讲明白的一个过程，他要去集成这个chat with image的能力，其实也很简单，就是本身ChatGPT就支持上传文件，任意的文件在这个位置，我们只需要去处理上传的文件就好了。在上个版本里面我们就已经去做了一个file extension的这样的一个变量。这个变量其实就是获取了上传文件的后缀，文件格式这个关掉。那么这个文件格式在之前我们只处理了WAVFLAC和MP3格式的那在这里我们会去处理JPG JPG、PNG和JPG。
	如果它既上传了一个图像，同时还有一些文本，那就说明他是有诉求的，他是针对这个图有他要问的问题的那这个时候，我们只需要把它的在这个里面敲的内容，作为我们chat with image里面的这个question传进去就可以了，是非常简洁的一个调用如果他啥也没说，那也没关系，我们就直接把图给到chat with image。然后我们的question的默认值就是去描述一下这幅图，这样的一个流程应该大家看看明白了，比较简单。那么我们启动一下我们的video。这次我加了密码，并且这个密码没有传到咱们的这个github上面。所以大家应该没法在我演示的时候操作了，我把这些关掉。
	这里在加载模型，那我们。
	看一看。
	不用，用这个。
	只能上下布局，只能分析那就。
	缩小一点，这样应该就可以了。好，这个是他现在的一个情况，然后我们加载进来之后，重新刷新一下我们的ChatGPT。
	在这个界面里面，我们现在试一试这个图像理解的能力。比如说我们在本地就有这么一个图。这么一个图。我们就可以问他，这是哪里？介绍一下，我要写一个关于他的对汇报材料。我不知道大家看到这个图能不能想起是哪儿？其实我还去过这里，在我小时候。
	我们看一下这个日志，这边其实开始去处理这个图了。应该不太好去捕捉到它的这个显存变化，大家看到就行了。这里我们看到这个模型输出了，然后右边是服务器端的一个图像解释。这个其实就是chat with image调用完之后在这儿打印出来的内容，image description有写到。这是一尊位于中国敦煌莫高窟的大型石刻佛像。莫高窟又名千佛洞，是中国著名的佛教艺术保护机。
	说一大堆。好，这个其实是我们看到一种很常见的一种做法，就是你要针对这个图再去提问什么的都可以。然后最终的我们的chat PPT的那个大作业，其实是需要把很多我们提供的基础能力再去做整合。当我们把它留给我们的整个结业，然后整个图像识别的集成。
	今天网有点差。整个图像识别的集成，其实核心用到的就是视觉和文本信息能够融合给到模型。然后这一块的输出的结果，这个结果其实也可以再去做处理，可以把它作为chatbot的历史记录去管理，也可以去不管它，就放在这里。因为如果你默认所有的这个都要存的话，整个上下文也会很复杂。因为它更像是一个单独的任务，会串成整个workflow。如果这些能力最终你有一定的前端或者做交互的技巧的话，你其实可以把提供的暴露的这些能力最终串在一起。你像AIPPT，它最终其实是做了一个在线可以去编辑PPT的这么一个UL。
	这么慢。
	好，那我们看一下，我们可以接着问。我们可以就问他介绍莫高窟的历史背景、艺术风格，内容。然后同时。
	他应该是网的问题了。这个还可以稍等一下。果然。网都断了。对，因为这个连接断了。
	我没有在后台启动，主要也是为了让大家实时能看见这个日志，但真实的这个服务启动还是按照我们github里面就教过的，变成一个demo service，然后你去看f log就好。
	好，很熟悉。
	我们再演示一下，这个地方刚刚网卡了。
	好，这边开始加载。
	这边是有跟刚刚说法不太一样，这个很正常。本来它就有这个temperature我们可以看到他给了一些有效信息，敦煌莫高窟代表了历史重要性研究我们可以尝试，当然直接拷进去肯定不好，我们可以试一下，看看check box的回复。
	有些聪明同学就知道如果work其实是可以串联起来的。
	生成了一个结果，敦煌莫高窟的历史与艺术价值，敦煌莫高窟概述，位置与背景。我们当下来看一下，一键生成。
	对，0.5版本也更新了我们的模板文件，变得更加的简洁。
	在下面。我们放大一点看啊。
	敦煌莫高窟位置与背景、历史沿革、建筑特点、历史重要性，佛教艺术的贡献。长这样的保护与传承，未来展望，结论参考资料等等，这个其实就是一个标准的一个任务流。切到这边来一个标准的任务流，就是输入是一个图像，可能就是领导的一句话的需求，然后你要基于这个去想一堆的内容。你说有没有可能你拿着这个图去网上搜，去找各种东西呢？可以。完了之后最终你还是得整理出一个PPT和内容加模板的格式。那你就算在网上搜到了一些信息，也可以复刻这个流程。你在网上搜的信息给到ChatGPT，它在帮你生成整个powerpoint里面的内容，然后这个内容最终在你可以再找一些图什么的再插进去，因为我们的模板其实是留了大量的空间去插图的，这个是我们集成图像输入图像识别能力的这个0.5的部分功能。
	我们下一个小节再来讲怎么把刚刚说的这个流程。就是不管我是拿到了一张图还是怎么样的，我最终还是得有一个完整的PPT。而且PPT都知道有图片会很好看，但是这个图片显然在这个聊天对话框里面是很难去编辑和指清楚我这个图到底差在哪儿，差在什么位置的。
	那一种显一种比较简洁的产这种流程，其实是我们在跟同事跟领导的交流过程当中给了你一个图。然后你自己又去网上找了各种各样的资料。可能你整理成了一个word文档，或者也有可能你的同事给你的就是一个word文档。然后你把这个word文档一键能够变成一个PPT，那就非常完美了。并且我们知道往一个word文档里面去塞各种各样的原材料，其实是简单的。这个过程其实挺像那种什么自动炒菜机等等那个逻辑，就是各种图，各种文字，你给我塞到这个文档里面，然后我再把这个文档一下变成一个精美的PPT，接下来进入下一个小节，看看怎么做到。
	好，我们接下来来讲一讲怎么把chat PPT的多模态输入功能给设计和研发出来。就是我们的这个像炒菜机一样的设计，给一个word文档，这word文档里面啥都有啊，你能够把它的对，这样的一个场景，就是很经常我感觉会有在公司，尤其是你的工作本身就要产出一些这种PPT相关的交付物的话，就会有这样的一个case。小李帮我把那个文档做成汇报的PPT。我明天要给大老板汇报，或者我明天要给我的加一汇报，也就是你的加二汇报。那个文档你其实可能之前都没见过，或者那个文档你你你也搞不清楚里面到底有些什么东西。
	举个简单例子，假设有这样的一个文档，多模态大模型概述，这个内容是GPT帮我生成的。然后这两张图，其中一个图大家见过了，构的架构图，另一个图大家应该很熟悉了，transformer的架构图。就这样的一个文档，你拿到它，然后你再想象一下，老板跟你说，小李我需要把它做成PPT汇报。因为我们在工作积累素材的时候，基本都是文档。不管它是一个word离线的文档，还是飞书、戚薇等等线上的文档，它它都是文档线上的文档。好，还更简单一点，大家如果了解的话，这些在线文档编辑都可以导出成markdown的那我们现在其实解决的是一些没有办法联网的更复杂的case，就是这些word离线文档怎么办？
	就这样的一个文档，你要去拷贝复制做到PPT里其实是非常烦的。而我们其实想要解决的就是企业办公里面的低效率问题，这是现在PPT的一个重要目标。那它变成一个PPT会是什么样？要花多久？现在它变成一个PPT，其实是长这样的。我们这是用chat PPT0.5把一个word文档一键转换成一个powerpoint的一个最终产物。但它还有在优化的空间，但确实就是一键变过来了。
	那这个一键变过来是怎么做到的？这里有哪些复杂的事情，是需要我们去一点一点处理的，包括这些一些小的细节，这是一种情况。就我们举了这个0.5想要解决的这个多模态输入的case，就是你突然接到了一个任务，来了一个你不熟悉的文档，然后你要把它做成一个PPT。第二个就跟我的工作很像，就经常要讲这个，要给大家讲课，要做很多的课件。那么我们讲一个时机点例子，就是有没有可能五分钟做一个PPT，今天我实际测了一下，可以，怎么做的呢？
	第一步你需要有一个原材料，就像刚刚的word文档一样，那个是PPT里的内容。假设我们要讲古巴雪茄，这个是我非常喜欢的那古巴雪茄它的内容我不可能现编，对吧？但是我们知道生成内容这件事情有人非常擅长，就是这样的一个word文档怎么来的？我们给大家简单看一眼。在古巴学家入门指南。这里我回头把这个U21分享给大家。
	其实就三次对话，每次它的生成应该在3秒左右。详细介绍一下古巴雪茄面向小白用户在内容中插入一些图片，后面我实际试了一下，它没有办法插入图片，所以这反而成了一个负担。但有一个好处是他给了我去搜索谷歌的时候用什么这个关键词。Anyway我们看到这里有他给的这个什么是古巴学家埃古巴学家的种类，制作过程如何享用，如何挑选，这个其实就对应着什么是啊种类，如何挑选等等。
	同时第二个部分增加的内容，增加购买渠道全球市场分布等等，购买渠道全球市场分布，价格因素、口味与香气、养护与保存、收藏价值。其实就三次，而且第三次其实我我比较复刻咱们想象这个场景，你时间就是很紧张，你根本就来不及去想还有什么东西了。你其实连上面这两块都没看啊，但是你就直接中间这一块有看。因为这个没有跟销售端跟marketing相关的内容，所以增加了这部分。下面这部分就是让他去补充了，口味、香气、养护保存、收藏价值、社交文化。
	接着将以上内容导出为一个DOCX的文件，就word文件保留格式。那么GPT4O是有code interpreter的，就是代码解析器的。它可以生成一个python代码，并且在它的沙盒环境里面去执行这个代码。所以他执行完之后，最终会生成这个文件，这个文件应该有效期过了。我印象当中他有A国人留下去过，但没关系，他在这里你可以打开看一眼，这个这里面的文字部分，就是我们通过最多一分钟时间，在这个GPT4O里面生成的。然后我删了两个部分，有可能养护保存，收藏价值可能跟绝大部分受众是无关的那接下来其实是要搞定的是图的问题。就是我们知道其实现在就炒菜的这个过程，我现在得有了买了很多菜了，需要买肉，肉去哪里买呢？其实。Google search. 
	然后我们假设刚刚我们说的场景是。这个图它没用，它没有办法给我们真正的这个价值，但我们可以把它再出来，去搜一搜。这里就会有很多我们可以用得到的图，因为是学术用途，所以我会找一些有水印的图也没关系。大家如果要商业用途的话，需要避免这个问题。
	然后整个这个手动过程其实也是可以自动化的，外乎是一个成本的问题。因为我们其实已经教过大家怎么样去调搜索引擎。就是我们在人群里面调搜索引擎，它是可以去把这个图request UIL拿过来的，然后我们找一个，然后我们不用每个都找，我们找到了这个像这个在阳光下晾晒，这个很重要，是一个头图。搞过这个运营营销同学就知道这几个门道有多深，要有投图的，然后种类这个得看一看，那我们就收古巴雪茄的种类，他就出来了。很多的这个内容，原话叫做各大品牌，我们搜一搜。
	这里就出现了这张图，类似的其实这个过程也很快，我印象中应该是最多两分钟时间，搜了几次，所以三分钟的时间我们造了一个word文档，这个word文档是古巴学家技术指南，对吧？然后这个word文档我们要怎么做呢？我们希望把它一键能够变成一个PPT。就刚刚说的那个文档好了之后，要把它变成一个PPT，其实就回到了case one，就是小李帮我做一个PPT，这有材料了，而且这是一个更陌生的话题。我们希望它能够自动化，最终出来的这个PPT长这样。有图之后你会发现这个PPT会更优雅一些，更好看一些。
	那实际怎么做的呢？其实在我们刚刚的这个代码里面，还多了一个文件的扩展，就是DOCX和DOC。就对于这种word文件，我们有一个新的workflow来进行处理。这个workflow我们看到第一部分有一个generate markdown from dogs，然后有一个叫做content for matter，有一个叫做content assistant。这两个内容的一个叫内容格式化。一个作家，我不知道中文怎么翻译，就content for matter和这个assessment，其实背后都是大模型他们做的工作也很简洁，一共就三个阶段。
	第一个阶段是把我们的word文件正确的解析变成markdown，啥也不干，这是不需要大模型处理的，你只需要能够熟悉，这个就跟PPTX1样，也有这个dox的一个库，你只要熟悉这个库，然后你正确的解析，然后按照这个word里面的格式转换成markdown里面的这个格式描述符就OK了。然后第二步，content formatter要做的事情是说那个word格式出来是很凌乱的，那这个凌乱的格式要怎么变成一个我们可以用的markdown的格式。然后第三个部分看名字大家应该能看到这个content assessment要做的工作叫adjust single picture。我们目前的layout group layout strategy里面还是没有去支持一个slide里面有多个image。我不知道这个大家有没有印象所以整个最后的这个样式就是我们去传一个古巴学家技术指南的dox。
	它会生成一个带有图片链接的内容，这个跟原来走chatbot生成不一样，因为我们在chatbot里面是不会生成这个图像的UIL的，因为我们不知道它在哪。待会儿我们会讲背后发生了什么，然后再点一键生成，就可以生成别的这个内容了。然后我们来看看怎么做到的，先记一下，这一节也没有homework，需要大家去好好消化一下0.5的代码。然后下一节课其实有一个homework很重要，就是把很多东西我们需要作为一个结业或者说结这个agent的一个作业，然后把这些能够整体性的调动起来。好，那我们先看看这个是怎么去实现的。
	首先我们要看到这个0.5的分值，我把这些关掉了。0点5分之在这个流程里面我们一点一点了解，分成三个阶段的事情。第一个阶段是这个dox文件，这里generate markdown from DOCX，这个文件word文件要能够被正确解析，怎么解析呢？我们看到它的定义，定义在这里。有一个DOCX passer，就是我们的word文件的解析，他其实要导入一个DOCX，跟PPTX有点像，DOCX在这里面他要做的工作主要调用了他generate markdown from dogs，从指定的docs文件生成markdown格式的内容。
	然后我们需要注意在这个word文件里，大家看一下word文件。在这个word文件里面是有图的，而我们知道在markdown里面是不能嵌入图的。在markdown里面我们是需要做这个图像的URL嵌入的。所以这些图我们还需要把它一个一个的解析到位之后，再保存到对应的这个位置。好，那么我们看到这里需要把图像另存为，然后插入到markdown里面。当然这个代码注释都很详细了，我就主要讲它的这个主流程了。
	然后我们在这儿同样的document就是去加载DOCX文件的一个类。把它框进去之后，然后我们一段一段的去获取内容，有hiding有title，就对应着我们在正常的我们的office的这个软件里，不管是word还是PPT都有这样的概念黑点title包括我们这个共享文档，然后去确定标题的级别，然后接着去检查每个段落的运行。这个paragraphs s这儿其实我们就能看到跟powerpoint有一些类似了。因为底层都是XML，所以这个paragraph round其实就是一段一段的内容，然后去查找图像关系匹配这些跟这个DOCX库有关的代码。大家不用想，肯定都是GP4帮我写的，然后去读取文件读取文件，然后把文件存下来，存的时候这里就有一个image path，存在哪儿呢？我们设计的这个存储方式就是长这样，在项目目录里面我们，就看他就这么来的，有images，然后images再去跟这个DOCX name新建一个二级目录，就像我们看到这个DOCX base name，就是我们的古巴雪茄技术指南，然后在这下面去存文件，存的文件就按照计数器来命名就好了，长这样123456789就这么存下来了。存下来之后，文档就更简单了，文档就往里面加，你是黑点就加这个，如果你是正常的正文，你就这么加。
	用缩进等于我们的bullet point，还有普通的段落，最终有一个从dox文件解析的大概内容，就得到了一个markdown的内容，那我们这儿可以来实际操作一下，给大家看一眼。然后我们有一个简单一点的word文件，就是我们的case one的这个word文件。在这里多模态的大模型概述，长这样，我们去处理一下。同样的这个dox father也是支持把它作为入口文件的那我们来执行一下新开一个。
	对。
	对它不会它不会打印，但它会直接生成一个文件。大家看对我改了一下这个逻辑，就是它输入的是这个dox，它会生成一个markdown的content，并且他把这个mark content写到这个本地了，就是这个。搜一下，这里是他刚刚新生成的on track的一个文件。这个就是把我们的dox完全按照格式解析出来的一个样式，我们可以对比着来看一下。
	多模态大模型概述，一级标题，然后下面是二级标题。1.1支持多种这三级标题，因为它确实比它要矮一级。2.1模态融合技术，这个确实也比他矮一级。
	在下面这有两个图片，这两个图片一、图片二对应的其实也都是他从这里捞出来的这个图。然后另存为的偏激，这儿有点慢，我就不展开了。对，这个格式显然它是没有办法作为我们生成这个powerpoint这个自定义数据结构的输入的。我们还有一个自定义的数据结构，大家应该没忘，叫做这个powerpoint。然后我们需要通过input puzzle才能把它变成这个powerpoint。现在我们第二步要做的工作，其实就是把咱们的这个。回到radio server。
	我们用面对PUA来看。
	应该更清晰。
	这边也实现了这个对应的扩展，就当它是dox的时候，我们先把这个输入的dox文件变成了一个roll content，我叫roll content。Content就是长长这样的，很原始，因为它不是我们这个chat PPT里要的那种markdown content，所以我们需要再把它变成一个我们接受的markdown格式，符合我们要求的。所以这个时候content for matter的作用就来了，这个content for matter是用来干嘛的呢？我们可以看一下，在这它是一个。
	GPT4 . 
	omni驱动的chatbot，然后这个chatbot自己的这个prompt，这个prompt file是这个content for matter，我就在这边双屏打开了，prompt里面有一个content formatter，这个content formatter我们做了扩展了，简单来看就是你是一个expert content for matter，然后要做的事情是什么呢？就是convert the provided markdown content into a structured format for presentation use。它的用途就是把那个保留了word文档里面的格式的原始markdown转成我们要的markdown。并且我们要的markdown的格式我也做了一些扩展，就这儿可以回头去看一看，包括是deadline，给了一个example，这个很重要。UM长这样，input是这样的。Input是长这样的，那么output我们整合了一下，做的更简洁了这样的一个prompt。然后他接受的这个输入format就是一个原始的content，返回的是一个格式化后的content。所以我们在这里。
	面点PY，在这儿我们看到返回的就是这个markdown的content，然后format之后，我们还有一个重要的工作是什么呢？就是我们看这个提示词里都有啊，就它一个slide下面有bullet point，有图有bullet point有图，这是它的一个格式。但这种格式其实是不能被正常生成PPT的。有一个细节我们没有专门讲，但读过代码就会发现，我们的slide content里面title只有一个bullet point，我们已经扩展成可以有多个了，甚至有多级。但是我们的image path目前还是一个slide里面只有一个，这也是为什么我们可以用二进制来进行编码，后面我们也会有一个homework，整个结业作业里面是另一个feature的扩展，把编码系统做成十进制，这样就能支持类型加内容的编码，这里我们不展开。所以目前我们的slight content在0.5里是只有一页一张图。
	在这样的一个结构下，不管是row content还是我们的markdown content都是不允许的，所以我们做了一个方法，叫做一个另一个check pot，叫做content assistant。它未来当然应该不只是只做一个，把你这个图拆成两个，它的目标他应该是一个内容增强内容增强的一个assistant。然后我们看这个convert状态就好理解了。首先这个是它的输入重点，就看这个第二页的slide，它会把它转换成拆成多页，因为它会补充内容，然后确保每一页只有一个对应的咱们的这个每一页只有一个对应的图片或者说图像，不会造成这个图像的忽略或者遗失，这个是content assistant，我们再来看面的话就好理解了。
	Content assistant再去调用它的adjust single picture，这个content assistant在这里，它的adjust the single picture，就主要是去把markdown的content变成一个最终可以用来生成PPT的input test这样的一个流程。那现在我们执行一下这个面点PY方法试试。它默认的是test input，我们让它去生成我们的inputs，也做了一些调整。我们的面点PY为了能作为一个方便大家测试的入口文件，这里也做了类型的判断，就是我们的inputs目录也做了调整，有了二级目录，按照文件类型去做划分，如果是markdown保持原来的逻辑，如果是这个DOCX，就保持另一个新就就实现这个新的逻辑这有这个记录。
	我们的。这个网又卡了吗？我考上了。
	这太慢了，稍等一下啊。VPN最近有点炸裂。
	在台湾和。
	日本间反复横跳啊。
	可以了。好。我们执行一下这个much moda，看到这里有一些日志输出，应该好理解。正在解析DOCX的文件，然后这个是格式化之后的，formatter格式化之后的长这样，太快了，这个formatter格式化之后的仍然有这种不合法的情况。然后重构之后，并且增加了一些内容之后，像下面这样，这是去加载了layout manager，然后每一页的内容，然后最终生成了一个powerpoint，我们可以把它down下来叫output多模态的大模型概述。这是刚刚生成的，你看一眼。多模态大模型概述。
	这个流程大家应该就熟悉了。所以我们最后其实是把这个流程再集成到我们的video server。我们看到。
	这里。
	To server，那刚刚降下来的。这个大模型的多模态的大模型概述，长这样已经比我们之前的版本又有迭代了。所以大家其实得得去钻研。一方面是这个模板本身也做了一些知识调整，其次我们的图片其实还做了知识性的调整。待会儿我们会在PPT generator的改动上再给大家看一下。
	到这儿为止，其实我们已经完成了前面的步骤了，就是PPT generator之前的所有步骤，就到这个地方为止，我们其实都生成了，这只是把它换了一种方式打印出来，这个是解析转换之后的我们自定义的powerpoint的数据结构，长这样。但是我们在PPT generator上面做了哪些改进呢？我们可以看一眼在。这里generate presentation，这里做了一些迭代，主要是图像的自适应，这个值得大家去看一看。后面如果大家要做扩展，这也是一个切入点。好。
	没怪。
	那香港还不让房东给他。
	生气了。
	加载出来了。大家看到我们之前的PPT generator里面的这个generate presentation，其实是比较简单的，就按照我们slide content的类型来进行生成。现在幻灯片标题没有什么大的变化，文本内容这里我们做了一个小的改进之前我们的上一个版本，为了这个格式上面不显得这个文本那么少，以及他跟这个标题之间的一些操作，我没有做这里的一个处理。
	大家如果去看0.5之前就0.4的版本，它生成的内容，它的所有的我们叫bullet point。在PPT里面就是这个tax frame那个文本框，它都会有个空格，它有一个空行。那个空行其实是因为这个方法，我们会先把place holder里面的那个文本框的内容全部先清掉，就play folder里面默认也有一些词的先清掉。但cyr方法它默认是说如果我清掉了，我会保留一个空行，所以那个同行是这么来的。
	但如果我们清掉了我们的这个到之前我们的代码里面，其实是会直接去add paragraph，就跟在dox里面的paragraph一样。但是我们可以先用第一个段落，而不是让它空着。它空着是因为我们没有用它，我们直接就艾特一个paragraph了，在0.4里面。那现在我们就把这个空行的那一个paragraphs先用着，给它取名叫first。这里是有一个变化给大家记一下。剩下的这个新的我们的bullet point就可以往下用了。
	然后插入图片结构比较复杂，我们做了一个insert image center的in play holder。这个是干嘛的？大家回头可以去把这个地方细读一下，里面做了很多的细节的操作，主要的作用就是把图片插入到slide当中的时候，让它能够跟他衷心对齐，什么概念呢？你可以看到。中心对齐就是它的plays holder，其实就在这里，这个是它的placeholder长这样的这幅图是用的这样的一个play hold。我们如果没有做中心对齐的话，其实很多时候你的图跟它是不可能跟这个框框刚刚好，这个应该是我们有这个0.1版本以来大家都在问的问题，你要怎么去做缩放？现在的策略其实就是我们可以把它复制过去看一下，他会发现他其实是给他做了长和宽的适配的。然后假设它是一个face holder比较窄的话，那他可能就会去比如说这个place holder假设变成了这样，就假设place holder变成了这样，那他可能就会。
	进一步。
	的去缩小，缩小到适配。
	它的宽度。
	就它始终不会出框。这是一个整个我们在insert image里面去做的一个处理，就是图片尺寸超过pace holder的时候，要进行缩小的适配。但还有一个反反方向的一个操作，就是假设假设我们的play holder足够大，但是我们的图太小了，就这么放着其实也不合适这么放着其实也不合适。所以我们同样的还会去做对他做一个放大的自适应放大也是放大到不出框，所以会有这样的一个操作，在这个代码里面去进行实现，这是一个知识印。同样的其实文本框的知识印，大家也可以去考虑一下怎么样去实现。就文本框的自适应，显然这个框框还有空间。那是不是我可以把它字号放大一点，甚至再放大一点，然后有的就像这个一样，我现在是这样的一个下面有这么多的空闲，我是不是能把它调成middle？这些其实都是可以，我们在其实就两条腿走路，一方面是你去改模板，一方面就是你去提升自适应的这个布局，这样去做操作的。
	然后我们把这个PPT generator也讲明白之后，其实整个从word到PPT的流程的代码就覆盖完了。那么接下来实际操作一下，看他会是什么样的一个情况。我们应该还是在校医这一直启动着的。对还在讲敦煌那我们现在他不讲敦煌了，讲讲雪茄上传。
	这网也太卡。
	刷新一下网连接断了。
	我靠，卡到CSS都没摘出来。什么情况？那应该是这边也断了武器。
	我靠。
	搞什么？
	稍等一下。
	难顶。
	不是吧。
	没有办法。我们把这个演示完，这节课大家就能去练习了。这个是正在加载mini CPM。
	好，刷新一下。
	over. 直接传。好，然后提交什么都不用干，为它是一个场景明确的任务，这里显示上传了一个文件。应该接着就开始解析。
	他尿，我靠，果然。我靠。这啥VPN情况。
	稍等一下，这个地方网确实今晚有点诡异，不知道发生了什么。
	这也没什么会要开。
	刚刚应该是网断了，连接断了。
	这是我我们看看能不能成功。
	我。
	好提交。上传文件。这边写的formatter格式化之后的一个情况。传错了，传成多模态的这个了。我说是右边是多模态，我靠这个不搭嘎，看看学家能不能长内容生成。
	好提交。
	么情况，可以，这个是formatter格式化之后的内容。图片一、图片2、图片3、图片。
	4567899张图。
	然后不同的页现在应该是在调content的assistant。出来了，这个是我们看到这有一个assistant内容重构之后，它的内容变得更加的规整，什么是古巴雪茄，种类、制作过程、如何享用，如何挑选，购买渠道、全球市场分布等等，然后最终这个文档内容。
	最终这个文档内容因为跟assistant输出的是一样的，只是它两个日志级别不一样，就长这样，然后我们点击一键生成好了4.9兆。正在下载。
	好，下载完了。打开，它长这样。
	但这个显然还有可优化的空间，就留给大家的就像这张图，他应该用这种布局会更好一点，就有点类似于他现在是随机去选的一个风险，我们应该是在这个位置，我之前生成的一个版本，因为是随机的，所以这是他现在的聊的策略决定的。我们在他上一版生成的内容里面，我们看到这个本身就有对称布局，就是对称的布局。然后这张图第二次生成的布局选择随机性没有这个好啊，如果是这样的话可能会更好一些。然后像这幅图也不应该是这样放，他选择的这个布局不好，这个布局其实用的跟他是一样，这是一个很窄的一个布局，它显然就超过了，这就是我说的文本框。这种情况下要么就是去缩小文本，但也不合适。最好的方式还是在布局这个层面上，也要去做自适应的策略，那么这最后生成的这个结果。好，这个就是我们0.5版本的主要功能。
	我们回看一下，就是我们集成了一个多模态的模型mini CPM，然后同时我们完成了一个非常常见的在工作当中的一个从word到power point的一个workflow。然后在这样的一个工作流里面，大量的重复劳动。我们其实是希望借助大模型的便捷，帮我们去快速的完成这样的一个工作。也演示了如何能够五分钟的时间完成一个课题的汇报材料。好，这就是我们0.5，也就是我们这个像PPT的图像集成和多模态输入的一个内容。
	10点37了，咱们是不是线上都没人了？看大家还有什么问题吗？可以提一些问题。
	大家有问题吗？是不是都不在了，还有人吗？有人在的话写个一。
	大家有什么问题，可以再提提问题。没有的话，我们就在群里聊了。你这个内容还是挺多的，但这个网络也耽误了一下时间。
	我没听懂这个同学问的问题，什么叫官网提供的版本？
	这个单独使用显卡的功能能做成服务抽出来可以。你去问一下，像GPT，他应该一下就能给你抽出来了。对，这是我第一节课就有提过，不要问这种很奇怪的问题，就是能不能都能，可不可以都可以。对。他现在不就是一个单独的服务吗？你想想，只不过我们把它的入口都做成了同一个按钮。
	大家要是没什么问题，我们就群里交流。我看大家问的都跟这个课程内容没啥太大关系了。对，行，那我们就到这里，最后讲一下域名配置和这个反向代理，做了一个文档给大家做说明。在我们的read me里面也有更新，就read me里面的这个文档有写。
	快速开始，然后如何运行？这里有写作为生产服务发布，作为生产服务发布，ChatGPT还需要配置域名SSL证书和反向代理详见文档。这里有一个域名和反向代理设置的一个说明文档，这里面会详细去写。
	从域名的A记录配置到安装配置，SSL证书到engines的这个反向代理的配置这里有一些值你需要替换，如果这个index还看不明白的，下面还有一个补充关于index的详细说明，就它的模板长这样，注释的说明。然后假设你的是这个可以套进去，然后假设你的证书在这里可以套进去，方便大家去详细的理解。好，那我们今天就到这里，谢谢大家。