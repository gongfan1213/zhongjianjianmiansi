	我们学习了从GPT1到GPT3的变化，一路风云变幻。很多人看不上这个OPI的技术上的一些迭代。但是OpenAI做出来，ChatGPT其他公司没有，那他到底赢在哪儿？ChatGPT为什么牛逼？哪做的很好？我们来了解一下，我们上节课学习了这个机器的一路的技术迭代，我们知道pre train就是预训练语言模型加find tree这个范式是非常好用的。但是这个范式是在GPT1提出来的，GPT1提出来这个范式之后，OPI也一直在沿着这个思路去走，一直到GT3的出现。从GPT3到ChatGPT，我们会发现OpenAI的这个思路，包括他对AI工程的理解和运用，有一个演进的过程。这个演进的过程，首先我们看到GPT3、GPT3.5、GPT4大家如果去深入研究过的话，他们的训练集没有特别大的变化，就是因为训练这个大语言模型的成本太高了。
	GPT3和GPT3.5和GPT4，他们的训练数据都截止在221年，就20GPT4虽然是今年发布出来的，但它的训练的大部分的语料也都是到2021年截止了。当然他线上这些用户的反馈，他也会作为他find two的一部分。这就说明无限制的去堆数据已经到头了。因为我们能学的大部分的数据都都差不多了。然后我们还能增加的数据就是互联网上每天新产生的这些数据。但是我们知道为什么要搞多模态？就是因为新产生的数据，更多的短视频，然后各种多媒体的内容这些还没有被用起来，所以要搞多模态。因为这些数据的价值还没有被学习挖掘出来，这也是后面GPT4要干的一个很重要的工作，包括其他的模型。
	我们说回来，ChatGPT作为GPT3和GPT3.5的继任者，OpenAI做了很重要的两件事情。首先他们在1到3这个阶段把预训练做得非常的到位，堆模型堆数据，做AI工程拿到了很好的结果。接着开始做微调，我们知道GPT3当时提出来我们不改模型了，我们做in context learning就好了。但是那个时候你会发现只是在那堆特的语料上学出来的模型，它是能生成一些NLP的基准测试的好结果了，但是离落地有距离，我想要做一些特定任务的时候，这些特定任务的数据一定不在那个大语料里面，就特定的下游任务，非常具体的特定的下游任务还是得要去用标注数据进行微调的，所以又回到了这个微好的思路上来。但不是去做这个fine turing了，他提出一个概念叫instruction to I，叫指令微调，就是针对一些特定任务的标注数据进行指令的微调。
	然后相当于让我的大语言模型去适应你的这个输入指令，这个跟直接去改这个大语言模型本身的这个参数是有区别的。大家去品一品这个区别。一个free training是大范围的去改给大规模的语料，然后去大范围的去变这个模型。就相当于这个区别是什么？就是高中分科17年就是我要选文科还是理科了。那学的这个内容就是语这个语数外大家都要学，然后英语也都要学对，英语就是外语都要学。
	但是你是学这个生物物理化学的，他是学这个地理政治、历史的，这是预训练的巨大差别，语料不一样。但是现在我们到大学了不分科了，可能分专业，这些专业我们都会上通识课，通识教育，你要上什么线性代数，这些都要学。但是学完这些通识课之后，我现在要去上专业课了。专业课就像这个指令微调，你过去这十几年的从小学中学的教育到通识课的教育，决定了你的预训练。专业课就是那个指令微调，针对一些特定人物，为什么有一级学科二级学科？就是干这个事儿，这个是巨大的一个不同指令微调就直接不断的通过这个指令微调，各种微调策略，让这个OpenAI玩的非常好。就是这个微调策略现在是一个指令微调这种思路，策略是一个非常重要的技能。如果真正说有懂大语言模型训练的人，这个微调策略是它的核心竞争力。
	然后我们可以看看它具体怎么做的。这个是一个OpenAI的模型迭代图，其中有些模型大家可能就比较熟悉了。最下面的ChatGPT这所有人都已经用过的，然后还有一个叫tex达芬奇003，如果咱们调过OpenAI的这个API的话，这个text达芬奇003应该是掉的非常多的，这是他非常重要的一个模型。我们看这幅图的迭代非常有意思，它体现了OpenAI对于微调这件事情的理解越来越深刻。
	首先我们看到GPT3第一代，它给它内部命名这个模型叫达芬奇。它的第一代模型就是我们刚刚在上节课讲的，GPT3训练出来了1750亿参数这个模型两条路线用来迭代这个GPT3模型。左边这条叫training and code，就是他去学代码。我们看到GPT3的训练语料里面是没有代码的。人类产生了这么多代码不得学一学，学了代码然后迭代出了code达芬奇001和q shi ma 001，到后面有code达芬奇002。
	学代码有一个很有意思的事情，就是学完代码之后，这个GPT的下面这些系列的模型，它有推理能力了，这个是他的论文相关的文章，大家可以去看一看，这个推理能力开始涌现了。以前是你给我参考示例，给我布置任务我就做，但是复杂任务我做不来。因为复杂任务需要推理，想步一步的想，就像我们写数学题，解第一步干什么，第二步干什么？学完代码有这个能力。因为代码本身面向对象面向过程的这些写的方式，给它增加了这方面的能力。现在只能从结果来看是这样。
	还有一部分就是我们刚刚所提到的这个指令微调instruction to，它做出来这个instruct达芬奇和这个tex WT001，就这种各种各样的指令微调专业课学习，把这两个合二为一，就是把这两个技术手段合二为一，训练出来的这个code达芬奇002，非常有意思。这个就被他自己在内部划分为了GPT3.5。所以从这个视角来看，GPT3和GPT3.5巨大的差别在哪？有没有训练代码对吧？有没有训练我们的编程的代码，有没有加指令微调？这俩事儿干就变成了GPT3.5了，那GPT3.5除了刚刚这两个黑科技手段以外，他还做了一个事情，就是做了一个叫做有监督的instruction tuning。这个有监督的instruction tuning做完之后，出现了这个tex达芬奇002，现在的GPT ChatGPT，很多时候你去用它的GPT3.5，就ChatGPT可以选3.5和4。用它的3.5的时候，他已经都在用这个tax达芬奇002这个模型作为它的基底。
	这些事情是什么时候发生的呢？其实我们一路从2020年到ChatGPT发布的2022年的11月有两年多的时间。这两年多的时间，20年的七月GPT3发布了21年的七月发布了这个有代码的模型code x然后一直到22年的七月，把betta测试的这个版本，就我们的instruct达芬奇贝塔，然后一路训练得到了GPT3.5。所以其实一直到22年中，GPT3.5才出现。然后接着，在22年的11月，发布了tex达芬奇003和ChatGPT。所以其实22年发生了很多事情，大家如果细细讲的话，然后这里我们再细看一下，这个里面有三个黑科技，代码上面去做去学习代码，去做指令微调，让他们合二为一。接着第三个就是我们能去做这个RLHF，就是基于人类的这个反馈机制来做训练。
	具体来看，其实GPT3.5相比于GPT3，按照我们刚刚的说法，它通过前两个黑科技做到了很重要的变化。为什么有代差？但它只有半代的代差。
	第一通过最后的这个RHF，它能够生成更恰当的回答。简单来说就是我们刚举的例子。你说的都对，但是你说话的方式让我很不爽，那我就不想听你说话了。这是RLHF是基于人类的反馈机制来做强化学习做的最重要的事情。当时OpenAI找了很多人去做标注，就是为了让模型的反馈结果更像是人说的，更让人喜欢听。
	然后泛化能力就是做了大量的指令调整之后，因为简单来说就是一个沟通能力差的模型，我们不是说人了一个沟通能力差的模型，他不太能听懂你到底要什么。如果又来一个沟通能力很差的人，这俩就牛头不对马嘴了。那我这个东西怎么推得出去？就是我我我有一身本事，我听不懂你说话，然后你也不太会布置任务，我俩就匹配不起来。
	这个时候OPI就很很会做这个商业化和产品化。我想我很难改变用户，用户可能就是不太会描述任务。那我能不能通过指令微调，让我的模型本身就适应各种各样不会说话的用户。你想象一下你的这个服务人员，他就天天笑脸相迎，你说什么他他都能理解，那就干这个事儿。这个就是他的任务泛化能力，通过大量的指令微调去做的。
	然后通过在代码上去学习，它能够开始生成代码和理解代码。所以这也是为什么我们现在喜欢用ChatGPT去生成代码，有这个方面的能力。还有一个原因就是我们通过学习代码，我们刚刚提到有复杂推理的思维链的能力了。这个我们后面再讲这个prompt learning的时候，会专门讲这个思维链，这个很牛逼，使得他能够去解决这个复杂问题，并且开始突破这个模型缩放的法则。
	这个scaling law scaling law就是你可以理解成这个模型的规模越来越大，这个是有成本上限的。我们不可能再花几个亿去训练一个模型，并且你的这个模型变大之后，这个效果是不是也会变好？不一定对吧？
	那这个模型缩放的法则，其实逐渐的会变成一个大家不再去谈论那么多的问题了。就是因为我们发现我们跟大语言模型的沟通能力变好了。而且大语言模型因为学了代码，它能更好的去拆分我们的任务，把我们的任务变成一个的小的目标，小的任务，那他就能去解决问题了。这个时候就不一定要把这个拆分目标理解的这个拆任务的能力硬硬要大语言模型学会。而是我们把跟他沟通能力的本事变好了之后，他就变厉害了。其实大家把这个弯转过来就会想明白，他是两个人的事儿，模型跟人的事儿。如果我们更会说话了，那这个模型的泛化能力，这个指令微调是不是也就不要要求那么多了，就我们更会说话，我们更会布置任务，简单来说是这样，那么ChatGPT有一个三段的训练方式，我们简单讲一讲。
	最后就在我们已经获得GPT3.5之后，其实到ChatGPT有一个最重要的一个改变，就是我们可以看到最后的这个RLHF，就是我们的人类的反馈。其实在这个阶段ChatGPT和我们有一个叫street instruct的GPT，就是基于指令微调的GPT相比起来最大的一个区别就是在这个阶段我们会进一步去训练一个有监督的模型，就是SFT叫supervise的fine tuning的模型。那这个模型做了什么事儿呢？第一，它要使生成的输出去回应。
	第二阶段的这个RLRM就是我们的奖励机制，reward mechanical，你可以简单理解成这个三段训练。第一个模型要去做有监督的一个微调，去改变这个模型。改变模型的目标是为了让这个模型生成的结果能拿更高的分数。那这个分数是什么意思呢？其实分数紧接着就是接着第二段，你先假设第二段这个奖励机制是固定的，奖励机制已经训练好了。那我第一段就是不断的去调整我的结果，让我的结果更符合这个奖励机制的要求，比如说第二阶段，这个奖励机制就会收集更多的比较的数据，然后去训练这个奖励机制本身。
	就我们讲接着来训练第二步这个奖励机制的模式。那奖励机制要怎么训练？第一奖励机制本身是需要训练的，它不是一个金标准。就没有人认为这个人说的话只有一种方式是对的。
	或者说你换几个人人听同一个回答，有的人会说这么说好，有的人说那么说好，对吧？那显然是一个比较主观的事儿，那怎么办？上标注。所以第二个奖励机制的训练，就有大量的标注人员他会去打分，就是我们可以理解成这个奖励机制的训练有一堆的标注人员，这些标注人员会针对我们刚刚说第一阶段的这个模型生成的结果去选。比如说这个模型生成五个结果，你这个标注人员从五个结果里面选了第一个结果最好。第二个标注人也是选择第一个结果最好，第三个人也是选择第一个最好，那可能第一个的这个分数这就会比较高。这个奖励机制本身就是通过这样的方式去训练，就是这个奖励模型。那那这个是第二阶段的一个的训练方式，相当于我们捋一下，第一阶段在改模型，改模型的这个调整方向，就是为了让他更符合人的想要听的这个答案，那么这个是前两阶段要干的事儿，人的这个标注本身就在训练这个奖励模型。
	最后我们还会看到有一个叫做PPO的东西，就是policy这个策略。那这个PPO最终是干什么事情呢？其实它就是把我们刚刚说的这个过程给它结合在一起。就是我们刚刚讲有两个部分，一个部分是大语言模型要调整，第二部分是奖励机制需要去做训练。最终其实怎么样联系起来，通过PPO来联系起来，就是通过这个策略来生成一个最终的输出。然后这个输出通过这个奖励模型去得到了一个输出对应的分数，这个分数就可以通过PPO的这个机制，用来反过来去训练整个模型。
	就是这样一个三个方式来进行的一个完整的ChatGPT的训练。那你要知道ChatGPT的输入就是一个已经被指令微调魔改了很好的一个GPT了。到今天为止，这个过程我相信仍然在不断的发生。然后我们这些用户其实就是这个奖励机制里面的现在的标准人员。大家如果有观察的话会看到ChatGPT会在你的生成结果旁边有一个点赞和觉得不好的这么一个标签，一个按钮。其实如果你去摁了，其实他会给他现在用的这个ChatGPT的模型背后的这个GPT的模型去做调整，其实整个这个过程就是这样去产生的。
	我们回过头来看GIGPT它已经在哪儿？第一，他们虽然都是基于transformer的模型，但ChatGPT用了黑科技对吧？用了这个代码的训练，指令的微调，包括基于人类的反馈。第二，他把这个GPT变成了一个应用程序了，变成了一个应用终端应用了。这个应用的特点叫chat，就是聊天。所以它以一个聊天的方式让人去使用它就非常的流畅。就是我们人都是聊天为主，那这个时候通过聊天，它充分的展现了这个大语言模型的能力，这个就很巧妙。因为大语言模型本来就是生成的能力，这个GPT，这个生成能力如果你觉得它生成质量很高，你就愿意一直跟他聊，就会像滚雪球一样，那他就火速的就引爆了全网，因为它生成质量确实不错，他对用户的输入进行了处理，这个处理是从GPT3到GPT3.5到ChatGPT。
	这一路他一直在做的事情。因为他知道人就是大家普通老百姓的表达能力都不强，沟通能力强的人很少的那怎么办？把模型的理解能力变强，让他能够提前去处理各种类型的用户输入。这样遇到一些表达能力不好的人，他也能生成还不错的结果。然后对于输出的生成做了基于人类的反馈，让你听的这个答案更符合人类要的结果。就是我们这个labeling奖励机制的训练。最后他还进行了一些安全性和道德的审查，我们在OpenAI的动手实战的部分也会讲，它有这个API叫是叫合规性的一个API。这些合规性的API使得他去推广的时候有一些监管上的压力，他提前去做了过滤了，这样就不会出现一些我生成的结果不符合公序良俗或者法律法规，以至于被封杀。
	当然它刚刚出现的时候，还是有人去像翻墙一样，或者说去越狱一样，让它生成一些不好的内容。但大家用的越多就会发现，它越来越能去防止你去生成这些有不恰当的内容。就是因为就是去过滤这些内容的本身也是一个独立的模型。这个模型会在最后去筛一遍这个结果。这个结果也在根据线上的这些用户越狱的这些手段，越狱的输入，他去再做调整，再去优化。所以他的这个规范的检测就会更好。这个是GPT模型。我们回看从18年的六月夏天，然后一路走来到2022年的ChatGPT年底的发布，到今年3月份GPT4的发布的一个过程，这个一路是非常的曲折。从只用decoder的一个transformer，到把两者结合起来一起学到提出了in context learning，一直到2021年，又提出了用代码来做预训练，接着在GPT3.5这儿一阵魔改，魔改之后又去针对人类的反馈去做强化学习的训练，最终又去发布了多模态的GPT4，非常厉害。