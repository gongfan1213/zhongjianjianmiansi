	咱们现在画面正常吗？能能看到这个播放的页面吗？这个配置直播软件很卡，电脑。
	我靠，可以是吧？好好好，正常。行，刚刚整了半天这个软件终于剥好了，每次开直播的时候就会很卡。行，那现在我们就正式开始今天的内容好，今天我们讲南券的新技术生态与LCL，也就是南茜的表达式语言的快速入门这两个主题。
	我们都知道其实南茜的整个开源社区，它的进展非常的快。那有多快呢？我们今天就通过上半节的内容给大家做一个一年来的南茜社区的整体性的一个回顾，和他对未来的一个发展的展望。然后这里这个重点可能我们放在南茜，从一个最早期大家如果上过咱们大模型的开发的这个应该叫大模型应用开发时代的这个训练营的话，可能会对南线有一些了解了。因为我们在那个开发营里面，其实有整个6节课的内容，从年欠的基础模块到三个实战，让我们系统性的了解了年欠的零点几，当时是一个快速迭代的版本，那会儿也主要是一个SDK，了解了南线的这些模块要怎么用，然后它的一个定位是如何的，然后也实现了一些比较有意思的agent的项目。
	但是这一年以来，其实关注南迁的同学都会发现，整个南线的代码在飞速的生长。并且它的代码仓库也已经从原来的一个report变成了一个group。然后下面有一堆的仓库，然后这个一堆的仓库最终组合出来的一个南茜AI的社区。其实从官方的角度来说，他是希望把整个南茜的社区做成一个agent的一站式开发平台，那我们待会会去讲一讲他是怎么如何排列组合变成这样的一个开发平台。
	然后最关心的同学最关心的这个问题，同学们最关心的问题就是能线的这个新版本。有了这个LCEL作为核心，那么之前的这些chance就可能在未来的某个时间会被弃用，那么新版本到底带来了哪些不兼容的升级，以及原来的这些chance未来要怎么去使用它们。假设我们的代码里面已经有了这些chance，我要怎么去做迁移，这是一些部分的内容。然后这个新的技术站，整个年前的这个新的技术站，我们其实会在讲它的这个发展回顾，整个过程当中会给大家提到。然后我们会以next miss作为一个案例代表来讲这个新技术站。None graph和nine serve我们这里只是简单给大家提到，后面我们会用到的时候再去详细展开，就是这些内容。
	然后整个这个过程，其实我们都希望通过上半节课让大家了解到原庆社区它的未来规划其实是很清楚的，就是要从一个开源的SDK变成一个由一家商业化的公司主导的agent的一站式开发平台。而且这个开发平台，不是说由年轻人一个项目就能够完成的。有很多的年轻人生态的项目共同组成了这样的一个开发平台，这是上半节的内容。
	下半节课，我们希望能够去讲一下，这个南线的expression language，LC也要这个表达式语言，它到底是一个什么样的概念。然后LCEL作为年前0.2开始主推的新的开发范式，它到底是一个什么样的东西，我们要怎么去用它，然后如果用LCL能带来什么样的好处？在这个过程当中，我们就不得不提到LCEL，它能够实现这样一个表达式语言的最核心的底层是因为他做了一个round table这样的协议。这个协议使得LCEL这样的表达式语言能够在这个0.2开始成功的运行。
	它就像我们经常使用从这个unix的同学应该就或者说linux也类似，会有很多命令行的指令。那在指令里面我们会去使用管道操作服务。包括我们让ChatGPT生成的第一个agent的这个demo service，有一个控制守护进程的脚本。在这个脚本里面也有一些管道操作服务。然后我们在构建镜像的时候也有一个脚本。然后这个脚本里面去检查了我们的自动化测试的结果，也有管道操作。这些其实跟LCEL里面用到的这个操作服有异曲同工之妙。但它的实现原底层实现原理不一样，核心就是做一个输入输出的标准的通路的管道。
	然后用了LCEL之后，其实我们会发现它可以进一步的去，但跟下节课我们要讲的nine graph会有一些关联但是这节课我们不展开讲，ALC也要用管道之后，大家会发现以前我们去实现各种各样的chain的时候，需要去定义。比如说LM chain，这个router chain，或者说这个transform chain。这种我们在能线当中已经预定义好的这些券我们是能够开箱即用的。但是想要去进一步的调试，去二开或者去改进它就会显得有一些难受。它开箱即用，但是你想要再去对它进行深度改造是一个很痛苦的过程，并且你很难对它进行调试。那么有了LCL之后，其实会变成更简单的一个事儿。当然可能他的写法上大家一开始会有点难以接受但习惯了就会发现其实会停挺轻松的。
	我们待会儿也会去给大家看一下，怎么样用LCL来实现一个多链马蹄券的一个简单应用。然后所有的这些LCEL实现的代码，最终都可以通过next mix这个平台来监控每一次调用。这些调用的结果有了nh平台之后，大家可以在这个平台上做的工作就更多了也是因为有了这样的一个平台的定位，所以能欠的技术生态和社区就可以往一站式开发平台去。请进，最后我们在今天会有一个用LCEL来，就相当于用新的语法来让大家去快速构建一个RAG的这样的应用。这个是我们这一节课的主要内容。好，我们接下来看一看。
	首先我们来看一下这个南券的新技术生态年前到底是什么？像很多媒体讲的年前可能不行，或者各种各样的声音都出现了。但通过我们今天应该是第八节课了，整个八前面七节课的这个内容，我们实现了第一个agent。这个agent我们面向的是完全零基础的，也没有上过咱们的应用开发的这个战营的同学去做的设计。从某种层面上来说，是属于新手保护期，里面有很多基础的概念，我们都讲的非常的细。但是到了第二个agent和第三个agent的开发之后，我们是希望这个同学应该是具有一些内嵌的基础，然后这个大模型的基本的使用的这些基础。如果这些同学没有上过之前的课程，也没有关系，因为这些代码也都是开源的，我们待会儿也会去讲一讲这个应用开发营的代码它开源在哪个位置，如果你自己有这个信心，然后有兴趣可以去自己学习一下。如果实在不行，你就可以左转去再学一学那个基础的用开发实战营，这个就取决于自己的选择。
	但无论如何，我们后面的agent确实会用到一些相对来说比之前要更难一点的，或者说更复杂一点的agent的设计技巧了。所以涉及到南茜的一些特别基础的概念，我们就不太可能会在一点一点给大家去讲了。这个在之前的课程都讲过了，今天这节课属于一个过渡的课程。我们会把年前的基础模块这些核心概念都带到了，给大家讲一讲它有什么样的作用。后面大家可以根据自己的这个学习情况，再去回看那些代码，或者去复习那些课程都可以。
	好，说回来，年倩其实这一年的发展是非常迅速的。我们看到他可以非常自信的在自己的官方网站上贴出这样的一句标语，目前它就是一个最大的生成式AI的开源社区，每个月的下载量是1500万，然后支撑了10万加的应用，有75000个给他的star数量，都还没有更新。因为他现在已经超过了这个数量了，然后有超过3000人的contribute。所以这是一个非常庞大的AI的开源社区，我们可以再对比一下，跟我们去年这个时候，一年前的时候，我们刚刚给大家讲这个应用开发实战营的时候，我们有这样的一个截图，幸好接下来让大家能看到那个时间片段的状态。左边这幅图是二三年8月的时候，南茜刚刚从个人项目变成一个社区的项目的时候一个转化。大家能看到右边这幅图还是属于年倩的发起人的个人一下，后来就转到了能迁AI这个公司所拥有的开源社区下面，它的发展速度非常快。在一年前的二三年8月的时候，我们当时有预言它应该会超过py touch，就是我们在现在的大模型训练的时候使用的这个hugging face的这个transformer，是大模型训练的一个SDK。这个SDK的底层就用到了PyTorch和TensorFlow，是属于上一个时代的深度学习的训练框架。
	从二三年8月的那个状态，我们现在来回看，其实整个南欠的一开始发起的这个核心项目，可以说是初露锋芒，在追赶上一个时代。其实也就是几年前这帮在深度学习领域里面做到了事实标准的前辈们。他当时的定位也比较简单，在南茜的零点这个叉系列里面，刚好我们这幅截图也是去年的时候，在V0.0.251。大家可以看到这个截图里面有一个，那是他当时的一个版本。他当时的月度下载是400万。我们看到他最新的这个官网上面写的已经是400万的，将近四倍了，是1500万。所以他的关注度其实并没有像国内很多媒体讲的，就是他不行了。相反这一年他是有肉眼可见的增长。那么在零点叉系列的时候，其实它一共迭代了应该有380多个版本。我不确定它后续又没有发这方面的一个新版本，就0.0系列的。
	在那个时刻，它的定位是轿用组合模块和能力的抽象去扩展LLM的一个助手，就像他这里写的building applications with ll然后通过组合的能力，那组合的是什么呢？组合的就是它的这些模块，也就是右边我们看到的这些基础模块。包括model IO，data connection，chance memory agents, 还有所有框架通常都会实现的。
	Coby这个是南茜在去年这个时候的一个定位。从这个描述包括它的这个模块的抽象来看，其实那个时刻它的历史使命也好，它的整个生态定位也好，都非常的简洁。向上去帮助开发者快速去搭建一个大模型应用，现在叫agent的原型。向下对接各种各样的大模型及其大模型周边的技术生态工具。比如说我们用到的这个向量数据库，嵌入模型等等。
	这个是在0.0系列的时候，一个标准的SDK和开发框架的这其中最典型的抽象，也就是很经典的model IO。在我们的这个开发云课程里面其实讲了非常多了。待会儿我们也会以一个我们第一个agent，当时留了一个小口子，就是我们的用欧拉玛这种私有化的大模型去生成github的项目报告。其实我们在0.8版本的githa city里面的这个problem做的没有那么好啊，这也是拘留的一个尾巴，有的同学发现了他做的不好。那个其实是一个可选的作业，你可以自己去改。本身这个是一个很重要的学习过程，就怎么样去优化你的prot但他留在那儿，其实也是为了今天这节课的过渡。我们在待会儿也会去给大家看一看，0.8.1的这个欧拉a github项目进展的prompt是怎么样在优化的。
	其实里面就用到了一些model IO，就是年前最基础的抽象的一些思想。这个思想的核心就是把这个自然语言变成编程语言去使用和定义你的提示模板。就像我们左边看到的，这个是的一个提示模板里面有提示词，当然有一些词是用占位符表达的，比如说X和Y在真正使用的时候再去赋值，那之前我们用这个欧拉玛去生成的时候，它的标题时间周期老是搞得不好。但大家如果去看了0.8.1版本，其实我们就用了类似这样的占位符去表达。
	然后中间这部分是模型的抽象，大模型的抽象。在我们的这个第一个agent里面，因为我们没有使用连chan作为我们的开发框架，所以我们自己实现了一个LLM点PY就是LLM的module，然后在里面去调用了我们的OpenAI的模型，或者说我们欧拉这个私有化部署的一些模型。那这两条线在大模型内部，其实它已经自己去实现了这个model的抽这样了。并且也支持不同的接口，LLM这个接口，就像我们前面讲的欧拉玛的文本生成generate这样的一个接口，对应OpenAI就是它的completion API。那么下面这条check model就是以对话的方式去生成我们的内容，对应着咱们在github set这个agent里面使用到的欧拉玛的API，就是那个chat的API，通过message的方式去构造不同角色的信息，然后最后生成一个特定的结果。在OpenAI这一次的这个大模型，现在几乎都是chat model了，它的整个除了3.5的一个特定的指令微调的版本以外，其他的都清一色的改成了check model。
	最终的第三部分叫output power，就我们整个大模型输出的结果，我们希望它能够有一个特定的格式的要求。这个格式的要求除了在其实模板里面去约束它以外，你很难控制大模型的生成结果。尤其是你的temperature值设置的比较大的时候，就它的生成的多样性比较高的时候。那这个时候你要怎么去控制生成结果呢？除了这个前面，你也可以在后面去做文章，这个就是output puzzle的一个重要的抽象。整个这三部分组成了最经典的model IO的大模型抽象，分别是模型input output。
	这个我们稍微给大家科普一下，到了今年24年9月份，就八月底九月初的时候，年轻人怎么样呢？其实我们现在来看，年前的到此时此刻的一个发展，我自己的一个评价是它的发展非常的强劲，而且后来居上。这个后来居上主要是指他的这个关注度也好，star数量也好，其实已经超越了patch ch了。
	Tenor flow可能有点难，因为TensorFlow的历史时间非常长从15年的十月还是11月开始开源的，到现在已经超过八年了。但我们看到左边这个图的时候，其实还是挺有感触的。在去年这个时候，其实能的这个项目离patos还是有一些距离的。但今年在去年底今年初的时候，就已经超过了这个patch了。而且pyto CH并不是一个已经没落没有人关注的项目，它也在持续增长，它并不是说没人关注了。
	大家可以看到右边这幅图，就是我们align就是把这个时间线对齐之后，就他的从项目开源的第一天起，这个时间线对齐之后，我们能明显看得到这个能线的斜率，其实是前期增长很快，现在也持续在增长。虽然它的增长范。这个斜率没有像之前那么夸张了，但这个速度依然是很大的，是比pyt ch和这个tens flow的这个斜率要大一些的。所以这个是我们看到南线到现在为止并没有像客人讲的没人关注了。
	那它有一个什么样的转型呢？这一年的时间，整个年欠变成了年嵌AI的社区。这个社区的定位从原来的一个SDK变成了一个开发部署管理agent的一站式的平台。在他自己的官方网站上面也做出了这样的一个定义，他希望能够build run和manage各种各样的大模型的应用，并且不局限于agent。在他的自己的这个描述里面，是没有把agent作为一个最高优先级的描述。因为从我个人的这个理解来看也是如此，agent其实也只是一种大模型的应用形态，未来可能会有其他的形态出现，就包括文达老师提的这个agented workflows，它跟agent是一个什么关系？
	因为现在agent这个概念已经越来越务虚了，所以我们这儿简单讲一讲，接下来我们再来看它具体怎么落地，这个南茜的生态其实是非常繁荣的。从这一年的变化里面，我们看到它的整个技术生态的技术站，已经从原来的一个report变成了好多不同的项目。那其中我们如果去梳理它的技术发展脉络的话，我们会发现在一年前的那个叫做南茜的项目，现在其实已经变成了一堆我们叫github上面的一堆的版本，或者说不同功能的代码组合。具体来看，其实在这幅图里面，我们可以通过这个红框高亮的部分去理解原来的那个年券。
	因为我们之前在前面的几节课也讲过，因为原来的这个年券内部有大量的代码是向下对接各种各样的第三方或者说开源社区里面的大模型，向量数据库以及其他的一些嵌入模型等各种各样的组件。包括agent用到的各种图，比如说这个搜索引擎的API，这个arc p上面的loader等等。所以这些全部是跟其他开源社区里面的项目和第三方API平台对接的代码都被统一的放到了中间这一层，能劝community里面这个现在也是一个最发展最频繁最活跃的一个部分，但是这些部分的代码也都还在南茜这个githa同一个report下面。待会儿我们也可以去打开github给大家看一下对应的代码在哪。所以原来的一个项目变成了好几个不同的包代码包，但他们的源代码还是放在一起进行了管理。
	这样的好处是，第一，我们整个因为他们之间的关联度是非常高的，是要搭配着一起来使用。通常你不可能不导入一个模型直接去使用蓝茜。但是如果还是像这个0.0时代，让我们的代码全部一起去发版，我们现在学过了发版要做这个测试等等各种各样的知识。在我们上一节课讲0.7和0.8的githa c tenure的时候有提过。所以为了去减少我们的这个测试失败，以及我们在这个迭代过程当中，因为代码的体量越来越大。
	我们去review这些新提交的PR的时候，也不希望什么事都找这个人来看，也会阻塞这个项目迭代。所以他把源代码做了一些拆分。从功能层面上第三方的就单拎出去成为了community。并且在能欠community里面还有一些级别更高的community。就比如说我们的能欠的OpenAI，那这个就是一个单独的包。因为它的更新速度很快，并且它的用户体量也很大，它就没有放在一个就像others是属于南迁的community。其中有一些热点的committee社区就会被拎出来，包括像google的金马等等，奥拉马这些都有单独的包，然后core其实是我们下半节课会去讲的LCEL，就是我们的从0.2开始要主推的这个表达式语言的代码。在core这个部分，然后原来的各种各样的抽象，包括我们的chance agents，包括我们的检索的一些策略，放在了叫做年前没有后缀的这个包里面，最终他们都在不同的去做各自的发板集成测试。
	然后除了这些以外，还有一些其他的服务。比如说我们这儿看到的其他的开源项目，比如说我们看到这个能，他其实现在的定位比较尴尬，我也没有发，每次都叫他的时候都都还在观察这个项目。他一开始的设定其实是为了方便把能chain里面做好的原型变成一个rest API。但这部分其实是一个跟它整个生态系统里面的其他的模块比起来，它不是一个特别核心的技术，粘性也不高。就像大家看到阿尔玛可以把大模型变成一个API，然后南茜要把咱们的开发好的agents变成一个rest API，在它内部叫chance。所以其实有很多的框架都在做这个部署的工作。
	那能欠的核心其实一开始是做开发和集成其他的这个模型等等这样的一些功能的。所以这个部分我观察到的是它的优先级可能是远远低于我们看到的上面这个红框里面的n chain以及n smith和n graph这样的框架的。但它的作用是把咱们的这个已经写好的chance，或者说在0.2系列里面叫runabouts变成一个API。
	那么next miss，我们待会儿会单独去讲。它是一个需要你自己去注册的一个平台，然后这个平台非常还是挺好用的。你如果写了一些LCEL的代码去调用的话，它能够非常方便的看到你的整个调用过程。包括你在哪个环节用的耗时比较高，可以更方便的去优化和调试你的年轻人的代码。
	Nine graph是我们下节课会去给大家做个介绍。我把它定位成复杂agent的开发框架，它是基于LCL为基础去开发的，以这个图计算机里面的图的这种数据结构来定义我们复杂的agents之间的管理逻辑。然后next mix提供的功能，在右边这个竖条这里都有标注出来。除了监控调试评估以外，它其实还可以打标注，有playground还有数据集的一些管理。这我们就在用到的时候再去详细给大家讲。
	我们回过头来看中间红框的这三部分。我们曾经在第二节课的时候跟大家提过，我们要做get up set。就是因为有些项目它迭代太快了。就比如说一个月的时间，我们看到年前发了五六十个版本，这个其实就是我们在第一个agent头里面反复去关注过的年欠杠AI，这个group下面的名为的这个report的一个进展。这里我们就能看到像年前OpenAI，这个能fireworks，然后年前的这个p call，这都是属于单独的，它其实是在community组建这一层去做的一些包。但因为他自己本身迭代速度很快，他也有专业的maintenance reviewer去管理这些包。所以它能够自己独立的去发布，快速的去修复自己的一些问题，以及添加一些新的功能。所以只要你看到了年前杠，一个第三方的社区里面的这个名字的版本发布，其实它都不会影响年券的主要功能，它只影响它自己的这个作用域。
	第二类就是我们看到的能券CLI和能core，这些是属于年轻人内部的一些功能改进。CLI就是一个命令行工具，这个名字大家应该很熟了，这是很早以前年前自己做的一个命令行工具，但它现在的作用也较低了。然后能券core是我们刚刚提到的LCL的功能。所以大家看到很多0.2点叉的这个版本号，其实跟南圈core走的更近一些。这里的右上角是一个能圈core，0.2.26等等，是这样的一个版本分布。所大家有了一个对于他能签现在0.2来的这个命名也好，包的这个分布也好，有了一个系统性的了解之后，其实也就没有那么复杂。并且我们能看到这些不同的第三方的包在频繁的发布，说明大家就像他提到的3000个人的contributor，就整个开源社区其实对于它的生态建设是持一个积极拥抱的态度的。
	这个南京AI的社区既然在快速的迭代，那除了我们刚刚看到的南倩本身以外，这个南倩本身就或者说就叫南茜名字的这个项目，它它实现了LCEL。整个LCEL其实也是为了我们未来去实现这种复杂的生产级的agent的时候，去做的一些有有价值的一些设计。在左边这个图其实就展示了咱们看到的LCEL怎么去实现使用的，在下半年我们会去在代码层面上教大家。
	这里我们能看到有一些南茜的基础模块，这些都是属于南茜里面的基础模块。我们刚刚提到的这个prompt model，包括这个output puzzle，这是model IO的基础抽象。然后在model里面其实又分成了两类，就是我们的chat model和LLM。然后除了这些以外，还有一些跟数据处理相关的，跟检索相关的。这些都是属于能欠的基础模块。而这些基础模块有了LCL之后，这里就像一个管道一样，一个模块的输出变成另一个模块的输入。通过这个LCL这样的一些设定，就可以快速的把它原来的这些基础模块去做排列组合，去实现一个agent的原型。
	这个是0.2以来我们整个看到的南茜这个框架想要去推进的。因为逻辑很简单，做模块的这些contributor和社区非常多，那他只要能够去管理调度好这些模块，其实南线就会越来越茁壮的去做成长。那怎么样去调度他们，然后怎么样能够调度的过程当中还能衍生出一些更复杂的这些应用示例？所以就有了nine graph。
	右边这个图其实是我们看到的这个nine graph的一个事例。在这个事例里面，它通过一个计算机的图数据结构最关注的。大家可以看到这个start的这个end，这个是整个用number of实现agent的起始和结束的一个。你可以认为就是起点就从上面开始，然后去执行一些逻辑。终点就是如果我们做完了所有的事情之后，它就像一个判断结束的一个判断符一样。
	在没有这个number graph之前，我们知道要去实现一个agent，在在训练营里面我们去做过agent execute的原理解析，那其实需要一个提示策略的支持。然后在这个提示策略的支持里面，他用到了react这种基础理论，一个大模型的reasoning加上action的两种能力的结合。其中有一个结束条件的判断，我们做了一个源代码和伪代码的解读，要写一个死循环，这个死循环就是去判断我现在人或者用户给的这个任务，他到底有没有完成。如果判断最终完成了，我这个agent就停止工作。那显然这种表达能力是比较弱的，尤其是如果我们要深度去使用一个agent的时候。所以通过N的这样的标志符，或者说这个节点去控制agent的结束状态，就跟我们在开发应用程序的时候，我们都知道很多应用程序都有自己的生命周期的一个设定，要通过一个状态机来做调整，那这里也是做了一些类似的思路，我们到时候再讲到ng graph的时候，再给大家深入去介绍。所以有了na chain和nine graph这两个开发框架之后，其实它就能够左手去实现快速原型的搭建，右手去实现生产级复杂agent的一个实现。而这两个开发框架的底层都是LCL，LCEL就是我们的年轻人call，我不知道这个就描述清楚，然后这些A键做好之后，我们需要调试，调试就使用我们的next miss，所以是这样的一个三位一体的设计。
	除了刚刚我们提到的这些生态这个生态的项目，或者说它技术占领的项目以外，济南建言还做了一些额外的有趣的尝试，这里我简单列了一些，大家可以看到左边这个是我们把能欠这个项目移除之后的一个情况。大家的这个时间线不一样，是因为他们开源的时间不一样。右边这幅图是加上年券之后的一个状态。所以大家能看到加上年券之后，因为它自己本身star数量太高了，下面的这些差异度就看不到了。所以我们才会回看左边这幅图。左边这幅图里面像联券GS这个是GS版本的内嵌，对应着我们平时用的这个是python版本的能除了这个以外，还有一个叫open GPTS的一个项目，这个项目我们下面就有写，就是第四行open source effort，effort to create a similar experience to open ASGPTS and assistant API. 
	这个是能欠希望能够做一个去模拟OpenAI的这个GPT s和assistant API的一个体验。因为这部分GPT s和assistant API是只能基于ChatGPT去使用的，或者说基于GPT的API去使用的。而没有办法像这个南线里面对接其他大模型一样，是一套标准的接口下面替换模型无感知的一个设定。所以有了这样的一个项目，除了这个以外，还有一个很有意思的项目叫chat南欠，这个是他的发起人自己又做了一个项目，他的定位也很简单，能嵌power的。
	Check what focused on QA over the年券documents。所有人都在抱怨年前的代码迭代太快了，然后文档又感觉好凌乱，看不过来，那怎么办呢？这个check能签的其实是他做的一个个人项目。希望能够通过一个聊天机器人。这个聊天机器人的这个知识库就是能签的官方文档。然后你去向他提问，他来对你进行这个回答，做了一个这样的项目，包括这个nso。但大家能看到nso其实现在是这几个生态项目里star数量比较低的，它开源的时间其实还蛮长的，整个这条线能看得出来，而且很早就开源了右边这幅图里的这个时间轴越长，就它开源的时间越早，他现在已经被被后面的这些新的生态项目给赶超了。
	所以去观察一些这样的数据，其实对于你了解AI社区的规划，以及他长期的资源投入，是非常有意思的一个角度，并且应该也能得出一些相对正确的结论。这个是我们看到的年券。除了他的技术站以外，其实它外围的这个生态社区发展也是非常的快，而且也是有很多的关注度的。
	我们刚刚提到的这个nice miss是怎么回事呢？它其实在我们的这个年轻的技术站里面开始有看到。那幅图里面是把年欠的这个就我们左下角的这个年券给展开成了三层，有欠柯南茜和community。这里我们把它收缩起来去看南茜的整个一站式平台的一个定位的话。Nice miss其实是贯穿了所有的其他的生态项目的一个平台。这个平台它的主要作用就是用来研发、测试、评估、监控等等这样的一些作用。
	这个平台我们通过右边这个图其实比较有意思的能看出来。其实就是在这个过程当中，每一个人欠的调用其实都会涉及到我们这里看到的这些关键的阶段。而这些关键的阶段的数据，以前在没有nice miss之前，你可能自己也通过一些python的做法去打印日志，或者说去做call back，去在捞一些这个中间结果。有了next mix之后，这件事情就变得非常的简单了。那么具体怎么去用next miss下环节我们会给大家展示的。好，刚刚其实是我们回顾了一下南茜这一年的一个进展，包括我们了解到他从一个简单的项目变成了一个生态社区。而这个社区里面其实有很多的包括内部的技术栈和外围的一些项目在快速的迭代，并且有大量的contributor在积极建设这个社区。
	我们现在再回过头来看，这一年它的基础模块有没有发生一些本质的改变，我们要怎么样去理解这些基础模块？首先这幅图应该是很多同学都都比较熟悉了，尤其是开发营的同学，或者我们之前开营直播，应该也或多或少讲过这幅图。这个图其实是非常早期，应该是在南茜刚刚开园小几个月的时候出现的一幅图。大家可以想象这已经是一年半之前的可能的一个基础概念的图了。这个图到今天它的的逻辑也是依然适用的。
	整个这幅图我们看到它的左边红色的这个部分，其实是构建一个大模型的应用最关键的几个基础概念。大模型就不用说了，LLM memory就怎么样给我们的大模型应用记忆，包括短期的、长期的等等。然后prompt这个是我们大模型的输入大模型有了这个prompt才能知道你到底要让我干什么活，下面是这个agent，agent内部又有两个关键的抽象，一个叫tookey，就是我们可以给agent加的各种外挂，反正去调的各种工具。下面这个叫agent execute，这是agent的运行时。在0.2系列以来，这个就会被替代掉了，不再使用agent excute，是用LCEL的方式去实现。但无论如何，这四个抽象就可以去排列组合出来各种各样的agent应用。
	因为大家想象一下，我们要做一个基于文档的问答，就像刚刚提到的chat chatbot能一样，它其实本质上就是需要有这个文档，你需要去自己准备，然后对文档去进行知识库的一个建设，这个就标准的一些操作了。那做完之后，通过年欠去把我们的prompt和大模型调度起来。然后在过程当中，如果你要提升这个用户体验，你还可以用memory去进行一些记忆，比如说FAQ的实现，比如说历史的提问的一些引用，或者说其他的一些应用场景都有可能。然后最终他们这种应用形态，右边的这些绿色的各种各样的应用形态被统一又抽象成了一种类型，叫做agent。这个就是连券的一个最核心的思想。然后这个过程当中的券发挥的作用就是有这么多绿色红色的方块都是它的基础模块。那这些模块要怎么被连接起来呢？
	最早是一种很朴素的想法，就我可以通过这个定义了这个基础的一个券，然后层层的去做这个子类的派生，然后最终能够串起来，然后数据在这些链条当中流动。后来发现这样的形式它难以为继，未来如果要给这个欠增加新的功能，要不断的给这个鸡肋或者说这个子类去派生新的方法和这个属性，这就是一个不是特别好的做法。所以后来整个0.2系列主推的LCER就是把模块都做成了扁平化的，大家都是平权的模块。然后我通过排管道操作符来进行串联就可以了，这个连接不用串联，串联这个词有混淆，进行连接，待会儿我们再去看。
	这个模块是值得大家去去深入理解它背后的一些概念的。包括这里的每一个链条里面的数据处理，其实就对应着南茜里面的数据处理模块。这个图里面的模块就已经很就是我刚才提到的0.2版本想要表达的概念了。大家不再是有前后依赖关系的，以及你是在链条里面，我是在红色方块里面等等这样的一些差异化。大家都是一样的基础模块。只要是基础模块，任何模块它就有跟大象装进这个冰箱一样，就三件事儿，输入是什么，输出是什么？过程当中这个结构要做什么样的一些变化，这个其实就很好了，就相当于所有的模块都一样了。都一样之后才能用同样的一个操作服务去去编排他们。
	这个是年前0.2想要去做LCER的一个核心。待会儿我们看到这里不同的颜色，当然是指它的角色和它的功能设计有所不同。最左边的这个绿色的部分是我们刚刚就已经提过的model IO prompts，language models output poser，从input model output共同组成了这个经典的抽象。然后上面这一行，其实熟悉的同学就知道，这是我们RAG的常规操作，也是在我们之前的这个开发实战已经讲过的data connection，数据处理流能欠的数据处理流程的一个五步法所有的RAG几乎也都是按照这个逻辑去做处理的。然后右下角的红色的部分是agent的基础模块，就我们刚刚提过的agent，还有各种各样的运行时，然后有各种各样的外部工具可以调用。那memory有我们的记忆系统，chance就我们已经实现好的各种各样的开箱即用的这个链。那这样的一个图，其实我们回到0.0系列，再给大家去做一个简单的介绍。面对我们有一些没有上公开扮演的同学，刚刚提到的model IO这就不再赘述了。就对应着我们刚刚看到绿色那一部分三件套。但这个三件套在0.0系列去做了进一步的抽象，叫做LLM券，大家可以理解。
	欠就是一个有输入有模型的最基础的应用单元。因为你想象一下我们用大模型的时候，比如说欠了GPT，你要让他给你完成一个任务，就你给他一句话，他给你一个结果，这个结果可能你自己要后处理，也是拿到这个ChatGPT以外的环境里面去做后处理。所以一次输入，然后一次模型的生成结果，就构成了我们最早期去使用ChatGPT这样应用的一个单元。所以LM券其实就是最开始是符合这样的一个抽象的。它包含一个提示模板，但提示模板里面可以有一些input variable able，就是我们提到的一些占位符的变量，然后同时也包含一个大模型，这个模型是一个确定的模型。这个就像我们的第一个github的city里面的ripple generator，其实就是一个LM chain的功能等价的一个抽象report generator里面包含了LLM，也包含了针对不同的场景里面去生成generate报告的对应的prompt。这个props它是以TXT文件的方式，在需要的时候再对应加载。大家可以去理解对齐一下这个概念，我们的report generator包括他自己在构造这个report generator实例的时候会去传入的LLM的实例其实就构成了这样的一个抽象，这也是最简单的一个欠那最简单的一个堑构造好了之后，我们刚刚提到的memory，这个在南迁内部其实也有实现这个memory system。
	我们看这幅图就能理解刚刚的这个券是上面这个带有灰色虚线的部分三件套。这个三件套如果我们没有memory的时候，其实用户的每一次提问都是新问题。没有历史记录，没有聊天记录，也没有知识库等等各种各样的外部信息。用户给你什么，这就是你拿到的唯一输入了。但有了memory之后，其实我们可以把刚刚提到的那些没有的东西都作为我们输入的一部分，跟你的具体应用场景有关。
	而这个my memory是哪来的呢？通常就是要么从已经生成的答案里面来作为FAQ或者参考答案等等。然后这个参考答案当用户问到相关问题的时候，可以给他，要么就是这就是一个对话场景，我会把你之前说的话记下来。那当你继续跟我聊天的时候，我得把之前的对话内容也记下来传给你。那这个功能像对话的这个history的功能，现在绝大部分的工具都是现了年限实现了。包括我们用的radio，他的聊天的这个interface也实现了。所以这就是一个典型的memory system叠加我们的欠抽象做出来的一种应用形态，这些其实都是年前的0.0系列机就已经能够做好的这个能力了。
	这个是我们之前在讲连券的这个基础的时候，我们花了一些精力给大家去讲的原生的数据处理流。当然这个没有上过应用开发的同学也不用担心，因为下半节课我们就会去做一个RAT过一遍这五个流程，只不过我们用的是LCEL的方式去做的，然后里面最核心的就是后半部分不用LCL来定义。前面有一些组件，因为它不涉及，它还是可以用原来的组件去实现。整个这个流程就包含了从原数据加载到南茜里面，变成一个南茜的关键抽象document。然后这个document在经过特定的转换，比如说裁剪、过滤，删除一些不必要的内容，变成一个一个的可以用于我们这个向量数据库里面去存储的知识。但是向量数据库里面存的不是我们自然语言，而是各种各样的高维的矢量，那这个矢量要通过embedding的model去做一次嵌入，所以就变成了这样的高维的向量了。这个过程当中需要涉及到一些embedding model，然后存到向量数据库里，最终被我们检索出来去使用，这个是一个典型的年欠的原生数据流，到这儿为止，我们其实就把这里面的绝大部分的模块都已经讲到了。
	大家想象一下，除了右下角的agents和tools我们还没有讲以外，原生数据流对应着orange这个颜色，然后这个model Green对吧？有各种各样的chance。最简单的AM chain我们讲了，然后加上memory system，其实这些就是能欠0.2的这个基础模块，在零点这个系列就已经都有了。
	那agent我们会去怎么演进呢？首先我们看到这个原生的数据处理流，可以跟外部的很多跟数据相关的系统去做对接包括我们看到的各种各样的数据来源，这些数据来源在第一步data no这个data或者data的loader部分都可以通过年前内置已经实现的一百多种各种各样的loader去直接加载进来。加载进来之后，它也可以通过，我们开始也看过了，在这个能券的community这个部分有像packin，mivers等等各种各样的向量数据库的包。这些就对应着这里最右边的vector storage，就向量数据库也做了大量的集成。包括中间我们要到相关数据库，还需要做一次嵌入，各种各样的in bedding model，不管是在线的，还是你可以私有化去部署的，这些其实都是整个南茜的数据生态系统已经实现了的部分。并且这还是一年前的一个状态，现在的数字应该还要更新了，所以能欠的数据生态非常强。
	有了数据，有了刚刚的能力抽象之后，就可以进一步的去实现一个agent。这个agent大家看到就把基础能力给加在一起了，memory tools，各种各样的tools。然后这个action和plane主要是我们的提示策略的这个部分，然后就可以做出各种各样的agents。当然大模型的集成也很多，这是它的最开始的做南迁这个项目就已经在努力去集成的事情。所以这个是我们能看到的南迁去实现agent的时候，在早期的时候以这样的一个形态可以去调度各种各样的工具。但是这里明显我们会发现他还是在尝试去做各种分类，有memory，有tour，然后plan里面又有LLM，又有agents，然后策略里面去组合出action，这个会有一些限制，我不知道有多少同学这个深度去用过，你会发现他他整个南茜被诟病的一点就是过于抽象，这个抽象不是中文语境互联网上聊的抽象，而是就是指这个概念的抽象本身。这个大模型的各种各样的调用和它内部的一些实现就比较复杂，尤其是大模型的生成过程当中一些不稳定，或者说换了大模型之后，相同的提示词造成的一些问题，就很难调试。然后南倩他他的初衷当然是希望让你快速的能搭出一个原型来，然后能看到原型就能进一步去推动这个项目。
	但一旦涉及到调试这一环，或者说我想要更自由的去这个组合的时候，原来的零点叉系列就会因为有这个基础模块的一些分类，以及各种各样的原因，使得这件事情变得没有那么的灵活，模块之间也是有层次的那0.2的这个LCEL之后，其实是把所有的模块全部打散成同一级了。大家不会再有这种二级甚至三级的这个模块概念。大家都是基础的这个模块都能够被这个rung协议所支持。然后最终大家都可以通过管道操作符去传递数据，这个就让整个人欠的模块这一层的设计就变得很清爽，待会我们看到代码也能get到。那么有了这个年轻的agent x system之后，大家应该就能理解刚刚我们看到的这幅图的这个核心模块，是就补全了。咱们没有之前没有去学习，过年前的同学应该就能对这幅图的模块有一个彻底的了解了。这幅图的概念就是要平权，大都是同一级的模块。这些模块我们通过去了解它的发展历史，知道他们历史上其实一开始设计的时候是有不同的功能定位的。
	Model IO是第一层的一级公民，下面才有各种各样的prompt。Prompt下面还有第三级的公民，分成了prom tempt和这个chat from tempt。Model里面还分了LLM和chat model out of the color里面你可以有直接使用这个内置的各种各样的outpower。比如说做时间的这个output，还可以自定义。然后we travel这个大的或者说data connection这个一级的数据处理流的模块里面，原来下面会分这个loader，然后transformer in bedding，然后再可以再往下去细分，甚至模块之间还有一些交叉，这个我们就不展开了。然后agents two也是类似的。
	在原来的0.0系列里面，它的设计调用非常的复杂，模块之间的跨层次的一些调用难免出现。到了0.2之后，大家就都是一等公民，都是平权的。然后我们都按照一套统一的方式去输入输出数据。底层用同样的积累去做实现，那这件事情就变简单了，所以这个是关于我们它的一些基础概念，有一些典型的实现，这里就涉及到我们之前这个开发课的一些内容。大家可以通过右下角的这个代码，不是右下角的这个UIL去看一下我们的原始的代码。如果没有参加过开发的实战营的同学，这里是我们用年欠去实现了一个PDF的任意语言之间的电子书翻译工具，然后也是用radio做的前端界面，然后RAG当然是更典型的一类用连线去实现A整个的场景。
	整个RAG的提出，其实在南迁之前，没有特别方便的去实现它的框架。然后我记得在我刚刚开始用这个能签的时候，甚至都没有RAG这个概念，就是一个检索器加上大模型的应用。就像更早的时候我们看到的这个应用类型的描述里面叫QA overdog，大家没有想出很炫的名字，RAT这个名字就显然就是很很会讲故事的人提出来的那这个RAT的整个要去实现RAG的核心，其实就是依托能欠作为一个核心，然后去调用三类不同的技术站。一类叫evadne model，就我们刚刚看过的用来做数据的嵌入的。
	把我们原来人体的自然语言变成一个高维的向量。然后再把这个结果去跟向量数据库，现在我们叫知识库去做检索比对，找出有没有一些可以用的相似的结果。最终拿着这个向量数据库的检索结果，加上用户的提问一起给到大模型。大模型回给我们一个答案，这个答案最终再由南倩去做处理之后给到用户。
	这是一个典型的RAG的场景。这个场景里面其实你想象一下这八个步骤，我们如果用没用原来的这个方法去实现的话，其实也是可以做到的。比如说我们已经实现的这个房产销售的这个示例，也是我们在应用开发实际上已经你做过的那这个事例里面我们其实用到了很高层次的一些南线抽象，这个retire QA等等。它封装了这里的好几个步骤，就相当于他把里面的这个234567封装成了一个年限的开箱即用的抽象。然后最终我们去用它的时候，就是直接把这个抽象去跟用户对接，相当于把后边这一块全部给包起来了。这样做当然很好去搭建这个圆形的时候非常方便。
	但如果我们想要去，比如说我们想要去改造这个第四个步骤或者第六个步骤的时候，就会变得相对来说有点困难。那么用了LCEL之后，你就会发现每一个箭头，每一个步骤其实都可以变成平权的单独模块之间的这个逻辑的串联。用这个操作服务去实现，这个事儿就变得很轻松了。好，这个是关于咱们对于连欠的基础模块，以及它的一些最佳实践，给大家做一个简单的一个分享。
	第三个小节我们主要来看一看，也是很多人关心的年轻新版本的不兼容升级的一个问题。在0.2其实当时这个推出的时候有很多的声音，主要就是在看到各种各样的warning。就我们去运行原来的LM chain，运行原来的一些底层的这个链条的时候，会报的一些警告，这个警告其实在一开始南迁社区是比较激进的，甚至认为这些欠很快应该就会被移除掉了。在我们准备这个agents这个训练营的时候，我也看到了这个社区的各种讨论当时很激进的说九月份会发布这个更新的0.3版本。会彻底的遗弃掉原来这些欠。但现在实际来看也没有这样做，只是说未来可能会在某一天去把它移除掉我们原始的这些实现，而全部转用新的方式来做。
	但这个过程怎么样呢？经历过tensor fo w1.0和2.0的这个切换阵痛的同学，应该都是能理解我想表达的这个意思。因为整个像TensorFlow w这样成熟的框架，在切换到2.0的过程当中也遇到了非常多的阻力和困难。主要是因为好多代码都已经上到了生产环境了，包括python 2和python 3的切换也是以这个十年为单位才完成的。所以在这样大的一个用户群的基础上去做弃用，其实是有很大的困难的。大家想象一下，11年前这个时候是每个月400万次的包的下载，他的用户群其实是非常大的那他有没有确实的去移除一些东西呢？其实也有，但很少，跟大家想的可能不太一样。
	我们现在来看一下0.2的弃用和重大变更首先0.2提出了一个原则，叫集成无关原则，就需要保持这个集成无关。从0.2这个版本起来，开始他的说这个通俗一点的意思就是他不再会像以前一样做各种各样的自动实例化，自动构造了。然后用户如果要去用这些基础模块的时候，是需要显示的去传递。比如说我们要用大模型，要用嵌入模型，甚至要用其他的一些基础模块的时候，要把这个显示的去传进去。到底要用什么类，要用什么样的函数，要用什么样的参数？这里就涉及到了一些具体的变更，我也列出来了。当然如果你要看更详细的信息，这部分我也放了这个官方文档的UIL，就是这里提到的这个需要显示指定的就会有一些变化。
	如果你原来用到了我这里提到的一些module，那你升级到0.2之后你需要显示是的，去在这些相关的方法或者类上面去做一些显示指定，而不是像原来一样连茜去在背后就你可以理解成原来它会有默认的一些实例化，或者默认的函数的参数。那现在不行了，这些像下面列出来这些是不行了。然后有一个下面我们看到的这个network chain，这个是直接被移除掉了。
	原因是因为没人用，所以它被移除掉了也不会影响绝大部分用户，所以这些都是经过了社区在迭代过程当中广泛的一些投票，所以这个移除应该是没啥问题的，我也确实没有用过这个方法。然后显示指定的这个调整，其实你升级之后，你不会影响你的这个内部运行逻辑。只是需要你显示的把原来的他帮你去做的构造和实例化的部分，由你来进行这个显示的设定了。
	这个部分是我想要再额外提一句的。在今年1月份的时候，年前发布了V0.1，然后当时他就非常激进的提到了下面这些东西会在V0.2去移除掉，当然现在我们都知道了，0.2也发了，这些也没移除，所以我说这个是待观察的。这个具体的完整的这个链接其实已经不在年欠的菜单栏的一级列表里了，但是我还是把它捞出来了。如果你去关注年前的社区进展的话，应该会在今年农历新年的时候会前后会看到这样一个change log。当时影响范围极广，这个net bot chain，这里有一行note use，这个是确实被移除了在0.2。但是剩下的这些写着use LCL under the hood这些东西都还在WIP的状态，或者说在移除中。但具体什么时候移除没有提，大家有兴趣可以去观察，是一个非常长的列表，所以没有大家想象那么快就不能用原来的代码了。所以大家如果之前学过开发音的同学，那些代码你还可以继续去运行，没有太大的问题。
	然后整个代码的升级和适配，其实是一个非常长期的工作。我们如果看到这个人欠现在的官方文档，我们会发现有大量的是关于代码迁移适配的一级的说明。整个左边的这个菜单其实内容非常少，tutorials how to和看这个概念的一些指南，包括它生态系统的一些指导，其实都是属于非常高层次的一些概念了。但是在版本管理这一块，他花了很多一级的菜单去提迁移。他肯定是希望大家能够去做这个迁移升级的。但道阻且长，大家也不用过于焦虑这个事情。如果你原来的能嵌代码运行的好好的，你就继续让他跑着。然后如果你要写新的项目，实现新的一种，去看一看LCE这个是我现在用年前的一个方法和这个思路这个过程升级的这个过程。
	要升级之前肯定得回应一下，就是想一个问题，我干嘛要升级，对吧？我用的好好的，我为什么要去升级？前面我们粗浅的提过，就是原来的券有一些问题，这里我具体的去列出来了让大家了解。首先在这highlight一下，还light了一下，这个旧的实现仍然是被支持的，这也是官方的一个态度，在最新的这些文档里面，但是不推荐用于这个新的开发了，用LCEL和name来进行开发。LCEL这里其实已经可以等价于南茜的，原来在0.1系列里面的地位了。
	只不过，因为南茜现在已经变成了一个不只是一个项目，而是一个社区，一个公司，甚至是一种一站式的平台。所以整个官方也在逐步把能欠这个词的使用变得更谨慎，会把这种快速实现agent原型的这些东西，统一用南茜的expression language，也就是LCEL或者叫nana core来进行表达。这个大家如果去看相关的一些文档，会发现会有这样的一个变化。然后复杂的agent n graph来承担这样的一个职能，那LCE药同时也是nine graph的基础，在这个nine graph是在LCE要之上去构建的。
	那么LCEL的核心就是实现了这个running的接口，然后提供了一套通用的方法，这个通用的方法非常好用。就简单来说就是它底层用runway作为了这个管管道的基础。它只要实现了这个ronal的协议，就能够变成一个管道可以去连接的模块。这个模块大家想象一下，我去调用它的时候有好多种调用，化调用、批量调用、流式调用、异步调用。就这里的四个方法，任何调用的这个过程当中，其实都支持这四种实现。那那大家就统一了，这个也是作为一个开发的框架最应该做的事情。然后通过组合这些原语，原语的意思就是我们第一行看到的这个统一接口这些内容，它就能够更简单的去组合这个链条，然后去实现组件的并行。
	包括我们下半年会讲的这个market chain等等，然后也可以去动态的去做这个链条的配置，这个是好处。N graph就更厉害了，它比LCEL的强大就在于引入了图这种数据结构就能实现更多更更像是要把自然语言变成编程语言要去做的这些基础的功能。我们前面讲这个年前的时候讲过，router chain实现了类似于编程里面的条件判断，EF else sequential chain实现了面向过程的编程。现在我们看来sequential chain可以直接通过管道就可以去实现了。Rota chain里面是通过提示模板去实现了多种destination的由大模型来判断，现在的输入应该交给哪个destination chain来进行响应。
	这些逻辑其实用途都会更好去能够表达，包括像循环这种结构。以前可能是像我们去解读auto GPT源代码的时候去解读过它内部要通过go task，然后各种各样的comments，包括结束条件，agent finish等等去实现这个循环。现在有nine graph之后，图天然就能够去做这些结构上的设计。这是number of用来做复杂的agent开发的一个定位。所以原来的南茜想要去实现LCEL和n grape的功能，你说能不能做呢？肯定也能做，但是会让这个代码变得越来越不可维护。所以原始的这种欠的鸡肋就会被弃用掉了，而转而使用了现在这个框架。LCEL和这个nine grab的优势，我这边再整理出来了四条，大家有兴趣可以去看一看。
	然后在0.2系列，它就没有像我们刚刚看到的change log里面这么激进了，它有一个专门的文档去描述migrating chance这些将来会被遗弃的基础的chain，都可以用LCEL或者n graph来进行对应的实现。就比如说我们的刚刚提到的最典型的LM chain，包括这个对话conversation chain，这个检索用的这个retire QA等等。这些都条件判断用到的这个LM router chain，然后它内部去调了mart prom chain，这些其实都开始逐步的可以迁移过去了。
	比如说AM茜，如果我们在零点系列用这个欠的积累来实现的时候，是长左边这样子的，我们可以搂一眼LM嵌在这一行里面，用两个参数来进行构造来进行实例化。一个是大模型，一个是提示词。就跟我们开始看到那个结构一样，它需要两个关键的内容。那这像不像我们的这个report generator？其实也是类似的，只不过我们的report generator比他做的额外的多了一步。是我们可以在这个内部用它的时候再去选各种各样的proof。相当于一个更在m chain上面又套了几层，我们三种不同的报告那个层次才是对应的M这个大家可以去对齐一下概念。
	然后在这儿我们看到MCAN把prompt和这个chat OpenAI这里默认就是去调OpenAI的这个check model，然后把它们构造出来的一个m check，然后在实际使用的时候在实际使用的时候，再去传入一个特定的值，这个值就是我们其实模板里面去需要去占位服务，放置在这里的。比如说我们讲一个什么笑话，这里讲一个fn的job，这是一个输出结果，那下面是它对应的一个结果。这个是标准的用原来的这种AM chain的方式去做的。在右边是LCEL，用管道的方式去实现。管道的方式就更像是我们开始给大家看的那个模块图里面的内容。
	一个欠包含三部分，input model和output。所以我们先看这一行券，给它的这个赋值从prompt开始，然后是一个chain OpenAI，然后是一个string output power，对于字符串的一个后处理的一个模块。那这个操作服务就特别像我们看到的model IO的这个系统流程这个图了，具体的我们在下半节课再讲。
	今天内容有点多好，我们刚刚对它的这个升级其实有了一个更真实的了解，就是不是像大家理解的，原来的代码都不能用了，然后可能明天就用不了了，实际不是这样子的，有一个缓慢的长期迁移的过程，那年欠的新的技术栈有哪些？这儿我们开始在讲的过程当中已经提到了有这个LCEL nine graph作为开发侧的框架。Nine graph我会放在下节课去讲。
	然后next serve是一个把刚刚我们看到的各种各样的应用变成rest API的一个框架。它现在逐渐的脱离出这个南茜的主要的历史舞台了。在他的文档里面我们也看到了，并没有他的身影，就像我们这儿看到的这个，这里也只有我们eco system里面也只出现了nasmith和non graph。但是现在的年欠的官方的很多价格图里，偶尔也还会出现这个nso。当然最新的版本很多都没有了，在其他的一些历史的介绍的一些文章里面，都还有这个男色的身影。
	但长期来看，应该会被nine graph的这个收，这个nine graph对应的有一个cloud平台。可以看一下这个图，大家有一个了解。在在这个图里面大家应该能看到，nine grap有一个cloud，这个是用来做部署的。所以从这个层面来看，它已经开始去取代原来生态里面的newer的一个定位了。所以我们可以在静待观察。今天我们主要去给大家讲一讲n miss，就是作为他的这个技术新的这个技术在里面，我们在以前的课程里面可能讲的比较少。但nice miss作为我们跟LCEL配套去使中的一个监控的平台是非常重要的。
	那next miss是什么呢？其实smith我们在联券的这个公司的官网上有一个单独的对它的介绍，它是一个平台，这个平台的目标很直接，定位就是把你的大模型的应用从一个原型变成一个产品，面向生产级去设计你的大模型应用。所以我们一句话来理解的话，nice miss就是一个用于构建生产级大模型应用的平台。怎么样能够帮你去做这件事情呢？它会提供可视化的监控，评估你的原型的生成结果符不符合你的预期，然后生成的这个速度性能符合你的要求。然后他自己其实是一个独立运行的平台，是由南茜这家公司来提供的。然后核心功能有四个，tracing、monitoring、dataset evaluation. 
	最初级的功能其实就是traffic和monitor，就是记录追踪你的生成内容调用情况，然后记录一下你的各项指标等等。那评估需要由数据集的一个配合，然后整个这个平台它的访问地址大家能看到，就在这个smith到能签到com，它也是需要去创建一个APIK的，待会儿我们会去大概了解一下，这个是它的一个典型的界面。比如说我们有一个chat now chain这样的一个项目，就是我们开始提到过的，是由南茜的发起人自己做的一个boat chatbot，是用来问跟南茜文档相关的一些问题。这儿就有一个how do I use a这个recursive UIL loader。这是一个数据加载器，里面已经实现好的这个loader预定义的来去从网页当中加载，结果他AI有回答。然后左边是它的一个调用情况，整个AI生成这个结果花了5.13秒，消耗了5846个token，然后调用是成功的，内部有一些调用栈，fan dox，retrial check，chain with巴拉巴拉这样的一个模式。
	那我们要怎么用这个平台呢？首先你需要去注册，然后左下角是它的这个网址，这边再写一遍。经常用这个经常在开源社区玩的同学，通常都有谷歌的账号或者github账号。你也可以用这个直接去登录，都不用再去用email去注册了。
	注册好了之后，登录进来之后是一个这样的平台，里面有各种各样的功能。然后它的这个P是在左下角这里，通过点你的icon配置，可以去设置你的APIT。现在大家做大模型开发这么久，APIP肯定不陌生了，就是你干啥几乎都需要他。在环境变量里面去配置一下这个next miss的相关的一些设置，主要是它的一个tracing的这个版本，we true设置为true。你的刚刚申请好的这个nice miss的key，他这儿要取名叫年轻APIK，然后就填好上面只是你如果要用open I的话，你还需要有OpenAI的APIT。然后在整个你配置好之后，其实它是按项目来tracking，就是来跟踪的。
	比如说这里我们看到这是我昨天做这个测试，这里会写一共用这七天一共用了12000个token，然后消耗是0.0038美金，那这个为什么会还有钱呢？这是很正常的，做什么事情都没有免费的对吧？那怎么会有钱呢？你想象一下这个监控是怎么来的对吧？这个监控，就是你的调用结果，调用过程被发送到了s miss这里去，所以他才能拿到这些信息。然后发送过去之后，在那个平台有一个可视化的界面给你呈现出来，那么这个消耗的这个token，就你给他发送的这个token，其实就是他的一个收费的依据，按token数量来收费，比较好的是说，它有不同的收费计划。然后对这个大家有兴趣可以再自己去了解一下。如果咱们是真的要去开发一些agent的话，那smith是可以有效的去提升咱们的这个效率，就是去查看这个效率。
	好，以上是关于咱们第一个部分，就是我们的南茜现在的新的一个社区的一个进展，技术生态的一个演进，包括它的很关键的nice miss这样的一个平台是怎么样能够去引入到咱们的这个生产环境当中的那下下半节课我们会来讲LCEL的快速入门与实战。甚至也会教大家去看一看，比如说我配置的这个s miss又是怎么跟我的调用关联起来的。好，现在我们有时间比较晚了，我们回答三个问题，大家可以在这个评论区里提问。大家有什么问题吗？我们可以提三个问题。
	少了个R吗？我看一眼，可能复制的时候出的问题。是那个vector store，还真是vector store index creator。是的，它少了一个R应该是我复制过来说这儿这调格式的时候出的问题。你看这个flare chain还是大一个字号，我待会调一下再发给大家。谢谢大家，nice miss. 
	支持私有化部署吗？是这样的同学，他是个平台，它它是南茜维护的一个服务。你这儿是客户端，你配的key，然后你调用完之后是把你的这个中间结果发给他那个平台，然后他那个平台会去把这个结果可视化呈现出来给你。所以他肯定不支持私有化的部署的。但是这里有钱的同学就不一样，你可以考虑一下这个最后一档，就是为企业进行服务的。我印象当中好像是可以去谈的，但我没谈过是可以私有化去给你提供服务的。既然数据要发给他，那这你能看到他这有一个data的region，就是他的数据是放在哪个区域的，你可以下拉。因为我那台服务器就在北美，就在美国，所以我就选的是US，你也可以选不同的region。什么叫平权？平权就是大家都是一等公民了，不再存在于我们在模块的抽象上分成了不同的级别，然后出现了一些不必要的你的层次化的抽象之后，我下面的一些二级三级的模块，我其他的不能调用，我看不到那个意思。
	这个同学也提醒我了，我还真得去细看一下这个收费，待会儿我们看一下平台就知道了。因为目前我还没有给他花过钱，一直在用在这里。费用。
	cost. 
	951。
	是吗？这个我待会儿我们看一下。
	这个跳到平台上来。
	你看这个界面就是0.11系列的文档生态里面还会有nor在这个eco system里面，然后names，还有这个deployment这样的一席之地，当你跳到0.2就没了，稍等我这个网有点小问题。
	然后在这个地方大家能看到products，它的产品分成三个跳到smith之后，应该是他的。收费，这个是他新的数据，2亿的。然后收费是在。
	私有化的sso这里对我印象当中刚刚同学问nice miss支不支持私有化？这一档可以的，企业级里面可以sell posted deployment m adoptions，这个只有这一档有，但这个是需要你填一个表单给到南茜的团队的。我印象当中我填过这个next serve的，然后就一直让我在等，后来就等着这个就没了。对，然后next mix这个我没填过，可以去看一看。然后我填过这个start，这个我填过，然后下面会有详细的各个版本的一些他能开放的功能。
	然后收费是啊对1000个base stress是5毛，确实是按这个来收的。然后看有没有关于token的，没有，那那他还是比较良心的对，没有按token来收。对，这个我们澄清一下。好。
	Smith这下赚嗨了，也没有？这个同学我们能算个账的。他这个是一千条才五毛钱吗？他现在好像刚刚有个数据是两个亿，是吧？两个亿，但还不到20亿，差5倍到10亿条tress，也就是现在是20万倍的。20万乘1000就等于这个数，所以就20万乘上0.5，也就是10万美金才赚了。对于他来说十万美金太少了，他的truss的费用才10万美金，而且你要是有一些别的什么鼓励计划什么的，估计就更少，没有赚很多钱的，跟大家想的不一样。
	监控如何分析？可以讲讲你的经验。这个我们后面会再具体开发一件的的时候再给大家讲，这个大家不用着急。
	对，因为nice miss绝对是我们要去讲的，也是一个开发debug的重点。好，那我们接着往后了，我看这个时间有点久了，所以今天可能会讲两个半小时，我们先往后面的内容来，因为大家知道这个nice mix是大概什么样的一个东西和它定位，那接着我们再来看一下LLCL。就我们看到的这个LCEL这个下半节课。
	好，那么下面我们来讲一讲能欠的表达式语言的快速入门。实战讲快速入门是因为后面的agent肯定也都会用到它，也不是说今天讲完就不讲了。但是对于之前没有上过课的同学，我们还是需要有一个偏科普的一些基础的代码让大家能上手的。然后这部分代码也都在我们的应用开发实战营里面，也就是OpenAI的quick star这个课程项目里面。待会儿我们会给大家看一看，那LCEL它到底这个是个啥东西，我们要怎么去理解它，这儿我是把代码当中的一些关键的，我把文档都写到了这个就拍里面，方便大家去运行代码的时候也能看到这些文档。但是我们也把它放到课件里，给大家能整体先讲一讲，这样你看课件的时候也能去做复习。
	LCEL其实是整个0.2的发放是主推的一种形态，它是一个声明式的语言是一种声明式的链式组合的语言。然后提供了一个统一的接口，这个接口就是round able接口。然后有了这个统一的接口之后，不同的组件，就我们开始看到那一圈组件，都可以用这个round able来连接连起来。然后像我们看到的invoke单次调用流式batch批量就可以通过这个type，就这个管道操作符在这个位置，这个数线，管道臭豆腐去连接起来。
	它的优势三部分来描述。一个就是接口统一了，大家所有的组件都实现了reliable，这个接口数据的流通就更简单了。第二是模块化，所有的组件都可以独立的去开发和测试，只要你的管道操作符能够去描述出来它的顺序，那就可以按照这个顺序去执行，聪明的同学就知道，那你只能顺序去执行，所以我们才会了解到后面有nine graph，可以去做分支，做更复杂的这个结构。第三个就是它有可扩展性，因为所有的这个roundtable都实现了相同的这套invoke stream batch，包括这个A系列的异步方法，所以它的可扩展性是非常强的。然后整个过程当中，我们待会儿会给大家看一看具体的这些组件要怎么去使用。
	这里我们还是以model IO这个最基础的作为理解。现在我们再来看model，应该就不陌生。这个课件里已经出现了三次了，那LCL去对model YO怎么处理呢？之前我们看到的M7是做了一个新的类，这个类由我们的prom template和我们的model来进行构造，然后就定义或者说实例化的一个M券。那pipe就是说你们也别再不断的去打包了，就相当于各种像乐高一样的。他其实这个也不能用乐高来比，就是你不要再造一个新概念，去把原来的最基础模块去给我凑到一起。然后再造一个概念，又不断的像这个套娃一样的去去套了。
	所有的模块都是直接通过pip去连接的，不要再去做套娃了，就用type来表达就好了。数据也都是能够只要实现了我就能流通的。所以用pipe管道操作符来定义的时候，其实就把这三个组件，这是三个基础的组件，用pipe进行连接，砍成了三段，而这个三段砍的这一下，就是比如说我们这个prom template，它的输出在管道这儿接收到了。然后这个管道接收到之后，就把它作为我们它下游的这个model的输入。然后model的输出就会变成它下游output poser的输入，就这样的一个模式。代码层面上来理解，其实就是这样的。
	这个竖线的这个type操作符是一个基础操作符，大家如果没用过的话也很正常，确实比较少见。在排场里面的集合，就我们用的集合也是支持这个操作符的。但跟这儿的概念略有不同，在我们聊LCEL这个场景下，它就是特定的去连接我们的技术组件的一个操作图。大家去这样理解，我们看到这个操作服务其实就把刚刚那个图叠加过来了。
	Prompt type model type of the part构成了一个chain。那在这里面我们看到要去实现一个LM chain，用LCL来实现的话，其实就是把这三个组件实例化做好。大家还记得LCEL我们讲过它的这个版本的变更有一个很重要的事情，就是减少不必要的自动化的填充数据就连线框架不必要的实例化就不帮你做了。你要显示的去做各种各样的实例化，尤其是基础组件这个级别的。然后这些实例化的组件就能够通过pipe去连接起来了。所以我们再看这个代码，就是一共有两个管道，三个组件分别实例化出来，再有两个管道把它们接起来，那就完成了一个AMT的定义。
	我们看到这儿有四行代码，model的初始化，model的实例化，指定使用GPT4o mini prompt。这里讲一个关于特定的topic的笑话，这topic是在真正去调用它的时候再传入的。然后初始化一个输出的auto part，这里用南茜内部实现的一个基础的对于字符串处理的一个out to partner叫string out power。把这三个已经实力画好的对象串起来就构成了一个圈。
	这个chain我们刚刚讲到了，首先这个prompt model和output poser他们都是基础组件。基础组件都已经是符合这个，或者说已经是支持这个running的接口了。Round table的接口实现了各种各样的方法invoke这个batch stream。所以这个管道你一个是支持roundtable的，串起来之后它也还是支持round able的。所以最终这个chain也都是可以用runners的那套接口去实现调用的那这样组合出来的东西也都可以去轻松的实现单次批量流式异步的调用了。所以最后的这个欠也可以使用in work方法，然后再传入这个topic的具体的值，这里就生成了一个运行结果，这个逻辑应该是清晰的。这个in work方法就是我刚刚提到的，它是整个LCU当中的一个重要的方法，也是大家用的最多的方法。它是一个单次调用，然后所有的这个组件也都可以使用于word方法，因为它是实现在running这一层的。
	那我们再细看一下round table是什么，这个算是LCEL的核心了，为了我们能够去方便的排列组合，就是要有一个管道。管道我们直观的人来理解，就是这管道最核心的是什么呢？一个管道能够运行的最核心的就是大家接口是一样的。管道可以有粗有细，但是它一定是能接上的。你这个尼康的单反和这个佳能的单反，我不知道现在能不能接上，以前应该是没法雇用的那这就不是一个好的接口，对吧？但是有转接器可以实现。
	所以任何要去聊这个管道也好，或者说聊这个LCL的round table也好，要去理解它它底层这个协议的设计原理，或者说它的设计哲学是用来干嘛的。就是让所有的连线可以去排列组合，去调度的这些组件都变成可以互相直接对接的一种实例。然后这个round able接口，因为它本身去实现了标准的这些调用方法，就我刚才我们再来重复一遍，这个流式的、单次的、批量的，包括异步的，因为它实现了这些东西，所以是基于它去实现的。这些组件也都可以排列符合了，这个是最核心的一个逻辑和理念。
	然后那他为什么能够通过这个round table协议去去对接上不同的输入输出类型的？因为理论上这些组件功能是不一样的，所以他们实例化之后，他们输入和输出的结果应该也都是不一样的。他为什么可以呢？
	这就要提到它里面的一个实现原理。Round able简单提一提，就是所有的runner's，它需要去针对不同的组件去显示的描述它的输入和输出的schema，就是结构输入是一个schema，输出也是一个schema。然后通过这个pandey，这是一个你可以理解成python是一个弱类型的语言。
	在一开始的时候，后来因为python的这个用户群体越来越大用python的研发越来越多。Python已经从一个胶水语言脚本也变成了一个可以做越来越多事情的编程语言。最典型的就是做这个从一开始做数据分析，各种囊派pandas meta proto lab，到后来深度学习，到现在我们看大模型的年轻用户群体。基础好，所以python要解决的问题范围也就越来越大了，以至于它不再能只是一个弱类型的语言了。
	因为弱类型语言虽然它写起来方便，但是运行起来会有一些不必要的麻烦。所以就有很多对python的数据类型去提供一些特定的检查，甚至校验的一些功能出现了。包括像python 3后面的版本也是可以引入type，然后我们可以去定义函数的输入参数是什么类型，返回结果是什么类型。
	Pandects就是一个用来干这个事情的库，他自己也有V一和V2两个版本。然后很不幸的一个事情就是南茜为什么升级这么痛苦？还有一个背后的原因是因为南茜非常早期就引入了pandects这个模型。那pandey的V1。和V2是不兼容升级的，这个你去深挖就能知道一些原理了。就是pandey v一和V2不兼容，然后南茜的零点叉用了pandects v1，然后它的0.1的中期开始到0.2用的是pandects v2，然后这俩是不兼容的，所以里面有很多然后这个玩意儿又是用来检查round able的输入输出的。所以你可以想象一下这个过程当中有多少恶心的事情是需要去被处理好的，所以还是那句结论，就是这个年券的升级适配是一个长期工作。
	然后对于我们的研发的最好的指导意见，就是老的代码已经在稳定运行的，先继续运行着，不用去折腾人欠的的升级适配，然后新的代码用LCEL这一套全新的库来做实现，就这样就OK了，不用去去想太多别的东西。然后这个input schema是怎么回事呢？其实我们在在这儿能看得到它的结构我们构造一个LM chain的这个不带out to part的实现。对，这其实是最原始的M千。就是如果我们回到零点叉的这个设定里面，MM圈的默认参数就长这样的，它也没有带out趴着，当然它是可以支持可选的参数去设定一个out of the puzo的那现在这儿假设我们定义了一个prompt加model构成的欠，然后这个chain它的输入就是prompt的。这个组件的输入输出是model这个组件的输出，中间结果通过管道连起来了。
	好，那现在我们再看一下这个schema，就chain input的这个schema，就我们这个prompt的输入是长这样的，下面这个是prom输入，然后欠的输入是长这样的，他们俩是完全一样的。当然大家想象一下，你一个链条的最前面的输入跟当中的这个管道的第一节的输入肯定是一样的。因为它就是一样的，那逻辑上也是这样的，所以chain的output schema和model的这个output schema肯定也是长一样的，这个大家可以去做试验，我就不去讲了。然后通过这样的一种方式去序序列化或者结构化的去定义了每个组件输出长什么样，它才能串起来让它能做转化，这样的一个逻辑去做的基层实现。
	通过这些部分的讲解，我相信大家应该有一个初步的概念了。就是所有的组件都要去实现runners的接口，然后大家能够去无限的去做串联的原因是因为round able的这个接口协议，它实现了对于每个组件的输入输出的描述。然后这个输入输出的描述又是符合pandev的这个模型的。这个模型会去做类型的检查，如果你出了错误，他会报错。所以通过这种方式去完成了所有组件的一个自由连接，可以去轻松的去做这个模块化的一个扩展。
	好，那么假设我们要去实现一个market change，比如说用这个LCL去实现。之前我们想在这个rotor或者其他的时刻，我们讲过这个蓝天的时候，去提过有一些场景我们输入了一个用户的输入。然后我们可能有不同的逻辑来进行处理，或者说有不同的决策。这个mark chain的这个欠本身层次不一样，可以处理的复杂逻辑也不太一样，可以是两个欠甚至是两个agent。那么如果是nine graph，这里就可以实现agent一和agent 2。那如果没有用nine graph还是仅仅使用了LCEL，可能就用两个千会简单一些，因为它没有更高的抽象，然后假设这里我们想要实现一个market给大家打个样，用这个LCEL实现多个链的这样的一种像编程的分支逻辑一样的的应用，应该怎么做？
	比如说我们在这儿去展示了一个示例，比如像这个辩论赛一样有一个场景，我们是针对一个辩题，比如说输入一个topic，然后由一个券去先产生一个评价，一个原始观点。你可以想象成就是微博上有一个超话，一开始有个大V发了一个原始观点，下面的粉丝就分成两波人，一波人会对这个原始观点给出正面的评价和论述，11拨人就会对他进行一个反面的论述。最终这个大V要总结一下，看到他自己的观点和下面2拨人的一个讨论，最终他得出一个结论，这个就是我们会给大家的一个case study，一个事例。
	具体怎么做呢？要实现这样的一个逻辑，大家可以想象一下，如果我们自己写python的代码来做，可能就是一堆的ef else，然后再去串联。但现在这样的一个看着像是图结构的，其实用LCEL的派也是能够实现的。只不过核心是那个分叉，那个分叉怎么做？这里有一个很重要的抽象一个类，叫一个可以用的实力，这个叫做runners pass through through pay through就是把一个输出同时给到多个后继电，有点像是一个增强版的LLM的router chain里面去调用的那个marty from的那个签，所以就这里就有一个LCL已经实现好的一对N的单一输入多输出的这么一个抽象。
	你可以想象就是有一个管道，一个水管。我印象当中在给排水这个专业里面应该有个专业的构建叫三通，就是一个输入两个输出，就是三个口的这个派输入就很像。只不过它甚至可以实现四通、五通、六通，就一个输入后面接一堆都可以，这是它的一个功能。
	那具体怎么做呢？我们看到这个就实现了完整的一个功能整个过程其实涉及到了几个阶段。第一个阶段是原始的观点，要生成一个大V要发的观点。但是我们在这里去做了一个实现，这个planner就是基于一个论一个话题生成一些论点。然后这就是一个标准的，我们前面看过的这个prompt LLM加上output poser构成的一个管道。只不过这个管道最后它不是直接输出一个字符串，而是在管道最后又接了一个叫做runners pass through这样的一个，就相当于这个管道又加了一节，不是直接把这个水像水龙头一样流出来给你了，而是这个水下面又接了一个特殊的一个组件，就是这个round world pass through。那这个趴数它是可以把原来原来的输出结果发给多个后面的组件，当然我们一定得把这个概念和这个弯给绕过来，这里的每一个管道连接的组件，他们可以组合出一个欠，这个也是等价于这个组件的。它们本质上没有区别，就你一个单一一个管道的一节和主和一堆祖传的一个。
	比如说这个10米长的一个管道和一个单独的一节管道，本质上是一回事。因为他们都是符合这个round able的设计的，所以我们看到这个定义好的planner，它本质上它的输入就是这个prompt的输入，它的输出就是这个round able pass through。所以它的输出也是可以接到一个新的很长的管道里，这个很长的管道就是我们要去做的正方和反方的这个逻辑了。
	正方跟反方内部怎么做呢？其实也是拿到一个特定的观点，然后针对他去生成一个正面的评价和一个负面的评价。所以我们看到这个argument for和这个argued guts的against，其实就正反面他们的输入也是一个提示模板。这个提示模板里面的提示词是列出关于什么东西的正面或有利的方面。
	这里有个占位符，这占位符就是前面的planner输出的结果，像不像我们在应用开发里面做的这个sequential欠欠是2个LM chain的串联。然后第一个LM chain是生成了一个应该是syntaxes，一个我不知道中文叫啥，这应该是戏剧的简介。然后第2个IM chain叫review chain，它的实物就是synapses。然后他自己要去评价这个sync生成一个review。然后这个其实就是location，就是一个固定的模式。然后我们还学过了有simple sequent键是单输入单输出，这个更通用的sequences是多输入多输出。但是你必须得按照他那套定义来做。现在有了管道操作符之后，你想一想你可以任意定义这个欠，然后大家也可以长得不一样。你可以一个前面类似于AM欠的东西，后面再接俩LM线都可以，那就很轻松的能实现各种各样的逻辑处理了，所以我们这里的planner去接了两条不同的处理链条，然后最终我们还得把它汇总。
	你想象一下这个汇总的过程是怎么回事。其实就是我们还是用水管。大家去想象第一个planner输出的结果，如果没有这个round able pass through，这结果就直接给你了。但现在他接了个管道，这个管道是像三通的管道一样，下面会有两条水流，一条留给argument for，一条留给arguments against。那这两条水流在各自的这个管道里面处理完之后，输出的是什么呢？输出的就是这个string out of the part，他俩是直接就把这个水流出来了，他俩没有再去接running part，所以你可以直接在这两个链的这个结论上，最终输出上拿到标准的python字符串。然后这个标准的python字符串是可以给这个final responder，就我们最终要合并的这个。
	就最终处理的这个意见，作为他的输入的他的输入是什么呢？他的输入当然也是一个以提示模板开头的一个管道。不过这个管道它一开始就需要多个input variable，用我们的各种各样的输入的应该叫什么？输入的这个占位符，在标准的连线里面就是它的输入变量，就是我们最前面看到的那些XY，在真正去在使用它的时候，再给它传入真实的值，就像我们的github的这个set nail的日期，这个项目名称一样的概念。他就把前面这三个banana 4和这个arguments against输出的结果，就直接依次给到我们这儿的正面观点、反面观点和这个original的response，最终他给出一个最终的回应，这个最终的回应是由这里定义好的chat OK chat OpenAI来的，这是这个string out of the puzzle，所以这里就定义好了最终的这个处理链条了。
	这个最终的处理链条，我们看到一个完整的这个就我们运行它的话，如果没有最下面这一行代码，就得分别去依次执行。Planner的这个involve arguments for的这个involve，然后这个arguments against the，并且我们还得想办法让这个planner和这俩能串在一起。那怎么串呢？最好的方式就是按照现在我们这看这个方式去串。Planner是作为就相当于管道再再接一接，就把我们前面做好的planner这一节管道接上了。这个arguments for和这个arguments against接上了之后，然后同时这有一个新的用法，就是去取了这个base的这个response。
	就它的第三条，就是大家想象一下，我们最终的这个final responder是需要三个输入的，就是原始的这个N的观点和AI生成的正面观点和反面的观点。所以我们还需要去构造一个original response，这个就跟咱们的final responder的这个input variable的key就对应起来了。所以中间这一环就是构造了这样的一个key value。然后这个results one，它的这个value是由arguments for输出的，因为输出的就是一个stream，arguments for执行完输出的是一个stream。The stream的这个输出结果就是这个result，这个key的value，类似的这个original response的这个值，是从我们的planner的这个base response，也就这儿去获取的。通过这个方式，把这三个最终的final responder需要的输入都拿到了，然后就可以了得到了这个final response的生成结果。那final responder生成结果也是一个字符串，最后我们要运行的其实是这个欠房地产低迷。
	这个房地产低迷这一条丢进去之后，大家想象一下调用了几次大模型，其实很好数，就是这里123444大模型，这个低迷先去生成了一个原始观点，然后又分别生成了正面和反面观点，然后最终生成了一个最终回应，所以调用了四次大模型，生成了下面这段话。全球经济这里也是一样的，用流式输出的方式生成了这样的一个结果，待会我们也会来实地跑一下这个代码，大家学习一下。好，刚刚这些掉了这么多次，对吧？我们想看到内部的结果怎么办呢？当然用nice miss也是可以监控的，我们可以用nice miss来进行这个监控，到时候大家可以去看一看，然后打开一个特定的，比如说这儿的rn able sequence，我问他什么是TOT，他这边就会展示一次这个trace，这就是一次的trace，这里每一行就是一次。他收费的单元五毛钱是1000次，对，0.5美金1000次内部有一些不同的耗时1.47秒消耗了985个token，然后开始时间结束时间这个差值就是它的这个消耗。然后有下面状态这个token，然后时延1.47秒，这样的一个顺利。
	好，这个是LCEL，我们学习了它的它是一个什么样的定位，然后它能够实现它的核心是因为runners这个协议让让大家能够把输入输出通过这个type操作符，type这个竖线操作符能够连接起来。那原来的所有组件就能够真正的实现自由的排列组合。然后通过s miss我们可以去监控查看一下这个操作服务内部的各种各样的调用逻辑是怎么回事。然后最后我们再来做一个实战，就是怎么样用LCEL的方式去实现一个RAG的应用。这就是我们这个小节要讲的内容。这部分的这个代码也都在OpenAI的quick star这个课程项目里了，待会我们去实操一下。然后这个事例其实是很有参考意义的。然后我也写了非常多的文档在这个代码里面，我们在这儿也快速过一下这个文档在那个周拍book里面，大家值得去好好读一读。
	我先带大家看一遍，就这个RAG的这个开发指南一共把这个实现RAG的方法分成了五个步骤。这五个步骤也对应着我们今天讲过多次的检索的五个步骤，load transform in embedding，然后加上这个存到这个Victor doors、Victor base、Victor store base里面然后我们的这个检索，最后是生成这五个关键的步骤。然后这五个关键的步骤里面也用到了很多年欠的基础概念，我这儿也就圈出来了，然后我们要去做一个RAT需要有知识库。这个知识库我们这儿选了一个线上的文档，就是LLM power的自主的智能体。然后这周KI的很厉害的一个人研发工程师，我们在第一节课的时候也提过他的一个观点，就是他关于agents的四大核心能力的一个观点，也是这个历练问的。大家有兴趣可以再翻译一下原来的课件，然后我们会从它的线上博客里面去录的文档，所以第一步加载文档，用这个wave base loader就可以去加载文档。加载文档之后，我们把加载好的文档转换成一个能欠的标准的document的对象。这个是所有的能欠的数据，最终只要去进行向量数据库的处理，都会转成这个基础对象。
	那这个document的对象是一个很长的博客，没有办法去把它整体存入到向量数据库里，因为这不方便我们检索一个博客，它有很多不同的章节，不同的段落表达了不同的含义，所以我们要通常对它进行一个transform。最常见的transform就是分割，我们通过这个recursive的character text split，这是非常经常会使用到的一个文本分割器，然后用它来进行分割，然后分割完之后，就变成了一段一段的内容。这一段一段的内容，我们使用克罗玛这个向量数据库在内存当中去进行存储。然后用它来进行用刚刚切割好的内容的嵌入之后的向量来建这个向量数据库，这里就会用这个from documents方法，然后建好的这个向量数据库，就是我我们这里看到的，建好这个向量数据库叫vector store这个vector store要检索的话，还要去使用这个检索器，因为我们待会儿看代码的时候再深入给大家讲。那所有的向量数据库在南区里面都是vector store的派森类。然后检索器，都是可以派生至vector store recovered这个类。然后所有的向量数据库都有一个方法叫as we travel，这个是在vector store的鸡肋里面就已经定义好的。所以你只要构建好了一个向量数据库，然后at一下之后，你就能获取到一个去检索这个向量数据库的检索器。
	当然你在ads的过程当中还要设定一下你到底要怎么检索它，用什么搜索方法，用什么特定的参数。比如说这里就是top 6，然后这里我们打印了这个检索器能看到它是这个vector stores base vector store recovered。而刚刚的咱们的这个coloma，它是属于vector stores的抽象，待会儿我们看代码能看的比较详细。那最终我们要还要生成一个回答。生成一个回答就是向量数据库的检索出来结果加上用户的问题合到一起，然后给到我们的这个大模型，大模型再来生成，所以这里会用到的一些代码的抽象我都列出来了，这里是每一个步骤都写的很详细。
	就算咱们没有学过之前的这个大模型的应用开发。13亿的同学去好好看一下这个文档，然后再跟g GPT去交流，或者去看一看我们的这个实战营的代码。应该也都能够理解这个应用的逻辑以及它实现的原理，然后这个能欠hub这里有个外提一嘴，就是我们其实看到了整个技术站里面有一个叫南茜template，就是之前一个技术站的架构图里面有个能当时这个n temp在0.1发布之后，就今年Q1的时候，24年一季度的时候，这个template还是希望说做一个提示词模板开箱即用的。现在这个部分的功能已经被集成到nice miss。所以我们看这URL也能看得出来，在nice miss上面有一个hub的这个模块。这个模块就是一个提示词模板的开源社区，都叫hub，这个hugging face hub，get hub, next hub，我们的课程agent hub对吧？那么smith能chain点com然后hub上面有各种各样的，其实smooth是可以去用的，可以跟我们今天的homework有关。
	然后我们这儿就下载了一个RAG的prompt，就是它的这段prompt，这里有两个input variable，分别是问题和上下文。这里有一段描述我就不念了。然后要去实现一个RAT要用什么呢？要有一个大模型用来生成回答，还需要有一段提示词用来整理这个用户的问题，以及用户的问题检索出来的就下面数据库里检索出来的上下文，以及由大模型来结合这两部分内容生成一个答案。所以还需要一个prompt，我们把这个这儿我们先跳过，待会儿再看代码，把这个IG线给大家看一眼。
	长这样的，它需要一开始的输入是三部分，这个应该是两部分是错两部分。一部分是这个context和这个question，这是跟我们刚刚看到的提示模板有关的，就我们的提示模板里面是有这两个input label需要填进去，那填进去具体怎么填呢？这儿就涉及到了一个操作符的使用。我看到这个retrained l是我们在在chroma上面造出来的这个检索器。这个检索器会去检索一些相关的问题。然后这个问题最终还会需要把输入的结果组拼接在一起，变成一个上下文。因为它会返回top 6，大家如果去待会儿看代码的时候就能get到了，它会有六条结果，然后通过这个former dox变成一条整体给到这个context。然后question就是我们要去填的各种各样的问题，然后最终再给到这个a prompt，在这儿给到这个prompt作为输入，这个context就是把六条，当然他这个你可以去改top 3也行，给到这个部分的占位符。
	然后question就是这个有你到时候这个提问的时候会提什么，就提到这儿来，这两个一起给到这个prompt作为它的上游，作为这个prompt的上游，然后prompt再给到这个LLMLM的生成结果，再给到一个string out of the powder。只要你希望这个券结束之后给你一个结果，你都需要在后面接这个string out of the powder，然后去生成一个问答。比如说我在问what is task decomposition这个问题，这个地方stream是流式回答。因为它本来这个chain这个runners就支持各种各样的方式来进行调用，那么这个问题就会传到这个runners pass through这里来。那传进去之后，打印出来这样的一个结论，然后后面再问他what is TOT，这里也会给出一个结论。那这个就是咱们的这个，咱们看到的用LCL去实现一个RAG券的代码。看起来就讲了很多页，但其实主要这一页就能够去概括他所有的这个内容。前面那几个步骤主要是面向没有能欠的这个使用和学习经验的同学去稍微展开了一下。
	我们先把homework讲了，再来实操实战。我们的homework有两个，第一个是必选，第二个是可选。第一个就是我们刚刚看到的这个RAG的开发指南，用LCEL实现的RAG，需要做一下咱们的上手的一些练习。我们简单讲一下。第一个就是使用其他的线上文档或者离线文件，不要用我们在这个patter里面已经用的这一篇博客文章，你自己再找一个线上的文档或者你离线的某些文件，去重新构建一个向量数据库，然后去你去尝试提出这个三个相关的问题。看一下你现在这份代码构建的这个IG线能不能成功召回，这个是要去试的一个。
	第二个就是我们现在去改了这个文档，就相当于改了向量数据库这部分没有改过提示词，你可以去重新自己去设计一下，或者说在nation hub上面去找一个可以用的RAG的提示词模板，不是我们现在代码示例中用的这个。那你就相当于换了这个提示词模板。这个时候我们为了控制变量法，你就不要再去改向量数据库了。你可以是在一的基础上用了别的向量数据库去直接对比，这个其实是模板的差异。也可以在我们标准的这个博客文章上去对比他们的差异，但无论如何，这个1.2的这个作业是为了去替换这个不同的提示词模板，让你去理解一下提示词对于RAG的效果的一个差异。
	这两者都是RAG生成质量最重要的两个核心支柱。一个是知识库的质量，构建它的这个过程很重要。一个是提示词模板的一个水平，如果写的不好，很容易出一些奇奇怪怪的问题。
	然后第二个作业是可选的，就是我们刚刚看到有一个多链的一个版本的代码，就是那个原始观点，正方反方观点。然后我们把那个代码其实也有对应的这个示例文件，希望大家能在那个基础上去改造一下。这个改造就是什么呢？其实很实用的一个代码，就输入一个特定的功能需求，输出两种以上的编程语言的代码实现，它比前面那个设置还要简单一点，因为它不需要把一开始那个输入来作为我们最终的输出了。它就是一分成两个输入的一个需求，变成两种。这个代码比我们已经实现的马蹄线还要简单一点。但是你要能够看懂它的逻辑，你才能改造成这个简单的版本，你可以想象一下，你输入一个生成一个快速排序的代码，然后一下输这个输出两段不同的编程语言的快速排序，就这样的一个可选作业，那现在我们来实操上手来学习一下。这里我们要讲一讲代码库在哪儿，这个在课件里也都有。那我们再强调一下。
	这个项目是我们的AI大模型的应用开发实战营的课程项目，叫OpenAI的quick star。这个项目里面我们实现了这个课程大，可能被我移到这里了，我们看了这个坑大刚大应该就能get到我想表达的意思。在在这个里面，我们其实把课程大纲也都写了。从我们的学过了这个课程的同学，像大模型的应用开发框架，上中下门欠以及三个实战，就我们刚刚看到的课件里的截图，这个能欠版本的OpenAI translator v2.0，以及这个能欠版本的劳动GPT的源码解读，以及能欠的实现REG，包括用GPT4去生成这个销售话术合成数据。
	这六节课其实是我们的AI大模型应用开发实战已经都详细，也讲的很仔细了，就跟我们这门课讲第一个agent一样仔细。如果你没有上过这个课程，没关系，这里是开源的，甚至这些每节课程对应的源代码也都在这儿，你可以自己去学一学，去赶赶进度。因为我们后面两个agent都会用到这些，但我们不会再从头讲一遍这个对于学过的同学也也非常不友好，赶一赶叮嘱，本来这个企业结构性的课程是一个就像大二的课程，这个大模型应用开发是在于像大一的课程，所以有这样的一个背景。
	我们的这个南茜的代码其实有非常多的具体的模块都在这里有包括data connection agents，然后我们现在要讲的这个LC也要，其实在这个。这个有点慢，在应用开发实战营里面是简单讲过，但没有讲的这么细。我们会在这个课里面把南茜的这个LC也要包括能讲的在就相当于南茜的高级用法和更难的更复杂的agent实现是这门课的一个定位。它跟前面实现的那个agent肯定是有难度上面的一些区分的。我们讲过三个agent的难度梯度，前面是一档，后面两个是一档，是各自不同的面。争取这个零基础的和有基础的都能学到一些这个东西，跟大家关注的点不一样。
	然后我们跳到左边这个代码库我们看到在Nancy有一个to Peter，有个LCL，这个里面就有我们刚刚这个课件里面的代码了，分别是这个march chain，quick star RAG和running。我就直接打开我的本地的这个，不是本这个就patter notebook，这个就对应着我们刚刚看到的这部分的代码。现在我们来带大家快速的去操练一下，去理解一下。首先quick star这个部分是讲LCEL的基础，就我们怎么样用这个LCL去实现一个LM券，这里有它对应的一些代码，全部都重启一遍，这样，restart一下。这儿我刚刚已经讲过，我们就不再念了，核心是这里我们实现好了三个特定的模块，再放大1.3个特定的这个模块，然后再构建了一个链，那这个链通过invoke方法就可以去单次调用。这里他就讲了一个特定的笑话，跟程序员有关的笑话。当然可以，这是一个巴拉巴拉的说一大堆。
	然后再往下面是这个prompt，这里就是我们提到的各种各样的基础组件都可以调用work。因为本质上他们是在vulnerable这一层实现的。所以原来的prompt明明是一个提示是模板，但是我们通过in work就能获取到它的传进去一个结果。因为prom的它本身是一个基础组件，输入一个特定的topic的特定的值，它就会变成一个最终的实际可以用的字符串的结果。
	这个就是当我们没有用LCEL的时候，在原始的不叫0.0系列的文件里面，是通过from template方法去format。这个其实是模板的。现在我们可以通过这个invoke来实现类似的to message，做string也是一样的。这个就是跟原来的我们的这个南茜的chat prom value是完全一样的这个用法了。只有这一步是不同的。Model也是类似。我们把这里的实际的字符串要去给大模型的这个结果给到model。Model达到了这个生成的结果，然后这个生成的结果。
	我们在去看一下这个生成的最终的效果长这样的那这个就是把我们上面的这个签单步去执行一个invoke执行的结果，然后这个结果再作为下一个组件的输入，他再invoke一下，再拿到这个结果，让大家去理解它里面发生的一个逻辑，长这样的。然后这个是我们的invoke跟RAG可以去做结合。因为其实就把这个管道前面再增加一个类似的跟RAG相关的对象就好了，这个大家可以自己再去细看一下。
	这个homework是不做要求的这是给用开发实战营的同学当时留的这个作业。我们的work就看课件里的就好了，然后roundtable是长这样的，这个只有一个round table的一个我们的课程代码的一个示例。主要是去给大家讲他单步执行能够成功对吧？我们刚刚看到的quick star那边，我们的一个chain有三个部分，prompt model和这个string out of the powder。单个invoke到下一段可以成功，为什么？其实就是因为这个round able对象的schema实现的很好在这儿我们就能看到这个也是restart，大家看看。
	好，我们看到这个chain的输入和prompt输入是完全一样的，因为这个构造是完全一样的。然后这个model的输入其实就是等于这个prompt的输出，它的输入单独执行也是可以的，就像我们刚刚quick star里面单独的这个执行也是可以的。然后这个any of就是指它输入要一个这个只要是字符串类型就行了。
	就是我刚刚提到的pandects做的数据类型的一些要求和检查，包括这里，这是一个ray，这个部分大家不用去深度研究，只要你理解了这个管道操作符的用法就好了。至于这个panic v1V2有什么差别，内部是怎么去去实现的？这个不用你花太多的精力去去折腾，那个是开发框架底层为你去提供的功能，不用去研究原理，除非你要去做对应的一些改造。Output也是一样的，这个是chain的一个output的schema，它跟这个model的skimmer其实是一样的。然后所有的券，包括其他的组件都是支持这几种方法的。
	这几种方法？Stream这个invoke batch，那么这儿我们就用stream方法来运行，能看到它是流式输出的，跟invoke是显然不同的，invoke是长这样的，执行完了之后一次性给到你的。然后绝大部分的大模型其实都是支持scream的，所以你不用担心模型那一侧不支持roundtable只要只要你的这个组件它本身是按照LCL来写的，它就是一个round table的符合rn able接口协议的组件，就能够直接一行代码，不就一个函数就实现流式的输出，是这样的一个逻辑。然后批处理和咱们的这个单词量也是一样的，就用一个特定的方法就能够实现了。比如说这儿我们用这个batch方法调三次，有一个批次里面就批处理，就批量调用我们的这个chain。但是每次给的这个topic不一样，三次分别是讲程序员、产品经理和测试经理的笑话，他就调用了这个三次的结果，这是一个关于产品经理的笑话，这是一个关于程序员的笑话，这是一个关于测试经理的笑话。而且他还很厉害的是，他都是讲的走进一个什么特定的酒吧或者咖啡店去点东西。这有兴趣大家可以去研究。
	然后这里是为了把这个批量输出的结果做的更好好看一点，更更能够去不像我这儿给大家高亮，通过这个方式去可以做，其实就是把输出的结果通过string out of the partner，再进行一些简单的处理就可以了。就把上面这一堆message给提出来了，关于程序员的，关于产品经理的，关于测试经理的，那异步的操作也都是一行代码就能够实现了。异步的三件套，就是在前面加上这个a asic异步的意思，这儿我就不再去单个执行了。然后march chain的这个实现，其实我们刚刚花了很多时间在课件里面去给大家讲了。我们看代码的话其实就是四个不同的基础的链，或者说基础的组件。分别是planner，正面观点、反面观点、原始的这个论述，最终的这个响应，构建了一个完整的链，然后去输出这个对应的结果，再执行一下。
	我们选一个好一点的模型，我们用这个GT4o mini，他默认的应该不是GP4O你对。
	看看他会不会讲的内容不太一样。好。
	好啊。
	这个地方其实原始观点也应该就这个地方其实他也应该有一个竖线能够下来。但是我折腾了半天都搞不好这个排列就没有弄了。讲道理这里应该还有一个，那就得把它再延长，就相当于他其实也是要给到这个最终总结的，应该是这样的一个流程下来，好像有戏。
	不行，我当时就折腾了半天，这个地方应该是有一个半角字符的问题。Anyway就这个意思，它的原始观点也会这样下来，然后最终形成了这个欠。好我们。
	好，定义好了，我们来执行一下，输进去一个就直接一下。我们看看房地产低迷这个话题，他会怎么样去讨论。
	有点久。这个地方因为要实现四次大模型的调用，所以会花不少的时间。这也就体现出下面我们用流式输出的这个意义这个是他给出来的一个结论，显然比前面我们看到那个结果要好多了。因为前面是拿3.5的这个turbo的这个instruct模型生成的，这儿就显示的很好了，就这个。
	批评意见，那我就不念了，这个话题比较敏感。我们看看这个全球经济它怎么聊呢？刚刚大家记住它很短的一个内容。
	这边开始生成了。
	好，这个是关于全球经济的一个说法。这里有一个小细节，就是大家会发现它确实是流式输出的，但前面还是等了一会儿，这是为什么？这个聪明的同学肯定就发现了，这是因为我们前面的这个三次，应该是前面的这三次调用了这儿有错了。这里planner调用了一次大模型，arguments four调用了一次大模型，result two调用了一次大模型，然后一直到这个final responder这儿也调了一次大模型，然后我们这里是让它流式输出的这个chain stream。
	但这个chain stream的stream方法到底是作用给谁的呢？其实是作用给整个欠的管道的最后面，就是作用到他身上的，而他的最后面就是他，所以你可以发现这个管道的好优势和便于理解的地方。就是这个管道最终你是可以找到它的最小值单元的最小单元就是这些组件这些组件，然后这些组件可以组合出一个欠，然后这个chain它是可以再被组合的。然后我们对它进行这个stream invoke和这个batch的方法作用于它的时候，其实它是作用于它这个管道的最后一节。然后这个最后一节，就可以一直作用到它的最小的这个单元上。
	Final responder, 其实是长这样的，它相当于只是用了一个快捷方式或者一个别名。你这个final responder最终在欠这儿，其实你把它展开写成这样，就是你把这个就不写这个别名，就是在这儿疯狂的把这些东西都复制下来。这样写本质上也是合理的，但是这样写就非常的不可读，就太难理解了。所以我们才又又给他弄了一个别名。但是从实现的原理上来说，和我把它复制下来是一样的。所以这个管道平权的概念就在这个地方。
	我这儿虽然有一个final responder，但是我其实跟直接去写是一样的。因为它就是平层的一个管道。然后我只要是一个管道，我对它最终去调用这些方法的时候，这个invoke也好，stream也好，就是作用到管道最终的最后一环上。然后最后一环因为它是管道的一部分，它是一个around的协议，所以这些方法也都能够成功的执行，就这样的一个逻辑，我不知道这个大家有没有捋明白这是一个可选的作业，实现一个多链版本的代码生成，输入的是功能的需求，生成的输出的是两种以上的编程语言的代码实现。然后最后是我们要讲的REG，这个RAG在LCEL里面又怎么实现呢？这文档非常健全，这是新版本的能欠的这个图例，我觉得没有本质的区别，跟原来那个data connection大家看看就好了，就这五个步骤，这五个步骤这里有一些依赖包，这里需要额外讲一个细节是新的我们要学LCEL和n grave。
	这周三和这周日的课程，其实是有一些新的代码包的。而这些新的代码包跟我们前面开发的第一个agent又没有直接关系，所以需要大家去跟这个没有直接关系，所以需要大家去close folk clone这个项目，就我们的OpenAI quick star这个项目。然后你也正好，如果你是没有学过这个大模型应用开发的同学，只好去学习了解。然后这个项目里面，你当然就需要一个额外的python虚拟环境了。然后我们也有怎么去搭建这个项目环境的文档，建议你去阅读一下，用这个mini cda去搭一个虚拟环境，这个是很有必要的。然后搭建一个虚拟环境给它命名为南茜，然后去装这个依赖包，装好之后，就可以去正常的执行了。
	然后这个是关于它的一个说明，然后我们再看一下这个RAG是怎么玩的。这里因为依赖包里没有这个，没有这个，我不确定有没有这个这后面两个包，反正我这儿写下来了。大家如果要去执行这个RAG的开发指南，记得去执行这行代码，去更新一下你的这三个包，跳完了之后我们来实际运行一下，看他怎么玩的。
	首先导入必要的库，这个库我就全部都写在这儿了。就我们这五个步骤需要用的所有的库都在这儿了，执行一下。然后加载我们的一个特定的文档，这文档我们可以看一眼这个线上的一个博客。就这样的一篇文档，2023年7 6月份写的，然后叫agent system overview，这样的一篇文档还挺有名的，这篇文档被对很多人都去阅读和讨论。好，那么这篇文档我们可以通过这个BS4，这是一个经典的去获取网页内容爬虫的一个方法。然后去获取到用web low best loader去把它转换一下来实际执行一下。
	然后加载下来之后，我们看到这个第一个文档的内容就很长，四万多的这个长度。然后我们不可能全部打印出来，那就很难看了。我们就打印前100个字符去检查一下就好了。100个字符是叫LLM power的自主智能体autonomous的agents。然后时间二三年的这个6月23，跟我们这儿看到的是0.11元样，就这个这是他的前100个字符。就这些，所以没问题，这个文档是成功加载下来了。接着我们既然这么长，我们就得进行分割，四万多行的在这儿去进行分割。分割之后，我们看到这里有一些关键的参数，比如说每分割之后，分割出来的结果就是一个一个的差，一个一个的快。
	每个块我们有一个特定的最大字符数的一个上限，以前然后块跟块之间有一些overland有一些重叠，那是因为如果我们只是按照字符去切的话，这种方式非常的粗暴，容易把一段话切成两半。所以用一些重叠字符来解决这种问题，这是常见的操作，在这个应用开发的时候都给大家详细讲过了。这刚刚已经切过了是吧，又执行了一遍。好，然后我们看一下切完之后的第一个块的结果第一个块是长度只有969，是符合我们切出来这个效果的，就不要大于1000。那内容就是这部分的内容，我们开头的在这个文档里看到的开头的内容building agents with LLM，这段话就是它的看出来的这个内容和他四个支柱的能力，就在第一节课的时候我们提过，然后打出这个原数据，这个是我们切出来的这个块，基本的一些原数据的内容，有兴趣的复习一下，对它的数据结构再了解。最关键的是要把这些切出来的块变成向量数据库里的知识。这里我们使用chroma这个向量数据库，用from documents来一次性建库。使用到的嵌入模型是open I的evidence。
	这里它应该默认会用text ADA002这个in body model。然后这儿再额外讲一句，就是我们之前去用这个向量数据库，在人群里面几乎都是用的from document这个方法。但是其实这个roma，包括其他的向量数据库，只要是在南茜里实现的，也有没有数据，只是实例化一个数据库的做法的对，这个大家捋一下，就是from document是你数据已经处理好了，因为它几个步骤处理好了，你现在就差一个向量数据库来建知识库了。但还有很多时候是你不是一次性把所有的知识变成这个向量数据库里的内容的，而是你需要分批次一步一步来的那这个时候，我们先把向量数据库给实例化出来。
	两种方法，一种是使用构造函数来初始化。就比如说我们从南茜chroma里面导入一个roma，对，导入一个chroma，然后来实例化。这个collection name就是在向量数据库里面的一个基础概念，就connection，你可以认为就是向量数据库里面的，类似于我们在关系型数据库里面的一张表。向量数据库其实是没有表的概念的，所有的向量都放在一起的。但如果你所有的向量数据库都只能为一个项目服务，这个也很笨，对吧？所以它实现了一个类似的概念就叫集合，就一堆相关的知识放在一个合点，比如说这个connection name就变成了我们去找特定集合的。
	比如说这个是为项目A服务的，它可以叫click name，等于这个A，那个是为项目B服务的，就BAB之间的向量是完全不相关的。去检索的时候也没有可能跨这个connection去进行检索，然后持久化存储的这个位置，然后使用到的embedding the function，这是我们用open I的evading，你也可以用client来进行初始化，这就是你建一个client的实例。这个就跟我们经常使用的关系型数据库的client是类似的了。就建一个client，然后去锁定你要去访问哪个collection，然后去为它添加一条向量，跟这个mysql le st great circle很像。然后这是我们额外讲一句，就是不管你是怎么怎么去实例化的向量数据库，这三种方式出来的向量数据库的实力都应该是在这个南茜里面，都是符合这个，或者说都是基于vector stores这个鸡肋来的这是它的详细的API文档，我做了一个reference，上过之前能劝课的同学就知道这个是授人以渔的方法，免得大家经常问这个是vector store这个鸡肋的。所有的API的文档，列出了它的各种各样的方法。然后我们的retrained l其实就是在它的基础上，就实现了这个玩意vector store的the travel，然后可以通过这个实例去访问他自己的向量数据库，当然我们也可以通过这个向量数据库去实例化一个检索器去哪去了这儿。所以他俩是都是在Victor stores下面。
	这个是从源代码层面上让大家更清楚理解内嵌的实现。这儿我们就建一个向量数据库叫vector store，看看它数据类型，在vector store下面有一个chroma，这chroma就是这个的派生派生类，然后我们来检索一下它，对吧？怎么检索呢？要先用这个方法，把它变成一个检索器。检索器就是一个实现了running协议的基础组件了，那我们这它的实现invoke一下，这个就可以invoke了，所有东西都可以通过这个玩意儿来调用了。单次调用，检索到了六个文档，是因为我们用的是top k的这个检索器的方法，这都年前的基础知识然后打印一下，这个是他检索到了六个，我们打印出来第一个，然后打印出来第一个是长这样的。好，那。
	我们接着。
	往下看啊，生成一个回答，生成回答就是RAG里面经典的做法，我们再来看一下，就是用户有一个问题，刚刚我们到上一步的时候检索出来的结果，就是这个大圈圈带着放大镜的检索出来的一个结果。这个结果要给到提示词，同时问题也要给到提示词，最终给到大模型去生成一个结果，这个是RAG的最典型的一个用法了，非常的经典。然后这个提示词可以从national hub里面去获取，这我们去给大家看一眼能hub。在smith. 
	这个人。
	券里面有一个对应的平台，这里面就有各种各样的，再有类型，大家可以看到agents 495个code writing，选一下交集，什么system instruction prompt，open interpreter system巴拉巴拉的。大家可以去研究，跟我们的作业有一定关系。包括学语言，再合并一下中文就只有一个text to circle，这他的提示词。这不骗人吗？这也是英文的打了一个中文的标签，其实是英文，这就很有意思了，所以大家还是谨慎乐观去去研究它。对，anyway我们回到这个地方来，然后这儿我们使用到的是这个RAG prom，这应该是官方经常会推的一个叫IOM这个同志的提示层模板长这样的来实际跑一下，这里使用GPT4o omi，那这个prom我们把它打出来，长这样的。为了很好的看它的这个结果，我们填充一个特定的值，然后给它打出来，这是它的提示词，这提示词你也可以通过我留的这个UIR去看一眼。
	就长那样的，跟我们在拍照留的文档里的内容一模一样。就从那来的，然后填两个词就填了这两个。然后这个是咱们把提示词模板搞定了，就相当于到这一步搞定了。接着我们得把这提示词模板里面的这个特定的值给它填上，不能填这个空白的question就从用户这儿取context就从我们上面这个检索到的文档来取。
	所以上下文我们有六条，不只是一条。所以我们需要把这六条合并起来。合并起来之后，就变成了传给他的一个值。这个retrained l有六条结果，六条结果是一个list，大家如果看的话是长这样的，这个就是we travel in work的结果。这个结果后面又去接了一个函数，接了一个这个函数，这个函数就把这里的list变成了一个字符串。这是一个把标准的python的操作，把这个list里面的元素join起来变成了这个字符串，然后还有这个换行，完了之后question就是用户传的这个做法，我们在这个martial market chain里面也见过了。那我们来执行一下21、20 24，这还没执行，定义一下，后面到时运行一下，我们再问问什么是这个task decomposition，返回、task decomposing，巴拉巴拉说一大堆，什么是TOT会写，TOT就是这个tree of thought，巴拉巴拉这就可以运行了。
	Homework就是我们刚刚在课件里看到过的，就是换这个数据源，重新构建项数据库或者换提示词模板去对比召回率和生成质量。为了方便大家能够去实现这个第二项，下面有一个对应的代码，去教大家怎么去用自定义的这个提示。是模板其实挺简洁的，就是这样的一个链，这样的一个customer REG chain customer的这个car这个REG chain，我们不再用南迁hub上面下来的，而是你这儿可以自己改这一段没改我这一段就是上面那个hub里面的原文，然后唯一的改动可能就是这一些小的细节没有这个key，这个healthful answer在这儿，你需要自己去去改吧改吧，然后去测试一下，对就把这个丢进去，然后就可以跑了。所以这个是给大家做作业完成的时候提供的一个参考的代码示例。
	好，那么这个就是咱们的LCEL的快速入门跟实战了，对应的这个代码再强调一下，是在OpenAI的quick star，在OpenAI的quick star里面，一定要去这个代码库，我们的应用开发实战营的同学应该破一下就能拿到新代码。有没有上过同学就克隆一下，book一下，然后去获取一下这个代码。希望大家能够通过这节课，能够系统的了解南茜的新版本，它的一个生态发展是怎么样的，LCL在其中的核心地位又是如何的，以及LCL它的基本用法。
	Round able是作用是什么？Runner ble为什么很重要？LCEL的优势是什么？为什么可以通过LCEL去快速搭建应用原型，并且LCEL作为nine graph的这个底层支撑？其实我们在mark chain里面就已经能端详到一些特点，它本身可以通过这个方式去做复杂的组合，只不过这个还是线性描述，这里偶尔可以通过这个roundtable pass through去实现一些分叉，但还不是图的结构。Nine grave是我们下一节课要讲的内容，通过nine graph我们可以实现更复杂的这个逻辑定义。那么好，这个就是今天的内容，将近3个小时。好，看看大家有什么问题。
	最新的康达虚拟环境能用3.12版本吗？你问的这个问题我不知道，你得问一下社区了。我只能确保我测试过的这个版本是OK的对，你为什么要用3.12？外挂文档是中文文档，对结果影响大吗？你是谁啊？大家别老问这种问题，很奇怪，你相当于在问我这个菜好吃吗？这个菜每个人都不一样，对，哪家店也都不一样，得试试。大家还有什么问题吗？
	今天其实讲了快3个小时了，因为内容确实很多，我已经尽量去去精简了。对什么时候需要使用round able pass判断依据是什么？什么时候需要使用round able pass through？这个怎么给你描述的？我想象一下他是个占位符。我不知道这个描述能不能比较清楚的去去定义它。
	我们再看一看这个代码，首先这个response，这个planner，它要能够运行这里的每一个管道，前后连接的环节都应该能够被正确执行。好，那现在这个管道的输这个第一个是一个提示词模板，它要执行的需要输入一个input，然后在在这儿这个input输进去，它就能够执行了。然后它执行之后，其实后面就能顺序的去挨个执行了。一直到这儿输出了一个结果标准的一个结果。然后这个结果如果我们没有这一行，就可以直接拿到输出出来了。但现在因为我们有一个使命是原始观点，还要给最终总结。那这个最终总结他拿到的是什么呢？
	是一个特定的key对吧？一个特定的key叫。叫这个。说错了，我把这个手机放下来，这个base response它这一环应该还是先给到这儿，先从这儿开始给大家看啊。
	这个base response是一个key，我不知道这个同学用没用过能签，其实在南茜里面我们教sequential chain的时候就已经讲过类似的东西了。简单来说就是一个模型它输出就可以输出很多个东西，然后那我现在要指定输出的这个T是这个，那我下游才能拿到它。你可以想象一下，就是有一个水管，这个水管里面它不可能只能漂流一样东西，它可以漂流ABC3样东西。现在我就把这个结果命名为A然后当然还有BC但是我现在的arguments for我就要A所以我就可以通过这个P去取这个结果。只不过恰好这里只输出了这个A而已。
	然后接着我们到final的这个responder再来看就更好理解了。这个final responder它的这个from message，因为它是一个chat model，它可以设定三种角色。三种角色里面AI就是planner生成的结果，它直接放到了AI这是年欠里面的角色定义，就相当于模型生成的结果，是叫original response这个key，那human是两个，他把这2个AI生成的结果当成用户给的意见，一个是正面观点，一个是反面观点。分别是result一和result 2333个需要输入的这个input variable，然后再给到这个模型，那最终构建这个券的时候就很有意思了。
	大家可以看一下，先是planner输出了一个什么呢？输出了一个结果，这个结果叫base response对吧？那base response可以给到他们俩，他俩是能拿到base response的，然后拿到之后，他们也能输出结果，就直接输出了结果。然后这个输出的结果，因为他不会再被别人拿去做任何使用了，所以他可以，而且他只输出这一个结果，所以他不去命名。没关系的，因为我只需要通过这个地方再命名就好了，因为他所有的输出都放到这儿来，所有的输出都放到这儿来。
	然后接着这个final responder还需要一个叫做original response，result一和result 2都已经有了，还需要一个original response是从哪儿来的呢？它是从planner的输出里来的，他去取这个planner的输出就跟咱们在这儿去取这个结果是一样的。他要能够去实现这个取，就得需要这个玩意儿了。因为它要同时给两个人去使用，一个是他们，一个是他，所以这仨其实都用到了这个原始观点。你看一二三这个runners pass through，就实现了把一个very response同时给到一二三的作用，你可以想象一下，就是是这样的一个逻辑。然后这个最终总结需要三个输入，result 1、result 2和这个original response，result一和result 2就直接把他们俩的输入全输出全部给我就好了。
	但原始观点我需要用这个pass through去取，不然他就会留给一个人。不够优雅。你说的很对，所以我们才需要nine graph。
	这位同学说的很对，是不够优雅。就是我如果要用这种管道的方式去表达这种复杂的结构，就会有这个问题。所以nine grab就说得了，你别这么干了，你就直接用图的方式来表达，有节点有边就简单很多了。主要还是管道操作符本身的。
	这个你可以想象一下，就跟我们用这个第一个github的一样。大家如果有印象的话，我们去看看代码，这也是有重定向的。这个logger模块它不就同时输出给别多个了吗？比如说我们看这个代码。这对输出流我们得有一个要是有一个这方面的了解就好理解了。这个是github的这个set now，我们有源代码，代码点进去这么慢。
	seriously. 这么卡。好，然后logo。你看我们这儿就一样的，我们这里。
	我们把。
	它输出到了这个APP点log里。然后同时我们也把它输出到了STDL和STD这个error，也是也给它做了对应的添加。甚至我们在在这个在这儿。
	我们甚至。
	在这儿还给它重定向输出到了demon process slog，这些就跟刚刚看到的管道是一样的。就在操作系统，我们这个是跑在lu x上面的，它也可以重建一下输出，就跟刚刚我们要的重定向输出是一样，它可以输出给两个同时，但你得指定对他输入就干这活的。Message那个里面可以分肉写是的可以分肉写是的，是刚刚那个retry为啥知道接收的输入是question？他不是，他们俩共同组成了下一个的输入同学，就跟这儿一样，他们仨是并列关系，他们不是管道，他们仨是并列关系，就跟这个rng一样，他俩是并列关系。我给你换个行呗，能这样的，不过这样太丑了，没必要，是这样的，兄弟。是这样的，他俩不是上下游关系，他俩是上下有关系，但是他俩不是，他俩就跟。就跟他们是一样的，能晓得吧？是这样的是这样的。怎么回事？
	这样的。
	看大家还有什么问题吗？对这个time plate的大家去去补一补基础，这个我们不讲了，这个太我们在get up的city name里面我做了更新，大家可以去看一下最新的欧拉玛的github. 
	的那。
	这不就是对吧？我们更新了这个problem。任务分两步，你将收到的这个整理成这个。第一步是把输入的这一堆close的issue分类整理。第二步是把这个整理结果生成一个中文报告。
	符合以下参考格式，这个参考格式repo，这个lama 3.18B怎么都能理解什么叫repo也能理解什么叫时间周期date。所以就能够正常的生成结果了，生成结果就是图里给大家看到的那样，也就是这样，我这个是怎么试出来的呢？就在这儿试的，我们的open YBUI还是很好用的，就在这改改完之后提交，然后提交了十回就试出来了。这个就试出来了，所以open YBUI对于我们去做私有化大模型的各种提示词的优化是很友好的。好啊，大家还有什么问题？
	这个问代码是什么意思的问题，我是真不想回答你每一个API怎么用，你最好还是跟大模型交流。好不好？我们就不问这种层面的问题了。
	Ra g返回的召回率如何计算？什么时候什么区别，这些都去去自学一下。这个没有不知道南迁，没有上过那个大模型应用开发的同学，去稍微花点时间去学一学这个年前的基础，然后对应的代码都有文档也很详细。如果觉得自学也很痛苦的，你就报一报那个那个应用开发营，那个就会讲的很细。
	然后这个向量数据库的。是必要的吗？是必要的对，是必要的，是必要的，你可以再查查文档。我们今天就到这儿，我看大家都在问一些很细枝末节的问题。对细枝末节的问题可以通过查文档问题，igbt和你自己去实际跑一跑来解决。好吧，我们就不在这儿做API文档的工作了。好，感谢大家。我们今天就到这里。