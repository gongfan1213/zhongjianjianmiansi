好，那么接下来我们就来看看注意力机制，这个非常有名，很很引起很多人关注，但是又很难讲明白的一个技术知识点。它到底什么叫注意力机制？我们通过一个这节课来让大家理解注意力机制它是如何为我们的大语言模型预热的，点燃了第一把火。
那什么是注意力机制呢？从名字的翻译来看，从中文的翻译来看，其实注意力机制翻译的是一个很好的一个说法。然后我们通过一个事例让大家理解为什么需要注意力机制，注意力机制是什么？接下来我会给大家放一个小短片，我希望大家带着一个问题来看这个短片。就是我希望大家看完这个短片总结一下，告诉我这个短片的这个发言人他到底讲了什么样的核心的主题，他的中心思想是什么？他到底在说一个什么样的事儿。
我们说当务之急是找到关键的问题，那么关键的问题是什么呢？是我们要找到问题的关键。那么一个如果在关键的问题，关键的领域、关键的这个环节上，我们找不到那个关键。我们把握抓手不在关键上，那么我们等于就是说无法解决找不到关键的问题，找不到关键问题，解决不到问题的关键，那么这个就舅妈，但是关键在于什么？关键就在于大家能不能准、能不能狠、能不能快。现在是这个是啊。
这里没有任何倾向性的这个意思，这只是想通过这个视频向大家展示注意力机制是什么。我们有看到这个字幕，就是这个发言是有字幕的，这个字幕可以丢给大语言模型去理解，对吧？但是大家现现在大家假设这节课你把把你自己当成一个大语言模型，你在学习，你在理解，那你能不能一句话总结告诉我这个发言到底说了什么，很难对吧？就是我我们现在能直观的体会到，对于一段发言来说，重点很重要。我们想要总结出来就是去抓重点。领导提到的要抓关键词抓重点，但重点如果你抓不到的话，你不就不知道他在说什么，对吧？所以注意力其实想干什么事儿呢？注意力就是想抓住这个重点，抓住领导到底在讲什么事情。
我们看一个相对来说没有这么复杂的例子，这个例子我们同样是抓重点抓注意力。我们带着问题来看，第一我们的问的问题是什么呢？问我到底去了几次咖啡店，就通过这样的一段发言去理解我到底去了几次咖啡店。我们看一看这段话是怎么讲的，昨天我在一个繁忙的一天结束之后，决定去我最喜欢的咖啡店放松一下。我走进咖啡店点了一杯拿铁，然后找了一个坐靠窗的位置坐下。我喝着咖啡，看着窗外的人们匆匆忙忙，感觉非常惬意。然后我从咖啡店出来回到了家中。
我去了几次咖啡店，大家想象一下，我说这个大家想象一下的时候，其实已经在让大家脑海当中有这个画面了。至少我是这样的，我不知道大家会不会这样去思考，就我会想象这个画面，他描述的这一段话的画面。但是一个大语言模型或者说现在的大语言模型是很难通过一幅或者说这么一段话去描述出一个完整的画面的这也是我们现在最新的大语言模型技术攻克的技术。
我们的多模态。大家知道我们Midjourney一直在做这个纹身图，通过一段话弄一个画面出来。但是它也很难解决这个时序的问题，包括像现在金to这个模型也juny发布的，想通过纹身视频。但是纹身视频最难的就是这个时序问题。我能通过一句话描述一个画面，但是这个画面是有顺序的，有时序的。这个时序怎么描述出来很难。像这里一段简单的话，就它是有时序的。那么这段话里面我们回到这个问题本身，到底去了几次咖啡店？
假设语言模型其实是没有想象画面的能力的，它只能通过这个文字本身来思考，那我们就忘掉画面他会怎么去解决这个问题。第一，这段话里描述了出现了三次咖啡店这个词，那我是不是就去了三次咖啡店呢？那肯定不是这样的对吧？因为咖啡店只是一个名词，我们真正解决这个问题，我去了几次咖啡店，更关键的问题是去了几次？
是去了次数，这是一个要找到关键动作的一个问题。比如说决定去我最喜欢的咖啡店放松一下，这个第一次出现咖啡店我去了吗？我还没有去？我只是想要这么做，我走进咖啡店这个动作很关键？我真的进去了，最后我从咖啡店出来，我走进我出来，其实我只去了一次咖啡店。
然后这中间当中有很多的信息其实是无效的信息。首先我们通过咖啡店这个词，这个关键的词，我们能找到它附近的一些词，肯定是我们回答这个问题最关注的问题。那中间的这些什么点了一杯拿铁，靠窗的位置喝着咖啡，窗外的匆匆茫茫的人流惬意这种感觉都不重要。对于我这个问题来说不重要。我当前想要问的最关键的问题就是去了几次咖啡店。所以找到咖啡店这个词很重要，同时去理解这个意义也很重要。换句话说，我们要回答这个问题需要理解的第一点就是咖啡店出现了三次，不代表我去了三次。最高频的这个词本身它不一定是唯一的重点。
我们需要去找到什么呢？找到关键的信息。第一，找到咖啡店这个关键的信息。第二要理解它。所以我们会发现要解决这么一个简单的问题，今天我们看这个问题感觉很傻，就是这种问题怎么还需要拿出来考虑。但是你要让计算机去理解这段话，并且回答出你的问题是很难的。甚至在大语言模型出现之前，这个问题基本上就答不出来，就很难能够回答出这个问题。现在我们有了大语言模型，我们把这个问题丢给GPT3.5，丢给ChatGPT，丢给国内的一些大语语言模型，他们大概率能回答出来。
那他们为什么能回答出来？第一，他们通过我们提到的这个注意力机制，他们找到了问题的关键点是咖啡店和走进。所以你看我们再来拆解这个问题，就会发现咖啡店很重要，走进是一个关键动作，但是点了一杯拿铁这个关键动作，虽然也是一个关键性的动作，但这个动作跟我们的相关性不高。所以我们的注意力不会放在点了一杯拿铁。当从咖啡店出来的时候，回到了家中也是一个关键信息。但它冗余了，我们可以把它这个冗余的信息去做一些处理。
然后通过我们用语言模型也好，用自然语言的处理技术也好，我们把这段文本进行了处理。找到了当中很多的关键动作、关键信息。然后关注到这些关键信息之后，我们要像人怎样去理解它。我们就会知道注意力我们搞定了有这些信息，但是这些信息是平铺在这里的，我们要理解他们。
问题就从一个我们能够去找到关键信息，进一步演变成了我们还得理解这些关键信息，就是我们相当于这个问题，我们再给大家做一下拆解，我们问了一个问题，这个问题现在给你的输入是一段话，这段话我们把一些停止时给删掉了，那些不重要的词删掉了。比如说昨天，比如说繁忙的一天，比如说然后比如说窗外的人群，这些都是包比如包包括这个感觉非常切。这些信息我删掉了，我找到了关键点，这是注意力机制能干的一部分。
但是注意力机制干了这部分信息之后，他还需要能理解理解这个信息，理解这个概念，理解什么叫走进，理解什么叫点一杯拿铁。就像我们现在用大语言模型的时候，有些特定领域的概念它是不理解这个词的。它虽然能找到一些关键点，你要去定义它，你定义的这个动作是什么？定义了这个信息是什么含义之后，这个大语言模型能做的更好。那就是因为它本身的基础的模型里面，它没有这部分的训练的语料，或者说它的权重不够高。那么我们的注意力机制它要解决的问题是把我们的这些关键信息找到什么跟什么是有关联的，这个是注意力机制要解决的核心。至于理解这个语言本身，这个是其他的一些技术要解决的，只不过transformer做的比较厉害的一点是他把这两件事情合二为一了。接下来我们不断的去通过这个理论课，让大家从技术上，从他的学术背景的发展上去了解这个注意力机。

![image](https://github.com/user-attachments/assets/c1c92445-5c61-446f-880b-036ae8010166)
