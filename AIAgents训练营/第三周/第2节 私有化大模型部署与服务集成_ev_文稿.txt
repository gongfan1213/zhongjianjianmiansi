	Hello, 咱们能看到画面吗？点了推流了，不知道有没有生效。
	没反应吗？
	我又点了一次了，大家看一下，能看到直播吗？我又点了一次了，看一下。
	大家能看见画面吗？我刚刚点过一次直播推流失败了，现在能看见对吧？OK好像现在是正常了，现在我这儿也能看见，我试一下这个PPT能不能播出来。
	好，那应该没问题了，人脸也是有的。好，没毛病。稍等，我在播放PPT，好有延迟，现在PPT的画面投出来了吗？我这看到的还是没有PPT播放的状态，是延迟的原因吗？
	这个画面没有卡住对吧？没有卡住。可以，对，应该是延迟了。好，那咱就现在正式开始，我这边应该是手机访问的时候延迟了。好好，那我们就正式开始。不好意思，稍微耽误了几分钟。刚刚我推流的时候，第一次推流好像推失败了。我想了一会儿发现我这个正常进去之后，这个没有画面。那好，那我们就正式开始。
	今天应该是第第六次第六次课程，没记错的话应该是第六次课程。然后今天我们要讲的内容叫私有化大模型的部署与服务集成。第六节课其实也是我们第一个agent实战的倒数第二节课，我们今天的内容主要是要给大家讲两个部分。上半部分是先给大家做一个欧拉A这个大模型管理工具的介绍，快速入门，让大家能对它有一个初步的使用。因为我们本身第一个agent对于这个大模型的定制化的能力等等要求并不高。
	但是在我们上一节课布置了这个homework，包括我们第一个agent的核心，其实是做一个信息采集加处理总结的agent。所以会涉及到很多问题，包括有的同学会反映说，我现在有了happy news，或者我能基于上节课的一些思路去做别的渠道的信息采集了。但是这个时候采集过来的信息又特别的多，我如果全部都去调OpenAI的这个GPT的API可能会比较麻烦。一方面是token可能会消耗比较多，另一方面是大家都知道众所周知的原因，去支付这个OpenAI的信用卡账单是一个比较麻烦的操作。有可能有的同学是刚刚注册的OPI的账号，所以他有赠送的这个五美金的额度。一旦因为咱们agent的这个消耗用完了，后面可能就不好开展这个学习了。所以我们考虑到这个问题，包括我们后面的两个agent也会有比较大的token消耗。
	所以我们从这节课开始，就是有条件的同学，或者说对于数据隐私要求比较高的同学，我们就可以开始通过像欧拉玛这样的工具来做私有化大模型的一个部署。它可以提供一个rest API的服务，以API的方式暴露出来。当他以API的方式暴露出来之后，就跟我们去调GBT的API是类似的一种形式了。
	只不过你的欧拉玛是一个模型管理工具，所以你想要去部署什么模型，只要你的硬件支持都是可以的那这个是我们今天要介绍的一个很重要的工具。我们会从欧ama的简介，包括它它的一个概述，然后它有什么样的使用场景，有什么样的核心能力和特点，从这个地方开始讲起，去讲怎么去装一个欧纳A对应的安装的文档。因为可能它散落在各处，其实我们在课程项目里面，read me文档已经更新了0.6的分支，大家应该刷新就能看到，有一个中文的文档去介绍欧ama怎么样去进行安装。
	包括欧ama它是一个大模型的管理工具，管理的对象是模型。那模型到底是个什么东西？学过前面这个微调训练的同学可能对他有一定的了解和认知了。因为我们用过transformer的库，用过PFT的库，知道有什么样的模型文件了。
	并且有不同的格式，有patch ch的格式，有hugging face transformer的格式，也有safe tensor的格式。这些格式之间有什么区别，我们应该用什么格式？Nama又是用的什么这样的模型文件格式，这些其实都是一个在这节课里面我们会去做的一个介绍。包括model file，就是在欧拉玛的这个平台上或者说这个工具里面去定义模型，然后这个模型要怎么样去使用，这个通过model file可以去定义，那我们待会儿会介绍一下model file的语法，接着就是怎么样去用欧拉马部署一个模型并且去调用它，这是比较重要的大模型的管理工具，欧拉玛的最初级的使用。
	接着我们会讲一个欧拉玛的社区项目。这个项目其实最早它不叫open YBY，它叫做欧达玛的web ui但它现在已经不断的去扩展了它的功能，已经变成了一个界面上完全跟ChatGPT类似的一个界面，一个类似的一个web UI的界面，所以我把它叫做TIGPT的私有化版本，open YBUI这个项目也，非常的好用，也是整个欧拉玛的社区集成里面排在第一的一个项目。待会我们也可以去教大家怎么样用docker去启动运行一个open web UI。这样的话，假设你的ChatGPT访问有一些问题，现在你有GPU，那你其实就已经可以使用open YBUI去平替掉ChatGPT了。
	因为lama的3.1还是挺不错的，然后下半部分我们会来讲一讲这个0.6版本的开源哨兵它的一些特性。其他主要特性就是承接了上半节课，我们会在0.6的版本里面去扩展LLM的模块，大模型的模块，让他去支持调用私有化的大模型服务，其中这个大模型服务就是以欧拉玛来进行部署的，第二个部分，就是它要能够支持这个私有化的大模型。那配置项，配置管理，conflict Jason和config PUI也需要去做对应的适配，那这个部分我们会去讲一讲如何去优化配置管理。这些内容都在0.6的分支上了，大家有空也可以去看一看。最后我们会来实战去试一试怎么样用我们0.6版本的开源哨兵来调用这个欧拉玛托管的私有化大模型。
	好，这个就是我们这一节课的内容，整个这节课的内容。我们希望通过这2个小时的时间，让大家能够了解欧拉玛的基本概念和功能，然后学习怎么样用欧拉玛去导入管理模型，以及怎么样用它来运行和调试模型。因为它有很多种使用方式，就跟我们的开源哨兵一样，有命令航空工具的形式，也有这个你看像open YBUI就是它的前端，还有一些rest API，调查API的这种提示，怎么样去运行和调试它，最后我们的0.6版本去集成这个欧拉玛，就算是一个欧拉玛在这种实际项目当中的一种应用这些我们都会在今天的这个一节课里面教给。
	大家好，我们接下来就正式开始讲欧拉玛这个大模型管理工具，快速入门的小结。首先我们要理解一个新的概念，有很多种学习方法论，这个5WYH是很常见的那5W里面最重要的就是what，它是什么？那欧拉玛是什么呢？我们尝试用两个维度来解释它。第一个就是它是一个开源项目，它是在私有化大模型管理这个领域里面最受关注的一个开源项目。我把这个私有化的这个大模型管理跟一个在海外，或者说大家在媒体里面经常听到的一个词这个领域结合起来。可能大家会比较有感觉一点，叫做LLM offs。有大模型的这个office运维管理。所以它是在这个领域里面非常受关注的一个开源项目。
	这个怎么能够体现呢？就是我们看到下面有两个在当下应该是同一个领域里面非常受关注的两个开源项目。一个是奥拉玛，一个是v LLM。因为他们在我们这节课第六节课的定位里面，其实本质是一样的。我们暂时就不去两个都讲一遍了，这个意义不大，因为它只是提供一个API服务。在未来我们需要去做这个VLM特有的一些功能的时候，再给大家去做它的介绍。
	但是我们光看这个star history就能发现，alama其实是比v LLM这个项目要后开源的，大家能看到，但是没有后太远，也就是一两个月的时间。我们看到二三年的这个也就是一年前一年前多一点，罗拉玛才刚刚开源，然后VLIM是相对来说要早一点点，早一个多月，应该是看这个X轴的话。但是我们看到欧拉玛的这个star数量其实是提升的非常快。尤其是我们看到右边把这个时间轴alien对齐之后，这个增长速度是远远超过VLLM的，完全就拉开了几倍的一个star数量，这个star就约等于关注度，大家可以这么理解。这个是从开源项目的角度来理解，他在这个领域里面目前就是大数量最多最受关注的项目。那它为什么会受到关注，提供了什么样的价值？是第二个维度，就是我们它是开源项目。那它是一个什么样的开源项目呢？
	它提供了一套用于下载运行管理大模型的工具和服务，简化了复杂模型的部署流程。其中这个服务更多的是以欧拉玛的第三方集成，就是基于它开发的各种社区生态的项目去体现的。就比如说我们刚刚在目录里面就提到了open web UI，open web UI就是基于欧拉玛去做了一个私有化的ChatGPT，并且比ChatGPT做的更多一点，它还能去支持RAG，去支持这个纹身图等等各种各样的前端界面的形式。所以这个生态也建设的非常好。
	这个是欧拉法我们了解了在LLM office这个领域里面最受关注的两个开源项目，当然他们各自侧重点有一些不一样，有一些差异，但是都是非常受关注的项目。单看这个图，大家会觉得好像VRM的关注度一般。但其实我们可以跟上一个时代去做对比的。是虽然是上一个上一代。也不叫上一个时代，上一代的这个模型管理，不是大模型的管理。ML offs这个领域里面有一些明星项目，这个还是跟ChatGPT一起沟通之后，他觉得有这几个项目，当然还有两个可能相对小众一点。
	Q flow刚好是有幸我有参与TF service，在刚刚这个TF出来的时候也没有这样的一个项目。是后来为了把政策flow的模型能够很方便的去做部署分发，才出现的一个项目。这个项目也有一个逐步成熟的过程，我们看这个图其实就更有感触了。我们看到左边这个star history，在上一代的模型管理的这个赛道或者技术门类里面，显然TF serving是最早的，也是最早成熟的。它的开源时间是在应该是在16年的下半年，我没有记错的话。然后cube flow这个项目，它的第一个项目就叫q flow，但后面出现了各种各样的creator的这个项目。
	Q flow这个项目是1718年的时候，我当时刚好有幸在这个财源科技带着团队和谷歌云一起去做的，一起去发起的一个开源社区，就叫q pro。这里非常有渊源的是这个q flow到org现在的这个官方网站，这个域名还是我无偿捐赠给这个开源社区的对。然后q flow这个项目其实现在应该在mlf这个领域也算是一种知识标准。
	Q flow这个项目大家能看到它的star数量，其实是很快就超过了tif。Sorry, 我们看左边这幅图的话，但是我们看到v LLM其实是远远超过了q flow的，而且它的时间非常短，尤其是我们把时间轴对齐，就所有的右边这幅图都是时间轴对齐的。大家从出生的那一刻开始，然后时间轴对齐不看绝对时间，就看我们从出生开始的这个成长速度。这个欧拉玛肯定是最离谱的，都已经超过了这个图例了，都已经在在后面去了。这个VRM它的斜率也是很高的，q flow相对tif serving是有一个比较高的。
	成长在早期，后来也触到了一个天花板，当然还在缓慢增长。因为它本身就支持各种各样的机器学习，深度学习的框架。所以看这样的一个图大家会发现大模型这一轮的技术栈，他的关注度明显是在各个维度上都超过了深度学习。能欠我们当时比较过南茜和TensorFlow，能和这个itouch欧拉玛跟tif serving和q flow的对比也是触目惊心，大家可以看到这个量级是差的非常远。如果有兴趣，可以自己把下面这一行URL复制一下去star history里面也去看一看，其他的一些项目的增长。
	如果你有关注的项目的话，那欧拉玛这么受关注它的定位是什么呢？其实它的定位我自己的总结非常简单，就是一个最简单好用的LAM offs，就是大模型的私有化大模型的管理工具，他自己的这个模型库就跟hugin face类似，他会第一时间去支持在开源社区里面最厉害的大模型，比如说lama 3.1，微软的V3，这个miss to，包括google的G82。这些其实都是现在在开源社区里非常强的大模型了。包括nama 3出来没多久的nama 3.1，也是很快就在欧拉马上支持了。现在几乎大家做这些大模型的前沿公司，都会优先去支持这个欧拉玛的私有化部署。就跟当时去南茜社区去支持一样，他已经成了在这个生态位置上的一个很关键的角色。所以这里就得提到我们第一节课就有的这张技术站了。我们看到南茜是在开发框架这个领域一骑绝尘的一个存在，尤其是在生态定位上，它没有谁可以跟他抗衡。
	而这个模型服务这一层，其实欧拉玛也是类似的。它通过欧拉玛的这个平台或者说工具，可以去托管host一大堆开源的大模型。就像我们右边看到的这些国内外的先进的大模型服务提供商，都会去我拉马的平台上去做支持。然后下面的hugin face同样也会支持能够在这些开源大模型上去做微调。微调之后的模型依然能够用欧拉玛去进行托管。当然我们待会儿会去讲它是如何做到的。这里有一些统一的模型格式上的支持，能够让这件事情很顺利的发生。所以它的定位很清楚，它是在模型服务这一层提供了私有化模型的管理能力，然后去支持了业界最先进的大语言模型。
	一个最典型的欧莱玛的使用场景，就是像我们现在这里看到的这样这样的一幅图，其实就跟我们刚刚的那个技术站的位置顺序几乎也是对应起来的。然后这样的一个架构，其实我们现在第一个agent也能改造成这样的一个架构，只是把大模型调用那部分，我们用南茜去稍微改一改就好了。这个部分我们不再第一个agent里面去做过多的展开因为第一个agent里面的提示工程用的并不是特别复杂。
	我们看到这个图里面，其实非常简洁，怎么样去理解这个典型使用场景的调用的逻辑关系，首先我们要理解，通常来说使用欧拉玛是一个你对数据，对这个隐私有一定要求的一个场景。或者说你是在一个开源的大模型上去做过微调训练的一个场景。所以这个奥拉马通常来说是由你或者你的团队来进行管理的，而不是像我们去用OpenAI的这个API，那个大模型你都不用管，模型是一定要你自己管的，无非是这个模型你有没有改造过。
	假设这个模型最终已经变成一个成型可以使用的状态了，不管有没有经过训练，那这个模型是一个模型文件，就是一个静态的文件，你需要把它变成一个服务。那么欧拉玛就是干这个事儿，他把这个模型变成了一个服务暴露出来。但是这个欧拉玛需要由用户去启动，所以我们看到第一条线呢就是stop alama，我把欧拉玛启动起来，欧拉玛本身它也是一个我们上节课讲到的守护进程，它有一个欧拉玛的这个也可以叫做后台服务，一直在这转着，跟多克demon很像。然后这边启动起来之后，他会去拉取我们要去提去托管或者说提供服务的这个模型。那把它拉起来之后，就变成了一个对外可以暴露的API。然后这个时候从第三步开始，这个用户才真正的开始去使用一个agent或者说一个大模型的应用，所以他开始跟这个应用去进行交互了。
	这个用户界面用radio是比较常见的，是一个图形化的界面。我们在应该是在之前的课程当中，已经教大家怎么样去用radio的一些常见的功能了。我们也提供了这样的一个入口，然后radio去转发了用户的输入，然后这些输入可以通过我们在radio里面学到的这个function去进行转换。转换之后这个function内部可以去调大模型，它可以是直接调用，也可以通过南茜去调用。然后这个请求就会被发送到欧拉玛这里。那欧拉玛去在模型里面，我们学过了这个应用开发实战和微调的同学应该都理解大模型的一个运作逻辑，用户的输入或者说用户的问题给到大模型之后，大模型会在内部去跑。
	这个主要是以transformer为主的这个神经网络的结构，最终会有一个输出。然后这个输出的结果通过这个token nier的decode，就变成了人能够看得懂的自然语言。然后这个自然语言又回到了中间这一层，就是我们的agent开发框架这一层。
	然后在这个过程当中，有的时候我们需要把这个场景做的更复杂一点。比如说我不是一个请求，一个回答就结束了。可能我还会提供像聊天记录机器人一样的服务，我会有多轮对话。这个时候我们会需要使用一些数据库，有可能是像sole light这样的轻量级的关系型数据库，也可以去使用向量数据库，那就是RAG。我们在很多个不同的场合都讲过的RAG的经典的八个流程，eva iy模型变成向量，向量存到向量数据库里。但无论如何，这里这个流程最终就可以通过这个模型加上数据库，最终拿到一个结果又返回到用户界面去渲染这个结果就是我们看到的radio里面拿到的结果，再转换成前端可以显示的形式，就走完了一个典型的场景。在这个场景里面，欧ama提供的这个价值其实就等价于我们刚刚在前一幅图里面看到过的GPT4。也就是OpenAI或者说SOO pic这样的公司，他们提供的大模型的服务，他们是完全等价的。
	好，这个是我们了解了欧拉A是什么，它的典型使用场景，它的生态定位是怎么回事之后，我们得再看一看为什么他能成功，对吧？我们知道的他what，但是why很重要。那为什么能成功？其实从它的核心功能和特点是能看出来的，它的长板很长。
	最近有一个很有意思的讨论，就是我们的很多同学的教育，其实一直在讲不要有短板。但这个思路其实我是这种观点其实我自己也是一直不认同的。但最近刚好有很多人在讨论这个事情，又开始讲了。其实不需要把每个短板都补齐，木桶理论的适用场景是很有局限的。更多的时候，作为一个这个人或者说在在社会当中的竞争的状态的时候，其实你是需要把你的长板做到足够长，做到可能全世界只有1%的人有这个长度，甚至只有1‰的人有这个长度。这个观点其实在我刚刚进入华为的时候，在有一次看这个任总的采访，他也讲过，对不对？就华为的这个任总讲就是要把长板做长，他也是一个长板很长的人，他讲他自己不会做饭，不会进厨房佬，被他的家人去去取笑，但是他自己的长板足够长。所以欧拉玛其实也是一样的，包括南茜早期的成功也是如此。
	我们看到欧拉玛它的优势在哪呢？第一它是足够简单，我们易用性是它对于绝大部分接刚刚开始接触大模型技术的用户来说非常重要的一个特点。包括PyTorch打赢了TensorFlow也是一样的原因，它的操作极其的简洁。待会我们可以看到他的指令没有几个，然后运行起来就可以了，然后也不需要你再做额外的各种配置。然后他也做好了很多预制的参数和模板，这跟年轻人非常像。然后第二就是他的扩展性很不错，他知道他自己的定位是什么，他没有想什么都去做什么都做。所以他把他自己的这个大模型的host，然后这个管理这一侧做好之后，别人都可以基于他再去做这个延展的一些扩展的应用。
	这个也是它的一个很重要的特点，那上面的核心功能就是它不断的演变之后做出来的，越来越累加的这个先发优势了。包括它支持多种模型格式，这个GGUF待会我们会去讲3 tensor，用过hugin face的同学应该都听说过这个模型格式，然后它的运行效率不错，可以支持这个大规模模型，就是尺寸比较大的，比如说这个千亿的这个模型，它也能够去做部署，然后支持这个呃多模型的管理。它可以在欧拉玛的这个命令行界面，包括带有这个YBUI的界面，像ChatGPT1样，来一个下拉框，然后里面去选我要用哪个模型，跟docker的这个思路也很像，就待会儿在使用的时候，我们再去看一看，就很方便你能去在本地拉一堆的模型权重文件下来，然后它能帮你去快速的加载卸载不同的模型。
	这个是我们看的欧拉玛的命令行的操作界面，左边是一些常见的操作情况，比如说我们看到左上角欧拉玛，我们装好之后，欧拉玛help就可以打印这个指令了。我们第一个agent其实就做过这个事情。大家如果有印象，我们的command 21开始在0.1版本就已经支持print help了。所以做一个命令行工具，这应该是第一步，得让人知道怎么用的，只不过可能我们写的没有他这么细，比如说usage里面有分这个flax和command，没有分这么细上面有一些指令。
	Serve就是去启动欧拉玛的这个demo service，这个守护进程。如果去细心的一节一节课学过来，会发现我们好多概念是一点一点垒上来的。如果你跳了的话，你又没有对应的知识，可能会比较难受。但如果你是一个小白，但是你又一节一节学过来，应该不会有知识上的太大的空缺。所以这个欧拉马本身它是一个demo service。然后启动起来之后，它的这些所有的加载卸载管理都是那个service在进行真实的操作。然后我们可以通过欧拉玛的命令行去list你当前本地有多少模型权重，然后也可以通过PS去看看你现在加载了多少这个模型到GPU里正在运行，当然常见的copy remove run都有。
	然后你也可以通过这个类似的方法，比如说这里我们看到在我的本地，在我的这个GPU服务器上有lama 3.1的最新版本，这个是一个80亿的版本，然后有金马兔的20亿的版本，有金马one的70亿的版本，还有这个nama 3的这个8B的版本，然后它有对应的修改时间。这个用过doctor应该就会很有感触。就很像，然后我们去展示对应的这个模型模型权重，它也会显示非常全的信息。就比如说我们这里展示这个nama 3.1，就能看到它里面会显示这个80亿，然后上下文长度最长支持多少，然后这个in em beijing的这个nse长度是多少？如果我们要去运行它，直接通过run方法就好了。
	就我们help里看到的run a model，alama run，然后后面跟一个具体的名字，这个就跟doctor run非常像。这个名字就来源于我们list出来的这个name。我们在做这个开源哨兵最早的这个命令行交互的模式的时候也是一样的。我们可以list出来一堆订阅的项目，这个项目就是跟这的这个name其实是一样的。我们去export一个项目，我们去generate一个项目，都是类似的思路。然后用起来之后，它是一个跟我们使用直接在命令行去使用python横向的一种格式，它是交互式的。然后你说一段话，我就给你一段回应。当然它也支持这个多行的输入，这个我们就不再赘述了。
	大家能看到右边是一个简单的示例，然后在这个交互过程当中，你也可以去让他打印出一些关键的指令，比如说这个逃逸制服加这个by就是退出它的一个意思，你直接写一个EXIT是退出不了的。这些就是大家只要自己去跑一跑，应该就能知道的。所以它整个命令行的操作是非常简洁的，但是总有的同学不喜欢，包括我。比如说我要去做历史记录的管理，我有多个对话框怎么办？这都是ChatGPT教育市场之后，所有的用户都知道的，我们不再习惯于用命令行了，命令行可能去管理一下模型，可以当我去用模型的时候，我希望有一个更舒服的界面，怎么办呢？这个欧拉玛的web UI，我们待会儿会去细讲。
	我先给大家抛一个图让大家看一下。这个界面其实就跟ChatGPT几乎是百分百复刻了，包括它的提示信息，就是这个大模型可能会有错误，这是一个非常重要的提示。在整个图的中心的下面的部分，然后整个这个欧拉玛YBUI，现在这个截图是它的新的名字叫open web UI了，它其实是功能非常强大的，待会我们会留一些时间给大家做这个介绍和演示。这个是它的图形化的界面，会非常好用。
	大家也能看到这能选不同的这个模型，怎么样去装一个罗拉玛？最简洁的就是去它的这个官网就有它的downal这个链接。大家能看到这个对应的网站链接，我也都放在标题下面了，大家可以直接去找。然后不同的操作系统，大家常用的mac、linux、windows都有。然后我们肯定整个课程主要是以linux为主的。然后linux是使用这儿这一个对应的安装脚本，install点SH这样的一个脚本。然后这里再去执行它这种管道操作符，其实大家看到这个管道操作符LCEL林茜的表达式语言也是这样的，因为整个这个管道操作符在需要脚本时代是非常常见的一种做法，所以写过笑的再去看LCEL应该就不复杂了。
	那用docker去运行我们的欧拉玛也是可以的，并且还挺方便的。只是需要大家对dock er有一个初步的了解和使用这个我们再再待会儿会去给大家讲一讲。像open YBUI，我们就直接用docker去拉起的。
	这个截图是因为我看了一下奥拉玛的中文的一些文档，可能有的不是很新浪。所以把这个相关的跟最重要的安装这部分的文档，包括它的命令行界里面的使用和这个rest API的一些相关的文档，就放在了我们的这个agent项目里面。有兴趣可以去看一下，在0.6的dox目录下面，read me里面也新增加了一个对应的跳转。
	好，如果我们把欧拉玛变成了一个容器应用的话，其实这个是一个比较常见的应用状态。我这边不展开讲，大家回头可以去看一看，就是把欧拉玛加载一个的模型，然后后面有slow flag这样的数据库，然后上面是一个向量数据库，整个是一个容器服务。这个容器服务就是我们刚刚前面看到的broker around这样的一个形式。
	多跳了一个，接着我们通过它就引入到一个很关键的问题，那就是欧拉玛。我们开始很抽象的聊了，它是去管理大模型的，但大模型有很多种，那欧拉玛具体是怎么管理这些模型的？这些模型又是长什么样子的？这个就涉及到我们接下来要讲的部分，就是欧拉玛的模型管理和他去描述模型的一个很重要的文件和语法，叫model file。
	要管理模型第一步就要解决哪里去找模型，对吧？模型是在哪里的，最常见的在这个时代我们看到的大模型的权重会首先在hugging face上面去开。比如说meta，它的lama 3.18B包括其他的一些权重，或者说其他的一些规模的。那么3.1也都在hugging face上面有开源的模型权重，对应的UIL也都在下面，大家有兴趣可以去了解一下。这个在微调训练我们讲了很多了。我们能看到他上个月的3.1B3.18B的这个模型权重就被下载了将近50万次，是非常夸张的一个测试。
	用这个下载量，那么欧拉玛它除了是一个工具以外，他自己也做了一个model library，就是模型库有点等价，类似于我们刚刚看到的hugging face hub的模型页，这个欧拉玛自己有一个页面，然后这个页面里面有一些很重要的关键信息，比如说它的nama 3.18B。我们左下角有一个不太明显，黑色的是它的UIL，就这个页面的UIL是lama 3.18B，这个形式就是在欧拉玛里面的模型的name，3.1，然后8B是它的尺寸，你也可以改成70B，只要你的GPU是够的。那么这个界面里面显示了我们看到的llama的一些lama 3.1的一些基础的属性，包括它的参数，它使用到的这个模板等等。这样的一个模型页，其实在欧拉玛的模型库也有很多。我不清楚用过他的同学有没有去逛过这个模型库，这里面有蛮多的信息，包括一些技术文章，然后这里也有一些下载量，那我们就能看到lama 3.1在欧拉玛的这个市场里面，当然它不是上个月，它是累计的。但因为llama 3.1才刚刚出来，5到6周的时间，只有我们能看到这是290万，跟刚刚hugin face上面的差距还蛮大的。我不清楚是不是因为欧莱雅的这个model library没有被大陆给强调，所以可能很多人都能直接访问他了。但他跟face那个是被强调了，这个我们是比较清楚的。
	所以大家也可以去通过这个欧拉玛的模型库去下载对应的模型权重，应该是能直接访问的。没有记错的话，那模型文件长什么样呢？就我们刚刚看到有有这个模型的规模，有一些模板，然后有不同的库都在维护和管理这些模型文件了。
	那具体长什么样呢？左边是我们看到的hugin face上面的模型文件，右边是在刚刚lama 3.18B，在欧拉玛这名字很绕，因为欧拉玛一开始就跟lama是有深度绑定的一个开源项目，它叫open的。我不知道这个O是open还是别的，这个O我印象当中是open，然后的lama，所以它也是个羊驼。他就想把开源的大模型管理这件事情做好。从拉玛依开源之后就带起了一股开源大模型的热潮。所以欧拉玛也支持了其他的大模型管理的这个事情。
	但是它的文件，它的这个模型页展开就不像左边我们hugin face上面能直接看到一个孤立的文件，在这个欧拉玛的模型页，它没有展示文件，它更多的展示了一些模型的结构化的信息。这个是两个平台，他们去对模型去呈现，去把他的这个信息放给大家的时候，一些不同的选择，很有意思。显然，右边是更面向人的一个信息，包括我们看到它的这个headcount headcount这个KV，就是它的一些关键参数，这些其实是对人的阅读是很有用的。左边这个其实就是简单的一个文件目录，那这里就出现了safe tensor，就我们这儿看到的这个safe。然后这个模型显然是被分片了，被分片成了四个部分。
	01到04这个是两个不同的平台对于我们的模型文件的呈现方式。那么除了我们这儿看到的这个sip tensor以外，这个模型的权重文件到底都有一些什么样的格式？我们平时去加载这个大模型的时候，其实上过之前课程的同学可能有一些印象，就是我们可以通过transformers from tree train就直接加载了hugin face上面的模型了。但如果我们现在不只是hugin face上的模型选中文件，像奥拉马有自己的模型库，别的地方可能也有一些模型问题。包括你自己微调完了之后，有一个模型的这个权重到底要怎么样去加载，这个其实取决于它是什么格式，然后用什么东西来加载。首先我们需要理解的就是到底有哪些主流的模型文件格式。这里我们特指深度学习大语言模型这些以神经网络为主的模型，这儿我们简单列了一下，但是作为一个大家可以去详细展开的一个影子。
	首先最上面这个叫GGUF，是由grow这个公司去设计和实现的一种模型格式，也是欧拉A这个平台主要支持的优先支持的模型格式。它的优点就还是比较明显的，它能够实现高效的存储和快速的加载。如果用过下面transformers的这个格式的同学应该是深有感触。就我们用包括我们在微调训练营里面教大家用hugin face的这些模型文件格式的时候，在加载的时候大家可能会感觉有点慢。尤其是可能只是加载一个小几十亿的模型，他会花10秒到20秒的时间。但是在欧莱玛里面用GGOF去加载的时候，会快个几倍的时间。这个大家待会儿有空可以自己再去体验一下。
	当然缺点就是目前主要就是欧拉玛的平台在用这个格式，它的生态系统和工具支持还不算特别的健全。而这个格式本身也不是一个像下面这几个非常久的一个状态。然后使用场景就是当我们在这个资源比较稀缺，然后网络资源和这个算力资源都比较稀缺的环境当中去部署大模型的时候，可能GGF会是一个还不错的选择，也是欧拉玛的主要格式。
	那下面这两个是我们刚刚已经提到过的，是由hugin face去主导的一个叫transformer format，就是最早期的在hugin face上面的模型格式。那下面这个叫也是现在hugin face的模型几乎都已经全面转向这个substance sors这样的一个格式。这个sentences它的名字就已经体现出来了。它在设计的时候有一个最重要的就是要保证安全性，防止一些漏洞。比如说有人去篡改了模型权重，然后再去去或者说做了一些别的修改。咱们没有发现，这些都是hugin face作为一个应该现在是全世界最大的模型权重托管平台要去负责的一些事情，所以它的侧重点不太一样。
	然后这个transformer的format，因为最早我们在讲这个微调的时候就提过这个transformers的库也深度的去使用它。我训练包括bert的这个全量微调，然后这个OPT等等这样的一些小企事业模型的高效微调，都用了transformers的这个库。所以它的优势就是它的整个开源社区很丰富，有大量的预训练的模型和工具可以去使用。场景不太一样，它是在对模型本身进行微调训练，这个阶段非常好用。然后又借助了hungry face这个平台可以去分享，是这样的一个场景。
	然后在上面的这三个，包括微软的unix，google的TensorFlow的save model，包括衍生出来的TF light的这个模型，patch的这个模型。这三个其实都是在上一波深度学习时代非常有影响力，非常受到关注的模型格式，各自有各自的框架的特点。Alex当时推出的时候，微软在主导meta也做了很多的贡献。
	它的这个设定就是我们要不同的深度学习的框架开发出来的模型，最好都能够去用onex去表达，它有一个像应该叫什么，像这个转接头一样的一个存在，大家都可以接过来，用我的这个框架的语法去描述，然后能够让不同的框架之间的成果都可以得到享用。比较像的一个现实生活当中的一个场景，就是索尼的这个微单它的镜头很少，但它可以有一个转换的头。这样佳能和尼康的这个单反镜头，索尼也都可以用了，它就像这样的一个设定。TF跟patos各自主导的，因为他们的算子实现差异太大了，所以几乎是没有办法互相转换的。
	这个是目前我们看到的以神经网络，以深度神经网络为主要原理的AI的模型，主要支持的一些模型格式。在大模型这一轮，其实主要就是我们看到的GGUF这个hugin face的两种格式，包括patch ch的格式了。当然TF也有，但是它的使用的总的人数相对来说还是少一些，然后alex就更少了，因为它的设定就是转接头，它本身不是一个对应有训练框架的一种格式。
	刚刚那个表可能很多人就看着很复杂了，所以为了这个有的同学不想去了解的太深，有一个too long don't read的一页总结，就是太长了。你不用看前面的表格的话，可以看看这一页，有一个一句话的总结，对于每个格式。好，这个是关于我们知道的大模型的这些文件格式的一个了解。欧拉玛的模型库文件我们刚刚看过了，它主要是使用的GGOF这个格式。这个格式我们稍微再讲一讲，因为这里就引申到了后面我们要提到的model file。
	GGF的全称叫grok generic unified format，是大家看了这个unified format，其实就有点像我们docker的这个思路了。然后它本质上是专门用来做大模型的存储和分发的一种文件格式。然后通过这个文件格式可以去高效快速的加载这个模型，也能够快速的去写到这个对应的持久化存储的上面的一个模型。它在资源紧缺的这种场景下是非常有效的。然后这个格式还可以尽可能的在不降低性能的情况下，让这个模型文件本身变得比较小。就可能是比如说我们在刚刚看到的pg in face的那个页面上，这个c tensor去以这个文件格式去存。3.18B需要20GB左右的一个硬盘的空间去存储它，但是用GCUF应该是只需要更少的尺寸，但这个具体数字大家可以回头去查一查。
	然后欧拉玛它是优先去支持导入GGUF这个格式的。然后待会儿我们也会去讲，怎么样去用这个格式去创建一个模型文件，然后用这个模型文件去创建一个实际运行的模型服务。然后这里得提一嘴这个GRG OKR grow这个其实是一家公司，它是一个专门去做AI的开发和优化的公司。然后在这一轮大模型的领域里面，也有一些声量，尤其是欧拉玛用了他们的GGOF之后，大家就关注到了他们。然后它本身这个欧拉玛的项目不是由这个公司去发起的，这里是需要给大家澄清和让大家了解的一个点。
	大家能看到GGUF就以它为后缀，其实是可以把它变成一个特定的我应该这么讲，就是他就跟我们用broker一样，它就像一个基础镜像一样，或者说他描述了一个原始的模型文件，就像这儿这个语法。比如说from mci a33B，然后量化Q40的1个GGUF。这个GGUF就是一个文件，然后里面装的是沃克尼亚33B，然后量化的这样的一个模型。从这个开始，下面可以去做一些额外的一些描述，然后我们基于一个特定的GGF，假设我们啥也不改，就直接基于这个GGF去做这个模型的创建也是可以的。然后就相当于我们把一个模型文件变成了一个可运行的模型，然后真正要去运行它，就通过run方法就可以了。
	这个应该是两个阶段能比较明确的说明，一个是模型文件的阶段，描述了这个模型是什么。然后你通过create这个命令就把它变成了一个可以运行的模型。真正要运行它要通过run方法。然后在刚刚提到的这个model file里面，除了from以外，还有很多其他的指令，这个from是一个必须的就必要的在模型文件里面必须要写的一个指令。就像我们刚刚看到的这个沃克尼亚的这个文件一样，它里面至少你告诉我基础的这个模型权重是什么，这个就是from。
	接着还会有一些其他的指令是可以去设置的。比如说这个parameter，temple int, system adapter，license的message。然后parameter是最相对来说是最多的，里面有各种各样的参数可以去设计。然后我们常见的比如说经常调API的同学知道这个temperature就是模型的生成多样性的一个参数，我们设置成0，它几乎就没有任何的变化。那你也可以设置成这个一，它可能同样的这个提示词，它每次生成的结果是不一样的。这个可以通过读对应的这个文档，大家可以去了解它的一些默认值什么的。待会儿我们也可以通过web UI去更简洁的去查看这些值。
	Temp late主要就是跟我们聊南茜的这个template ate很像，这里有一些prom tempt，可以去直接发给我们要去运行的那个大模型。然后system就是有一些系统级的指令，就像我们在前面的这这个课程当中教给大家的这个开源哨兵的agent也有一个系统级的message，和这个system的这个message里面就是告诉他一些特定的角色任务参考格式。Adapter主要是考虑到我们真实的托管大模型的时候，有的时候是因为开源的大模型权重不支持我们的需求，所以这个时候我们需要去对它进行微调，也就是咱们微调训练营主要教的内容。然后在这个微调里面，大家都知道loa，包括这个QLA，应该是最常见的PFT的训练方法了。然后他训练完的成果其实是一个适配器，一个adapter。
	这个adapter是在原始的权重上面，就像plugin一样去直接加上去的。我经常举的这个例子就是，如果喜欢看看这个高达的这个高达seed里面，刚出场的这个强袭的这个高达就是裸装，什么装备都没有。后来通过不断的剧情推进，有了这个翔翼型的可以在天上飞，推进力比较好。这个重炮型的就是远程攻击比较好啊。有这种重火力设备或者是这个巨舰型的给近战破防破甲的能力比较强。那这个Q罗A的逻辑是一样的，因为罗A的训练方法就是保证原始的模型权重不变。
	然后我单独训练一个小的adapter，这adapter的参数可能只有原始模型的千分之几甚至万分之几这样的一个比例。Adapter直接加回到原始的这个模型上，让它真正在使用这个模型的时候会同时走旁路的这个Laura adapt。把这两个结果再一合并，就得到了微调之后的一个结果，所以adapter也是一个很重要的指令，那这些指令大家可以在下面的这个文档里面去详细的看一下，它可以设置什么值，我们这儿就不展开了，后面需要用到的时候我们再去提。
	然后一个最简洁的事例就是欧拉玛提出来的这个角色扮演。就各种各样的character AI就是模拟人的这个AI，都是最近应该是在各种各样的场合都很火，包括也有些公司拿到了不错的融资。这儿欧拉玛也举了一个类似的例子，就是我们假设用lama 3.1这个模型，我们这个模型还给了他一个出厂设定。这个出厂设定就是让这个模型去扮演一个超级马里奥里面的那个马里奥，就这么一个简单的事情。
	好，那他具体怎么做的呢？我们看到首先他要去把基础模型给破下来，要把它的模型权重给拿到。所以会使用欧拉玛的pull拉马3.1就拿到这个模型文件，模型权重的文件，然后这个是模型的权重。接着我们要去创建一个特定的应该叫马里奥的模型，这个马里奥的模型需要有一个，对，所以我们用两种术语，nama 3.1是模型权重。这个model file是这个模型的文件，用来描述我们即将要运行起来的这个模型的，跟docker file特别像。然后这个model file里面就会写，下面就是它的主要内容了，from lama 3.1，就是我从这个lama 3.1作为我的基础的模型，然后这个parameter就是我们刚刚看到的第二种指令，除了from以外还有别的指令吗？那parameter里面把这个temperature设置为一。因为我们不希望这个马里奥的模型运行起来之后，我每次问他同样一句话的时候，他都给我同样的回答，那就很不好玩了，我们希望这个模型足够的有趣，同样的问题有不同的回答。
	System就是我们的system message，这里就跟我们常见的写这个系统指令一样，你是来自于super这个超级马里奥兄弟里面的马里奥，然后扮演一个assistant，通过这个就写完了一个model file，这个地方我们用欧拉a create，然后杠F有特定的这个model file去指定。这里的这个name，就是指我们要创建的这个模型的名字，然后run一下就可以了。所以整个欧拉玛的语法，它也借鉴了非常多的来自于docker的这个命令那这样就可以去很好的让大家理解整个欧拉玛的模型的管理。
	首先有模型权重，这个模型权重有不同的文件格式，我们前面讲到了很多，它可能优先支持的是GGUF，但它也支持sip tensor，支持pyto ch等等，就包括我们刚刚看到的这个adapter。显然这个adapter现在都是拿transformers的这个hugin face的transformers加上PFT这样的库来进行训练。大家如果之前没有参加过微调训练营的同学也不用纠结参加过的同学可能就知道我在说什么。因为这个就是我们主要的训练方法和用到的的工具。那用这两个训练出来的adapter，通常来说就是hugging face的这个文件格式，他肯定不是GGOF。
	所以这里的adapter这个指令如果我们写到model file里面之后，就可以去直接指定一个我们的adapter的文件。它就是sap tensor的这个格式，它也能够去转换。那这个时候其实我们就会发现model file就有点像doctor里面有一个from的基础镜像。比如说我们from一个特定的操作系统，或者from一个特定的tn sor flow的版本，然后在里面再去做一些额外的安装工作。包括我们未来后面要去做的agent，可能也是一样的。我们from一个特定的操作系统，然后在里面装这个python，装连嵌，装各种各样的工具，装完之后，最后打包成一个可以直接用的应用。这里的model file就是做类似的事情。
	可以在nama 3.1的基础上，我再用adapter去指定一个我训练好的na adapter。在这个nama 3.1上面去加上去，这样的一个方式去创建一个新的模型出来。好，怎么调用我们的大模型服务呢？我们首先通过run就把这个模型起来了，就变成了一个可以在命令行界面去交互的模型同时我们也可以除了在命令行界面调用以外，也可以通过这个API的方式去进行调用。这个因为比较简单，就是调2个API，我们就不再坠这个讲太多。但对应的这个主pattern book也在我们的0点6分支里面有，就是在我们source目录下面有一个jupiter，新加了一个欧拉A的jupiter，有两个调用的这个API的文件，大家到时候可以去看一看。
	一个是文本生成的API，这个就对应着我们在之前讲这个呃GPT的时候，它最早期的API就是纯文本的生成，它的参数就是prot，跟我们最早去调这个年欠的LLM这一类模型也是一样的。当然除了用prompt去生成文本以外，现在更多的大模型是通过这个chat对话聊天的方式去对外暴露服务的。就比如说我们这里看到的这个messages，也就现在我们的开源哨兵使用到的GPT3.5的这个GPT4o mini的这个模型，其实也是通过这个check的方式去给大模型信息的。然后这个chat支持不同的角色，user system assistant。
	这里我们需要注意一个点是说，这个案例并不是指金马二支持chat，lama 3不支持chat。它它其实是欧拉玛在把这个模型做好之后，它就会同时对外暴露这2个API，这也是它非常有优势的一个点，而且它的这个API的形态跟OpenAI的这个接口几乎是一样的。那么这儿也支持stream的方式去返回，流失的方式去返回。这样如果我们有前端的这个界面的话，就更简洁了，更方便的能够看到了。好，这个是如果我们最终要去集成的时候，其实就是用类似这个方式去调用的。
	那有了这些基础的概念之后，我们知道了可以有欧拉玛这个框架帮我们去管大模型，然后欧拉玛这个框架还是目前这个领域做的最受关注的一个框架，然后它的使用也比较简洁。并且它还支持把这个微调之后的模型。包括我在这个简单的开源模型上去做各种各样的模板。然后做做系统级的一些提示，提示词的一个注入等等。这些东西都是它可以通过model file来做启动时候的出厂设置，因为最终它变成了一个服务。那除了这些以外，它也能够对外同时暴露我们的generate和chat两套rest API。这样我们原来如果是调的OpenAI的GPT的API，它现在也都能去调对应的API了，直接无缝切换，换一个模型的这个值就好了。把这里改成model，然后UIL从open I的这个UIL改成我们去部署欧拉玛的这个UIL，这个都是欧拉玛的基础的一些能力，然后也很好用。
	接着我们想到一个问题，就是现在的这些使用的展示相对来说都还比较零散，不管是我们用命令行还是用这个request库来进行API的调用。就感觉好像不是很很对新手友好，尤其是不会写代码的同学，或者说之前很少用网络编程，或者这个命令行的人。所以说有没有可能给我们一个带有前端界面的，甚至是跟trend GPT1个级别体验到的这个YBUI呢？这个事儿其实欧拉马推出之后没多久就有人想到了，并且做得也还不错，这个项目现在叫open web UI，从他的介绍里面我们也能看到，它是一个用户友好的web UI for LL ms，就是为大模型提供的一个用户友好的图形化界面。然后它之前formally之前就叫lama web UI，现在改名叫open的web ui因为他已经不只是服务于这个欧拉玛，他还做了更多的扩展。
	好，这个是咱们看到的一个很有意思的项目，待会也会介绍一下怎么用。这个是它的一个界面的技术，在这儿能看到它的这个界面跟咱们的ChatGPT几乎一致。然后左边是这个对话框，各种各样的对话框上面可以选各种各样的模型，然后也有一些这个模型预制的一些问题，你可以直接选这个问题，对话的形式也是左一句右一句。当然更多的不只是包括这些内容，这个是我们实时的一个截图，也是待会会给大家展示的。比如说我们这个左边就有三个不同的对话框，然后不同对话框可以有不同的system的prompt，就是系统级的这个提示词。这里的translator就是做好了一个，我们把这里展开就能看到这个system from就是一个专职的翻译，也就是咱们的专门用来做翻译的一个对话流。那下面有对应的一些premature，这些parameter在model file里面其实都是支持的。好，这儿我们也可以在待会儿可以看到，在这个界面里面也都可以去做调整。
	好，那到这儿其实我们这个快速入门的这些就介绍完了，接下来我们来实际看一看怎么去使用，然后接着我们可以再讲下半本的内容。好。首先我们在看一下这个带有前端的页面，这个应该是对用户最友好的这个open web UF。还越放越小。这个open YBY它的安装其实是写的蛮蛮详细的了。但我不知道大家这个阅读怎么样。其实简单一点就是我们看到在它的安装里面，我相信绝大部分同学应该是有有GPU的时候会选择这一套今天讲的这个私有化的这个思路。如果咱们有自己的GPU，然后这个时候更多的。稍等一下，卡住了。
	分手。在这里这有这个installation，安装。然后这里有一个是只为OpenAI的API用户提供的，可以走这条。但我觉得这可能绝大部分人用不到，因为没必要没必要。下面是说我们不用在在这里我们不需要用这个open I的API了，我们只需要有它一个类似的前端页面。然后这个前端页面背后的大模型是刚刚我们讲的所有的内容里面的那些基础知识。就是这个前端页面它的大模型其实是欧拉玛启动出来的一个模型服务。然后前端页面上的这些所有的对话，其实背后都是欧拉玛在提供对应的这个模型的结果的生成。然后如果咱们有GPU，并且是NVIDIA的GPU，就可以通过这行的around去拉起这个对应的服务。
	然后这里其实就是我之前通过这个docker run就已经拉起来的这个服务。Docker的一些基础的安装，我们在课程项目里面0.6的分支已经有一些文档给大家做了储备了。这个是0.6的分支，往下面这个配置更新了。当然这个是下节课的内容，下一个小节的内容我们先不讲了，这儿新增加了一个文档，就欧拉玛的安装与配置。那我们点过去，因为它太多了也是一个独立的内容，就没有放到read mi里面。点过去之后，会跳转到这个dox目录，下面有一个欧拉玛，里面会有它的安装部署和服务发布，这个nex是我们最常见的安装办法。然后要去启动它，让它支持各种各样的模型。对它的模型库的地址，然后命令行工具的形式有哪些相关的指令，l rest API，包括我们刚刚看到的请求，它的服务端暴露的就是这些API。
	然后下面的这个docker是要重点给大家提的。如果大家有没有网络问题，然后也习惯去用docker启动这些demo service的话，那那是特别好啊，是一个好习惯。显然欧拉玛跟刚刚我们要看到带有前端的这个web UI都是可以通过docker来进行启动的。
	然后因为我们有有做这个对应的推荐，如果你自己要用私有化的话，至少是一个invidia t4的级别的GPU，这里的这台GPU服务器也是对标着我们课程里面的最低的GPU要求来做的。所以在课程里面讲的这些性能，如果你用的是T4，那几乎就跟他一致。但如果你自己本地有一些卡，像这个4090之类的卡，理论上是比T4要快得多的那也可以用对应的适配的操作系统的这个指令，你来进行一些对应的安装。那这里的欧拉法可以使用这个doctor来进行安装和启动对应的容器，这个是英vidia GPU应该去使用的这个容器。
	然后它最终的这个启动方式其实很简单。如果你本地的docker都装好了，这个是依赖包，这个都是一些前置依赖。如果你的docker都装好了，然后你本地也有GPU，理论上通过这一行就可以启动起来。这个dock road杠D使用你的所有的GPU，然后把你本地的这个硬盘上的目录映射到这个欧拉玛里面去。暴露的端口是11434，这个是欧拉玛的监听的端口。然后这个容器就叫欧拉玛，然后最用的是欧拉玛最新的这个doctor镜像。关于这些部分，大家可以回头去详细看一看怎么样去使用。
	好，我们回到web UI，web UI也是类似的一个思路，那这里我们可以使用这个版本为GPU support，这个版本是替代了open web UI的前端，也有对应的欧拉玛的后端去做支持的。然后我们这一行运行完之后，启动起来，这个杠D就是以demand service的方式去启动，docker的基础知识，大家可以稍微跟ChatGPT聊一聊，这行指令告诉他，这里面的每个参数什么意思都会告诉你，我们就先不去扩展太多外部的知识。那这个启动起来之后，我们会看到就有一个这样的界面了。那你刚启动之后，它应该是一个我开一个利民的，应该是长这样的，就是他的一个logo。
	这么卡吗？
	对他也是带有这个注册的，所以大家也不用担心，我也不担心你们那个，但你们现在访问它是是可以注册的，就能直接进去了。但我会会关掉的。一会儿那这里可以注册，你自己注册一个账号，有邮件什么的。那我已经注册过了，我就登录，然后对邮箱，密码。登录进去之后，这个界面这个匿名的这个界面就跟ChatGPT几乎是百分百复刻了。然后我们刚刚看到的这个关于我们的聊天的一些参数设计，包括我们的to call和这个function calling之类的一些功能，其他也都支持了。整个这个open web UI是非常健全的功能。这个是推荐给大家用的一个工具，尤其是你自己有这个GPU的资源的话，用它就相当于你拥有了一个私有化的HIGPT了，包括这些参数其实也都可以调，那在哪儿调呢？这里有个管理面板。
	在管理面板里面有这个city，这可以调非常多的内容，包括我们的模型，这个模型如果我们要新加一些模型，可以在这儿直接去输一些对应的这个名字。比如说我们现在看到的这个workspace里面有两个模型，一个是金妈兔的20亿的模型，一个是lama 3.1的这个模型。假设我们现在想新增一个模型，可以通过这个去做。但这些大家都可以回头去看他的文档，就不逐个介绍了。但是也可以比较好的跟我们的这个模型库联动起来。我这儿就用回非匿名的模式了，不然很多都用不了这个历史记录。就比如说我们这儿看到有一个。把这两个都展开。
	我们还是到管理面板。
	然后这里sitting。假设你现在新启动的，你应该是一个模型都没有，那我们这选模型。然后这里假设我们想新增加一个模型，大家能看到这里有写的pull model from allama点com。然后我们在这儿这个模型库，这模型库就是在欧拉玛的官网里面，对欧拉玛点com，这俩其实是完全对应起来的。然后这里有models。
	比如说我们刚刚看到的nama 3.1在这里，金巴兔在这里，比如说千问二在这里。假设我们去想要去获取一个千万二的模型，千万27B的模型，那这样我们其实只需要QWEN two。其实就跟这儿对应起来的，QWEN two它默认你什么都不写，它这个前面是model后面是tag。什么都不写的话，这个千万二就拉的是7B的。但如果你在这儿选了千万21.5B你会发现这个tag就会变成1.5B那假设我们这里就去获取一个千万2的1.5B，然后为了去获取这个道路的你。
	可以在这儿也可以跳转过去。然后这个写好之后，你可以直接点这里的下载，它其实就在拉这个对应的。模型，然后拉开之后，它有这个many face，拉开之后应该在这儿就能看到，还没刷新。这里已经验证好了。好，它就弹出来了这里我们就看到这个千万二的1.5B就给拉下来。这儿大家看到这一段就跟我们在前面讲到的model file的部分是一样的。你可以给他一个名字，千问2，假设我们这个啥也没改，这些部分就对应着我们前面讲的model file的一些语法，把这个拖过来大家简单讲一讲。这里你看它有一些预览的这个字，其实就对应着我们刚刚讲到的model file里面有各种各样的语法。
	From这个require的部分已经不需要了，因为在左边的这个界面里面，这个model就是它的from，就我们这里的千万21.5B显然就等于这里的from了。然后接下来我们在这个model file里面要写的，一个是这个create，我们要有一个名字，就是我们在mario里面看到的，这个最基础的启动操作里面，这个create a model，这里的填其实就对应着这里的名字。然后接着我们下面要填一堆的这个内容，model file的内容其实就对应着这里，我们支持的各种各样的model file的指令和语法。只不过from已经被代替了，已经有了。那这些语法就包括我们这儿看到的各种各样的，尤其是我们常见的这个parameter在下面，常见的各种各样的parameter temperature等等，这个top k，top p等等，这都是一些常见的可以设置的参数，这个大家有兴趣可以去详细了解，在这儿就能创建一个模型了。那我们看到拉出来这个模型之后，我们回到workspace。
	它就已经有三个模型了，千万二的1.5亿。现在假设我们要新开一个new chat，这个new chat里面就可以选，我先把这边打开，让大家看到后台的运行，这个是我们看到的open web web UI的这个前端的页面。左边其实是同一台机器，我们可以看一下GPU的使用情况，用watch指令就可以了。大家看到现在是这个GPU是空转的状态，因为我没有用，没有对话。这也是欧拉马做的比较好的一点。它不会让你的这个GPU被加载起来之后，就一直把那个显存给占着。
	我们用过transformers来训练模型，微调模型的时候就会发现很烦。因为它底层用的是PyTorch排第二期是不太好能帮助你去把模型卸载下来的。因为它的一个训练的这个场景里面，但是欧拉玛是一个部署推理的场景，所以它会自动去识别。如果你有一段时间没有去用这个模型，它就会把这个模型从GPU里卸载出来，这个是它的好处。
	但是这里也有一个小的坏处是什么呢？就是我们想象它的这个优化是为了GPU的最大利用率，你不能站着什么不拉什么，对吧？那那如果你确实有一段时间没有用它，现在你要用它的时候，你的第一次调用是会有一点点的额外开销的。这个开销就是它重新把模型从一个模型文件加载到GPU里去。
	这里是有一些开销的那它的实现方法也很就要解决这个问题的方法也很多。假设你是一个需要这个模型服务长期就待在这了，那么你可以通过这个心跳或者一些别的方式，让他能够一直把这个模型放在这个显存里。好，那我们可以来实际操作一下，看一下这个千万21.5B然后我们直接就用它给我们的推荐实例。右边这里大家看起看到没有？这里有一个欧拉A的nama server就拉起来了。拉起来之后是用了1762兆的显存，然后这个是我们的一个示例，直接让他给我们的这个示例。
	然后同时大家可以看到，假设我们再开一个，让我们选一个别的model，比如说这个金马two。看他的代码实例会给我们影响，这里会再拉起一个新的进程。这个进程其实就是金妈兔的一个进程，这儿就是在等，这个等就是因为大家看到这里刚刚一直是162兆，162兆其实是欧拉玛这个进程刚刚拉起的时候的一个占用。然后直到把整个G8 two加载进来之后，变成33G多一点点的这个输出。它才33G多一点点的这个显存占用之后，它才会有对应的输出，因为模型才加载回来。这个就是我说的会有一点时间上的开销。但它一旦加载起来之后，基本上这个输出就是实时的了。因为它使用了streaming的这个输出。比如说我们这里跟他说的是show me a cold sleep，就是给我一个网站的这个sticky header，用CSS的GS的的方式来实现。但是我们知道金马two，我们看它中文字是怎么样，我印象中他应该是支持中文的。
	这里它就已经能很快的去做输出了，这个merge sort。对，他是能听懂这个中文的输入的提示词，输出的结果会更优先的去生成这个对应的英文。从这个角度来看，其实会解决两个问题，第一个问题是大家如果对这个ONI的网络访问有一定的问题的话，你只要有一台带有GPU的设备，其实你自己就可以用开源的模型去实现类似于ChatGPT的体验了，就包括我们刚刚看到的这些，大家如果对多克尔不太熟，我们比如说就这一行。
	我看看能不能用中文回复。已经可以了。并且大家还能看到金马兔使用到的一些这个叫什么？叫提示工程。This break down this docker command, stay by step in chinese. 
	这就是一个很常见的思维链，在星巴克里面它他把这个也输出出来了，这儿他还在运算卡的有点久。但因为我们的右边的这个T4，其实是有16GB的一个显存资源。所以理论上我们这个时候再加载一个lama 3.1的8B它也是能够正常运行的那我们试一试金巴兔去回答这种问题的时候是有。
	当然这里它也会重新拉起一个大家如果有看到右边的话，应该能很明显发现这个加载的过程。
	这里占用了六点几个G但这个生成的结果就快得多了。大家看到比如说这个nama 3.1的这个81的版本，已经把整个结果都生成出来了。解释的也都比较清楚，就杠D就是DM的模式，杠P是端口映射。
	刚GPU就是指这个docker能用多少的GPU，GPU后面可以跟01234GPU的编号，GPU number, 就我们这儿看到的这个其实就是GPU的number 0。如果你有多个就01。然后刚刚的咱们的这个金马兔，不知道生成没有一点。
	他他生成到一半就没了，好吧。这个应该是比较常见的一个情况。如果它生成到一半我们切走的话，它会把这一次的生成到一半的结果给遗弃掉，做不掉。
	然后细心一点的会发现，其实这里我们已经演示到一个想要的效果了。就是这个是欧拉A的服务端的行为。因为它背后启动的其实就是一个欧拉A的server。然后这个服务端大家会注意这里只剩两个了。然后因为这个模型尺寸不一样，所以很明显能看出来最开始我们启动的这个千万二的1.5B已经释放了，这里的资源已经没有了然后现在应该就是活跃的最后这两个金巴兔的两笔和这个nama 3.1的这个芭比是还在运行的那一段时间。如果我们不跟这个金巴兔去进行对话的话，它这个资源应该也会释放掉，这个是前端页面我们能看得到的。然后除了这个OpenAI的外部UI，除了我们能在刚刚看到的欧莱雅的模型库里面去找各种各样的模型以外，为什么要给大家提这个前端？是因为它其实有好多模型库是这个open IVBOI的社区自己在在玩的。
	从刚刚我们这个地方启动之后，这边是create model，下面有一个叫做made by open BOI community，在这里面大家可以去找到各种各样的模型，包括但不限于这个文本生成的模型，也有很多这个纹身图的，包括多智能体的，要稍微放大这个top models，这有一些对应的排序热点。然后它下面应该还有标签的assistant，然后做数据分析的，包括做各种各样角色的character的，然后做这个编程相关的，然后纹身图的应该是，我先看看这个。
	纹身图的，我记得是在这个。应该有一个特定的标签，比如说类似于这个mida geri的prom generator，就专门为纹身图去生成对应的这个提示词的。做这个优化的也有，做这个character的也有，这个就是大家自己去探索，我们就不一一去介绍了。
	然后这个是咱们的前端的一个页面，然后这边现在金马兔也没了。然后接着我们再讲一下，就回到我们的这个agent里面来，agent开发里面来，就是前端页面我们如果想要用类似于ChatGPT的私有化版本，有这样的一个方式可以去用了。但我们最终开发的这个agent还是要去调API的。所以我们在咱们的项目里面。Source目录。
	下面大家。
	看到to Peter新增加了一个欧拉玛。这个欧拉玛就是去展示咱们调对应的这个欧拉玛模型的一个示例。网的问题我们直接看这个，这就是对应的这个欧纳玛的文件，主要就两个request库的使用，其实非常的简洁。一个是用API generate做文本生成的调用，一个是API chat做这个对话聊天的调用。然后它的对应的这个参数payload跟OpenAI的这个格式几乎是一致的。硬要说去这个差异的话，可能就是这个chat返回的这个response的结构，会跟这个OKI返回的response结构略有不同，这个是它的返回结果，这个我们待会会在这个集成里面去去使用。好，我们第一个小节的内容其实就是这些，给大家再总结一下。
	欧拉玛是用来管理大模型的一个工具开源项目，也是目前这个领域最火的项目。它能够像docker的这个指令一样去管理我们的模型文件，模型权重。然后通过model file一种类似于docker file的这种描述方法，把一个模型权重加上启动这个模型的时候的一些配置，包括但不限于这个system message，然后各种各样的parameter，比如说temperature top k这种参数，然后去启动这个模型，变成一个实际运行的模型服务。然后这个模型服务可以以rest API的方式暴露出来。并且欧拉玛本身就帮你暴露了两种接口，一种是文本生成的接口，一种是对话聊天的接口。它默认会走这个11434这个接口。但是如果你希望以docker的方式去运行欧拉玛的话，那么我们也有对应的文档给到大家，让大家去学习和了解。在这个dox里面可以去看一看。
	然后用docker来运行的时候，就会有一个好处，就是你可以在本地。刚刚我们也讲了这个端口映射的这个方法，你可以在本地用无数个端口去映射到里面11434。那么在容器里面，欧拉玛的server用的是11434。但因为有docker的这个好处，那你就可以把容器内部的11434映射到你本机的任何一个端口上。那这样你就可以用别的端口，甚至你也可以去挂载一些这个域名等等。那通过这个欧拉玛，我们可以很方便的把一个在开源社区里面的大模型的权重变成一个模型服务。甚至通过open web UI这样的一些社区的项目，让我们可以像ChatGPT1样去使用开源大模型，提升我们的这个体验。然后如果我们在这个过程当中有一些模型不用了，欧拉玛服务它本身还会帮你去卸载资源，不会造成这个OOM或者资源的抢占。
	这个是关于我们上半节的内容。好，接着我们待会儿会讲一下我们的0.6是怎么去集成它的，这个其实就比较简单了。好看大家到这儿有没有什么问题，我们针对性的来提问。
	目前居然没有评论，是我的网断了，还是大家真没提问，我刷新一下。
	老师用的是阿里郎的虚拟机，怎么配置成现在的这种使用方式？你说的现在的使用方式是指我右边这个VS code吗？右边这个VS code是一个IDE，我不知道你用过VS code吗？如果你用过VS code，你就熟悉右边这个界面。然后VS code有很多的插件，其中有一个插件就叫remote explore，就是给你看一下。
	对我看有评有同学也评论了，这是一个非常好用的插件，建议这个好好用。对，然后你把它装完之后，你就可以再去配置你本地的SSH的这个配置文件，那就可以有无数台机器了。对，就是这样。
	然后再看下一个问题。欧拉玛是一个增强部署和界面的hugging face，我觉得还不能完全这样理解，因为两个公司的定位是不一样的。我们回到这张技术站的图，为什么今天要再提这张图？就是大家的生态定位是不一样的，两者其实目前是没有抢占关系的。我们看到在这幅图里面，模型服务这一层，左边因为是商业化的，所以是一体化的，就是完全在一起。GPT4就是GPT4，我怎么训练的？你别管我这个模型权重是什么，你也别管，反正对外暴露的就是API。所以对于这些闭源的商业化的公司来说，它的最终的暴露出来的东西就是欧拉玛暴露出来的那个rest API。
	但是对于开源社区来说，欧拉玛是最后那一步就是把一个模型已经固定下来的模型权重变成一个API，当然也可以变成别的这个服务形态。然后hugin face是想做的，它的上游就是我怎么方便的把一个模型经过数据的训练迭代变成另一个权重，这个是hugin face做的事情。Hugin face的这个host就是他的欧拉玛下游这做的这些事情，他跟face目前是没有花很多精力，并且我觉得他应该长期来看也不会去做的事情。这两者是一个上下游关系，他们不太像是竞争关系。然后他们各自擅长的点也不一样，他跟face擅长的这些模型训练的东西，欧拉玛是完全没去碰的。
	然后open web UI必须和nama捆绑安装吗？应该不是的，以前可能是，现在不是了。但是这有一个过程，我们这个同学问挺好的。
	你看到这个open web UI，它也有讲，就是他可以去有默认配置的安装。然后这个默认配置安装主要区别在哪呢？就是比如说我现在的这个服务器，就是有GPU的服务器是在北美，然后我现在想在我的本地mac上面去装一个open的web UI可以吗？肯定可以。因为这个open web UI其实就只是一个前端。所以你看到这儿有，如果欧拉玛在你的computer上面，就是那个能跑大模型的GPU在你的computer上面，那么可以有这个方式来启动。这个就相当于适用于什么环境呢？就是你自己买了一个4090或者2080，然后用这个指令是最合适的。
	如果我现在想要把这个web UI放在我这台mac上，然后远端的那个服务器就通过alama的base UIL去注入，这样也是可以的。但我不推荐是因为相当于你的很多请求，就你这个地方的很多的请求，就要从你的笔本地的这个笔记本去发送给这个远端的服务器。然后远端的服务器再把结果给你呈现出来。这个过程当中但凡出现了网络的问题，这一次生成就会遇到一些不必要的麻烦。
	但如果我们是这个界面在这儿，但如果我们是说它它的这个前端页面也是跑在远端的服务器上的。我只不过是去打开了它的这个远端的3000号端口而已。那这样的话它其实就是local host去发这个信息，理论上很难发丢的。对，是这个逻辑。
	然后它也支持这种只用OpenAI的API的话，那它就是这样的一个界面。这个是什么意思呢？就是OpenAI的API不等于ChatGPT。我不知道这个大家应该理解它的区别，就相当于我这儿我不去连欧拉玛的服务器，我连的是OpenAI的API。
	就跟我们待会下半节课要讲的那个事情是倒过来的。就是我有一个前端页面，我连的是OpenAI的GPT。他跟ChatGPT的区别就是它不会被做太多的内容审查和删减。所以我可以用它绕过这个moderation，也就是那些各种内容审查的一些操作。在这儿就可以直接调GPT来实现一些内容的生成。其实最早的套壳不就干这个事儿。
	欧拉玛支持多线程，这个是什么概念？我没没太理解你说的这个多线程具体是想指什么，你是指那个欧拉玛的server吗？还是指什么？欧拉玛和v LLM用哪个好？你用在哪儿？你准备怎么用？
	欧拉玛在生产环境怎么做高可用，万一挂了呢？你是这你这个问题其实挺好的，但它其实不是单独通过欧拉玛来实现的。因为它本身只是一个服务端的，它本身只是一个多块D这样的角色。你的负载均衡其实是要在上面去加一层的，然后那一层其实就本质上跟传统的去后面接一堆web server的技术当然是一样的，没有太大的区别。我不知道我这个描述能不能get到。
	通过模型参数如何计算启动后的GPU占用大小？有没有上过模型微调的同学来给他讲一讲这个模型参数去算要用多少GPU的显存，这个应该是我们微调训练的基础知识。但这个问题你直接问7i GPT也可以。对它其实一个很简单的算术问题，就每个模型参数要用多少的这个资源。比如说他是用的16位的浮点数，还是用的四位的还是八位的，你就可以算他一共要多少字节，这个字节就可以转换成诏KG，这是一个简单的转换，对。
	为什么提供两种类型的API？Chat和文本分别适应什么场景？这个同学这两个问题是不是体现出可能之前没有上过前两节，前两门课这2个API是作用很不一样的。对，你可以简单理解成长期来看chat应该是会替代掉generate。Generate更多是因为开源社区有些模型它的对话训练能力比较差，它只能支持generate强一点的模型现在都支持chat，因为chat可以描述的场景更多，就可以描述更复杂杂的生成的需求，它也就能生成更复杂的结果。
	自己用使用欧拉玛确实比较方便好用。公司生产环境貌似用欧拉玛部署方案用的不多。这个同学，你们公司生产环境是怎么在部署的？不太理解模型权重是什么概念，就是一堆矩阵里面的具体的值可以回头去翻一翻。如果你上过微调训练营，就可以回头去翻一翻里面的课件。如果没有上过的话，应该那个微调训练营的开课开营直播里面有讲过。我不知道那个应该是公开的。你可以去搜一搜。
	欧拉玛只能使用request API吗？如果把之前的OpenAI的接口切换到欧拉A的场景，怎么统一？好问题，能欠帮你统一了，你这个问题问的特别好，就是这样的一个技术战，为什么存在？我妈妈对接了这么一大堆的模型，南茜又对接了这么一大堆的模型，既有开源的又有闭源的那我们能不能用一个统一的接口呢？当然可以，我们那个应用开发实战营是教的主要内容就这些。能欠通过统一的接口去把你刚刚提到的这些API调用的事儿给做好了。包括这个semantic kernel其实也想做这个事儿，但他也只是只做了这个事儿，那现在做的比较多，我们这个企业级的agent。
	第一节课讲过，可以回看一下。我看看欧拉玛是默认从欧拉玛到com下载模型，如果想要的模型欧拉玛的话没有，这个其实开始有提到过，只是我没有去展开讲，这个同学问的，因为它支持很多的地方去下载模型，包括我刚才讲adapter的时候，其实就已经提过了。就你可以看一眼这个adapter对吧？就我们刚刚还专门花了应该几分钟去讲了的，这个adapter就是各种Laura的适配器。然后这个adapter的指令看，这里接了一个pass to sub tensor adapter，然后这里就下一个具体的文件对吧？这个bin就不是一个GGUF了，是一个safe tensor的文件。
	那这个病是怎么来的呢？这个病就从hugin face上面来的。所以首先最简单粗暴的方式，也是通常在生产环境上比较稳定的方式，就是直接把模型权重的原文件给下载下来，作为你的项目的一部分。所以这个病是可以下来的，所以首先它本来支持不同的模型文模型权重的格式，所以都可以加载进去，包括from也是一样的。所以你只要能拿到那个模型权重，你就能够去启动它。然后你刚刚说的那个欧拉玛点com这个下载，那是open的web UI的界面。那个不是欧拉，就是两个东西，它是给欧拉玛做的前端，对吧？你刚刚提到的只能从这儿下载，这是这个open web UI的界面做的。
	假设用妈妈岁70B做私有化的agent能达到生产机的需求，私有化专属客服机器人，私有化调用function calling。你这两个问题我没看懂，联系。然后你的生产及需求能量化一下吗？怎么定义生产及需求？需要先转换成欧拉玛支持的GGUF格式。不是不是，这个同学们不对，他他支持别的这个格式的哟。我们在在。
	Read me里面搂一眼。
	这里有啊，可以从pyto ch或者substance sor来进行导入。
	这里有。
	可以看一下，它都直接支持的。
	现在已经支持了。
	欧拉玛支持多并发吗？你说的多并发是什么概念？我举个简单例子，你看这是不是你说的多并发。我们在这启动了一个金巴兔的模型，然后随便跟他聊一句。
	这里有一个金巴兔的模型被加载起来了，它生成了一段内容，然后我们再启动一个。这里没写。再启动一个界面跟他聊天，这个算多并发吗？我不知道这个跟刚刚那个同学的问题是不是有关联的。就是我们这儿两个窗口同一个金马兔在进行服务，这算多并发吗？我不知道你的这个多并发是什么概念，这里其实是欧拉玛server它自己加载了一个模型。然后这个模型其实它是能支持多个窗口的，然后也许这个OpenAI的YBI前端它自己会去做队列，或者说欧拉玛的server他自己做了这个队列。本地有模型文件，怎么对接到欧拉玛上面部署？这个应该刚刚讲过了，通过你的model file就创建出来了，还专门讲了两阶段的权重，通过model file变成一个model，然后run起来。
	这个同学你还是在群里交流。因为你这个生产机的需求描述的还是很抽象的。就是你来了一个GPT70%的能力，这个东西怎么定义呢？什么叫GPT70%的能力呢？
	好。
	我们QA先到这里，已经9点50了。我们接着讲一下这个0.6的集成。这其实集成的这个实现很简单，所以我们多留了一些时间。QA因为本来新工具每个人的知识储备和技术储备不太一样，所以很难一下服务于所有人。都都讲到你的点上，所以多多QA了一些时间，好吧，我们先接着把0.6讲完，我们再来讲好。
	好，那我们接下来看一下这个0.6版本的这个开源哨兵是怎么去集成这个私有化的大模型服务的。其实代码的变动并不多，很简单。第一个就是我们要去扩展一下LLM这个模块，这个模块一开始大家如果有印象，在0.5版本里面，我们是直接在它的构造函数里面就初始化了一个self client。也就是0.6版本里面的这个self model等于OpenAI的这条路径，直接去构造了一个OpenAI的python SDK climate的实例，就self climb这一行。
	现在因为我们要支持欧拉所以我们会给整个LLM这个大模型的构造函数新增加一个参数配置对象，这个配置对象传进来，这个配置对象的这个做法像不像我们上节课讲的email notify的做法，也是给notifier传了一堆email的配置。这里的LLM也是类似的思路，我们给他传了一个config里面我们会取这个model type LLM model type就是我们这里的OpenAI或者欧拉玛。然后当他是欧拉玛的时候，我们需要知道这个欧拉玛的服务是部署在哪一台服务器上的。它可以是一个IP加端口，包括后面的这个使用的API generate test，API chat. 
	因为我们整个agent是由这个chat来构成的，所以我们就在配置那边到时候会写成chat。这样就简化一下配置项，就不要再拆太细了，也不同时支持chat和generate了。然后就是当我们有不同的这个配置项的时候，当然这里就会有同的这个成员变量。如果既不是OpenAI也不是欧拉玛的话，那0.6版本暂时是不支持的，会抛一个错误出来。然后同样的在原来的这个实现里面有这个generate danger report，会去构造一个系统的提示词，这里都不变。然后我们的这个用户的报告内容就是我们和关掉的issue，然后这里就分别实现了两个内部的函数，一个叫generate report I一个叫generate report alama。然后这两个内部的是我们generate data report实际调用的，相当于他会去根据我们的情况去实际调用不同的函数来实现。那上面的这个report OpenAI是比较简单的，跟原来一样，就跟我们原来的这个版本的实现是一样的。
	下面这个欧拉玛略有不同，打印的这个日志是使用欧拉玛的lama模型开始生成报告，那这个日志也可以去做调整，因为我们可能不用拉玛模型，然后payload，这个payload就是我们刚刚在jupiter里看到的，然后self config，欧拉a model name，这个我们待会讲conflict里面配的那个模型名称，然后message，stream pose。因为我们没有前端的那个页面，需要它一个流式的去输出，我们整体一次性请求回来就好了。然后response就是我们去实际请求，然后拿到了一个结果。这个结果再去获取里面的内容，然后再去返回这个对应的内容，就跟我们的OpenAI的调用逻辑是一样的了。
	好，那conflict这边的新增配置项，configure节省文件里面新增了LLM。跟之前的email一样，我们新增加了二级配置。LLM里面有model type，这个model type可以是o en I也可以是欧拉玛。就对于我们刚刚看到的LLM的构造函数里面的那个判断，然后当它是OpenAI的时候，我们可以选不同的模型，这里也都支持对应的适配了。比如说我们默认用GPT4o mini，比较便宜，你也可以把它换成GPT4O换成别的都行。
	然后如果我们是用的欧拉玛，这里就取决于我们在服务端那边已经加载了多少模型了。就是我们通过这个欧拉玛list能看到的有多少个模型，或者通过前端页面的那个workspace或者下拉框能看见有道模型，就是本地已经把多少的模型权重变成了一个可以随时跑的模型，这是两回事儿。然后这个APIURL就是我们调用的这个模型服务，它的这个UIL到底是什么样的？因为是在同一台机器上，我们这里就填的local host 11434的聊天对话的API，然后要去加载这个对应的配config点PY也对应的做了一些调整。其实就跟我们的电子邮件的配置是类似的，用了这样的一个方法去做对应的获取，然后就拿到了大模型相关的配置。然后这个配置就是我们刚刚在LIM点PY里看到的加载进去的，这里给他了一些默认值。
	好，那我们先把homaro给讲了，代码变动其实并不多。对，homework k其实也是考虑到了上一次的作业布置然后我们会有同学就反馈这个hacking news上面的内容很多，然后这个内容很多有可能第一这个token会花很多，第二就是各种各样的原因。现在如果你自己有GPU的这个算力资源，那其实就完全可以切到用你的欧拉玛的大模型服务来完成这个hike news里面的多级信息的获取和报告生成。这个就几乎就没有太多成本了。所以这里是一个可选的作业。
	因为我们没有要求每个同学都要去有GPU的资源，所以可以在0.6的基础上，0.6这个版本基础上去完成第一个作业。因为第一个作业本来就需要去扩展，就是去把这个信实现一个happy news client，然后实现一个hike news的趋势报告的生成。我们会在下节课去讲怎么去做这件事情。现在这个作业大家还是要自己去去完成。然后这节课是给大家提供了一个可以用本地的算力资源来生成这个报告的途径和方法，一个可选的作业。好，那我们来实际跑一跑，看他是怎么去去运行的。
	我们看到这个。
	LLM. 
	点PY文件去做了扩展，这个是我们刚刚看到的LLM点PUI文件，做了对应的更新，在generate data report里面，我们在这儿根据不同的模型会选择对应的generate方法，对应的这个尖锐方法。然后这个部分，我们其实学会用连线的同学就知道，也可以用连线的方式去做替换。那我们为了不增加复杂度，在第一个agent里面就没有引入南茜，然后南茜的有一个南茜community的包，里面也支持欧拉马，就是南茜community也支持欧拉玛。他们俩本身就是在agent的开发的不同层，开发框架层和模型服务层。所以能欠的community支持lama，这个欧拉玛就能很方便的也可以去调，就相当于实现了这一部分的封装，就能劝community导入一个欧拉玛，就实现了这部分的封装。
	它的实现方法，这里有个相当于彩蛋一样的东西，就是大家可以去看一下应用开发实战营。因为最近给google的这个gm I spring做了一个小的demo，其实就是用了有点卡，就是用了年前的这个欧拉玛那个实现方式的参考。大家可以看一眼，在其实应用开发十3营的项目，这个项目里面有一个新的分支就叫金马。然后这个分支里面我们有一个OpenAI translator，这个open I translator就使用了金马来做实现，跳到中文的文档里面，这里就实现了金马。然后对应的是咱们包括它的配置文件也都支持了这个欧拉玛来调这个调欧拉玛来实现对应的私有化大模型的服务了，这里就直接改一改就可以了。
	然后怎么用的呢？我们可以看到我应该有一个主拍摄图，对，这里有去讲怎么去用能券的community里面的欧拉a module因为是给这个google global的团队做的demo，所以都是英文的文档，这个大家可以自己翻译一下。然后这个地方我们能看到是导一个from template，然后这儿就是我刚刚提到的能欠community里面有这个欧拉玛，这个欧拉玛就可以通过这个方式来来使用。所以也比较简洁，但这个前提也是你你有装装这个我你有装这个欧拉吗？这个大家也可以参考。熟悉应用开发实践同学应该就找不到这个路径那大家回头看一下，这个我到时候也可以发到群里。
	然后我们再回到这儿，回到咱们的这个LM点POI里面，所以这里就能调用咱们的两种不同的模型，OKAI或者是欧拉法的模型。然后为了让大家能够去测试你的欧莱玛部署和这个模型运行的情况，这里我像noti fire一样增加了一个用来做单元测试的启动支持。展开一下，在LM点PUI里面跟note fire一样，我们加了一个这样的知识。具体来看就是咱们不需要再走一个全流程，而是直接去把LM点PY作为我们的主函数来进行入口函数来进入口这个脚本来进行启动的话，它就可以直接去运行。因为它只需要一个配置，也能检查你自己的配置是否正确。
	然后传进去之后，这里是我们写好的一个单元测试的，这个报告的内容，这个其实就是实际来源于八月20到8月21号的年前的进展。我只截取了前面的三个issue，这边就在生成对应的这个报告。然后这个比较慢，是因为我们刚刚看到的它的模型还没有拉起来。这样来实际看一看。然后这个窗口应该是可以。我印象中应该是可以分个窗口出来。
	支持吗？对，支持的是这样，然后换一下。
	我们看到这边就是我我们刚刚在这个地方去测试，他刚拉起来的一个llama的模型，为什么是lama模型用的这个配置，就配置大家可以看到拉马3，默认这个拉马三就是8B的模型，然后我们第二次调用它应该就没有那么久了。这个迅速就出来了，然后我们可以实际的去启动一下。比如说用radio，用带有前端的页面来看，我三个启动方式都改造了，大家可以看到对应的代码。7860。
	选一个欧莱玛过去七天的提交，这边就有显示。使用欧拉玛的这个nama模型开始生成报告。一会儿我在做一个简单的调整，这里理论上应该是把这个欧拉玛的model name传进去，不能直接写实用的lama模型。你假设我用的是金马兔的话，这里应该就是金马兔才对。这个日志需要做一个调整。我们看一看。对，这个已经生成了。但是这里大家注意它其实用的是这个英文，这就大家可以再去优化了。
	就是一个就是想通过这个例子给大家讲一个逻辑，就是我们看到这儿这个提示模板其实是没变的，然后我们用nama 3的3.1的这个8B的模型，我还把它展开。我们这个提示模板其实没有变的，只是模型换成了咱们的这个lama 3，这个lama 3的模型没有让它生成对应的中文结果。这个其实是相当于简单来说就是你的nama 3的8B肯定是没有GPT4欧美Y要强的。所以针对这种情况下，我们就得可能再去做适配的这个提示模板的优化。理论上来说越强的模型，其实模板的要求其实是越低的，就你你的就相当于那个模型越强，你的提示技巧就可以弱一点。但如果那个模型很弱的话，你的提示技巧就得非常强。不知道这个描述清不清楚，那这里我们可以再看看这个，我们先把它荡下来，把这个模型下载下来，把这个报告下载下来。然后我们可以对比一下金马兔生成的这个版本。
	就停一停，保存一下。把这一行改掉。这里不能叫lama模型。
	我们还是选择这个欧拉玛项目的过去七天看一看。这应该会。会等得更久。因为他没有。没有拉起来，要我们去提交这一次生成日志的时候才会拉起来。这个地方就是想告诉大家，就我们开始提过的欧拉玛的这个server，它为了资源的最大化的利用，所以如果一段时间你不去调查，他的那一次访问就会特别久。这个只要用过大模型服务的人应该都有这个体会，包括这个SD也是类似的，这个是金马兔生成的，这个肯定就更差了，能看到这个报告，下载下来对比一下。
	这个是。这个是咱们的nama 3.18B生成的，这个是GM two的两B生成的。所以不得不说这个GPT4欧minia它虽然便宜，但它也是一个千亿的大模型。这个80亿的和这个20亿的，肯定还是有差距的。至于这个20亿的，显然必须得改我们的提示模板，才有可能能用这个80亿的lama 3其实更多的是一些小问题，其实你强行让他用中文，比如说去强调中文这件事儿，有可能是可以的。但更好的做法是因为lama的这个预训练数据里面中文太少了。所以有条件你可以找一个nama的chinese版本。
	所有了解nama开源生态系统的都知道，妈的中文预训练没有用多少中文数据，甚至前几代就没有中文数据。整个中国的好几家大模型的公司，其实都是在开源的lama上又做了中文的预训练，所以你可以用他们的模型，那样的话其实本质上用的也是lama的绝大部分的基础能力，但是他对中文做了专项的优化，那样应该会对中文的提示是响应的更好。好，这个其实就是咱们0.6版本的一个迭代，然后也是通过这个去抛出一个影子来让大家了解开源的大模型，尤其是小几十亿的模型，其实肯定不可能直接等价于GPT4o mini这个目前OpenAI最便宜的模型。然后这个提示词要怎么样再去调整，其实是咱们在不断使用的过程当中可以去做的研究和探索。
	然后下一节课，其实我们就会把整个agent第一个agent开源哨兵去做一个收尾。这个收尾就会去讲把hike news的这个信息渠道，我们会给他做一个实现，包括我们布置的第一个作业。因为第三周其实是到今天这个第三周就结束了。因为我们第一节课是周日开始的，所以理论上这周日的课程已经是第四周的这个课了，所以大家可以赶赶进度。然后如果学有余力的，可以多试一试不同的信息渠道和这个模型，足球对比。
	好，那么咱们这个github的这个开源哨兵，0.6版本就是这些更新行好，那看看大家还有什么问题，我们接着来QA一下。欧拉玛支持分布式部署欧拉玛不支持分布式部署，因为它是一个demo service，它在本地，然后一个推理模型怎么分布式部署的这个东西。南茜好像支持动态替换模型，这个动态是什么概念？现在欧拉玛算是动态替换模型吗？
	看大家还有什么问题吗？关于这节课的。
	是的，这个资源释放问题是一个配置项。就是大家提问的时候提具体一点，就是这个有好多概念，大家不一定对齐了。
	大家还有别的问题吗？包括这个同学提的双路服务器，这概念也不是一个大家都能达成共识的概念，所以我也不知道这个支持双路服务器是什么概念。
	大家还有问题吗？没有问题的话，就咱们群里见。欧拉玛40B是不是效果好一些？欧拉玛哪有40B啊，你是想说拉玛吗？
	你的模型参数规模越大肯定效果越好。不是我们这个场景所有场景都是这样的。然后那个同学说这个CPU上跑欧拉玛的时候，我也不知道你跑的是哪个模型。然后理论上你拿CPU跑就挺玄乎的对，可以部署除了文本生成的其他模型吗？我不知道你想说的是什么模型，比如说stable diffusion吗？
	我印象中他好像不行，我可以看一眼，不然现在怎么样，而且到现在可以了。我关注他比较早，没有看到最新的好多东西，最新的都是在看他周围的一些东西。它可以支持。SD? 应该不行。
	其实他也写了他的所有的这个支持的模型就在这个library里面了。如果在这里没有，那就没有了。因为它本身是要把这个模型最终转成它的可以去托管的格式。然后这个过程当中，他本来就有一步额外的工作，然后这个额外的工作要针对模型来搞的。然后diffusion的架构跟transformer还是不太一样的，所以实践上不是同一回事儿了。就这个里面，这儿应该就能搜，这个里面你搜不出来，应该就没得搞了。
	没有。没有，应该只有这个大语言的模型。这个同学问那个模型怎么样？我不知道，这怎么样？这个事儿很难说，whisper还有whisper它也不支持。讲道理weser large two应该是开源了的。对，这肯定开源的。我们微调训练营还做了这个last two的微调。对这个模型怎么样的问题，这都没法回答的，你就没有办法问这个没有目标的怎么样。对，行，那我们今天就到这儿，大家我看问的也都比较发散了，有什么问这个问题再在群里大家再展开讨论。我们今天就到这里，感谢大家。