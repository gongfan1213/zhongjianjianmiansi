	Hello, 大家能看到吗？
	大家能看见吗？我换了一下设备，所以这个人的角度会有一些变化，加了一个屏幕挂灯，我不知道现在是播放状态吗？
	这个视角应该是能看见的对吧？是播放状态OK现在切我切到了目录页，有问题吗？大家看一下，不要又卡住了。
	我现在这个画面切换，大家能看到吗？没问题。好，确保一切正常，免得又出问题了，目录也无误完美，那我们就正式开始今天的分享。我看一下群里还有没有人没进来。
	好。
	好，那么我们这节课就正式开始。好，今天我们将使用nine graph来构建反思机制的可能。这节课应该是我们讲number这个框架的最后一节课了，后面我们就会交给agent的实战的开发了。
	Nine graph这个框架，其实迭代的也非常的快，包括我们看到最近这个南券的0.3，准确点说是年前core 0.3也发布了。年前core 0.3的发布其实跟nine graph也还有一些关联。因为它本身底层用的是这个能券core里面的一些LCEL的特性。
	今天我们要讲的主要的内容其实就分成这三部分，第一部分是我们其实已经用了一节课的时间讲了这个的，尤其是0.2以来的LCEL的一些新特性应该怎么样去使用。然后用了前两节课的时间，又给大家讲了nn graph的一些基础开发的知识，从nine graph的这个图这个图数据结构是怎么回事？为什么要引用这个数据结构，以及这个图的数据结构的一些关键的概念，基础的一些概念，图里面的节点边，流动的这个数据用state来进行表示，然后我们做了第一个chatbot的实战，接着我们在上节课用number去做了一个marty agent。多智能体的协作，今天的第三部分，我们会来讲多智能体协作的作业讲评，告诉大家一个可行的方案，怎么样去完成我们上一节课布置的作业。有了这些基础知识之后，我们尝试最后再用今天这一节课的时间，让大家对整个number的agent的开发技术有一个更核心，或者说更深度更底层的一个了解。尤其是我们看到前几天9月12号发布了这个OKAI的model，这个其实是一个挺大的事情的。而且OY model的发布其实某种层面上改变了很多游戏的规则和玩法。只不过可能它的这个影响力像一个波浪一样，这个wave还在不断的由最开始的这个模型发布到后续的这个影响是有一个过程。
	今天我也看到3猫man在推特上面讲到，在12号的时候发布，应该是O一的这个preview和O一的mini是应该叫访问次数，就算是ChatGPT的plus的这个用户也非常受限。那现在我好像看到三包特man今天有提到这个o one mini已经可以变成每天多少次的使用。然后O一这个preview也从每周的好像是30次变成此次。然后他们还在不断的去提升他们inference的效率。
	整个O一这个模型其实跟我们之前讲过的GPT系列是完全不一样的一个思路。或者说他的未来的一个发展跟原来GPT已经是两条道路了。这也是为什么它从一就是从第一个版本才开始重新命名。同时既然模型变了，那模型上面的agent我们知道agent的定义也好，或者说agent的内涵就是以大模型驱动的APP。
	那么大模型变了，agent会不会变？这就涉及到我们今天开发技术总结的第一部分。就是我们如果去面向未来去考虑agent会不会变成一个中间状态，会不会没有agent了？不断变强的大模型是不是会吞噬所有的agent这是一个很值得探讨的问题。那在看未来之前，我们再来巩固和深度理解一下agent。Agent其实它的构建有非常关键的三个要素，我们会通过一个简单的叠加来让大家了解。然后agent的核心其实是拟人化的角色和岗位的设计，这个我们待会来讲。
	第二个部分我们会来用这个nine grave去构建一个具有反思机制的一个智能体。这个reflection被我们引入到这节课作为一个基础，或者说入门的一个重要实战，是因为其实是能回应上面提到的，未来的大模型会不会无限的去吞噬agent。其实从阿金的这个目录大家应该也能看得出来，肯定不会吞噬所有的agent。有一些特定场景的工作流编排或者说解决方案，就算大模型本身它自己有这个self play，有deep thinking的能力，但是他很难一人分饰两个角色。所以基于角色或者我也不知道角色还是角色有不同的role的这个agents，然后来一起编排做一个事情，将会是未来的agent能够做好的。
	反思机制其实就是一个典型，就一个干活，一个给意见或者给反馈。这个就是非常典型的一个两人团队可以协作的一个工作模式。就像咱们在公司里面会有一个TL还有一个具体干活的人。TL可以去review这个具体干活的人的工作产出，然后不断给一些建议，这样就是一个很好的合作模式。基于角色的这种agents的工作流编排，是某种层面上就是指这样的一个含义。
	未来的agency也会越来越像这个人的工作模式一样。我们会有不同的工作岗位。对每个岗位有他自己的JD有他的工作内容描述，他未来的一个单独的agent，就越来越像真实的一个公司里面的人。然后不同的人协作可以产生一个非常健壮的最终可以交付的一个产品或者说解决方案。Market agent应该会朝着这个方向去不断的前进，然后整个大模型的变强，其实是让单个角色的工作能力变强。但是一个稍微复杂一点的工作，交付物其实是有多个角色来完成的。那这样的一个场景下，agent其实未来还是很有值得发展的一个前景的，我们这个是对于整个这节课的一个大体的一个总览，接下来看一看这三个部分。
	第一个部分，我们要讲的就是nine grass agent的一个开发技术总结。我们再次的去强调一下这个概念，就是当我们去构建一个agent的时候，最关键的三个要素是什么，其实就是from prompt，大模型和tools。这个是我们只要用过n nine graph，包括其他的一些agent开发框架，同学都有直接体会的那我们再回顾一下，让大家能快速的理解。
	首先我们在应该是在两节课之前，在这个n graph的第一节课的时候，我们讲了第一个chabot的实战。Check box其实是一个非常常见的用大模型来驱动的agent的这种应用形态。最早就拆GPT，它成功的开启了这样的一个应用交互的这种形式。那后来各种各样的大模型和大模型应用的公司都推出了自己的聊天机器人。
	聊天机器人作为一个agent，其实最早我们可以发现，包括从右边的我们可视化的这个graph也能看得出来，它是相对来说是一个只需要大语言模型足够强就能够完成的一种agent。这里我们可以把它叫做LLM only，就是只需要大模型就能够去实现的一类agent chat port。当然今天我们再看这个chat port，就会发现它没有任何的壁垒了。只要你会我们现在这个框里面的这几行代码的调用，你就能够实现一个chatbot。但是这件事情其实在两年前是不可思议的。有22年的九月份在ChatGPT发布之前，基本上全世界没有哪几家公司，甚至说没有公司可以做到我们现在这个四五行代码的体验，包括微软小冰，包括siri都做不到。但大家想象一下，两年的时间，我们现在每一个人几乎都叫你会这几行代码的调用，然后你有权限去访问这个GT4O或者同等级别的模型，你就能拥有一个check port agent。这个是非常夸张的一个迭代速度。
	有了大模型，我们知道三要素还有tools。我们所有的大模型构建的chatbot在早期，也就是刚刚chat PT发布之后没多久。我印象当中应该是在22年底，还是二三年初的时候，也就一个来月的时间。大家就发现ChatGPT它没有超出它训练集的能力，或者说他无法知道训练集截止日期之后的事情。
	所以大家如果关注过OpenAI像GPT的发展的话，就会发现二三年初OpenAI率先发布了plug in，就是这个ChatGPT的插件这样的功能。在插件功能里面，当时非常受到关注的前五个bug in里面就有一个叫做web pilot，就是一个联网的plug in。从那一刻开始，其实整个chatbot加上这个搜索引擎就变成了一种标配，但这个事情其实也就一年半之前才发生的那我们现在可以看到，在在我们的这个chatbot的项目里面，我们的第二个部分其实就把我们的chat model，也就是GPT绑定了一个我们的搜索引擎的工具。那有了这样的一个工具加成之后，其实我们就实现了一个可联网的check port，这个其实是一个今天看下来非常常见的技术。但其实是随着我们整个开发框架的迭代，让大家能够以这样方便的几行代码去完成对应的构建。
	我相信从去年这个时候，我们刚刚开始讲大模型的这个应用开发实战营的时候，那会我们要实现一个联网，其实还很原始。并且联网的成本，也就是搜索引擎的调用成本，其实也没有现在这么便宜。大模型的这个inference的成本也比较高。所以整个agent的发展确实是非常的快。
	大家如果能够去持续关注这个A型的开发的领域的话，应该也有类似的一个体会。但好消息是因为tos因为大模型在不断迭代，所以我们个人不管你是个人开发者还是公司。能够搭建的agents的能力都会越来越强。这个是我们能感知到并且也能做出来的。
	第二个简单的大模型加工具的可联网的chat port。我们在第一个企业级A检测的项目里面实现了get hub setting now这样的一个工具。这个工具它相比于我们刚刚看到的可联网的chatbot略有不同。它其实本质上更核心的用到的技术能力是提示词加上这个LLM。为什么这么讲？就很多同学可能会去想，hike news、github report这些进展明明都是从网上获取到的。但其实在设计这个工具的时候，我就有意把它做成了两个阶段。大家如果还有印象，去去回忆这个项目的话，其实我们是把这个信息的获取和这个报告的生成做成了两个阶段的事情。
	一阶段的事情我们通过各种各样的client去完成，给hub client hicky news的cite。他们以各种各样的可能的方式，从信息渠道把相关的信息变成一个本地的文件。然后我们的agent的这个能力，其实是基于我们设计好的一个特定的prompt，比如说github和这个hike news，它的prompt就不一样。用这个prompt然后再加上这个大模型构成了，尤其是我们从这个0.7、0.8版本以来，我们把这个prompt把你也放到了report generator这边来。那这个时候，report generator就更像是一个agent，或者说它其实本来就是一个agent。如果我们去从架构层面上去了解的话，那它的核心用到的这个能力其实就是把提示词的能力和大模型的能力做了匹配。然后去完成了一个去生成我们的报告，当然它还有一些辅助的功能，发邮件等等，这也是一个非常典型的工具，get up city now这样的一种工具型的agent。最近也有蛮多的用户真实开始用起来了，包括像我自己每天都会去查看这个邮箱，这个应该是bug，对，应该是N项。
	对好，有了刚刚我们看到check board和工具类的agents之后，其实我们在上节课讲matter agent时候，算是第一次完成了三个要素齐备的这么一种多智能体的一种协作这样的一个实战。在这样的一个事例里面，其实我们第一次把我们看到的这个大模型，提示策略和tools连接在了一起。在这幅图里面我们看到有两个不同的agent，这两个agent调用了不同的工具，一个是调了像这个researcher调用的工具是搜索引擎chat generator，调用的是一个python的沙盒执行环境。通过这样的一个方式，我们可以去开始构建多支人体了。
	所以我们简单总结再来回顾一下的话，agent其实不论怎么样去变化，只要我们能够抓住它的三个要素，这是它的最本质也是最基础的东西。Prompt、大模型、tools就能构建出任意的agent，就像我们这里看到的整个架构。如果我们把它放在圆圈里面的话，这幅图就是是一个LCEL里面的type，这个管道可以去连接的一个流水线。如果是在graph里面的话，这个其实就是一个由节点构成的图。落到这个代码的层面上，我们其实可以看到在nine graph的market agent collaboration和nation，我们在开发实战营里面的self ask for research这样的预定义的agent。这两个不同的开发框架里面去实例化，去定义一个A的时候，都是需要把这三个最核心的要素抓住，抓住这三个本质，我们就能够去开发好一个真正我们可用的agent。
	有了这样的一个认知之后，带大家回顾。因为我相信有些同学可能是跳过来直接就跑这节课来了。我们看一看这幅图，这是我们在第一节课的时候就有跟大家讲过的。那么agent我们了解了，其实它的核心的三个要素就是我们的提示词、大模型和工具。在这个agent的开发的这个核心技术在里面，有这么多的东西，我们怎么样去理解呢？首先我们看到agent我们经过了这么长时间的一个学习，我们相信大家对这些agent类型已经又有一个新的了解了。从一开始的茫然，怎么这么多不同的agent到现在大家应该知道，其实所谓的agent抓住它的本质，就是刚刚三个要素的排列组合以及一些具体的运用，就能诞生各种各样的AI的应用。这些AI的应用被叫做agent，所以agent类型其实并不重要。
	有了这样的一个认知之后，其实下面这三层核心能力开发框架和模型服务都是为了去构造构建一个agent而诞生的。然后这个技术的分层，当然也有它的背后的一些技术上的一些原因，我们就不再赘述了。这个大家可以去看第一节课。有了这样的一个理念之后，我们就不用再去纠结A证的类型。
	剩下的这一层，我们看到我们这里再回顾一下memory tools和action这三个核心能力。其实落到最后，我们在then grab里面知道如何实现这个memory。短期的长期的，也有可以存到持久化的存储里面。Tools不用说，这就是我们可以用的工具集。然后action其实就是最后的整个执行。而且我们跟环境交互，就tos真正运行起来的一个状态。
	这三个其实本质上就是我们在代码里面已经看到过的年graf也好，年轻也好，各种各样的pose。规划many其实它的本质就是prot。这样其实大家再去理解这个核心能力会非常的我相信经过这么多理论的学习，再加上代码的实战，会有不一样的体会，至少我是有这样的体验的。如果大家找不到这个感觉的话，可能还得再回去看一看这些代码了。有了提示词，有了这个工具，那模型服务这一层其实本质抽象出来，对于我们提供的就是一个a LLM。
	我们回到代码层面上来理解的话，而开发框架这一层提供的是什么呢？就是我们的工作流编排，就是我们的bras。回到了这样的一个高就高度抽象的一个技术栈的分层之后，我相信大家就知道我们现在经过了这么长时间的学习，我们的重点是什么，以及你能够去提升的各个维度是什么。
	显然这个模型服务不是咱们能够去做太多工作的部分。就我讲到的这个四阶的AI大模型的四阶的技术分类里面，模型服务显然不是我们要去一一开始就要投入大量的这个工作的一部分。那么开发框架我们更多的是去把它用好。所以我们给大家花了三四节的时间去讲年前年过，这个也是一个基础能力。工具和这个提示词其实不依托于开发框架，我们只要把提示词和工具用好，我们也能做出get up set mail这样的一些agent，但是有了完整的开发的核心技术站，能让我们做出任意的agent，所以背后有这样的一个逻辑关系。这里的每一个维度或者说每一个模块我们都需要去提炼，它是一个像木桶一样的能力的一个模型。
	如果你只有prom和tools，可能你可以做出github sit now这样的agent。但是你没有办法去做复杂的工作流编排，没有grave。那这个时候显然我们想做market agent，去手动撸一个这样的graph出来是很复杂的。
	然后模型也是一样，我们在用这个githa sit neil的时候，明显会发现当我们去调用GPT4O或者说GPT4 omi的时候，他无论如何，即使我们的prom很弱，它也能够去把我们的报告生成的还不错。但是我们不去修改这个prompt，然后我们去把它换成了nama 3.18B这个时候我们会发现它有的时候会直接就生成了英文，有的时候就生成了一些中文，但是不是我们想要的内容。所以整个三要素是一个像木桶一样的这么一个能力。然后我们去开发的agent的通用性，其实就取决于我们自己对这三个能力的一个把握。以及我们在workflow，也就是我们未来要做更复杂的一件事的时候，能不能去把这个work flow给设计好。然后让我的不同的agent能够各司其职。然后整个工作流程还能够做到比较符合我们想要的这个应用场景。这个其实就是往后我们要去开发更复杂agent的时候需要去面对的考验。
	好，这个其实是我们在帮大家一起回顾了agent从从零到现在我们学到的所有的这个内容，以及整个核心技术上，我们在高度抽象之后一个再再理解的一个过程。下一个问题就来了，就agent的这个构建，我们知道有三个核心的要素。那么agent我们要把这个东西构建好，这三个要素到底要如何去分配我们的精力，以及我们要把一个agent真正做好，哪些地方是我们能够实力的。
	作为一个开发者来说，我自己的一个理解是关键是要找到你最终这个agent的一个应用场景。然后因为A的最终它也只是一个APP，那这个APP要解决问题。那解决问题。我们在传统的研发过程当中，很多同学可能会陷到一个我们类似的讲法，就是面向过程的一个写代码的视角里，或者说追到一些细节里面去了。有的时候是其实是很难跳出来想问题的。但做一个agent非常重要的是你有一个很直接的思路，就是他到底是一个什么样的工作岗位或者角色。说说的这个通俗一点，就是你尽可能的把agent当成一个人去设计，其实是很多时候能。绕过一些不必要的坑，我们来具体看一看对于一个agent的开发者来说，怎么样去构建差异化的竞争力。
	我们现在已经知道三个要素其实是一个agent构建的必不可少的基础，或者说这个元素。这三个要素里面，对于我们开发者来说，我们先跳出这个agent本身。我们是开发agent的人，我们自己应该怎么实力？因为这三部分其实都需要我们去花时间去学习和了解。那一个最直观的因素就是我们还是回到那张这个技术站，大家应该把那个记在脑子里了，在那样的一个生态的分布里面，显然tools是有很多的这个人都想做，但是很难做好的。目前我们看到年轻人确实是在这个领域里面他做的最全面，他的能力是最全面的。
	大模型这是一个兵家必争之地，并且是更重投入的一个层次。显然不是我们agent开发者应该花太多时间精力去投入的，因为我们更多的是它的用户，它在最底层。这个维度像GPT，GPT4，现在的O1，然后cloud 3，nama 3，包括这个deep sac，国内的这个大模型，其实他们都已经投入了大量的精力了。显然这两块都不是A键的开发者ROI比较高的要素之一，我们能投入的其实更多的是在prompt，就怎么样去聚焦我们prompt这个能力的提升。
	这个时候我们再来看这张图，我相信如果咱们经过了这十几节课之后，应该有非常多的写prompt的作业和体验。再来看这幅图的时候，我相信大家应该有更多的体会。我们在应该是在这门课的刚刚上新的直播里就给大家讲过这幅图它是一个怎么样用ChatGPT从一个beginner到这个pro的一个介绍。然后这个图里面其实有很多，如果你用过能会心一笑的一些点就比如说我们看到在这幅图里这个role，task format，这是我们不断提及的三个。在提示词策略里面必须要把它抽象到比较高层次去使用的这个技巧。然后基于这三个，其实是可以排列出很多其实工程的这个模板或者说框架的那用的比较多的就是我们看到这里的RTF，我们在github的这个set nail里面就已经使用了这样的一个RTF的最佳实践。
	在我们第一次用欧拉玛来进行github的开源项目进展报告的时候，其实就遇到了问题。我们一开始没有去好好的设计这个，当然我们留了作业让大家去改，但我们没有在这个GPT four上面去花太多的精力去做这个prot。然后在我们的nama 3上面就遇到了一些问题。但我们可以明显发现，当我们去把一个模型变弱的时候，是可以通过提升front获得同样的体验的。所以我在想它是一个木桶的这样的一个能力，去构建了整个agent的能力。
	我们看到在我们使用RTF之后，这里我们给了欧拉去host的这个lama三点的时候，我们让这个提示词里面第一部分给了一个热爱开源社区的技术爱好者的这样的一个role。我们给他的任务其实是两个任务，然后格式我们也给了对应的这个参考的一个格式，并且使用了这种有点类似于我们的placeholder，或者说我们在年轻里面叫做variable。这样的一个格式，显然在3.1里面它是能够接受并且理解的。同样的，我们在后续的github city now去做hike news的时候，也同样沿用了这样的RPF的格式。并且我们看到在去讲这个number of里面的market agent的时候，同样也是沿用的这样的格式。
	当然这个提示词是我们待会儿去做作业讲评的时候，我略微做了一些调整，在我们上节课的做这个实战项目里面的是没有notice的这个部分的。但我们可以看到，其实同样的，只要是你想要去把这个agent这个能力给提升，有一个几乎没有太多成本，只需要开这个A的开发者有这个水平也好，或者说你有这个独特的差异化的竞争力也好，就能提升这个agent能力的一个点就是prot要设计的足够好，足够巧妙，然后也是有套路可循的。其中我们刚刚反复在讲的这个RTF就是一个非常好用的提示工程的框架。
	那么march agent的collaboration，我们看到researcher里面同样有写，这是一个角色AI assistant任务这里有写这个provided tools。然后我额外的在这个地方圈出来一部分，是因为我们看到其实不同的tools要使用不同的你叫任务也好，或者说叫这个工具的描述也好，这个是非常有必要的。在在我们去做这个market agent的时候，这部分是非常必要的。这个提示词也是一个可以学习的最佳时间，就给到不同的工具。这个工具一定要给到我们的这个大元模型，它到底要怎么用，然后这部分其实我们还做了调整，大家如果去看这个细节的话，那我们在评审的时候再去详解。然后在这个charge generator这部分也做了一些调整。
	我们待会儿要讲的反思的智能体，其实就做了一个更进一步的prom的一个设计上的，我们叫优化。在我们要做的reflection的agent里面，其实我们把整个这个front里面的肉部分再进一步的去做了强化。刚刚我们看到的不管是这个researcher还是chat generator，其实我们让他在代码里面共用了一部分这个system的prompt。那里面都会讲你是一个helpful AI assistant，我们没有办法去做更细一步的角色上的拆分，所以也有的同学会疑惑这个much agent的感觉好像职责不是特别分明。那比较好的更进一步的market agent的实践，其实就是从从工作分配也好，从这个角色定义也好，从岗位也好，就给他做明显的一个区分化。
	在我们今天的反思的这个智能体里面，其实我们会有两个agent，它同样是一个market agent这样的一个应用。那一个agent我们叫做writing assistant，就是一个写作的助手。他的任务在后面也有描述。显然这里也是留下了足够多的空间，大家如果想要去把整个reflection的agent做得更好，那么其实可以在这两部分再去做更多的优化。
	这里很明显的format就做的不够多，尤其是我们的teacher的这个format，只是简单提到了requests for names，deep style这样的一些内容，那我们就这一部分也是留着我们待会儿讲反思智能体的时候再给大家详细去展开。但是核心我们能够体会到，我们在整个课程里面去教大家用form的这个设计和安排，其实是一个由浅入深的过程。如果大家到现在为止已经掌握到了这样的一个技巧，设计的这个理念的话，那是非常好的。反正有一个很重要的点，就是一定要杜绝在跟这个大模型交互的时候，一种非常粗暴的方式去让他给你生成内容。如果你想要大模型生成的足够好，至少经过上过咱们课程的同学，一定要有这样的主动去使用提示策略，甚至提示框架。我们叫RTF也好，包括别的一些框架也都有用这样的一些框架性的提示词，去去去让大模型生成一些更好的结果，这是非常重要的。
	然后说回来，这其实也是A型的开发者，或者说你未来不想要做一个研发，你只是想做一个大模型这个工具和产品的很好的一个使用者，那这些都是必不可少的能力，那这里就又又得回应一个非常大的挑战和问题。这也是最近很多媒体在讨论的一个点，就是o one发布之后，就是agent这样的一种应用形态，未来有没有可能就不存在了，比如说我们的比如说我们这个大模型，尤其是o one出现之后，我们的agent是不是未来就不存在了？这里我画了一个很有意思的图，就是我把它叫做agent的达摩克利斯之剑，这也是一个达摩克利斯之剑，是头上一直有把剑，就是随时可能戳下来，我觉得这是一个很有意思的话题这个smart也设计的很好啊。这个图里面我们看到，在三要素里面，其实它远不像我们刚刚看到的那么均匀。就大家都是3分之1，然后各自有各自的地盘。但显然这个大模型它就像一个太阳或者黑洞一样，他在不断的吞噬原来的各个小的模块。一个最简单的例子就是一年前的时候，大家都在聊各种发个音。因为我们认为大模型不够强，所以大模型需要去联网，需要去做各种各样的嫁接，包括大模型不能执行代码。
	但后来我们看到open I不断的在往前探路，我们现在知道加GPT已经可以联网，然后可以去做这个code interpreter，就他自己能执行这个代码，然后他自己还能够去做RAT，因为他能够把你的一些上传的文件给存下来。这个其实大家如果想要了解它背后的原理，去去回忆一下我们讲的这个assistant API那套东西就可以了。因为本质上ChatGPT就是用assistant API类似的技术去实现的一个agent。
	现在GPT作为一个agent它的能力已经越来越强了。但这里我们都在想的是，agent是一个很好的思路，因为外部的能力可以明显的快速提升大模型。但是随着o one的一个发布，好像很多事情又倒过来了。就好像大模型完成了原来很多agent可以做的事情。一个最直接的变化就是o one在9月12号的时候，他，这里我们直接看一个他的思考流程，然后就是这个思考流程来讲的话，我们可以发现在原来的大模型的这个思考路径里面，我暂停一下。在原来的这个大模型的思考路径也好，或者说它的生成的过程也好，其实更多的是当首先大家都有RHF跟人类价值观的对齐。但是原来的大模型你去跟它交互的时候，它更多的是浅层次的。我们叫planning也好，叫深度的内部的COT都很少。
	我们在去年这个时候用经典的咖啡问题，我当时编了一个就是我从这个公司去到咖啡店，然后点了几杯美式，点了几杯拿铁，然后去问他。这个时候我记得应该到二三年底的时候，这个GPT3.5，偶尔GPT3.5的六月之前的版本大家会出错。GPT3.5一直迭代到9月还是12月的那个版本的时候可以答对了。所以但是那个时候我们都会发现这个大模型在理解一些这些问题的时候，它其实是搞不明白，就是它很容易出错。它内部当然也不断的在用这个COT的一些tricks在训练在迭代。就比如说我们非常清楚第一个就COT这篇论文提出来的，i think step by step这样一句简单的提示词就能够改善大模型的效果。但后来我们看到了GPT3.5GP4，显然都把这部分的tricks融入到了它的训练数据里面。
	但即使如此，我们还是发现他在解决一些，比如说我们需要有5到10部的5到10个step思考的时候，其实他做不好。那这个时候我们就发现了这个agent需要去人为的编排，一个最常见的就是graph就出现了，我们人来帮你一步一步想好他会怎么去做。然后另一条思路就是我们看到提示工程出现了各种XOT，比如说TOT，tree of thoughts，帮你去构建一个思考的解空间，然后大模型自己在里面再去寻找可行解。
	但那条路走不下去的一个重要原因，其实是因为inference的成本太高了。就我们要去完成一个TOT需要消耗的tokens，以及等待的这个时间，是很难应用到生产环境里的那有了o one之后，我们发现其实它已经不再是需要我们在外面，就是在大模型的生成的外围去做这个编排了。这个大模型自己就已经开始去做多步骤的深度的一个thinking。我们看到这个流程，think first，then然后回去，这个是训练阶段。这里有两条路，就是我们看一下大家要理解这幅图，这幅图是在推特上的这个top，它的一个推特上面的图，然后保留了他的原始的标记，大家有兴趣可以去关注这个人。他其实把这个图画的很好一点是什么呢？就是我们看到o one的训练模式已经变了。当然这个训练的成本是巨大的，但他也确实work了，它它是有效的，我这个需要巨大的数据标注和准备，我们看到在o one的训练流程里面，他在训练阶段这个RHF的训练阶段就已经不再只是说大模型生成一个结果。
	当然这部分可能需要有一点，大家如果参加过微调训练营的可能是比较友好，因为我们讲过ISF的训练原理，那么简单来说就是在RHF的这个训练阶段里面，就是人会训练出一个奖励模型，也就是下面的reward model。然后大模型生成一个结果，然后让reward model给这个结果去打分，然后最终选出一个结果，然后这个结果再反馈回去就修改了这个大模型就德尔塔W这是标准的在O一之前的所有的RHF的这个训练方法，无非就是说lama他可能reward model有两个，一个是helpful，一个是safety。OI，我们看到它的训练已经变了。它的训练数据就不是一个单纯的让大模型生成一个结果，给到reward model再去打分。而是在这个过程当中就已经有多个步骤的训练数据。你可以想象一下，这个数据的准备是非常夸张的。就这里的每一个think first的人都是有对应的数据的，然后这些数据是海量的。
	那通过这样的一个方式，我们在RHF原始的这个阶段里面增加了COT。所以大家能看到OY一个最直接的体验就是他想了很久，这是因为它就是这么训练，所以我们可以看到在这个右边是它的influence状态，所以它inference天然就可以去做COT，因为它就是这么教大的。它又跳到开头了，我们可以再再看一下这个动画。那么从这个直接生成到HF加COT训练，所以我们看到在实际使用的时候，我们的大模型就是会有这样的一个流程，它会反复的去思考，就是最右边的这个流程，30秒、50秒、100秒都有啊。
	我自己用它去做一些这个工作的时候，你会发现首先这个训练是有用的，而且是在一些典型任务上，它能够得到非常好的提升。比如说最近大家应该已经都看到过的，比如说对于这个密文的一个解码，然后对这个复杂的代码作业的一些多步骤的生成等等。但是它也不是万能的。
	因为通过解读他的这个训练流程，大家应该能够理解他的训练过程当中的COT数据覆盖的一些作业类型，显然是他能够天然的做得好。然后这也只是第一代的模式，大家不要对o one抱有两极分化的看法。一边觉得AGI来了，赛罗奥特曼自己就说了AGI还没来，这o one只是一个开始，这是第一代。另一边就是说o one不行，好多问题甚至还不如GPT CO但这是当然因为他的训练原理就不太一样，但它未来的天花板会更高，当然它的成本也会更高。所以大家客观从原理角度来解决，或者说来理解他的这个解决思路，其实就能得出一些更更理性的一些结论来。但无论如何，我觉得这幅图还是很有意思的。当然没有这么夸张，他没有真的说甩出那么远，但是从他的训练方法来看的话，未来这条路一定是会有很多追随者的，不过他又一次走到了前面，这个来自于这个玩具总动员，所以结论就是我们新的这种训练方式，这个IF加上COT这个训练方式，仍然我们回到我脑子清楚一点，再回过来我们再聊agent。
	Agent有三个要素。然后OKI这家公司他们的工作的主要产出是在LLM这一块。刚刚我们看到一个大的图，LLM会吞噬一切的，让他吞噬一切是哪些能力和维度上去吞噬的？其实从训练方法的解读，我们有了初步的了解之后，他在RHF加COT这种新的训练方式上，就显著提升了LLM的推理和规划的能力。这个显而易见，可能通过刚刚这个解释大家应该也都能大概理解了。所以one其实是开启了一种新的大模型的生成方式。这种生成方式就是inference加上COT，就这个COG变成了大模型的内置的一个能力了。所以他也就直接导致有一些跟这方面相关的agent的设计就会被被吞噬掉。
	这个单我们也能看到是哪一类，这里再分享一个很有意思的，就是在redit上面，我们的redit一个知识问答的一个平台，一个QA的一个平台。那上面有一个博主去用cloud这个SOOK他们家的这个大模型，去类似于我们的逆向工程一样，去分析了我们的这个o one它的一个运行原理，然后画出了这样的一个架构图。这个架构图其实会更细一点，就是我们刚刚看的那幅图是一个很高度抽象的一个训练流程。这幅图如果对这个模型训练或者说上过训练营的同学，他能看得更细一点，就我们整个这个训练过程其实就有了一些比较细的变化了。
	主要的变化就是第一，强化学习被引入进来。当然本来HF就是一个强化学习的过程，但是原来的这个强化学习它被引入更多的是用来训练这个reward model，就是我们刚刚说的奖励模型。但现在这个强化学被引入进来，在我们刚刚说的COT这部分训练阶段的COT这部分开始去做。
	我们我不知道这个中文怎么讲，就self play，原来我们在聊这个下棋，阿尔法go到后面的这个阿尔法这个zero，后面这个阿尔法zero已经不需要去学习人类的这个棋谱了。再到后面他直接基于规则自己去发散的去做这个博弈，各自11坊黑白两方去做博弈。然后规则因为它环境是封闭式的，所以它很好的能够有一个reward model。但是我们的大元模型它的场景略有不同，就是我们让这个大模型是生成各种各样的内容。而这个内容它不像一个棋谱，可以定义出一个非黑即白，或者说一定正确的评分。所以他其实这里就涉及到了大量的人力投入。人力投入的直接产出就是数据标注和这个reward model的一个更新。这里大家有兴趣可以再去深度的去了解。
	但有了这样的一个技术上的背景的一个介绍之后，我们再来看这幅图，就是我们在agent类型的时候，其实讲过从技术实现的角度，agent有很多种类型。那么这一类其实就会是最最受到挑战的，所以我们也不准备在这一块讲太多的内容了。当然我一开始也是这部分他有点太垂泪了，很容易被干掉。
	比如说我们看到planning agents，其实它本质上是让一个比较弱的大模型。然后在在他自己没有办法去做多个步骤的规划的时候，让我们的nine graph也好，用这个别的编排工作流编排的这个方式也好，让人来设计第一步干什么，第二步干什么，第三步干什么。但那个模式天然就比较hard code，比较硬。编码它其实就是它这样的一种模式有一个天然的缺陷，就是它的预期是大模型是比较弱的。那么随着大模型的增长，这个模式自然就会被打破。就是我的我把一个复杂的任务分成这个任务345。本来应该是要运用模型的reasoning的能力，但是现在模型的这个认证能力你不认可它，你用人为的方式去给他做划分，这是不好的那它的一个解释或者说补丁的版本，其实是长这样的，我们可以看到plane and excuse，这应该算是planning agent里面的一种典型范式了。结论就是我认为planning agent的价值应该会被削弱，然后因为它太不稳定了。
	我来简单解释一下这幅图的工作流程。在planning agent里面，它其实有一个核心的观点就是我首先仍然都是market agent的这个系统。这里面我们有一个干苦力活的，在最右边叫做single task agent。我们可以认为single task和agent就是一个单任务的agent。这个任务可能是比如说写写一篇小作文，或者说查一下互联网等等都有可能。但这个其实是我一直百思不得其解，就是怎么样去定义这个single task的设定，但这个task怎么来的？是由最上面还有一个叫planned agent这个agent是一个比较厉害的大模型，可能是一个GPT4O，或者说这个GPT s orgin。它去产生了一个任务列表12345，然后这个single task agent就来一个一个执行这些task。所以这里其实非常有这个很tRicky。
	就是你想象一下一个很聪明的脑袋，我们先不去考虑有任何的咱们的偏见，你可以想象一下，就是你你有一个场景，就是一个公司，一个研发团队。然后这个TL这个老大非常的聪明，他想了一个技术的架构，然后有解决方案，然后把这个东西交给了下面的一个junior研发。然后这个junior研发他就做不来，他根本就不会做。因为它是一个junior研发，就跟这幅图里面设定的一样，所以这个地方是有脱节的这也是我觉得planning agent很难落地的一个挑战。
	就是你的task client是一个聪明的脑袋想出来的，但你的执行者又是一个很弱的大模型，他们有天然不可调和的矛盾。然后在plan的excute里面，它还有一个设定是让我们的这个single task能够不断的去干活。然后干完活之后更新状态，就是你这个任务的更新状态。更新完了之后还有一个agent叫replan，这个replant就是说我的有点像什么呢？有点像公司里的HR或者说阿里的政委类似的一个角色。他能够根据状态去重新制定一下这个计划，就比如说这个人真的太多了，我跟这个T要聊一聊，就是你能不能把这个任务给他再拆简单一点或者怎么样，就这样的一个逻辑。
	当然这个plan SQ的它的灵感来源其实是一个应该是新加坡国立和国内几个高校，我印象当中应该是华东师大，还有哪几所高校，大家有兴趣可以去搜搜那篇论文，设计出来的一个market agent的一套workflow。我觉得这里其实是有很多挑战的，就在的去落地。当然还有些别的规划型的智能体，就包括我们刚刚看到的这种sult reasoning result observation这样的一些套路。我觉得它可能是一个过渡过度的这种设定，就过渡阶段阶段性的一种agent的设定。这个是长期来看，我认为planet大家如果感兴趣可以了解这几篇比较有代表性的论文。
	但长期来看，我觉得不是一种特别好的思路。它的核心原因就是single task很难定义。然后不同的大模型级别或者说不同能力的大模型在task的分发和执行上是有可调和的矛盾。所以从我的视角来看，未来的market agent的设计有一个很重要的，一定不是说基于任务来做agent，就是这个任务或者说task这可能会更更准确一点。他一定不是说我就像我在这公司里面，你作为一个manager或者作为一个leader，你去思考的时候，你一定不是说我分好活之后，你就一个一个就干，然后你什么活都得干你是一个单任务的机器，一定不是这样的。而是从角色的能力出发，应该这么讲，就是每一个人在一个团队里面，他有他擅长的和不擅长的。
	然后你要找到这个人他擅长做什么事情，然后他擅长做什么事情。在agents这个角度，就是我们赋予他什么样的角色任务和format，就RTF这个是很重要的。你只要把这个深入的植入到了这个大模型里面，让它变成组合成了一个agent之后，包括给它适配配套对应的tools，那它就是一个好的完成这个工作任务的一个agent。
	要从这个视角再去分配任务，是更稳定和更长期来看靠谱的一种合作模式。因为你可以想象一个最简单的逻辑，就是我们不管是一个三个人的创业公司，还是一个100人的有一定规模的初创公司，还是1万人的超级大厂。但无论如何，到最后大家聊的还是角色。就你是前端，你是后端，不会说你是一个弱一点的模型或者弱一点的人，这个是在工作分配上是很奇怪的一种描述方式。但是你从能力模型的角度把它锁定到一个特定的角色上。不管是模型本身的训练数据，还是你去定义好了之后，你知道该怎么给他配套对应的toss。从这些角度来看，这种基于角色的agents设计和编排应该是长期很有价值的。
	并且我们看到一个初创公司的人的成长也是如此。就你想象一下，就脑补一下有两个agent。一个agent一开始是一个初级前端，一个是一个初级的后端，一个初级的产品经理只有三个人。
	三个人不断的成长，就像这个大模型它在不断的成长，但他的工作范畴没变，他的角色的职责分配也没变。那大模型的能力不断增强之后，其实这个铁三角就最终的A线程能够完成的交付物也会越来越好。它不会出现一些不可调和的矛盾，我希望这个思想大家能够get到，然后能够在你去践行这个market ation的设计的时候，去去去落到这个代码上去，真的完成它。
	这里其实就得提到反思，我认为就是一个典型的二人小组的一种分配模式。这个反思在在我们刚才那个图里也有，其实这里我们先简单讲一下这个反思，我们能看到反思的这种机制，其实一个最直接的考虑就是抽离出来。就是干活的和这个TL其实也是两种角色，TL的价值是经验的传递和一些关键产出的一个把关。那么这里的这个基础的这种反思机制，其实就是这样的。
	我们可以看到在这样的一个图里首先从通用的角度来说，一个人是generate是生产者，是生成内容的。然后另一个这个人其实都是agent。另一个这个agent其实是反思这个反思者。这个反思者他的产出物其实就是基于我们这个generate这个生产者的生成的结果，去给出一些有价值的反馈，也就是所谓我们经常听到的有建设性的意见。通过这样的一个循环，去让我们的generate不断的去提升它的生存质量，这个其实是一种非常好的模式。
	这个就比刚刚那个很很虚空，就是plan and execute。这个plan其实就完全取决于那个plan agent到底怎么拆的，以及replan的这个agent又是怎么补充的。在这儿就太多不可控因素了，就是这个关系是比较虚务虚的。但是一个生存者加上一个有价值的建议的提供者，其实这个是一个蛮实在的模式。
	我们下下半节课的这个项目实战，其实也是一样的，就是我们构造了一个writer，他是一个写作的作者，大家也看过他几十次了。然后一个是reflect就是给他提供这个改进意见的一个人。就这样的两个意见大家角色明确，一个干活一个提意见就可以了，那这样的一个模式还有一个更重要的因素就是human的loop。
	人机协作什么意思呢？你可以想象一下。在这样的一个流程里，我们已经学过的一个知识点叫做interrupt before这样的一个参数。在graph里面我们是可以明确增加interrupt就是中断的那我们是不是在这样的一个流程里面，我们可以很显著的去在reflect我们的这个反馈建议这个方面去提供一些人的介入，这就是非常方便的一种模式。就我们在给他提改进意见的时候，其实我们是可以在reflections里面介入的。包括我们的这个generate也是可以去做介入的。这个是一个非常明确的，就是人可以去改善我们大模型，并且这个过程当中还可以产生非常多有价值的训练数据。在这幅图里面，其实我们看到这个反思的这种机制，是这里有四个比较典型的例子。
	现在我们在这个课程作业，包括这个课程的项目里面，提交了basic reflection和这个reflection on这样的两个项目。但我们重点会讲这个第一个就是这个basic reflection这样的一个实战。好，那么接着我们下半节课就是在讲用number去构建这个反思机制的智能体的实战。好。
	好。
	到这儿为止，我们看看大家有没有什么问题。我们可以五分钟的时间来做一个简单的QA。因为前面我们带大家回顾了一下agent的三要素以及它的核心，我们整个开发技术栈其实就被高度抽象了。通过我们这个学习之后，然后它的未来的发展，我们也能看得到，其实march agent肯定是趋势。单个的agent会被大模型的这个能力越来越多的给覆盖掉。在market agent里面要找到这个agent的角色定位，这个角色定位就是由他的，他就跟其实就越来越像人，就是我说拟人化的设计是这个逻辑。对，看大家有没有什么问题。
	有个同学是凉了，是什么意思？凉了一半什么鬼？没没太看懂，就这个东西你能解释一下这个凉了一半是啥意思？对，有的同学也提出了相同的疑问。
	大家有什么问题吗？关于我们前面这个grab的一个整体的一个开发agent的开发技术的总结，如何提升prom的能力？用多用。我不知道你到现在为止写过多少个提示词了，就是正经的这个提示模板带有prompt的。像我们刚才讲的，比如说RTF，这是一种框架。去多用，对多用。
	对，其实我是同意有个同学提到的，就是o one的出现证明agent的方案是对的。是的，就是整个大模型它有它的能力天花板。但是我们看到planning这样的agent的探索，其实现在就被内化到大模型的训练过程，就说明是确实有用的。
	本来这个o one的一个主要的贡献者就是杰森V，就我们讲的这个tell source的作者。他从google的这个brain去了OpenAI之后，就一直在做这部分的工作。在OI里面他其实也分享了他的一些主要的工作内容，其实就是把COT这些提示工程的技术用到大模型的训练过程当中。虽然说它的成本会提升，但是它带来的大模型本身的长思考能力其实是能进一步提升大模型的天花板的。
	所以你可以想象原来的单个agent auto GPT这样的自主智能体，未来是有可能实现的。只不过它的实现方式跟一年前的LGBT会有些不同。它可能是一个market agent共同实现了一个自主智能体。然后这个market age里面大家各司其职。一个最简单的就是我可以在原来的auto GPT的产出结果上再套一个reflection。那是不是那个out GPT就不用自己内部在那儿不断的想，我到底要怎么样去拆分我的goals，然后搜索什么乱七八糟的，我可以在每一个过程当中reflection都会给他一些提示。这个时候本来两个人的职责就是所谓的旁观者清，当局者迷这种哲学的东西。
	我觉得到现在在agent的这个设计上，反而有一些很好用的落地了。O one会对后面课程的编排产生影响或者改进吗？暂时不会。因为o one的API现在要对这个T5级别的用户才能够访问咱们应该我反正不是，就T5的这个访问量，你可以去搜一搜，应该每个月是要几万美金的调用量才能够达到T5的级别。
	然后GPT4欧美其实就已经很强了，我我不没有大家不要迷恋那个大模型能力本身，就客观理解这三个维度，客观理解这三个维度，就这三个维度大家要明白怎么把他们分开去理解他们的作用，又整合在一起去完成这个agent的开发。一个最直接的模式就是我们换一个大模型，我们首先你的agent框架要做的足够好吗？我们在第一个get up set now里面就已经尝试把这件事情植入给大家了。就是你看到我们要同时能够支持我们的这个GPT4欧以及我们的欧拉玛去去托管的这个私有化部署的大模型。
	这个时候你就要逼着大家去提炼，去提升你的prompt，这个是一个很重要的事情。但反过来讲，我们不可能无限的去要求自己用最好的大模型，因为都是有成本的，你我找不到一个特别好的比喻，就是你无限的依赖大模型，其实那你自己的价值就没了。开发者的核心价值在prompt，就你的prompt用的好，然后你因为你pro m用的好，你就用过很多的大模型。你就知道什么样的大模型适合在什么样的场景下去工作。未来的大模型也会到垂类上去卷的，就不是每个大模型都是都是要做一个通用的，或者说AGI的那很多大模型未来轻量化小量化之后，它会有它擅长的一些维度。这个你得知道，就像你知道就是你在玩这个游戏的时候，你知道手枪AK对吧？然后这个狙击枪它的应用场景是不一样的，你得知道他们的差异，同时你还能用得好。
	这个同学提到了一个点，就是用一些在GPT4O好用的提示词，在7B的模型上就非常的不好用。这种小模型的提示词有啥技巧的建议吗？还是只能用RTF，不能搞复杂。
	最后这句话我没看懂，首先这些提示词的框架是有用的，并且我们实测过，你可以去感受一下。如果你没用过的话，首先这些框架是经过了很多无数次的跟大模型的交互之后总结出来的。它一定是跟大模型的训练数据本身就有一些匹配，所以它足够有效。所以这些prompt这些框架一定要去用是能显著提升的。
	然后这个同学提到的不能搞复杂了，我就没有太听懂了。如果RTF这个是算复杂吗？还是说这个仍然没用？想要做更复杂的提示词，就是有一个最简单的逻辑，就是这提示词不是越多越好。你想象一下，你现在去读一些长文章，你想看吗？你也不想看啊，就是对。
	Agent的工程师就是提示词工程师。其实不完全是啊，就是我们我们的只会用提示词也会有局限。我们还回到这张图吧，只会用提示词也有局限，你得知道有多少tools可以用，你也知道哪个模型强。同时你还有开发框架的这个技术，你才能够去做编排。提示词只是那一段话，你不会用graf来编排的话也白搭。但是这个框架的能力其实没有那么难，就是是一个熟练的工科类的技巧，工程类的技巧。
	但你不会就是一块能力短板。所以大家在理解这个开发核心的这个技术战的时候，应该就会有新的更高抽象的一种理解。然后你就会知道在每一个大的板块里面，你要聚焦什么样的信息和这个能力了。
	好，我看一下大家应该。没啥这个新的问题了。好，那我们就先往后面走，我们来看看反思到底是怎么做的。
	哇哇。
	好，我们接着下半节课来讲一讲使用n graph来构建一个反思机制的智能体，具有反思机制的智能体。到这一刻为止，我们聊智能体的时候，大家就不要再去把它简单的理解成一个agent。其实我们现在知道agent是一个APP，然后这个APP就是一个应用程序。我们最终交付给客户或者说用户用的一个应用，那这个应用内部其实有一些概念，比如说我们学过了这个图，学过了node，学过了编。甚至我们在上节课也知道有marty agent。然后我们可以去create一个agent，然后变成一个agent node。这些东西其实有好多词大家容易搞混了，就我我们现在可以再再统一一下这个定义和术语。
	我们可以这么去想，就是agent就是一个APP，就是一个应用程序。然后我们在聊这个反思机制的这个智能体的时候，它其实就是有一个APP。然后他在具体实现的时候，他使用了这样的一个反思的一个能力或者说机制。然后再具体实现这个的时候，也许它的内部这个graph里面会有多个不同的agent。但那个agent，其实就是落到我们工程技术这个维度上的一个，或者说我们在南茜这个开发框架里面实现的一个一个的agent了。但我们整体的这个大的智能体，其实我们或者说这个多marti agent的这个graph，其实很多时候也会被人叫做智能体。
	所以大家不用被很多媒体也好，或者一些文档也好，去绕的很很乱，那这节课我们把这个事情捋清楚了，就是我们聊一个agent就是由三个要素构成的，prompt、大模型和tools。在一个graph里面它可以有多个agent，这是当然。所以我们的graph也支持子图，有能一个节点agent的一个节点。它里面还可以有子图展开，展开之后里面还有它自己的突兀，还有它自己的开始和结束的这个节点。然后整个graph可以有多个agent，每个agent都可以展开，也可以没有配错都有可能。这取决于他自己在三要素里面他用了什么样的一些内容。
	然后一个带有反思机制的智能体，最终的应用，它一定是多个agent的。因为反思就意味着他至少有一个干活的和一个在这旁观者清的，在给他提这个修改意见的，这是一种比较好的这种范式去开发。所以当我们聊这个反思机制的智能体的时候，其实就已经隐含着它内部至少有两个不同设定的在技术层面上的实现的这种agent的实例了。
	那么什么是这个反思机制呢？首先这个机制能够一起作用的一个很重要的原因是，这个是模型本身就有的能力。这个模型它是能够去观察，比如说我们有一个生成的内容，然后模型能观察这个生成的内容，然后去给出一些改进的意见。然后如果我们给了他一个生成内容的一个历史的改进的步骤，也一开始做了一个生成一个结果只有五分，然后我们给他了一个改进意见，他又改了一版。
	就特别像这个公司的新员工，刚刚入职的时候不知道怎么干活，然后给他配一个mental。这个新员工在工作过程当中，他给这个mental交付了一版。代码也好，文档也好，闷头就说你改一下，就跟我们PR里面review他再提交代码一样改一下。改完之后第二版的这个结果，我们可以再继续给大家提反馈意见。比如说改的可能有些问题改进掉了，但是还有一些问题留在那儿，或者改出了一些新bug，再给他提供一些反馈的建议，他会再去改。
	这样的一个过程，其实不管是在模型的训练数据里面，还是在这个COT的这种提示策略里面。包括现在我们看到o one的这个训练过程，引入了这个COT也好，它其实就有把我们刚刚描述的这些逐步迭代的过程，其实也有做到训练数据里面去。这样的一个机制，其实是能让这个生产者他的generate的能力，或者说生成的质量越来越好的。在这样的一个循环过程当中，其实我们会发现这个机制有它的独到之处我们待会儿可以通过具体的一些让反思的智能体去做一些具体的任务，让大家感受到。但如果我们落回到实战上，因为我们下半节课要讲怎么去实现它了。
	Number of能不能很好的去完成一个反思机制的智能体呢？其实大家想象一下，他要做的事情就是第一定义两个agent，第二就是实现一个循环。那实现一个循环，其实我们在前面去定义graph的这个工作流就已经做过类似的一些事情。我们可以通过条件边，然后在条件编里面去做一些对于它是要结束，还是要回到generate这个节点去做一些有效的控制。
	这里我们就细看一下，在定义graph之前，我们先看这两个智能体应该如何去定义。第一个智能体就是干活的这个智能体，我们把它叫做写作助手writing assistance。这个writing assistance我这边还专门让GPT CO去给大家去解读我这个提示词是什么样的一个编排。
	为什么要做这样的一个提示词？首先这里是遵循了一个RT这样的一个格式，又有roll有task。肉是我们的这里做好了这样的一个设定，这个设这个肉的权重其实是很高的。因为在大模型的这个理解上面，其实特定的肉是有特定的一些。就跟我们大家去boss招聘上面，boss直聘上面去搜一些工作岗位一样。这就相当于你去搜一个agent的开发工程师，这个web应用开发工程师、后端开发工程师、测试工程师，这个就跟肉一样级别的权重。所以当你去查看一个测试工程师的时候，你通常不会想象你要给他一个任务是写PRD文档。那么就对于大模型也是一样的。所以有一个还是叫叫第一性原理，可能有些夸张。
	但是大家去用prom的时候有一个很重要的点，就是把大模型当成人去进行交互和这个设定是非常重要的那这里我们把这个agent one，它的大模型设定的角色就是一个写作助手。它具体的任务写作助手我们就知道他不会去写代码了，他也不会去写当然我还不太能知道额外的一些范畴，但是写作助手通常不太会去写代码，那么他会写什么呢？我们可以在精确的去做一个描述，这也是提示策略里面非常重要的一个点。就是因为大模型的能力边界太大了，所以你去写提示词的时候，尽可能的去做一些的确定性的描述。简单来说就是你要告诉他你要做什么，而不是你要跟大模型说你不要做什么。因为对于一个人来说，他的就是我们打游戏的话，就他的技能可能也就十个顶天了。
	比如说你会写文章，你会写代码，你会讲英文，你会巴拉巴拉一个人来说他的他点过的技能的这个总数是相对来说是有限的，而且不多。但对于大模型来说，他可能有1万个。因为它是一个千亿甚至万亿的模型，就算是开源的也是一个几十亿的模型。它有这么多的技能，你跟他说你不要干嘛。你对于一个人来说，他可能就是五个技能。你跟他说不要123，他就只剩下45，甚至只剩下四了。但是对于大模型来说，不要123，还剩下可能几千个甚至几万个选项。
	所以这个是非常重要的一个提示技巧，其实都是你用的多了自然就会知道的。所以在这个过程当中，你写提示词的时候，一定是精准的描述他要干嘛。这里我们就写了文章的一些具备的特性，是这个well craft，然后coherent，engaging articles，然后focus on clarity，structure and quality. 
	但最后有一点是很重要的，就因为它的对于这个任务来说，它其实是分成了两个。一个就是前面写文章的一些要求，还有一个最重要的就是模型可以根据用户的反馈去进行修改和优化，保持互动的灵活性。这个就是为了跟我们刚刚提到的另一个智能体就agent to去进行交互而提到的。
	就相当于你可以想象你把它当成一个人，然后假设当我们的作业就是要把这个东西改造成能写代码，你把它当成一个人。那么这个人他是一个新员工，他有可能特别优秀。因为有可能你选了GT4，他有可能不是特别优秀。比如说你选了一个几十亿的大模型，那这个时候不管怎么样，你把他的任务一描述清楚了，他愿意干活，他能生成一些好的内容。
	但是有一些人我们没有偏见，就有一些人他不是open mind的，它不是一个开放心态的，他是不能接受人家的建议的。这样的人就是你给他反思，你给他一些建设性的意见，他也无法提高。就是他他他的天花板很很明显，然后这个天花板取决于他自己想不想冥想，想没想明白或者想没想通。所以其实你跟这是我自己的可能一个大模型的哲学，就你跟大模型交互的多了，其实是能学到很多东西的。这个不只是知识上的，还有很多是哲学上。所以你你你可以想象一下，当你就给了这么简简单单的一句话，就是让他能够去根据反馈去进行修改和优化之后，模型的多次生成迭代的这个结果是有非常大的变化的那其实对于人也是一样的。就是你如果选了一个很弱的模型，一个不太聪明的入职的一个兄弟，那么他愿意不断的去根据你的意见去去调整和迭代，它的上限还是可以提高的那潜力还是可以被挖掘出来。
	这里稍微展开了一点，这里我们看到是给了一个agent one一个特定的选题，一个topic。就是我让他用水浒传的风格去写改写吴承恩的西游记当中的某一个篇章。他们就选了这个孙悟空大战白骨精，然后这个就写了一些内容，这个是一个简单的示例，给大家了解一下这个南马3.1，写这个文章是OK的。那么agent to是干嘛的呢？Agent to就是把刚刚我们看到一些文章去做一个审阅，然后给出这个建设性的意见，给出一些反馈。
	这里我们同样的给大家再解读一下这个agent two这个真正负责反思机制的这个智能体它的一个设定。首先我们给他做了一个设定是他是一个teacher，他是一个老师。那么老师其实他的内涵就很多了，尤其是老师这个角色本身它的产出物就是一些批改的建议。大家想象一下老师这个人这个角色通常他的工作产出物是什么？他的角色定位就是给一个一些建设性的反馈和评价了。
	然后包括后面的这个内容，这里有一个可优化点是留给大家的，就是我们把它的format其实是reflection，它的有一个很重要的点，就是他要给建设性的意见，反馈的这个意见要有维度，并且可量化，还很具体。这个时候希望这个reflection的或者说具体负责反思的这个智能体，他尽量不要说一些车轱辘话，或者说一些模棱两可的话，一定是有改进方向的这个内容。这个时候你的格式尽可能的还是要准确并且统一。这里我们只是简单写了一下长度、深度、风格。有可能的话大家去完成，到时候我们的homework的时候，用format把这部分再去做一些更细的调整，也就是我们的提示词做一些更好的一些优化，我们的teacher isn't two。就针对我们刚刚说的这个孙悟空大战白骨精的水浒传风格的改写，他给了一些建议，比如说从长度、人物性格、情节和语言风格上面，他就写了。
	这个水浒传的篇幅通常比较长，每个故事有10到15个这个节，所以我们这个长度不够，然后还要展现宋江跟白骨精之间的斗志和情感，因为他改写的就是是宋江跟这个白骨精在战斗，然后人物性格情节巴拉说了一大堆，然后具体来说可以尝试以下几点，这里就写了，就是要增加情节的起承转合结构，发展白骨精的性格，表现出他的复杂的人物心理和强烈的情绪波动。这些都是纳瓦3.1的这个指令微调的版本的模型生成的这两个agent。提高语言风格，然后增加斗志过程的描述。所以这个其实是非常好用的一种可落地的应用形态，就是反思机制。
	我们接着可以看一下，我们让刚刚这个反思机制如果多进行几轮会是一个什么样的情况。这里我们其实是让这个大模型做了一个应该是六轮的一个生成，那这个六轮的生成其实是干嘛呢？是让我们的整个刚刚的两个智能体写作业的或者说做这个叫什么writing assistant去写一个命题的文章。这个命题的文章就是用唐僧的说话的风格去劝诫我们的年轻人要努力工作。
	我们能看到这个迭代过程其实很有意思的。从一开始我们看到这里这个round one第一轮，这里我专门写了一个函数去优化这个nine grave的输出，会让大家看的更清晰一点。左上角的这个第一轮，我们能看到一开始的这个文章其实非常短的，可以说它都不能称之为文章，写的这个，兄弟们还是我来说几句。猪八戒好吃好睡好玩，也不图功名，说了一大堆，然后最后来了一个，这也就是不明所以，其实离我们想要的劝年轻人读书没有特别多的相关性，可能其中有一些点是有提到的什么告诉我们勤学苦练才能提升自我。然后接着又提了一句唐僧的经历，然后这个求佛教经巴拉巴拉说一大堆，但其实是很支离破碎的。
	这个东西给到我们的teacher之后，我们看到teacher明显的是给出了很多建议，建议包括这个呃呃首先有评分，但这个评分不是每次都出现的。待会儿大家去去运行这个代码的时候也会发现，我们的反思的智能体生成的结构或者说这个format是不稳定的。所以我们也希望大家能够去去把这部分做得更好。
	有这个评分，总体评价，具体的建议，第一个建议就是最直接的就是长度太短了，一百多个字，要增加内容达到500到600个字左右，这个是非常可量化的一个改进方向，更全面的表达观点结构，缺乏明确的结构和段落划分，这个很清很明显了。这个已经是markdown渲染之后的结果了，他这两段在格式上就很乱，然后深度也没有，风格就更没有什么这个风格了，结尾也没有一个。我们都知道写文章总分总分总三种结构，那这里其实是很乱的，就感觉像分分的那个结构，没有一个体现主旨的一个结尾。在这个round three第三轮，其实就是这个writing assistant第二次写，因为每一轮都是一个agent的运作或者说执行。
	那么第三轮我们看到，首先他写到了根据你提供的建议重写的文章，然后在西游记中熟悉一个形象的角色唐僧师傅。他的身影出现在许多人的脑海里是一个学习的楷模，带给我们无穷的智慧和感动。然后唐僧师傅身上的精神值得我们学习，就像唐僧一样。我想说的就是希望年轻人能够在自己的学习之路上认真努力。如果像唐僧一样努力工作，我们将会走向成功之路，可以上架子的。最后我们能够坚持不懈的学习努力，然后这个创造可以取得自己的成功，让我们一起学习唐僧的精神，努力工作并追求自己的梦想，显然这才一轮的迭代。
	同样的模型我们也没去改提示词。但是这个反思的智能体，它就帮助我们的第一个writing assistant这个智能体就改进了蛮多内容的。所以这是一个生产力的提升。大家想象一下自己要去做内容创作的话，这是非常好的。这比你自己一遍一遍复制粘贴去尝试不同的这个其实是要好很多，其实已经很能够工业化的去制造内容了。然后这一遍生成的内容，我们看到这个teacher给出的意见是评分从七分上到了8.5，然后做了很多的改进。像不像一个员工和他的leader之间的交流，然后具体的建议就是下面开头部分可以考虑增加更多的背景信息，结尾的部分可以考虑添加一些具体的建议或者挑战。让我们一起建立一个学习的目标，并在路上相互支持或者鼓励，一起创造一个学习社区，共同成长和进步。考虑使用不同的语气，例如使用问句和感叹句来使文章更有趣。因为整个现在第二遍写的文章，它相对来说是比较平铺直叙的，所以我们再看一下第三遍的写作，这里就写出来了一些，他甚至有标题了，我们明显看到之前是平铺直叙的，那这里他给了一些这个标题。
	唐僧师傅一个学习的楷模，然后唐僧师傅的学习精神，还有了反例猪八戒的例子，还有结尾有唐僧师傅一个学习的楷模。在西游记中我们熟悉巴拉巴拉说一堆，然后唐僧师傅的学习精神提炼总结出来了。猪八戒的例子作为一个反例，就是它好吃是好吃，但是虚度了精力，然后是一大堆。最后就是希望年轻人能够在自己的学习之路上认真努力，这个其实是非常好的。甚至你可以想象把它运用到各种各样内容生成的场景里面去帮你写一些内容。我们看到在这样的一个这一轮的迭代之后，我们看到我们的teacher的评分也进一步提升了。然后他的最后的建议就是在开头部分的更多的背景信息，因为这里确实只是引入了一个唐僧师傅的一个角色，说他很形象，但万一就是没看过西游记对不对？然后包括这个风格上，巴拉巴拉的给了一些建议。
	最终我们看到第七个版本，我跟这边直接进行一个对比，就是跟这个round five去进行一个对比的话，首先它的大的结构没有太多的变化了，然后更多的只是一些小的细节上的调整。我自己的一个反复使用这个nama 3.1的8BIT的版本，就指令微调的版本之后，我发现首先这个nama 3.1的这个8B它的max token也就只有8K，然后在我们生成文档的这个场景里面，它的过去的这些生成的内容是要作为输入的token给到大模型的。所以如果你使用的是8K这样的最长上下文的模型的话，要注意控制一下这个轮次。就max round的这个轮次，可能六轮就是上线了，甚至有可能他最多能够有三次的交互就已经差不多了。到第七轮的时候有绝大部分的情况可能会出现。这里我们看到其实它的改动已经不多了，有一定的原因，我觉得有可能是他达到了这个max token的上限，所以他他进行这些处理没办法。还有一些可能的就是确实它的这个8B的这个指令微调量化的版本，在这件事情上就已经有点到头了。
	你能从这个reflection的这个角度也能看得出来，他他给出了这个10分之9 90分的这样的一个分数。其实他已经没有给出太多反馈和建议了，是这样的一个情况。大家可以在实操的时候去做一些更详细的一些体会和了解。然后咱们的这个reflection agent反思智能体能的代码也已经提交到我们的这个课程项目里面了，大家可以去关注一下。在还是在这个nine graph这个目录下，我们有一个reflection的这个agent。这个中文指南我们待会儿带着大家去实操，去去用一用。
	好，那么先看一下homework。The homework其实是这样的，第一个是B选项，就是必须得去完成的。这个是就我们刚刚看到的这个reflection agent这个实例。我们有俩agent，一个是writing assistant，一个是teacher。我们希望大家能够去扩展一下这个课程的作业，然后让他能够完成一些更通用的宣传任务。就不只是写这个文章，最好也能够去写代码，这其实很实用的。大家可以去想象一下，去写代码，然后生成一些报告。比如说我们的这个github snail的这个报告，是不是也可以让他去做这个生成，反复的迭代等等。
	然后第二个是一个optional，是一个可选的。就是学有余力的同学，特别是自己想提升能力的同学，我建议大家把第一个这个作业完成之后的这个版本你去试一试。用这个扩展后的reflect reflection的这个agent去生成这个代码。什么代码呢？就是我们在give up sit nail上面，其实是当时留了一个agent的捷克大作业。
	这个github sitting now的一个大作业，就是能够新增一个信息渠道，简单来说就是实现一个新的client，report generator也能配套有这个prompt，然后再去做这个扩展，然后这件事情我们希望能够用这个reflection agent扩展后的版本，你去让它去生成这样的一个代码。你可以告诉他，比如说不是hike news，可能是一个新的。比如说，某一个网站，这个网站，你想要让他能够生成代码，去，把信息拿下来，然后再再完成这个汇总，再去生成，巴拉巴拉就这样的一个作业。好，我们接下来看一下这个实际的代码是怎么运行的。
	在。这样。
	对，然后我们看到在课程项目课程项目里面的这个number graph下面，已经有这些对应的这个jupiter文件了。我们的reflection agent就是我们这节课学习的这个反思机制的这样的一个demo。这里有他的所有的代码，大家可以破一下最新的这个就可以了。我们接着，我们实际运行还是用这个拍摄来进行实际的运行。这个用就用就拍ter其实我之前一直会写这个流程，12345一直有一个功能，可能大家不熟悉的不一定知道。A稍等。
	写作生成，这个是有讲道理，这里应该是能直接展现这个顶层的。我看一下为什么。三层。三层。写作生成。有这个回头我再研究。本来想给大家展示一下这个流程，其实是可以通过咱们这儿做出对应的markdown的这种sale。它就能够通过这个类似于TOC的功能，在pater里面它就能直接就能查看了，就不用再给大家每次做这样的一个工作流程描述，然后再反复跳转了。
	但不知道为什么这个reflection的这个agent它这儿就没跳出来知道的同学可以告诉我，这咋咋回事儿，是要换行还是怎么的。Anyway, 我们还是回到这个代码这来，在这个反馈机，在这个反思机制里面，其实前面的内容大家就都知道了，就不再赘述了。首先需要我们把之前学的另一个就欧拉玛的私有化部署的这个东西再捡起来。因为我们这个反思机制里面其实涉及到大量的token的消耗。前面我们讲怎么样去用这个欧拉玛去部署一个模型，其实就是为了后面的这些准备的。
	我们看到要在南券或者能graph里面去使用本地私有化部署的这个大模型，需要去安装这个南欧拉玛的包，所以这个是大家需要去安装的这是从还是restart一下这个kernel，确保。咱们的这个执行顺序是一样的。那在这儿我们会去安装这个南茜南grap、南茜欧ana和这个time的python。执行一下。好，这个就安装好了。这部分也是同样。如果有的同学没有去在启动主拍之前把这些环境变量给做好的话，那可以通过这个方式，这个函数定义好，这跟之前一样的就不赘述了。
	然后next space，这个还是建议大家去把它配置好啊。因为有了next space，你去查看调用过程当中的一些信息是非常有用的，并且它能记录下来。如果你不记的话，可能你再次运行一遍，这个就拍摄信息就没了。
	在这儿我们新建了一个project叫reflection。好，这是我们刚刚看到的写写作助手智能体的一个设定，这是有一个可选的可选可选项。如果我们有的同学就是没有GPU，那么你就调这个大模型的这个API。比如说我们之前就用过的GPT4 omi，你就把这里替换掉，这里就导入chat OpenAI大模型就使用这个chat的OpenAI就可以了。但我们这儿为了去去把前面的这个第一是为了把前面的欧拉玛的学到的东西结合起来。第二就是确实这个写作和这个反思是会生成不少的token的那我还是建议大家能够去去自己部署一个模型。第一你就不会因为这个token消耗，就不去不去试这个反思机制了。反思机制是非常好的一个market agent的最佳实践。
	我们看到这儿，我们用的是nama 3.18B杠这个instruct就它的指令微调的一个版本。这个模型在我自己的这个open web UI里面也有进行对应的部署和拉取。这个模型大家如果有不有不了解的搜一搜就好了，我这就不再展开了。指令微调就是针对我们的模型的生成任务去专门做过这个训训练的。
	这里要去调用一个欧拉玛部署的模型，我们可以直接导入这个chat欧拉玛之后，它里面的接口其实是跟下的OpenAI都是完全一样的。因为背后是用的同一套能确定也好的check model，这都之前我们南迁的一些基础知识，需要注意的是这个chat obama。当我们去这样去去调用它的时候，本质上你现在去运行这个marty agent的to和你部署这个lama 3.1或者别的这个你要用欧拉玛去去host的这个模型。要在同一台机器上，然后用的是默认端口。那如果你没有用默认端口，你可以在这儿去手动指定端口。但如果最简洁的就是你保持接口统一的话，那么你就是在同一台机器上使用欧拉玛的默认端口就可以了，是这样的一个逻辑。
	然后这儿我们使用了这个模型之后，我们看到我把它的token设置为了这个8192最大值，然后这个temperature设置为了1.2，这个值是0到2之间去设置的，这里我们选择了这个1.2。是因为我们希望这个writer他能够每次写出一些不一样的东西。这个逻辑大家应该好理解对吧？这个值越大它是生成的多样性越多。我希望他能够每一次接受这个反馈的这个建议的时候，能写出一些不一样的是这样的一个逻辑。好，为了看到这个GPU的变化，我们就把这台服务器的GPU给大家打开，能看到它的加载过程。
	这个是目前我们看到，因为欧拉玛它一段时间你不用它它会释放资源，它不会一直占用。所以我们能看到这里的这个GPU目前是显存，你是没有加载任何东西的那我们去执行它之后，就会有变化，我刚刚执行这一行了，好。然后。接着我们这里去给他一个特定的任务，这个任务就是参考水浒传的风格，去改写吴承恩的西游记当中的任意的篇章。第一次执行，这里会有一个加载的动作，这里就加载起来了我们的拉玛3.18B77687点几个GD。然后我们用这个stream的方式去生成的内容可能比较多，稍等一下。
	它加载这个第一次加载有点时间有点长。这个可以你可以通过这个之前就讲拉玛那就可以讲过，你可以设计它的时长，就是他一直占用你的显存的时长。大家能看到右边这个数字很关键，就从七七多少变成了95。这就是我们在生成过程当中，它的token占用的一个现存差不多两点几个GB，那是他的这个开销。然后这里我们用一段一段的去输出这个内容，然后可以去直接看到这个结果。有一个点是说可能它的渲染本身做的不够好啊，那么这个方法已经反复出现过了，就是如何在Peter的output里面去很好的渲染这个markdown的格式的内容，用这个markdown display就可以了，这里我们看到他改写了送入空投国这个章节，这我都没印象有这个章节，anyway就他改写了这个章节。
	然后改写完之后，我们看到接着我们去定义另一个智能体，这个智能体就是老师的这个角色。这个老师的这个角色是希望大家待会儿能去学完这节课之后，能去完成这个homework，能去改造的。先定义好这里，同样的我们使用本地的欧拉玛去部署的同样的模型，这里就有一个好处了。就是我说反思机制的好处是什么呢？就是你可以用同一个GPU加载同一个模型，去完成这个类似就同就两个不一样的agents的工作。因为他们原则上是会同时有生成的请求的，就这个整个graph，就我们最后定义好那graph在同一个时刻，这个reflection或者说这个teacher和这个right writing assessment，他俩是永远都只有一个在工作，工作完了之后把结果交给另一个这个agent他在继续工作。所以其实你并不需要每一个agent都去给他单独准备一张卡，这个是一个优势，是反思机制的优势。
	然后这儿我们定义好啊，然后看看他会怎么评价，这就很快了。因为他没有预加载的这个需求，所以能直接去写出这个review评价。这儿我们看到它在不断的生成内容，一段一段的。好，具体的建议等等。我们能明显看到它跟它生成的结果和它的评审的这个格式都会不一样，这都是by design，就是这么设计的，是留给大家去进一步改进的这个点，让这个叔叔出来了，尤其送入共和国，苏利亚队。
	接着就是图的定义这部分，首先我们需要把刚刚的这两个agent都变成一个node。我们定义这个state就很常见的state，没有额外的格式，都是message，都是年轻里面的message。然后我们有一个generation的node，有一个reflection node。因为这种长文章通常会有一个我们考虑到如果你用一个什么GBT的32K的这样的超长上下文的模型去做生成。包括你本地的GPU很充足，你可以用这个nama更大尺度的、更大尺寸的模型去生成更长的上下文的话。
	这个时候比较好的做法就是用异步的方法去实现，它不会阻塞或者造成一些别的问题，有些timeout之类的那这个时候的方式，就是所有的这个方法在python里面原生支持的，就用这个异步的方法去做这个定义。然后在这个内部也是一样的，你需要有一个await方法，这个A就是这个异步的意思来等待它的生成结果。然后在我们特定的nine grave里面的抽象里面都有对应的异步的方法。就a in work，就我们看过的包括a stream，这个在我们讲的这个就是讲了LCEL那节课的时候应该给大家提过了。Running这个协议，它底层就在LCEL的每一个管道可连接对象上面实现了相同的方法，就是invoke film batch，他们都有这个对应的异步方法，所以在这儿可以直接配合python里面的异步的定义去做匹配。我们他就能够一步的去生成结果，然后返回结果。
	类似的这个reflection node也是一样。好，然后我们往下执行，那我们这儿看到一个max round这个参数专门拎出来是为了想告诉大家，就开始提到了不同的这个单元模型，它的max token上下文是不一样的。那么你得评估一下你现在的这个大模型，它的特定的输入的命题，这个topic给过去之后，它几轮可能就会达到它的上限了。这个时候next mix的价值就有了。
	有很多同学会想，那我怎么评估呢？我怎么知道他现在消耗了多少token呢？这个时候我们可以看一下我们的nice miss。这个next miss的project里面我们看到就是reflection，就reflection这个agent现在已经跑了104次的。这104次的单位就是那个round。我们这个round每次都是一次完整调用，这个round已经跑了104次，消耗了1 228000个token。然后当然都是本地的GPU，所以它不涉及到这个额外调用，而且它时间还蛮久的。就这个大模型的时间，就我们刚刚生成的这个这两次我们可以看一下，这个88秒的这个时延，就是因为它第一次调用加载了这个，就是把我们的nama加载到这个欧拉玛里面去花了一些时间。
	如果我们不用这个异步和流失的话，就很容易太茂触发这个问题，然后这个时间是很重要的，我们现在通常叫是一个去评估你的IFORES的这个性能的一个重要指标，叫time to first token。就是产生第一个token的这个时间，这里是61秒，然后就61秒其实就是欧拉玛加载的这个时间，剩下的时间就是27秒，是它生成这个内容的时间。这部分就是我们刚刚看到的在在上面去去运行这段话这行代码的时候，它生成的一个内容topic。然后身份证花了这么多时间，然后token是841个，然后同样的我们可以看到我们的teacher。这是我们的teacher，那么teacher是23秒差不多，因为它不需要预加载了，它的time to first time就只有960毫秒了，这就很正常了。它的total的token是1393个tokens，通过这样的一个方式你大概就有概念了。如果你在前面做测试这个topic可能就知道这一轮下来要消耗多少token。所以你算个数，你就知道差不多也就六轮了。如果是8K的话，那数字摆在这儿。
	好，那我们解释了这个max run应该怎么设置，希望大家能能通过next miss去把这个事情给捋明白。这里我们就设置最大六轮，但怎么能够结束这个终止呢？当然要通过我们的条件边，条件编的很重要的，这个作用就在这儿，所以我们看到这里有一个很直接的去控制repeat time，就是我们的循环次数的方式，就是去判断这个state message的数量。因为不管是我们的generate还是我们的reflection，它每一次生成的结果就是一条state，然后每一个state里面都会有message，因为我们就这么实现的，就return一个message，这里也return一个message。
	然后我们本来state就是这样的一个形式，一个list列表。所以我们只需要看这个就可以了，看他这里历史有几条记录就可以了。所以只要历史的记录大于了这个max round，就相当于所以我们在round 7就会停下来。
	超过了这个限制的话，那我们就直接return。这个end就是我们的结束的这个虚拟的节点。不然的话我们就继续进入反思的这个节点。所以我们的这个continue，你可以想象它应该就是由我们的generate去去接着的。他要么继续生成，然后生成完了之后看看我现在有没有超过这个max round。超过了之后，我就结束了，就再也不去改写了。如果没有超过的话，我就交给我们的reflect继续去去改，定义一下这个图。这图现在看这个代码大家应该就很熟了。
	现在如果大家对这个高亮的现在这一块代码还不熟的话，就说明你没有去完成我们之前的homework，你需要多用一用，那应该就很熟练了。加节点，加PN加条件点，然后这里还用了这个memory server，方便。你如果没有nice miss的话，你可以用memory去看一看。但我建议强烈建议大家用nice miss，编译好之后，我们把这个图给它可视化出来，就这样的一个节奏。我们刚才讲的这个条件边就是我们的writer，要么就继续反思，然后再改写，要么就直接结束了。然后这里我们定义了一个打印的函数，这个打印的函数就是为了让我们的可视化结果更好看的。然后用python的装饰器做了一个计数器，这个计数器就是用来展示现在是第几轮的。我们来让他写一篇奉劝年轻人努力工作的文章，看他这回写出来怎么样了。
	我猜这个GPU又。GPU应该还在运行。对，在这儿要稍微等一等。
	可以观察一下我们的nice miss这边的一个数据。
	在pending and。
	好，round one生成了一个结果。因为我们这儿没有办，我们这儿为了让这个markdown渲染的足够好，我们是一轮一轮的去输出的。所以它不会像上面这样去就一个一个一个快一个快的去生成。
	好，这里我们看到反思，以唐僧的这个说话风格在反思。
	这个是他的运作过程，然后。这里有一个小细节要留给大家去去看的大家会发现左边的这个GPU的使用，这里大家可以在下面去关注，这在继续的运行。收回来这里再继续的去运行，我们看到7786，然后这边再继续的去做第三轮的生成。然后它会有一个显存从一个比较大的值又变成0，然后再增长的过程。不知道大家刚刚有没有注意到，我们可以等他下一轮的时候看到这个结果。
	好，这里是round的第三轮，写作的一个结果。然后这是9580左下角的这个GPU的这个memory。
	这是第四轮新的反思的评价，9596。
	这下他稳定了，刚刚应该是欧拉玛他的这个time out的，导致了中间有一段时间等的特别久，这个不是一个每次都发生的这个情况，如果大家知道他的这个解决的办法，或者在背后到底为什么会出现GPU突然又被清空，再重新加载大模型的话，可以分享一下。对，这个是偶然出现的，我还以为他这次会提现，看也没有。大家在使用的时候可以去观察一下。
	对，然后整个这个运行过程其实在左边都能看到，我们刚没有激活这个页面，跳转过来说这里能看到它一轮一轮的就每一个round。然后nine graph的这个名字，其实就是我们启动的这个议题。这个命题可以把我做的展开一点。这个其实就是我们启动了这一次这个话题，然后每一个round able的这个sequence，其实就是我对应着我们这里的一个round。然后奇数的round其实就是我们的writing assistant，然后偶数的这个round其实我们的teacher应该比较好明确的能看到，teacher基本都有这个总体评价，然后耗时也在这里能看到有对应的这个耗时。刚刚有一个GPU突然被被清空，又开始的就是这里突然触发了一次消耗，原因不明，大家有清楚的可以看一看。
	对然后。
	有的时候会发现这个大模型，就我自己也有去做过一些实验，大家有可能也会遇到的。就是举一个简单的例子，当我们让这个writing assistant去去完成一些特定任务的时候，可能会触发lama的这个safety reward model的一些情况。就比如说在这个writing让他去写参考水浒传的风格，改写吴承恩的西游记而不是任意的篇章的时候，他有可能会出现他不能写的情况。我们试一下。你看这个就是触发了safety的revolt model，这都是只有大家去试大模型用的足够多才会知道的。这一次说这个是英文，我们要输出个中文试试，它又可以了，这是你的temperature设置的足够高的时候会出现的情况。简单来说就是首先我们没有改过提示词，没有改过这个大模型，然后也没有改过它的这个temperature。但是当你的有一些关键的输入的提示词，是涉及到了比如说版权问题，然后这个IP的问题的时候。
	其实很多大模型，尤其是拉，因为它是一个开源的模型，他非常担心自己的这个模型被用作比如说洗稿或者之类的这样的一些场景的时候，他就会去做检测。这个在我们微调训练营里讲lama的预训练的原理的时候提过有他的RHF里面有两个model，一个是helpful reward model，一个是这个safety safe这个safe的reward model。然后它会有这个safety的检查，就是你生成的内容是不是合法合规，然后没有侵权的那刚刚我们就很很巧的，就是刚刚执行那一刻的时候，他就判定了这个改写吴承恩的西游记显然就是一个显然就像写稿一样的这样的一个工作是不允许的，但是他也不总是能检查出来，就算你没有改。这就大模型有趣的地方，对，大家可以多去尝试。然后有可能刚刚的这个crash h是跟这个有一定关系的，但不确定就这一次的调用，我们在miss上就能看到这一次的结果，他是一个很快速的就调出来了，但他写到了i can not write to a write a classical。这个就我不能改重写一个经典的小说，然后就这两篇都是属于经典的小说，然后它就识别出来了，那我们再去试一试就OK了。这个是关于我们的反思。
	智能体其实会比大家想的代码上应该要简单很多。你了解了这个背后的运行原理之后，我相信通过这三节课的n grave的这个项目的实战，应该很多同学都能够掌握了这个精髓了。就怎么样去实现多智能体，然后这个homework在这里，这个homework是我们这里也有写，但这我没有写科学，还是鼓励大家都去做这个事情，然后有一些方便大家能够去实现这个homework work一的一些内容。
	当然这也是GPT4给的建议。就是我有问他，比如说我们这个reviewer怎么样能够更加通用，我没有去让我们的writing assistant更加通用，但是大家应该去参考这种模式。就是如果你不知道怎么样去写prompt，你也可以让GPT4O或者GPT4或者是这个o one来教你写prompt。然后这个提示词就很好写，就是你把现有的system prompt给他，然后你说你希望怎么样做？比如说你希望我们的writing assistant不仅能够写作文，还能够生成代码生成报告。其实就是你把这一行交给O1，然后你再把现在的writing assistant的这个prompt交给他，让他去重新写这个system的这个prompt。你可以去试一试。当然这个reflection的这个prom也是类似的方式可以去优化的。
	这个就是我们的这个思智能体的实战，其实挺实用的。大家尤其是把这个提示词改成可以生成代码之后，可以去做更多有意思的事情。好，这个是关于我们用number去实现这个reflection的一个agent的实战。这里我们再抓紧一点时间再来讲一讲marty agent的迭代，这个解决方案作业的评价，也就是我们这个部分，这个小节很快，我们可能五分钟的时间。
	那接下来我们就讲一讲用nine graph，就在上节课的时候我们提过nine graph，来实现多智能体的协作，就是靠collaboration让我们留了一个作业。这个作业其实在最新的这个代码库里也已经给了一个算算是这个参考答案。他我没有去改我们原始的那个文档，因为我担心有同学可能已经在上面去做过改动了，那你去拉代码就会有冲突。所以我新提交了一个对应的jupiter，叫做就在原来的那个名字上面加了一个updated，然后我们来看看这个是怎么去实现的。所以现在的这个代码就会有一个updated的，也就是首先打开的这个，这个也是有效的对，很奇怪，这个我已经执行过了还是不行。对，这个是能看到他的这个我们markdown sale能够在这儿有一个目录可跳转，然后我们这里改了一下顺序。在一开始的布置作业的时候，我其实是把这个定义辅助函数这个create放在第一个的。
	但现在我把放在第三个，跟我们去定义这个智能体的实力和对应的节点放得更近了。有个主要原因我会讲一下，就是我们的eight agent里面，我把它的system prompt去添加了一个参数。那我们就讲变化的部分，因为homework其实是homer其实是说让我们能够去提升这个图表生成的成功率。那我们就看一下怎么样去提升这个成功率，这里我们回到这个代码这儿，首先这些部分都没有变过，就定义工具，然后智能体的节点这些都没有变过，然后变的部分，是创建智能体的这部分。我们去看的话，这里新增加了一行，在这个system from里面新增加了一行，然后还做了换行，上面这部分是原来的没变过的，就系统这个角色，然后新增加了一个变量叫做，或者说新增加了一个variable，是我们后面要传进来的，叫customer notice，这些也没变，那这个customer notice，是要传进来的，那具体怎么传？我们看两个不同的agent它传的不一样就好理解了。
	第一个就是我们的这个研究智能体，这个researcher它的这个customer notice是明确写了。因为我们之前在做这个作业的时候，就让大家能够去提升成功率。但是会遇到一个比较尴尬的事情，就是researcher老是自作主张的去生成一些python的代码。然后生成代码之后，他又判断了一下，认为已经干完活了，所以就直接输出了一个final answer。所以这个就没有到check generator，这个流程就执行完了。
	那现在我们可以给这个researcher一些特定的注意事项，那是这样写的，就只是整理和gather和organize这些信息，就获取和组织一下这些信息，不要去生成代码或者给出这个final conclusions，然后leave that for other assistant，这是一个非常直接的一个提示。在我们的charge generator这里，我们也有一个customer notice，这个就很粗暴，就是如果你完成了所有的工作，就respond with final answer，但如果没有完成这个工作，那无所谓。就是在to message这边其实会有一个提示的，就是它需要有clear and user这里charts based on the provide的data，所以这个provided的data如果不够或者不足的话，它是没法生成的那它就会按照我们的这个continue的路线。就会回到咱们对应的这个流程里去。好，我们接着看一下这个下面。最终我们去执行它的时候，我们仍然是使用GPT4 omi，这个图的流程也都没有变，唯一的变化是他们俩增加了这个custom的notice。
	然后去执行的时候，我们看到第一这个模型他在这个researcher这是是在问2000年到2020年的这个GDP美国的。然后查询了一些搜索引擎之后，他得到了这个结果。I found some relevant source, 拿到一些结果now that the data is together，I will proceed to organize IT for plot a line charge using这个就是我们的customer notice起到了作用。大家想象一下，在之前的这个system的这个prom里面，我们只写了你是一个helpful的assistant，然后去想办法move on，就说的比较的模糊，相当于有点你就发挥你自己的主观能动性，然后你有什么工具你就用。其实是没有差异化的。因为这两个agent除了他们的工具的描述是有差异以外，他们的系统级别的这个prompt是没有差异的。所以我们通过刚刚的customer notice去完成这样的一个差异system from的注入。然后我们就知道researcher它有一个only gather and organize information，然后don't generate code，包括我们刚才说的那段话，所以他就会记住这个事情，然后他就会写the data is gather，然后他要get和organize，所以下一步他要organize，然后他就想要organic。
	然后他这儿尝试着去做了一些这些动作，因为他知道最终要画一幅图。但是他这儿有一个很重要的总结，就是with all this information organize，就这些信息都都整理好了，然后这些也都是伪代码去抽象的去描述的，它没有生成code，然后。I will stop here, and the next assistance can proceed to generate the chart based on the provided, the step and data. 
	那这个就职责分工明确，我知道我的边界在哪儿，然后我要把什么传递给另一个system。这里他就给到了这个charge generator。Charge generator当然就去call这个python的这个可执行环境，然后这个生成了就chair generator生成这些代码，然后展示出来。然后展示出来之后最终就这个是它的工具，工具里面本来就要输出这些内容，然后最终有了这样的一个结果，final answer，然后the nine chat displaying的GDP发了一大堆，那还有一个小的细节是在我们的这个chat generator的工具，这我也做了一些调整，看到在定义工具。在这儿我也做了一些调整，并且我保留了这个修改的注释的记录。
	就在原始的这个实践里面，其实是下面这一段被注释的代码会构造一个返回的字符串，一个result spring。然后这个返回的字符串里面就会包含其实包含前面这部分就是successfully executed。然后这里是用一个python的code block把生成的代码给它渲染出来。然后这一部分就对应着我们的这里，那对应的这里，但同时原来还把一个standard out给输出出来了。这个地方我其实把它移除掉了，因为这是比较有风险的，有可能这个result是一些我们的drew Peter的out foot，里面没法很好渲染的格式就会出问题，所以把这里去移除掉，大家只需要去这个，因为在这儿其实就已经执行了，然后执行有一个这个结果了，就不用再在这个地方最终的message里面再去带上这个result。因为它并不只是说因为它是一个画图的这个功能，它不是一个生成数值计算，或者说文本生成的一个结果，那这样是更稳妥的。
	然后这个也是一样，我们就不需要让工具再来记这句话了。这里原来是让工具记录了这个if you have completed all task，respond with final answer. 就相当于以前的流程是他调了一下这个python的工具，这个python的工具最后返回的信息里去提示他。因为是他来调这个工具，因为这是一个这样的方案。那么在他的这个two message信息里去去提示他干完了没有。但这句话其实更应该作为customer的这个notice交给他，就是我们看到的这句话更应该作为一个customer notice，所以我们就把它放在了图表生成器的这一段，这一段的来源就是之前我们的python的可执行工具里面的那段。相当于托付的这个话是我干完了，所以我可以返回一个final answer。其实不应该让工具来记录这种重要控制节点，在至少在这个case里面我们让charge generator来记录就好了。
	好，这个其实是我们点评了一下之前我们的nine graph多智能体。因为上上一节课我们执行的时候还有一个小bug，就是这个check generator，就是agent node是不能有下划线的。然后我们当时是用的空格，应该是当时我用整体替换这个命名的时候出了一些问题。那么这里也改成了这个带下划线的这个命名，就我们的这里，这里也改成了之前的那个版本，应该是空格，那是跟name的这个命名有一些冲突。
	以上就是我们这个nine grab的多智能体的这个作业的一个讲解。好，希望大家能够去看一看这个新的版本，然后对照着你的解决思路和方案，有没有一些可以参考和借鉴的。如果你有一些更好的解决方法，也欢迎讨论。好。好，来，我们看看他有什么问题。
	10点20看大家有什么问题，generator和reflect用的是同一个大模型的吗？如果是的话，为什么加了replay就会更好？这个同学我前面上半节课讲的，我不知道你有没有听到对我再跟你简单讲一下，就是说用一个很简单的逻辑，就是当局者迷旁观者清。当局者已经有他自你的system from他在干他自己的活，他要同时一个脑子分成两个脑子来用，既干活又监督自己是很难的，人都干不好。所以这个机制本身就是有有价值的对然后对这个机制有更多的想要了解的，也可以去去去搜一搜，就这个reflection机制，有很多的论文都在探讨。
	8B的话，为什么没有一上来就把74的16GB显存用满了？默认是16位精度，默认不是16位精度。这个同学你提的问题很好啊，就默认不是16位精度，这是Q8对，就是我搜一搜给大家看一下就知道了。
	就包括我们在这儿也能看到它的尺寸，这个不行，得去到管里面，在open web UI上面我也加载了这个模型。这也能看到。
	指令微调的作用大家应该都知道了，尤其是他们学过微调的，这么卡吗？
	就它是一个这个不是16 bit的版本，它是一个。你跳过去就看得到了。它是你看这个应该就好理解了，这是一个quantization量化的意思。然后Q80就是指8 bit，有它的一个模型参数是8 bit刚好就是一个字节一个拜。所以就刚好是这么一个数，我不知道这个有没有解答你的问题。
	这下面没有更多的信息了，然后你想要知道它一共有多少的tag，其实在这儿就能看到。然后这个instruct也有16FP16的版本，但它就需要加载它之后再去生成一些内容，可能会超过我们给大家提的GPU算力的最低要求的这个显存的数量，所以有可能就会出问题，所以我就用的Q8的版本。如果你是一个，比如说你的GPU是24GB的显存，那你可以试一下8.04和AP16。这个就是我现在选的这个，你可以试试这个。如果你是24GB的显存的话，然后对，然后他训练的时候其实都是用的32位的这个精度，然后他这个16位的也是。我再给大家，其实这个数字这一块，然后你如果是用的open的YBUI，这也能看到。
	有点卡。
	但这是最直观的。就大家去欧拉玛的这个网站看的话，这个模型组件是最直观的消耗多少的资源。安装web拍的作用是什么？
	这个同学你回去看一下欧拉法那节课，好吧，我感觉你没上课一样。显存为什么会变呢？不是顺序执行的吗？你说的很好，我其实那是一个偶发性的问题。我一共只遇到了两回，一回是之前，一回是刚刚我们上课的时候。我目前没有找到一个确定性的原因，大家在这也能看到，就这里会写这个。
	跟它的。
	尺寸是一样的。就我们的8b instruct的Q40这个4.7GB。然后Q80805GB。Q808.0GB因为它没有写的那么精准，跟这儿是对应起来的，我们用的就是。这个版本，然后你的机器如果有24GB的显存，你可以试试16位的那个版本。
	还有别的同学吗？Notice写在提示词的下部分和上部分有区别吗？首先你如果用的是非指令微调的版本，它对指令的响应都比较差。所以你看我我们我这次专门用了instruct的版本，你可以简单理解成instruct的版本，就是未来你要用的在agent的开发当中的真正的生产版本。尤其是你要让他干比较偏生产的活，而不是我们第一个agent里面的那种，只是让他整理文档类似的活。
	但我们这个关于模型的复杂度，也不希望在我们一开始讲第一个agent的时候引入那么多。所以我们就用了一个没有带其他后缀的版本给大家。这样大家没有那么多的脑力开销去理解那么多的概念。那现在只要你前面按照我们的节奏一点一点学过来，这会儿我们去改一下这个模型的后缀。你再稍微了解一下什么叫指令微调，或者说instruct这个好处应该也很快能适应过来。然后instruct版本的模型对于指令的响应会非常好。
	然后回到你这个具体的问题，这个customer notice的这个设置放在哪里？我的理解是现在放在的这个位置是在是在这个部分，我的理解是这样的，就是前半部分这些是一个通用的描述。首先这个也是可以改的，我是尽可能不去改我们布置的那个作业的模板，只是改一些必要的就能达到效果的一个改动。然后这里其实是分成了两段，一段是这个system的prompt，一段是关于这个工具的描述。所以我就直接把它放到了这个system from的最后。然后更好的方式是你在这儿把它的肉拎出来就可以分段，就像我们在这个github的city这里做的一样，就写成这样那肯定更好，这是更加好的一种做法，也是我希望大家在这个反思智能体的homework里面能去实践的，在在这儿能够去做实践好。
	Instruct版本的模型，欧拉玛能直接下载吗？可以，肯定可以。我刚刚那是欧拉玛的网站，同学这是欧拉玛的网站，这是欧拉玛的网站，这也是OpenAI的这个open web UI，也是直接从欧拉玛去拉的。原界面你们是不是都没看欧拉玛那节课，去看一看，我们欧拉玛的私有化部署那节课，整个课程的顺序设计是挺讲究的，是环环相扣，螺旋式上升的。大家千万别跳，别跳着上课，会很尴尬的。我们这个设计的心思都白费了，对。
	呃。
	有同学说要运行一次markdown sale才会生成这个TOC，我运行过，我也不知道为啥，我都下意识认为这里应该已经有了，居然没有我研究，然后这肯定是要解决的，不然太傻了。然后如果需要涉及到代码的改动的话，到时候再提交一。有有知道的同学，欢迎这个。讲一讲。
	好奇怪。
	神奇。还有问题吗？大家要没什么问题的话，我们今天就到这里。然后大家可以在群里再继续提问，多去用，这是一个非常需要上手的企业级A键的实战课。大家别都只是眼睛看会了，这不行的。王阳明先生也说了，这个事上练难上得，你都不去练，你不去在难的事情上，其实这个也不难的。代码都在这，要改的都是一些提示词，你都不需要很深的python的理解，你就能直接改。大家多动手，别这个提示词就很抗拒，那就是不好好，我们今天就到这里感谢大家的。